Ein randomisierter Algorithmus ist ein Algorithmus, der als Teil seiner Logik oder Prozedur einen Grad der Zufallskraft verwendet. Der Algorithmus verwendet typischerweise einheitlich zufällige Bits als Hilfseingang, um sein Verhalten zu führen, in der Hoffnung, eine gute Leistung im "mittleren Fall" über alle möglichen von den Zufallsbits ermittelten Zufallswahlen zu erzielen; also sind entweder die Laufzeit oder der Ausgang (oder beide) zufällige Variablen. Man muss zwischen Algorithmen unterscheiden, die den Zufallseingang verwenden, so dass sie immer mit der richtigen Antwort enden, aber wo die erwartete Laufzeit endlich ist (Las Vegas Algorithmen, z.B. Quicksort) und Algorithmen, die eine Chance haben, ein falsches Ergebnis zu erzeugen (Mon Carlo Algorithmen, z.B. der Monte Carlo-Algorithmus für das MFAS-Problem) oder nicht ein Ergebnis zu erzeugen, entweder durch Signalisierung eines Ausfalls oder zu beenden. In einigen Fällen sind probabilistische Algorithmen die einzigen praktischen Mittel, ein Problem zu lösen. In der Praxis werden randomisierte Algorithmen anstelle einer wahren Quelle von Zufallsbits mit einem Pseudozufallszahlengenerator angenähert; eine solche Implementierung kann vom erwarteten theoretischen Verhalten und mathematischen Garantien abweichen, die von dem Vorhandensein eines idealen wahren Zufallszahlengenerators abhängen können. Motivation Als motivierendes Beispiel betrachten Sie das Problem, ein „a“ in einer Reihe von n Elementen zu finden. Eingabe: Eine Reihe von n ≥2 Elementen, in denen die Hälfte ‚a’s und die andere Hälfte ‚b’s sind. Ausgabe: Finden Sie ein „a“ im Array. Wir geben zwei Versionen des Algorithmus, einen Las Vegas Algorithmus und einen Monte Carlo Algorithmus. Las Vegas Algorithmus: Dieser Algorithmus gelingt mit Wahrscheinlichkeit 1. Die Anzahl der Iterationen variiert und kann beliebig groß sein, aber die erwartete Anzahl der Iterationen ist lim n → ∞ Σ i = 1 n i 2 i = 2 {\displaystyle \lim {_n\to \infty }\sum _i=1{n}{frac *) Da es konstant ist die erwartete Laufzeit über viele Anrufe ist Θ ( 1 ) {\displaystyle \Theta (1}) .(Siehe Big O notation) Monte Carlo Algorithmus: Wird ein „a“ gefunden, gelingt der Algorithmus, sonst scheitert der Algorithmus. Nach k Iterationen ist die Wahrscheinlichkeit, ein "a" zu finden: Dieser Algorithmus garantiert keinen Erfolg, aber die Laufzeit ist begrenzt. Die Anzahl der Iterationen ist immer kleiner oder gleich k. Unter k ist die Laufzeit (erwartet und absolut) Θ ( 1 ) {\displaystyle \Theta (1}) . Zufällige Algorithmen sind besonders nützlich, wenn ein schädlicher Gegner oder Angreifer konfrontiert wird, der bewusst versucht, einen schlechten Eingang zum Algorithmus zu füttern (siehe schlimmste Komplexität und wettbewerbsfähige Analyse (Online-Algorithmus) wie im Dilemma des Gefangenen. Aus diesem Grund ist Zufall in der Kryptographie ubiquitous. Bei kryptographischen Anwendungen können Pseudo-Zufallszahlen nicht verwendet werden, da der Gegner sie vorhersagen kann, wodurch der Algorithmus effektiv deterministisch wird. Daher ist entweder eine Quelle von wahrhaft zufälligen Zahlen oder ein kryptographisch sicherer Pseudo-Zufallszahlengenerator erforderlich. Ein weiterer Bereich, in dem die Zufallslichkeit inhärent ist, ist Quantenrechner. Im obigen Beispiel gibt der Las Vegas-Algorithmus immer die richtige Antwort aus, aber seine Laufzeit ist eine Zufallsvariable. Der Monte Carlo-Algorithmus (bezogen auf die Monte Carlo-Methode zur Simulation) wird in einer Zeitdauer, die durch eine Funktion die Eingabegröße und ihren Parameter k begrenzt werden kann, aber eine geringe Fehlerwahrscheinlichkeit ermöglicht. Beachten Sie, dass jeder Las Vegas-Algorithmus in einen Monte Carlo-Algorithmus (via Markov Ungleichheit,) umgewandelt werden kann, indem es eine willkürliche, möglicherweise falsche Antwort ausgegeben, wenn es nicht innerhalb einer bestimmten Zeit abgeschlossen. Umgekehrt kann, wenn ein effizientes Verifikationsverfahren besteht, um zu überprüfen, ob eine Antwort korrekt ist, ein Monte Carlo-Algorithmus durch wiederholtes Durchlaufen des Monte Carlo-Algorithmus in einen Las Vegas-Algorithmus umgewandelt werden, bis eine korrekte Antwort erhalten wird. Computational Komplexität Computational Komplexität Theorie Modelle randomisierten Algorithmen als probabilistische Turing Maschinen.Sowohl Las Vegas als auch Monte Carlo-Algorithmen werden berücksichtigt, und es werden mehrere Komplexitätsklassen untersucht. Die grundlegendste randomisierte Komplexitätsklasse ist RP, die die Klasse von Entscheidungsproblemen ist, für die es einen effizienten (polynomial time) randomisierten Algorithmus (oder probabilistic Turing machine) gibt, der NO-Instabilitäten mit absoluter Sicherheit erkennt und YES-Instabilitäten mit einer Wahrscheinlichkeit von mindestens 1/2 erkennt. Die Komplementklasse für RP ist Co-RP. Problemklassen mit (möglicherweise nicht-terminierenden) Algorithmen mit polynomaler Zeitmittelfalllaufzeit, deren Ausgang immer korrekt ist, sollen in ZPP sein. Die Klasse der Probleme, für die sowohl JA als auch NO-Instanzen mit einem Fehler identifiziert werden können, wird als BPP bezeichnet. Diese Klasse wirkt als das randomisierte Äquivalent von P, d.h. BPP stellt die Klasse von effizienten randomisierten Algorithmen dar. Geschichte Historisch war der erste randomisierte Algorithmus eine Methode, die von Michael O. Rabin für das nächstgelegene Paar Problem in der Rechengeometrie entwickelt wurde. Die Studie der randomisierten Algorithmen wurde durch die 1977 Entdeckung eines randomisierten Primalitätstests (d.h. Bestimmung der Primalität einer Zahl) von Robert M. Solovay und Volker Strassen angestoßen. Bald danach Michael O. Rabin zeigte, dass der Primeltest von 1976 in einen randomisierten Algorithmus umgewandelt werden kann. Zu dieser Zeit war kein praktischer deterministischer Algorithmus für Primalität bekannt. Der Miller-Rabin-Primalitätstest beruht auf einer binären Beziehung zwischen zwei positiven Zahlen k und n, die ausgedrückt werden kann, indem man sagt, k "ist ein Zeuge der Kompositität von" n. Es kann gezeigt werden, dass wenn es einen Zeugen für die Kompositität von n gibt, dann n Komposit ist (d.h. n ist nicht Prime), und wenn n zusammengesetzt ist, dann mindestens drei Viertel der natürlichen Zahlen weniger als n sind Zeugen für seine Kompositität, und Es gibt einen schnellen Algorithmus, der ermittelt, ob k und n ein Zeuge für die Kompositität von n ist. Beachten Sie, dass dies impliziert, dass das Grundproblem in Co-RP ist. Wählt man zufällig 100 Zahlen kleiner als eine zusammengesetzte Zahl n, so ist die Wahrscheinlichkeit, einen solchen Zeugen nicht zu finden, (1/4)100, so dass dies für die meisten praktischen Zwecke ein guter Urtalitätstest ist. Wenn n groß ist, kann es keinen anderen Test geben, der praktisch ist. Die Fehlerwahrscheinlichkeit kann durch die Durchführung ausreichender unabhängiger Tests beliebig reduziert werden. Daher gibt es in der Praxis keine Strafe, die mit der Annahme einer kleinen Fehlerwahrscheinlichkeit verbunden ist, da mit wenig Sorgfalt die Fehlerwahrscheinlichkeit astronomische klein gemacht werden kann. Auch wenn seither ein deterministischer Polynom-Zeit-Primalitätstest gefunden wurde (siehe AKS-Primalitätstest), hat er die älteren probabilistischen Tests in kryptographischer Software nicht ersetzt, noch wird davon ausgegangen, dass dies für die vorhersehbare Zukunft der Fall ist. Beispiele für Quicksort ist ein bekannter, häufig verwendeter Algorithmus, in dem Zufall nützlich sein kann. Viele deterministische Versionen dieses Algorithmus erfordern O(n2) Zeit, um n Zahlen für einige gut definierte Klasse von degenerierten Eingaben (wie ein bereits sortiertes Array) mit der spezifischen Klasse von Eingaben zu sortieren, die dieses durch das Protokoll definierte Verhalten zur Schwenkauswahl erzeugen. Wählt der Algorithmus jedoch statistisch gleichmässig Schwenkelemente, so hat er eine nachweislich hohe Wahrscheinlichkeit, in O(n log n) Zeit unabhängig von den Eigenschaften der Eingabe zu beenden. Zufällige inkrementelle Konstruktionen in der Geometrie In der Rechengeometrie ist eine Standardtechnik zum Aufbau einer Struktur wie eine konvexe Hülle oder Delaunay-Triangulation, die Eingabepunkte zufällig zu permutieren und dann nacheinander in die vorhandene Struktur einzufügen. Die Zufallsbildung sorgt dafür, dass die erwartete Anzahl von Änderungen an der durch eine Insertion verursachten Struktur gering ist, so dass die erwartete Laufzeit des Algorithmus von oben begrenzt werden kann. Diese Technik ist als randomisierter Inkrementalaufbau bekannt. Min cut Input: Ein Graph G(V,E) Ausgang: Ein Schnitt, der die Vertiken in L und R unterteilt, mit der minimalen Anzahl der Kanten zwischen L und R. Recall, dass die Kontraktion von zwei Knoten, u und v, in einem (multi-)graph liefert einen neuen Knoten u ' mit Kanten, die die Verbindung der Kanten auf entweder u oder v sind, mit Ausnahme von jeder Kante(en) Verbindung utex und v. Nach der Kontraktion kann das resultierende Diagramm parallele Kanten aufweisen, enthält jedoch keine Selbstschleifen. Kargers grundlegender Algorithmus: beginnen i = 1 Wiederholung Nehmen Sie eine zufällige Kante (u,v) ε E in G ersetzen u und v durch die Kontraktion u', bis nur 2 Knoten das entsprechende Schnittergebnis Ci erhalten bleibeni = i + 1 bis i = m den Mindestschnitt unter C1, C2, ..., Cmend ausgeben Bei jeder Ausführung der äußeren Schleife wiederholt der Algorithmus die innere Schleife, bis nur 2 Knoten übrig bleiben, wird der entsprechende Schnitt erhalten. Die Laufzeit einer Ausführung beträgt O (n ) {\displaystyle O(n}) und n die Anzahl der Vertiken. Nach m-fachen Ausführungen der Außenschleife geben wir den minimalen Schnitt unter allen Ergebnissen aus. Die Figur 2 gibt ein Beispiel für eine Ausführung des Algorithmus. Nach der Ausführung erhalten wir einen Schnitt 3. Analyse des Algorithmus Die Wahrscheinlichkeit, dass der Algorithmus gelingt, ist 1 - die Wahrscheinlichkeit, dass alle Versuche scheitern. Durch die Unabhängigkeit ist die Wahrscheinlichkeit, dass alle Versuche scheitern, By lemma 1, die Wahrscheinlichkeit, dass Ci = C die Wahrscheinlichkeit, dass keine Kante von C während der Iteration i gewählt wird. Betrachten Sie die innere Schleife und lassen Sie Gj den Graphen nach j Randkontraktionen bezeichnen, wobei j ε {0, 1, ..., n - 3}. Gj hat n - j vertices. Wir nutzen die Kettenregel der bedingten Möglichkeiten. Die Wahrscheinlichkeit, dass die bei der Iteration j gewählte Kante nicht in C ist, da vorher kein Rand von C gewählt wurde, beträgt 1 - k | E (G j ) | {\displaystyle 1-{\frac k}{E(G_{j) .Note that Gj\ still has min cut of size k, so dass von Lemma 2, es hat noch mindestens (n - j ) k 2 {\display- 1 - k | E (G j ) | ≥ 1 - 2 n - j = n - j - 2 n - j {\displaystyle 1-{\frac k}{|E(G_{j}}}}\geq 1 ) .So durch die Kettenregel wird die Wahrscheinlichkeit, den Min-Schnitt C zu finden, bei Stornierung Pr [C i = C] ≥ 2 n (n - 1 ) {\displaystyle Pr[C_{i}=C]\geq {\frac 2}{n(n-1) .Durch die Wahrscheinlichkeit, dass der Algorithmus gelingt, beträgt mindestens 1 - ( 1 - 2 n (n - 1 ) m {\displaystyle 1-\left(1-{\frac 2}{n-1)}\right)^{m . Für m = n ( n - 1 ) 2 ln n {\displaystyle m={\frac n(n-1)}{2}\ln n} ist dies gleich 1 - 1 n {\displaystyle 1-{\frac 1}{nDer Algorithmus findet den Min Cut mit der Wahrscheinlichkeit 1 - 1 n {\displaystyle 1-{\frac 1}{n , in der Zeit O (= O (n 3 log n ) {\displaystyle O(mn)=O(n^{3}\log n}) . Derandomization Randomness kann als Ressource betrachtet werden, wie Raum und Zeit. Derandomisation ist dann der Prozess der Entfernung von Zufall (oder der Verwendung so wenig wie möglich). Es ist derzeit nicht bekannt, wenn alle Algorithmen derandomisiert werden können, ohne ihre Laufzeit deutlich zu erhöhen. In der Rechenkomplexität ist beispielsweise nicht bekannt, ob P = BPP, d.h. wir wissen nicht, ob wir einen beliebigen randomisierten Algorithmus nehmen können, der in Polynomzeit mit einer kleinen Fehlerwahrscheinlichkeit läuft und ihn in Polynomzeit ohne Verwendung von Zufall zu laufen derandomisiert. Es gibt spezifische Methoden, die zur Derandomisierung bestimmter randomisierter Algorithmen verwendet werden können: die Methode der bedingten Wahrscheinlichkeiten und ihre Verallgemeinerung, pessimistische Schätzungen Diskrepanztheorie (die verwendet wird, um geometrische Algorithmen zu derandomisieren) die Ausbeutung der begrenzten Unabhängigkeit in den zufälligen Variablen, die durch den Algorithmus verwendet werden, wie die paarweise Unabhängigkeit, die bei universellen Zufallsaufgaben verwendet wird. Diese Technik wird üblicherweise verwendet, um einen Probenraum zu suchen und den Algorithmus deterministisch zu machen (z.B. randomisierte Graphalgorithmen) Wo Zufälligkeit hilft Wenn das Modell der Berechnung auf Turing-Maschinen beschränkt ist, ist es derzeit eine offene Frage, ob die Möglichkeit, zufällige Entscheidungen zu treffen, in der Polynomzeit einige Probleme zu lösen, die nicht in der Polynomzeit ohne diese Fähigkeit gelöst werden können; dies ist die Frage, ob P = BPP. In anderen Zusammenhängen gibt es jedoch konkrete Beispiele für Probleme, bei denen die Zufallsbildung strenge Verbesserungen liefert. Basierend auf dem ersten motivierenden Beispiel: Bei einem exponentiell langen String von 2k-Zeichen, der Hälfte a's und der Hälfte b's, benötigt eine Zutrittsmaschine 2k-1 Lookups im schlimmsten Fall, um den Index eines a zu finden; wenn es erlaubt ist, zufällige Entscheidungen zu treffen, kann es dieses Problem in einer erwarteten Polynomzahl von Lookups lösen. Die natürliche Möglichkeit, eine numerische Berechnung in eingebetteten Systemen oder Cyber-physischen Systemen durchzuführen, besteht darin, ein Ergebnis zu liefern, das der richtigen mit hoher Wahrscheinlichkeit (oder wahrscheinlich etwa korrekte Berechnung (PACC)) nähert. Das harte Problem, das mit der Auswertung des Diskrepanzverlustes zwischen der angenäherten und der korrekten Berechnung verbunden ist, kann effektiv durch Zufälligkeitsrückgriff angesprochen werden In der Kommunikationskomplexität kann die Gleichberechtigung zweier Strings mit Hilfe von log ≠ n {\displaystyle \log n} Bits der Kommunikation mit einem randomisierten Protokoll auf einige Zuverlässigkeit überprüft werden. Jedes deterministische Protokoll erfordert Θ (n ) {\displaystyle \Theta (n}) Bits, wenn sie gegen einen starken Gegner verteidigen. Das Volumen eines konvexen Körpers kann durch einen randomisierten Algorithmus auf beliebige Genauigkeit in der Polynomzeit geschätzt werden. Bárány und Füredi zeigten, dass kein deterministischer Algorithmus das gleiche tun kann. Dies ist bedingungslos, d.h. ohne auf irgendwelche komplexitätstheoretischen Annahmen zu vertrauen, vorausgesetzt, der konvexe Körper kann nur als schwarze Box abgefragt werden. Ein komplexeres, theoretisches Beispiel eines Ortes, an dem zufälligerweise die Klasse IP unterstützt wird. IP besteht aus allen Sprachen, die (mit hoher Wahrscheinlichkeit) durch eine polynomial lange Interaktion zwischen einem allmächtigen Profi und einem Verifier akzeptiert werden können, der einen BPP-Algorithmus implementiert. IP = PSPACE.Wenn es jedoch erforderlich ist, dass der Prüfer deterministisch ist, dann IP = NP. In einem chemischen Reaktionsnetzwerk (eine endliche Menge von Reaktionen wie A+B → 2C + D, die auf einer endlichen Anzahl von Molekülen arbeiten,) ist die Fähigkeit, je einen bestimmten Zielzustand aus einem Anfangszustand zu erreichen, dezidierbar, während auch die Wahrscheinlichkeit, jemals einen bestimmten Zielzustand zu erreichen (unter Verwendung der Standardkonzentrations-basierten Wahrscheinlichkeit, für die die Reaktion als nächstes auftritt) unbestimmbar ist. Insbesondere kann eine begrenzte Turing-Maschine mit beliebig hoher Laufwahrscheinlichkeit für alle Zeit simuliert werden, nur wenn ein zufälliges chemisches Reaktionsnetzwerk verwendet wird.Mit einem einfachen nicht-deterministischen chemischen Reaktionsnetzwerk (alle möglichen Reaktionen können als nächstes erfolgen) ist die Rechenleistung auf primitive rekursive Funktionen beschränkt. Siehe auch Probabilistische Analyse von Algorithmen Atlantic City Algorithmus Monte Carlo Algorithmus Las Vegas Algorithmus Bogosort Prinzip der abgeleiteten Entscheidung Randomized algorithms as zero-sum games Probabilistic roadmap HyperLog count–min Skizze ca. Zählalgorithmus Kargers Algorithmus Hinweise Referenzen Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest und Clifford Stein. Einführung in Algorithms, Second Edition. MITTEL Presse und McGraw–Hill, 1990.ISBN 0-262-03293-7.Kapitel 5: Probabilistische Analyse und Randomized Algorithms, pp.91–122. Dirk Draheim. "Semantik des probabilistischen Typierten Lambda Calculus(Markov Chain Semantics, Termination Behavior, und Denotational Semantics.)" Springer, 2017. Jon Kleinberg und Éva Tardos. Algorithm Design. Kapitel 13: "Randomisierte Algorithmen".Fallis, D. (2000). "Die Zuverlässigkeit von randomisierten Algorithmen". The British Journal for the Philosophy of Science.51 (2:) 255–271. Doi:10.1093/bjps/51.2.255 M. Mitzenmacher und E. Upfal. Wahrscheinlichkeit und Computing: Zufällige Algorithmen und probabilistische Analyse. Cambridge University Press, New York (NY,) 2005. Rajeev Motwani und P. Raghavan. Zufällige Algorithmen. Cambridge University Press, New York (NY,) 1995. Rajeev Motwani und P. Raghavan. Zufällige Algorithmen. Eine Umfrage zu Randomized Algorithms. Christos Papadimitriou (1993,) Computational Complexity (1st ed.,) Addison Wesley, ISBN 978-0-201-53082-7 Kapitel 11: Randomized computation, pp.241–278.Rabin, Michael O. (1980)."Probabilistic algorithm for testing primality". Journal of Number Theory.12: 128–138.doi:10.1016/0022-314X(80)90084-0.A A. Tsay, W. S. Lovejoy, David R. Karger, Random Sampling in Cut, Flow, and Network Design Problems, Mathematics of Operations Research, 24(2):383–413, 1999.