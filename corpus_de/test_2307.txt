Die menschliche Bildsynthese ist Technik, die angewendet werden kann, um glaubwürdige und sogar photorealistische Renditionen von menschlichen Ähnlichkeiten zu machen, sich zu bewegen oder still. Sie existiert seit den frühen 2000er Jahren effektiv. Viele Filme, die computergenerierte Bilder verwenden, haben auf das reale oder andere simulierte Filmmaterial digitale Kunstbilder menschlicher Figuren gezeigt. Gegen Ende der 2010er Jahre wurde eine intensive künstliche Intelligenz angewandt, um Bilder und Videos zu synthetisieren, die wie Menschen aussehen, ohne menschliche Hilfe zu benötigen, nachdem die Trainingsphase abgeschlossen ist, während die alte Schule 7D-Route massive Mengen menschlicher Arbeit erforderte. Zeitleiste der menschlichen Bildsynthese 1971 machte Henri Gouraud die erste CG-Geometrieerfassung und Darstellung eines menschlichen Gesichts. Modeling war seine Frau Sylvie Gouraud. Das 3D-Modell war ein einfaches Drahtrahmenmodell und er hat den Gouraud-Scheinwerfer angewendet, den er am bekanntesten ist, um die erste bekannte Darstellung der menschlichen Ähnlichkeit auf dem Computer zu erzeugen (Bilder ansehen). Der 1972 Kurzfilm A Computer Animated Hand von Edwin Catmull und Fred Parke war das erste Mal, dass computergenerierte Bilder im Film verwendet wurden, um bewegte menschliche Erscheinung zu simulieren. Der Film zeigte einen Computer simulierte Hand und Gesicht (hier Uhrenfilm). Der Film Futureworld 1976 nutzte Teile von A Computer Animated Hand auf dem großen Bildschirm wieder. Das Musikvideo 1983 für Lied Musique Non-Stop von der deutschen Band Kraftwerk wurde 1986 ausgestrahlt. Erstellt von der Künstlerin Rebecca Allen, verfügt es über nicht realistisch aussehende, aber deutlich erkennbare Computersimulationen der Bandmitglieder. Film 1994 Die Crow war die erste Filmproduktion, um die digitale Zusammenstellung eines Computers simulierte Darstellung eines Gesichts auf Szenen zu nutzen, die mit einem Körperdoppel gefilmt wurden. Die Notwendigkeit war die Muse, als der Schauspieler Brandon Lee den Protagonisten porträtierte, versehentlich auf der Bühne getötet wurde. Im Jahr 1999 eroberte Paul Debevec et al.of USC das Reflexionsfeld eines menschlichen Gesichts mit ihrer ersten Version einer Lichtbühne. Sie präsentierten ihre Methode auf der SIGGRAPH 2000In 2003 Publikumsdebüt von fotorealistischen Menschlichkeiten in den Filmen 2003 Die Matrix Reloaded in der Burly-Brawl-Sequenz, in der bis zu 100 Agent Smiths Neo und in The Matrix Revolutions kämpfen, wo zu Beginn der Showdown Agent Smiths Wangebone von Neo eingestanzt wird und das digitale Aussehen unnatürlich unhurt lässt. Die Matrix Revolutions Bonus-DVD-Dokumente und zeigt den Prozess im Detail und die Techniken, die verwendet werden, einschließlich Gesichtsbewegungserfassung und körpereigene Bewegungserfassung und Projektion auf Modelle. 2003 Die Animatrix: Final Flight of the Osiris a state-of-the-art will-to-be human likenesses nicht ganz täuschen die Uhr von Square Pictures gemacht. 2003 wurde die digitale Ähnlichkeit von Tobey Maguire für Filme Spider-man 2 und Spider-man 3 von Sony Pictures Imageworks gemacht. 2005 wurde das Projekt Face of the Future gegründet. von der Universität St Andrews und Perception Lab, gefördert durch das EPSRC. Die Website enthält einen "Face Transformer", der es Benutzern ermöglicht, ihr Gesicht in jede Ethnizität und Alter zu verwandeln sowie die Fähigkeit, ihr Gesicht in ein Gemälde (im Stil von entweder Sandro Botticelli oder Amedeo Modigliani). Dieses Verfahren wird durch die Kombination der Fotografie des Benutzers mit einem mittleren Gesicht erreicht. 2009 Debevec et al.presented new digital likenesses, made by Image Metrics, diesmal der Schauspielerin Emily O'Brien, deren Reflexion mit der USC-Lichtbühne 5 Motion aufgenommen wurde, sieht ziemlich überzeugend aus, im Gegensatz zum Clunky Run in der Animatrix: Final Flight of the Osiris, die im Jahr 2003 hochmodern war, wenn der Photorealismus die Absicht der Animatoren war. Im Jahr 2009 wurde für den Film Terminator Salvation ein digitaler Look-Alike eines jüngeren Arnold Schwarzeneggers gemacht, obwohl das Endergebnis als unbesorgt kritisiert wurde. Die Gesichtsgeometrie wurde von einer 1984er Form von Schwarzenegger übernommen. 2010 Walt Disney Bilder veröffentlichten ein Sci-Fi-Sequel mit dem Titel Tron: Vermächtnis mit einem digital verjüngten digitalen Look-alike des Schauspielers Jeff Bridges spielen die antagonistische CLU. In der SIGGGRAPH 2013 präsentierten Activision und USC eine echte Zeit "Digital Ira" einen digitalen Gesichtsausdruck von Ari Shapiro, einem ICT-USC-Forschungswissenschaftler, der die USC-Lichtstufe X von Ghosh et al. für Reflexionsfeld und Bewegungserfassung nutzte. Das Endergebnis sowohl precomputed als auch real-time Rendering mit dem modernen Spiel GPU hier gezeigt und sieht ziemlich realistisch aus. 2014 Das Presidential Portrait von USC ICT in Verbindung mit der Smithsonian Institution wurde mit der neuesten USC Mobile Light Stage gemacht, in der Präsident Barack Obama seine Geometrie, Texturen und Reflexion gefangen genommen hatte.Im Jahr 2014 stellte Ian Goodfellow et al. die Prinzipien eines generativen adversarialen Netzwerks dar. GANs machte die Schlagzeilen Anfang 2018 mit den Deepfakes Kontroversen. Für den 2015 Film Furious 7 wurde von Weta Digital ein digitaler Look-Alike des Schauspielers Paul Walker, der bei einem Unfall während der Dreharbeiten starb, durchgeführt, um die Vollendung des Films zu ermöglichen. Im Jahr 2016 wurden Techniken, die eine zeitnahe Fälschung von Gesichtsausdrücken in bestehenden 2D-Videos ermöglichen, glaubwürdig nachgewiesen. Im Jahr 2016 wurde für den Rogue One-Film ein digitaler Look-Alike von Peter Cushing gemacht, wo sein Auftritt gleich alt sein würde wie der Schauspieler während der Dreharbeiten des Originals von 1977 Star Wars Film. In der SIGGRAPH 2017 wurde ein audiogesteuerter digitaler Look-Alike des oberen Rumpfes von Barack Obama von Forschern der University of Washington präsentiert.(Ansicht)Es wurde nur von einer Sprachspur als Quelldaten für die Animation nach der Trainingsphase zum Erwerb von Lippensync und breitere Gesichtsinformationen aus Trainingsmaterial, bestehend aus 2D-Videos mit Audio, durchgeführt. Ende 2017 und Anfang 2018 sah das Surfen der Deepfakes Kontroversen, wo Pornovideos mit Deep Machine Learning promoviert wurden, so dass das Gesicht der Schauspielerin durch die Meinung der Software ersetzt wurde, wie andere Personen in der gleichen Pose und Beleuchtung aussehen würden. 2018 GDC Epische Spiele und Tencent Games zeigten Siren, ein digitaler Look-Alike der Schauspielerin Bingjie Jiang. Es wurde mit folgenden Technologien ermöglicht: CubicMotion Computer Vision System, 3Laterals Gesichts-Rigging-System und Vicons Bewegungs-Capture-System. Die Demonstration lief in naher Echtzeit zu 60 Frames pro Sekunde in der Unreal Engine 4. 2018 präsentierte die Xinhua Nachrichtenagentur auf der World Internet Conference in Wuzhen zwei digitale Look-Alikes, die auf die Ähnlichkeit ihrer echten News-Anker Qiu Hao (chinesische Sprache) und Zhang Zhao (englische Sprache) ausgerichtet waren. Die digitalen Look-Alikes wurden in Verbindung mit Sogou gemacht. Weder die verwendete Sprachsynthese noch die gesturing der digitalen Look-Alike-Anker waren gut genug, um den Zuschauer zu täuschen, um sie für echte Menschen mit einer TV-Kamera abgebildet. Im September 2018 Google fügte seiner Verbotsliste „unfreiwillige synthetische pornografische Bilder“ hinzu, so dass jeder die Suchmaschinen-Block-Ergebnisse anfordern kann, die sie als „nude oder in einer sexuell expliziten Situation“ fälschlich darstellen.Im Februar 2019 öffnet Nvidia Quellen StyleGAN, ein neuartiges generatives adversariales Netzwerk. Direkt nach diesem Phillip Wang machte die Website ThisPersonDoesNotExist.com mit StyleGAN, um zu zeigen, dass unbegrenzte Mengen von oft fotorealistisch aussehenden Gesichtsbildern von niemanden automatisch mit einem GAN gemacht werden können. Nvidia's StyleGAN wurde Ende 2018 in einem noch nicht geprüften Papier vorgestellt. Auf der CVPR im Juni 2019 präsentierte das MIT CSAIL ein System mit dem Titel "Speech2Face: Learning the Face Behind a Voice", das wahrscheinliche Gesichter basierend auf einer Aufnahme einer Stimme synthetisiert. Es wurde mit massiven Mengen von Video von Menschen gesprochen trainiert. Seit 1. Juli 2019 Virginia hat den Verkauf und die Verbreitung von nicht autorisierten synthetischen Pornographie kriminalisiert, aber nicht die Herstellung., wie § 18.2-386.2 mit dem Titel 'Unlawful Verbreitung oder Verkauf von Bildern eines anderen; Strafe.' wurde Teil des Code of Virginia. Im Gesetztext heißt es: "Jede Person, die mit der Absicht, das Bild zu koerzieren, zu schikanieren oder einzuschüchtern, bösartig verbreitet oder verkauft irgendeine videographische oder immer noch Bild, die auf irgendeine Weise erstellt wird, die eine andere Person darstellt, die total nackt ist, oder in einem Zustand der Unress schuldig ist, um die Genitalien, pubic Area, Buttocks, oder weibliche Brust, zu entlarieren, wo diese Person weiß oder hat, ist er weiß oder hat Grund. Die identischen Rechnungen waren House Bill 2678 präsentiert von Delegate Marcus Simon dem Virginia House of Delegates am 14. Januar 2019 und drei Tage später wurde eine identische Senatsrechnung 1736 vom Senator Adam Ebbin in den Senat von Virginia eingeführt. Seit 1. September 2019 Texas senate bill SB 751 Änderungsanträge zum Wahlcode trat in Kraft, so dass Kandidaten bei Wahlen eine 30-Tage-Schutzzeit für die Wahlen, bei denen die Herstellung und Verteilung von digitalen Look-Alikes oder synthetischen Fälschungen der Kandidaten ist eine Straftat. Der Gesetztext definiert das Thema des Gesetzes als "ein Video, erstellt mit der Absicht zu täuschen, das scheint eine echte Person darstellen, die eine Handlung, die nicht in der Realität auftritt" Im September 2019 luftte Yle, das finnische Rundfunkunternehmen, ein Ergebnis des experimentellen Journalismus, ein Tiefpunkt des Präsidenten im Büro Sauli Niinistö in seiner Hauptnachrichtensendung, um die fortschreitende Desinformationstechnologie und Probleme hervorzuheben, die sich daraus ergeben.1. Januar 2020 Kalifornien das Staatsrecht AB-602 trat in Kraft, ohne die Zustimmung der abgebildeten Menschen die Herstellung und Verteilung der synthetischen Pornographie zu verbieten. AB-602 bietet Opfer synthetischer Pornographie mit einstweiliger Erleichterung und stellt gesetzliche Bedrohungen gesetzlicher und strafrechtlicher Schäden an kriminellen, synthetischen Pornographie ohne Zustimmung dar. Die Rechnung AB-602 wurde am 3. Oktober 2019 vom kalifornischen Gouverneur Gavin Newsom ins Gesetz aufgenommen und von dem kalifornischen Staatsversammlungsmitglied Marc Berman verfasst. 1. Januar 2020, chinesisches Gesetz, das verlangt, dass synthetisch gefälschte Aufnahmen eine deutliche Mitteilung über seine Fälschung in Kraft treten sollte. Nicht einzuhalten, könnte als ein Verbrechen angesehen werden, das die Cyberspace-Administration von China auf ihrer Website erklärt. China hat dieses neue Gesetz im November 2019 angekündigt. Die chinesische Regierung scheint das Recht zu behalten, sowohl Nutzer als auch Online-Video-Plattformen zu verfolgen, die sich nicht an die Regeln halten. Schlüsseldurchbruch für den Photorealismus: Reflexions-Erfassung Im Jahr 1999Paul Debevec et al.of USC hat die erste bekannte Reflexions-Erfassung über das menschliche Gesicht mit ihrer extrem einfachen Lichtphase. Sie präsentierten ihre Methode und Ergebnisse in SIGGRAPH 2000. Der wissenschaftliche Durchbruch, der erforderlich ist, um die Untergrundlichtkomponente zu finden (die Simulationsmodelle leuchten von innen leicht) was mit der Erkenntnis zu erkennen ist, dass Licht, das von der Öl-Luft-Schicht reflektiert wird, seine Polarisation behält und das Untergrundlicht seine Polarisation verliert. So ausgestattet nur mit einer beweglichen Lichtquelle, beweglichen Videokamera, 2 Polarisatoren und einem Computerprogramm, das extrem einfache Mathematik und das letzte Stück benötigt, um Photorealismus zu erreichen. Für ein glaubwürdiges Ergebnis muss sowohl Licht, das von der Haut (BRDF) als auch innerhalb der Haut (ein besonderer Fall von BTDF) reflektiert wird, die zusammen das BSDF bilden, erfasst und simuliert werden. Entrückung Die 3D-Geometrie und -Texturen werden mittels 3D-Scannen mit einem RGB XYZ-Scanner wie Arius3d oder Cyberware (Texturen von Fotos, nicht reiner RGB XYZ-Scanner), stereophotogrammetrisch aus synchronisierten Fotos oder sogar aus genug wiederholten nicht-simultanen Fotos auf ein 3D-Modell erfasst. Die digitale Bildhauerei kann verwendet werden, um Modelle der Körperteile zu erstellen, für die Daten nicht erfasst werden können, z.B. Teile des Körpers, der durch Kleidung abgedeckt ist. Für glaubwürdige Ergebnisse muss auch das Reflexionsfeld erfasst oder eine Annäherung von den Bibliotheken zu einem 7D-Remissionsmodell des Ziels ausgewählt werden. Synthese Der gesamte Prozess, digitale Look-Alikes zu machen, d.h. Zeichen so lebens- und realistisch, dass sie als Bilder von Menschen weitergegeben werden können, ist eine sehr komplexe Aufgabe, da es photorealistisch modeling, animating, Cross-Mapping und die weiche Körperdynamik des menschlichen Aussehens erfordert. Die Synthese mit einem Schauspieler und geeigneten Algorithmen wird mit leistungsstarken Computern angewendet. Die Rolle des Schauspielers in der Synthese ist es, sich um die Nachahmung menschlicher Ausdrücke in noch bildsynchronisierender und auch menschlicher Bewegung im Bewegungsbild zu kümmern. Algorithmen werden benötigt, um Gesetze der Physik und Physiologie zu simulieren und die Modelle und ihr Aussehen, Bewegungen und Interaktion entsprechend zu ordnen. Oft werden im Syntheseteil sowohl physikalisch/physiologiebasierte (d.h. skeletale Animation) als auch bildbasierte Modellierung und Renderung eingesetzt. Hybride Modelle mit beiden Ansätzen haben beste Ergebnisse in Realismus und Leichtigkeit gezeigt. Morph-Target-Animation reduziert die Arbeitsbelastung durch eine höhere Niveausteuerung, wo verschiedene Gesichtsausdrücke als Verformungen des Modells definiert werden, die Gesichtsausdrücke intuitiv abstimmbar machen können. Morph Target-Animation kann dann das Modell zwischen verschiedenen definierten Gesichtsausdrücken oder Körper posiert ohne viel Notwendigkeit für menschliche Intervention. Die Verwendung von Verdrängungskartierungen spielt eine wichtige Rolle bei der Erzielung eines realistischen Ergebnisses mit feinem Detail der Haut wie Poren und Falten bis zu 100 μm. Ansatz des maschinellen Lernens In den späten 2010er Jahren wurden maschinelles Lernen und genauere generative adversariale Netzwerke (GAN) von NVIDIA verwendet, um zufällige, aber auch fotorealistische humanähnliche Porträts zu produzieren. Das System namens StyleGAN wurde auf einer Datenbank von 70.000 Bildern von der Bilder Depository Website Flickr ausgebildet. Der Quellcode wurde 2019 auf GitHub veröffentlicht. Die Ausgänge des Generatornetzes von der zufälligen Eingabe wurden auf einer Reihe von Websites öffentlich zugänglich gemacht. Ähnlich, seit 2018, Deepfake-Technologie hat GANs erlaubt, Gesichter zwischen Schauspielern zu schlucken; kombiniert mit der Fähigkeit zu gefälschten Stimmen, GANs kann so gefälschte Videos erzeugen, die überzeugend wirken.Anwendungen Hauptanwendungen fallen in die Bereiche der Stock-Fotografie, synthetische Datensätze, virtuelle Kino, Computer- und Videospiele und verdeckte Desinformationsangriffe. Darüber hinaus schlägt einige Forschung vor, dass es therapeutische Effekte haben kann, als "Psychologen und Berater haben auch mit Avataren begonnen, Therapie an Kunden zu liefern, die Phobien, eine Geschichte von Trauma, Sucht, Aspergers Syndrom oder soziale Angst haben. " Die starken Speicherdruck- und Gehirnaktivierungseffekte, die durch das Anschauen eines digitalen Look-alike Avatars von sich selbst verursacht werden, sind der Doppelgänger-Effekt. Der Doppelgänger-Effekt kann heilen, wenn verdeckter Desinformationsangriff als solche den Zielen des Angriffs ausgesetzt ist. Verwandte Themen Seit der Einführung der Sprachbearbeitungs- und Generierungssoftware Adobe Voco, einem Prototyp, der als Teil der Adobe Creative Suite und DeepMind WaveNet, einem Prototyp von Google, eingesetzt wird, ist die Sprachsynthese von einer Aufnahme einer echten Stimme des Menschen völlig unentscheidbar. Die Fähigkeit, andere Volksstimmen zu stehlen und zu manipulieren, stellt offensichtliche ethische Bedenken. Auf der Konferenz zu Neural Information Processing Systems (NeurIPS) präsentierten Forscher von Google auf der Konferenz 2018 die Arbeit 'Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis', die das Lernen von der Lautsprecherverifikation überträgt, um Text-zu-Speech-Synthese zu erreichen, die fast wie jeder aus einer Sprachprobe von nur 5 Sekunden klingen kann (hören). Sourcing Bilder für AI-Training stellt eine Frage der Privatsphäre, da Menschen, die für die Ausbildung verwendet werden, nicht zugestimmt haben. Digitale Sound-Alikes-Technologie fand seinen Weg in die Hände von Kriminellen, wie in 2019 Symantec-Forscher wussten von 3 Fällen, wo Technologie für Verbrechen verwendet wurde. Dies gekoppelt mit der Tatsache, dass (Stand 2016) Techniken, die eine nahezu Echtzeit-Fälschung von Gesichtsausdrücken in bestehenden 2D-Videos ermöglichen, glaubhaft nachgewiesen worden sind, erhöht den Stress auf die Desinformationssituation. Siehe auch Motion-capture Acting Internet Manipulation Mediensynthese Propaganda Techniken 3D Datenerfassung und Objektrekonstruktion 3D Rekonstruktion aus mehreren Bildern 3D posieren Schätzung im Allgemeinen und artikulierte Körper stellt Schätzung vor allem mit der Erfassung menschlicher Ähnlichkeit zu tun. 4D Rekonstruktion Finger Tracking Gesture Erkennung StyleGAN =References ==