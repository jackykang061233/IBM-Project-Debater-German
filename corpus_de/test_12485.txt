In der Statistik sind naive Bayes Klassifikatoren eine Familie von einfachen "probabilistischen Klassifikatoren", basierend auf der Anwendung Bayes' Theorem mit starken (naïve) Unabhängigkeit Annahmen zwischen den Merkmalen (siehe Bayes Klassifikator). Sie gehören zu den einfachsten Bayesischen Netzwerkmodellen, aber in Verbindung mit Kerneldichteschätzung können sie höhere Genauigkeitsstufen erreichen. Naïve Bayes Klassifikatoren sind sehr skalierbar und erfordern eine Reihe von Parametern linear in der Anzahl der Variablen (features/predictors) in einem Lernproblem. Maximale Wahrscheinlichkeitsausbildung kann durch Auswertung eines geschlossenen Ausdrucks, der lineare Zeit benötigt, statt durch teure iterative Approximation, wie sie für viele andere Arten von Klassifikatoren verwendet wird, erfolgen. In der Statistik und Informatik Literatur sind naive Bayes Modelle unter verschiedenen Namen bekannt, darunter einfache Bayes und Unabhängigkeit Bayes. Alle diese Namen verweisen auf die Verwendung von Bayes' Theorem in der Entscheidungsregel des Klassifikators, aber naïve Bayes ist nicht (notwendig) eine Bayesische Methode. Einführung Naive Bayes ist eine einfache Technik zum Konstruieren von Klassifikatoren: Modelle, die Klassenetiketten Probleminstanzen zuordnen, dargestellt als Vektoren von Merkmalswerten, wo die Klassenetiketten aus einem endlichen Satz gezogen werden. Es gibt keinen einzigen Algorithmus für die Ausbildung solcher Klassifikatoren, sondern eine Familie von Algorithmen basierend auf einem gemeinsamen Prinzip: Alle naive Bayes Klassifikatoren nehmen an, dass der Wert eines bestimmten Merkmals unabhängig von dem Wert eines anderen Merkmals ist, angesichts der Klassenvariable. Beispielsweise kann eine Frucht als Apfel betrachtet werden, wenn sie rot, rund und etwa 10 cm Durchmesser ist. Ein naive Bayes Klassifikator betrachtet jedes dieser Merkmale unabhängig von der Wahrscheinlichkeit, dass diese Frucht ein Apfel ist, unabhängig von möglichen Zusammenhängen zwischen Farbe, Rundheit und Durchmesser Merkmale. Für einige Arten von Wahrscheinlichkeitsmodellen können naive Bayes Klassifikatoren sehr effizient in einer überwachten Lernumgebung ausgebildet werden. In vielen praktischen Anwendungen verwendet die Parameterschätzung für naive Bayes Modelle die Methode der maximalen Wahrscheinlichkeit; d.h. man kann mit dem naive Bayes Modell arbeiten, ohne Bayesische Wahrscheinlichkeit zu akzeptieren oder Bayesische Methoden zu verwenden. Trotz ihres naiven Designs und anscheinend übersimplifizierten Annahmen haben naive Bayes Klassifikatoren in vielen komplexen realen Situationen sehr gut gearbeitet. Im Jahr 2004 zeigte eine Analyse des Bayesischen Klassifikationsproblems, dass es solide theoretische Gründe für die scheinbar unplausible Wirksamkeit von naiven Bayes Klassifikatoren gibt. Dennoch zeigte ein umfassender Vergleich mit anderen Klassifizierungsalgorithmen im Jahr 2006, dass die Bayes-Klassifikation durch andere Ansätze, wie z.B. verstärkte Bäume oder zufällige Wälder, übertroffen wird. Ein Vorteil von naive Bayes ist, dass es nur eine geringe Anzahl von Trainingsdaten benötigt, um die für die Einstufung notwendigen Parameter zu schätzen. Probabilistisches Modell Abstractly, naïve Bayes ist ein bedingtes Wahrscheinlichkeitsmodell: Bei einer zu klassifizierenden Probleminstanz, dargestellt durch einen Vektor x = ( x 1 , ... , x n ) {\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{n}, die einige n Merkmale (unabhängige Variablen) darstellt, ist es dieser Instanz Das Modell muss daher reformiert werden, um es attraktiver zu machen. Unter Verwendung von Bay' theor, kann die bedingte Wahrscheinlichkeit als p (C k ∣ x ) = p (C k ) p ( x ∣ C k ) p ( x ) {\displaystyle p(C_{k}\mid\mathb\ {x} {=\)frac p(C_{k\} In der Praxis gibt es nur im Zähler dieses Anteils Interesse, da der Nenner nicht von C {\displaystyle C} abhängt und die Werte der Merkmale x i {\displaystyle x_{i} angegeben werden, so dass der Nenner effektiv konstant ist. &  of,  − x_{n},C_{k})\\\&=p(x_{1}\mid x_{2},\ldots x_{n},C_{k\) p(x_{2},\ldots x_{n},C_{k})\\\&=p(x_{1}\mid x_{2},\ldots x_{n},C_{k\) p(x_{2}\mid x_{3},\ldots x_{n},C_{k\) p(x_{3},\ldots x_{n},C_{k})\\&=\cdots =&p(x_{1}\mid x_{2},\ldots x_{n},C_{k})\p(x_{2}\mid x_{3},\ldots (x_{n},C_{k}) p(x_{n}\mid C_{k\}) p(C_{k})\\\end{ausgeglichen Nun kommen die naiven bedingten Unabhängigkeitsannahmen ins Spiel: Nehmen Sie an, dass alle Merkmale in x {\displaystyle \mathbf {x} } voneinander unabhängig sind, bedingt durch die Kategorie C k {\displaystyle C_{k} .Unter dieser Annahme, p ( x i ∣ x i + 1 , ..., x n , C k ) = p ( x i ∣ C k ) (\displaystyle p(x_{i}\mid x_{i+1},\ldots x_{n},C_{k})=p(x_{i}\mid C_{k,\}) .Dadurch kann das gemeinsame Modell als p (C k ∣ x 1 , ..., x n ) a p (C k, x 1 , ... , x n ) a p (C k ) p ( x 1 ∣ C k ) s Dies bedeutet, dass die bedingte Verteilung über die Klassenvariable C {\displaystyle C} unter den obigen Unabhängigkeitsannahmen: p (C k ∣ x 1 , ..., x n ) = 1 Z p (C k ) ∏ i = 1 n p ( x i ∣ C k ) (\displaystyle p(C_{k}\mid x_{1},\ldots ,x_{n})={\frac 1{Z}p(C_{k})\prod i=1}^{n}p(x_{i}\mid C_{k}), wobei der Nachweis Z = p ( x ) = Σ k p (C k ) p (x ∣ C k )\displaystyle Z=p(\mathbf {x}=\)Konstruieren eines Klassifikators aus dem Wahrscheinlichkeitsmodell Die bisherige Diskussion hat das unabhängige Merkmalsmodell, also das naïve Bayes Wahrscheinlichkeitsmodell, abgeleitet. Der naive Bayes Klassifikator kombiniert dieses Modell mit einer Entscheidungsregel. Eine gemeinsame Regel ist, die Hypothese auszuwählen, die am wahrscheinlichsten ist; dies ist als die maximale eine posteriori oder MAP Entscheidung Regel bekannt. Der entsprechende Klassifikator, ein Bayes Klassifikator, ist die Funktion, die ein Klassenlabel y ^ = C k {\displaystyle {\hat y}=C_{k für einige k wie folgt zuordnet: y ^ = argmax k ε { 1 , ... , K } p ( C k )  i i = 1 n p ( x i ∣ C k ) . {\displaystyle {\hat y}={\underset} {,\ldots ,K\}{\operatorname {argmax}}}\ p(C_{k})\displaystyle \prod i=1}{n}p(x_{i}\mid C_{k}) Die Parameterschätzung und die Ereignismodelle A-Klasse kann durch die Annahme von äquiprobierbaren Klassen (d.h. p (C k) = 1 / K {\displaystyle p(C_{k}) = 1/K) oder durch Berechnung einer Schätzung für die Klassenwahrscheinlichkeit aus dem Ausbildungssatz (d.h. <prior für eine bestimmte Klasse> = < Anzahl der Proben in der Klasse>/<total Anzahl der Proben>) berechnet werden. Um die Parameter für die Verteilung einer Funktion abzuschätzen, muss man eine Verteilung annehmen oder nichtparametrische Modelle für die Merkmale aus dem Trainingsset erstellen. Die Annahmen auf Verteilungen von Merkmalen werden als "Ereignismodell" des naiven Bayes Klassifikators bezeichnet. Für diskrete Features wie die in der Dokumentenklassifikation (inklusive Spamfilterung) vorkommenden Multinom- und Bernoulli-Vertriebe sind beliebt. Diese Annahmen führen zu zwei unterschiedlichen Modellen, die oft verwirrt sind. Gaussian naïve Bays Beim Umgang mit kontinuierlichen Daten ist eine typische Annahme, dass die mit jeder Klasse verbundenen kontinuierlichen Werte nach einer normalen (oder Gaussian) Verteilung verteilt werden. Nehmen Sie beispielsweise an, die Trainingsdaten enthalten ein kontinuierliches Attribut, x {\displaystyle x} . Die Daten werden zunächst von der Klasse segmentiert und dann in jeder Klasse die mittlere und Varianz von x {\displaystyle x} berechnet. EPMATHMARKEREP EPMATHMARKEREP EPMATHMARKEREP EPMATHMARKEREP EPMATHMARKEREP ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Eine weitere gemeinsame Technik für die Verarbeitung von kontinuierlichen Werten ist die Binning zu verwenden, um die Merkmalswerte zu unterscheiden, um eine neue Reihe von Bernoulli-verteilten Merkmalen zu erhalten; einige Literatur in der Tat schlägt vor, dass dies notwendig ist, um naive Bayes anzuwenden, aber es ist nicht, und die Diskretierung kann diskriminierende Informationen wegwerfen. Manchmal ist die Verteilung der klassenbedingten Randdichten weit von normal. In diesen Fällen kann die Kerneldichteschätzung für eine realistischere Schätzung der Randdichten jeder Klasse verwendet werden. Diese Methode, die von John und Langley eingeführt wurde, kann die Genauigkeit des Klassifikators erheblich steigern. Multinomial naïve Bayes Mit einem multinomialen Ereignismodell stellen Proben (Featurevektoren) die Frequenzen dar, mit denen bestimmte Ereignisse durch ein Multinom erzeugt wurden (p 1 ,..., p n ) {\displaystyle (p_{1},\dots ,p_{n}), wobei p i {\displaystyle p_{i} die Wahrscheinlichkeit ist, dass Ereignis i auftritt (oder K solche Multiclass cases ist. Ein Merkmalsvektor x = ( x 1 , ..., x n ) {\displaystyle \mathbf {x} =(x_{1},\dots ,x_{n}) ist dann ein Histogramm, wobei x i {\displaystyle x_{i} die Anzahl der Zeiten zählte, die in einer bestimmten Instanz beobachtet wurden. Dies ist das Ereignismodell, das typischerweise für die Dokumentenklassifikation verwendet wird, mit Ereignissen, die das Auftreten eines Wortes in einem einzigen Dokument darstellen (siehe Fallbeispiel). Die Wahrscheinlichkeit, ein Histogramm x zu beobachten, wird durch p ( x ∣ C k) = ( Σ i = 1 n x i) ! ∏ i) 1 n x i ! i) , p k i x i {\displaystyle p(\mathbf {x} \mid C_{k})={\frac {\sum i=1{n}x_{i}}}}}}{\prod I=1{n}x_{i}}}\prod ) Der Multinomial naïve Bayes Klassifikator wird ein linearer Klassifikator, wenn er im Log-Raum ausgedrückt wird: log ‡ p (C k ∣ x ) a log ‡ (p (C k ) ∏ i = 1 n p k i ) = log ‡ p (C k ) + Σ i = 1 x i ⋅ log protokoll p k i = b + w k ⊤ x {\displaystyle begin{aligned}\log p(C_{k}\mid \mathbf {x} \&)varpropto \log Left(p(C_{k}\prod I=1^{n}{p_{ki}}{x_{i}}\right)\\&=\log p(C_{k}+\sum I=1{n}x_{i}\cdot \log p_{ki}\&=b+\mathbf {w}{\top \}mathbf {x} \end{just} wobei b = log ‡ p (C k ) {\displaystyle b=\log p(C_{k}) und w k i = log ‡ p k i {\displaystyle w_{ki}=\log p_{ki} .Wenn in den Trainingsdaten ein bestimmter Klassen- und Merkmalswert nie gemeinsam auftritt, wird die frequenzbasierte Wahrscheinlichkeitsschätzung Null sein, da die Wahrscheinlichkeitsschätzung direkt proportional zur Anzahl der Ereignisse von a ist. Dies ist problematisch, weil es alle Informationen in den anderen Wahrscheinlichkeiten löschen wird, wenn sie multipliziert werden. Daher ist es oft wünschenswert, eine sogenannte Pseudocount-Korrektur in allen Wahrscheinlichkeitsschätzungen so einzubeziehen, dass keine Wahrscheinlichkeit jemals auf exakt Null gesetzt wird. Diese Art der Reglementierung naive Bayes wird Laplace Glättung genannt, wenn der Pseudocount ein ist, und Lidstone Glättung im allgemeinen Fall. Rennie et al. Probleme mit der multinomen Annahme im Rahmen der Dokumentenklassifikation und mögliche Möglichkeiten, diese Probleme zu lindern, einschließlich der Verwendung von tf–idf Gewichte anstelle von rohen Termfrequenzen und Dokument Längennormalisierung, um einen naive Bayes Klassifikator, der mit Unterstützung Vektormaschinen wettbewerbsfähig ist. Bernoulli naive Buchten Im multivariate Bernoulli-Eventmodell sind Merkmale unabhängige Booleane (binäre Variablen), die Eingaben beschreiben. Wie das Multinomialmodell ist dieses Modell für Dokumentklassifikationsaufgaben beliebt, bei denen binäre Begriffserscheinungen anstelle von Termfrequenzen verwendet werden. Ist x i {\displaystyle x_{i} ein Boolean, der das Vorkommen oder Fehlen des i'th term aus dem Vokabular ausdrückt, so wird die Wahrscheinlichkeit eines Dokuments, das eine Klasse C k {\displaystyle C_{k} erhält, durch p ( x  of C k) = ∏ i = , p k i x i ( 1 - p k i) ( 1 - x i) {\displaystyle p(\mathbf {x} \mid C_{k})=\prod i=1^{n}p_{ki}^{x_{i}(1-p_{ki})^{(1-x_{i, wobei p k i {\displaystyle p_{ki} die Wahrscheinlichkeit der Klasse C k {\displaystyle C_{k} ist, die den Begriff x i {\displaystyle x_{i} erzeugt. Dieses Event-Modell ist besonders beliebt für die Klassifizierung von Kurztexten. Es hat den Vorteil, das Fehlen von Begriffen ausdrücklich zu modellieren. Beachten Sie, dass ein naive Bayes Klassifikator mit einem Bernoulli-Eventmodell nicht der gleiche ist wie ein multinomer NB-Klassifikator mit Frequenzangaben, die zu einem gekürzt werden. Semi-supervised Parameterschätzung Mit der Möglichkeit, einen naïve Bayes Klassifikator aus markierten Daten zu trainieren, ist es möglich, einen semi-supervised Trainingsalgorithmus zu konstruieren, der aus einer Kombination von markierten und unmarkierten Daten lernen kann, indem der überwachte Lernalgorithmus in einer Schleife ausgeführt wird: Bei einer Sammlung D = L ⊎ U {\displaystyle D=L\uplus U} von markierten Proben L und unmarkierten Proben U, beginnen Sie mit der Ausbildung eines naïve Bayes Klassifikators auf L. Bis zur Konvergenz, tun: Predict class probabilities P (C ∣ x ) {\displaystyle P(C\mid x}) für alle Beispiele x in D\displaystyle D}. Die Konvergenz wird anhand einer Verbesserung der Modellwahrscheinlichkeit P (D ∣ θ ) {\displaystyle P(D\mid \theta )} bestimmt, wobei θ {\displaystyle \theta } die Parameter des naïve Bayes Modells bezeichnet. Dieser Trainingsalgorithmus ist ein Beispiel des allgemeineren Erwartungs-Maximalisierungsalgorithmus (EM:) der Prädiktionsschritt innerhalb der Schleife ist der E-Schritt von EM, während die Umschulung von naïve Bayes der M-Schritt ist. Der Algorithmus ist formal durch die Annahme gerechtfertigt, dass die Daten durch ein Mischungsmodell erzeugt werden, und die Komponenten dieses Mischungsmodells sind genau die Klassen des Klassifikationsproblems. Trotz der Tatsache, dass die weitreichenden unabhängigen Annahmen oft ungenau sind, hat der naive Bayes Klassifikator mehrere Eigenschaften, die es in der Praxis überraschend nützlich machen. Insbesondere bedeutet die Entkopplung der klassenbedingten Merkmalsverteilungen, dass jede Verteilung unabhängig voneinander als eindimensionale Verteilung geschätzt werden kann. Dies hilft, Probleme zu beseitigen, die aus dem Fluch der Dimensionalität, wie die Notwendigkeit von Datensätzen, die exponentiell mit der Anzahl der Funktionen skalieren. Während naive Bayes oft nicht eine gute Schätzung für die korrekten Klassenwahrscheinlichkeiten, dies kann nicht eine Anforderung für viele Anwendungen. Zum Beispiel wird der naive Bayes Klassifikator die richtige MAP-Entscheidungsregelklassifikation vornehmen, solange die richtige Klasse wahrscheinlicher ist als jede andere Klasse. Dies gilt unabhängig davon, ob die Wahrscheinlichkeitsschätzung leicht oder sogar grob ungenau ist. Auf diese Weise kann der gesamte Klassifikator robust genug sein, um schwerwiegende Mängel in seinem zugrunde liegenden naiven Wahrscheinlichkeitsmodell zu ignorieren. Weitere Gründe für den beobachteten Erfolg des naiven Bayes-Klassifikators sind in der unten zitierten Literatur beschrieben. Beziehung zur logistischen Regression Bei diskreten Eingaben (Indikator oder Frequenzmerkmale für diskrete Ereignisse) bilden naive Bayes-Klassifikatoren ein generativ-diskriminierendes Paar mit (multinomalen) logistischen Regressions-Klassifikatoren: jeder naive Bayes-Klassifikator kann als eine Art der Anpassung eines Wahrscheinlichkeitsmodells angesehen werden, das die gemeinsame Wahrscheinlichkeit p (C, x ) Die log  log 0 {\displaystyle \log {\frac p(C_{1}\mid \mathbf {x} )p(C_{2}\mid \mathbf {x} =\})log p(C_{1}\mid \mathbf {x} \-)log p(C_{2}\mid \mathbf {x} >) Seit naiv Bayes ist auch ein lineares Modell für die beiden diskreten Ereignismodelle, es kann als lineare Funktion reparametrisiert werden b + w s x > 0 {\displaystyle b+\mathbf {w} {\^top x>0 . Die Einhaltung der Wahrscheinlichkeiten ist dann eine Frage der Anwendung der logistischen Funktion auf b + w ⊤ x {\displaystyle b+\mathbf {w} {\^top x , oder im Multiclass-Fall die Softmax-Funktion. Diskriminative Klassifikatoren haben niedrigere asymptotische Fehler als generative; aber Forschung von Ng und Jordan hat gezeigt, dass in einigen praktischen Fällen naive Bayes kann logistische Regression, weil es erreicht seine asymptotische Fehler schneller. Beispiele Personenklassifikation Problem: Klassifizierung, ob eine bestimmte Person ein Mann oder ein Weibchen ist, basierend auf den gemessenen Merkmalen. Die Funktionen umfassen Höhe, Gewicht und Fußgröße. Trainingsbeispiel unten. Der Klassifikator, der aus dem Trainingsset unter Verwendung einer Gaussischen Verteilungsannahme erstellt wurde, wäre (wenn Varianzen unvoreingenommene Mustervarianzen sind): Das folgende Beispiel nimmt äquiprobierbare Klassen an, so dass P(male=) P(female=) 0.5.Diese frühere Wahrscheinlichkeitsverteilung könnte auf Vorkenntnissen von Frequenzen in der größeren Bevölkerung oder im Trainingsset basieren. Im Folgenden wird eine Probe als männlich oder weiblich eingestuft. Um die Probe zu klassifizieren, muss man bestimmen, welche Posterior größer, männlich oder weiblich ist. Für die Einstufung als männlich wird der Posterior durch Posterior (männlich) = P (männlich) p (höhe ∣ männlich) p (gewicht ∣ männlich) p (fuß size ∣ male ) e v i d e n c e {\displaystyle \text{posterior (mänlich)}={\frac P({text{m{male}\)\,p({f} Für die Einstufung als weiblich wird der Posterior von posterior (weiblich) = P (weiblich ) p (höhe  female weiblich ) p (Gewicht ∣ weiblich ) p (Fußgröße ∣ weiblich ∣) e v i d e n c e {\displaystyle \text{posterior (female)}}={\frac P({\text{female}})\,p({\text{height}}\mid text{female}})\,p({\text{weight}\mid text{female}}\,p({\text{f}}}}} {m} {m} {m} {m} {m} {m} {m} {m} {m} {m} {m}} {m} {m} {m} {m} {m}} {m} {m} {m} {m}} {m}\m}\m} {m} {m}\m}\m} {m} {m} {m}\m}\m} {m} {m}\m} {m} {m} {m} {m}\m}\m}\m}\m} {m} {m} {m} Angesichts der Stichprobe ist der Nachweis jedoch eine Konstante und skaliert somit beide Posterioren gleichermaßen. Sie wirkt sich daher nicht auf die Einstufung aus und kann ignoriert werden. Die Wahrscheinlichkeitsverteilung für das Geschlecht der Probe kann nun bestimmt werden: P (Männchen ) = 0.5 {\displaystyle P({\text{male}}) = 0.5 p (Höhe ∣ male ) = 1 2 π σ 2 exp \m \ n Beachten Sie, dass hier ein Wert größer als 1 OK ist – es ist eine Wahrscheinlichkeitsdichte anstatt eine Wahrscheinlichkeit, weil die Höhe eine kontinuierliche Größe ist. p (Gewicht ∣ männlich ) = 1 2 π σ 2 exp ‡ ( - ( 130 - μ ) 2 2 σ 2 σ 2 ) = 5.9881 ⋅ 10 - 6 {\displaystyle p({\text{Gewicht}\mid text{male}}}}={\frac 1}\sqrt {2\pi \sigma {^2}}}\exp left({\frac {(130-\mu ^)2}{2\sigma 2}}}}\right)=5.9881\cdot10^-6 p (Fußgröße  male männlich) = 1 2 π σ 2 exp *** ************************************************************************************************************************************ Sachverzeichnis Hier ist ein Beispiel der naiven Bayesischen Klassifikation zu dem Dokument Klassifikation Problem. Betrachten Sie das Problem der Klassifikation von Dokumenten durch ihre Inhalte, zum Beispiel in Spam- und Nicht-Spam-E-Mails. Stellen Sie sich vor, dass Dokumente aus einer Reihe von Dokumentenklassen gezogen werden, die als Sätze von Wörtern modelliert werden können, wobei die (unabhängige) Wahrscheinlichkeit, dass das i-te Wort eines bestimmten Dokuments in einem Dokument der Klasse C auftritt, als p (w i ∣ C ) geschrieben werden kann. {\displaystyle p(w_{i}\mid C,\}) (Für diese Behandlung werden die Dinge weiter vereinfacht, indem angenommen wird, dass Wörter in dem Dokument zufällig verteilt werden - d.h. Wörter sind nicht abhängig von der Länge des Dokuments, Position innerhalb des Dokuments mit anderen Worten oder anderen Dokumenten-Kontext.) Dann ist die Wahrscheinlichkeit, dass ein bestimmtes Dokument D alle Wörter w i {\displaystyle w_{i} enthält, bei Klasse C, p ( D ∣ C) =  of i p (w i ∣ C ) {\displaystyle p(D\mid C)=\prod i}p(w_{i}\mid C,\}) Die Frage, die zu beantworten ist: "Was ist die Wahrscheinlichkeit, dass ein bestimmtes Dokument D zu einer bestimmten Klasse C gehört? " Mit anderen Worten, was ist p ( C ∣ D ) {\displaystyle p(C\mid D,\}) ? Nun nach Definition p (D ∣ C ) = p (D С C ) p (C ) {\displaystyle p(D\mid C)={p(D\cap C) \over p(C}) und p (C ∣ D ) = p (D С C ) p (D ) p ( C ∣ D ) = p (C ) p ( D ∣ C ) p ( D ) {\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D Assume für den Moment, dass es nur zwei voneinander exklusive Klassen, S und ¬S (z.B. Spam und nicht Spam, e) gibt, so dass i p (w i ∣ S ) {\displaystyle p(D\mid S)=\prod i}p(w_{i}\mid S,\}) und p (D ∣ ¬ S ) = ∏ i p (w i ∣ ∣ ¬ S ) {\displaystyle p(D\mid \neg S)=\prod i}p(w_{i}\mid \neg S,\}) Mit dem Bayesischen Ergebnis oben kann man schreiben: p ( S ∣ D ) = p ( S ) p ( D , i p (w i ∣ S ) (\displaystyle p(S\mid D)={p(S)\over p(D)}\,\prod i}p(w_{i}\mid S}) p (€ S ∣ D ) = p (€ S ) p (D ) ∏ i p (w i ∣ ∣ ¬ S )) {\displaystyle p(\neg S\mid D)={p(\neg S)\over p(D)}\,\prod i}p(w_{i}\mid \neg S} Eins nach dem anderen gibt: p ( S ∣ D )p (€ S ) D ) = p (S ∏ i p (w i ∣ S ) p (€ S ) ∏ i p (w i ∣ ∣ ¬ S ) {\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S)\,\prod i}p(w_{i}\mid S) \over p(\neg S)\,\prod i}p(w_{i}\mid \neg S}) Das kann wieder hergestellt werden wie: p ( S ∣ D ) p ( ¬ S ∣ D ) = p ( S ) p ( ¬ S ) ∏ i p ( w i ∣ S ) p ( w i ∣ ∣ ¬ S ) {\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S \over p(\neg S)}\,\prod i}{p(w_{i}\mid S) \over p(w_{i}\mid \neg S} So kann das Wahrscheinlichkeitsverhältnis p(S | D) / p(€S | D) in Bezug auf eine Reihe von Wahrscheinlichkeitsverhältnissen ausgedrückt werden. Die tatsächliche Wahrscheinlichkeit p(S | D) lässt sich anhand der Beobachtung, dass p(S | D+) p(€S | D=) 1. Den Logarithmus aller dieser Verhältnisse erfassen, erhält man: ln ‡ p ( S ∣ D ) p (€ S ∣ D ) = ln ‡ p (S ) p (€ S ) + Σ i ln ‡ p (w i ∣ S ) p ( w i ∣ ∣ ¬ S ) {\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}=\ln {p(S) \over p(\neg S)}+\sum {_i}\ln p(w_{i}\mid S) \over p(w_{i}\mid \neg S}} (Diese Technik von "Log-Lihood-Verhältnissen" ist eine gemeinsame Technik in der Statistik. Bei zwei sich gegenseitig ausschließenden Alternativen (z.B. diesem Beispiel) erfolgt die Umwandlung eines log-ähnlichen Verhältnisses zu einer Wahrscheinlichkeit in Form einer sigmoiden Kurve: siehe Logit für Details.) Schließlich kann das Dokument wie folgt klassifiziert werden. Es handelt sich um Spam, wenn p (S ∣ D ) > p (€ S ∣ D ) {\displaystyle p(S\mid D)>p(\neg S\mid D}) (i. e,. ln 0 {\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}>0 ), andernfalls ist es nicht Spam. Siehe auch AODE Bayes Klassifikator Bayesian Spam filtering Bayesian networkRandom naive Bayes Linear Klassifikator Logistische Regression Perceptron Take-the-best heuristic Referenzen Weiter lesen Domingos, Pedro; Pazzani, Michael (1997). " Auf die Optimität des einfachen Bayesischen Klassifikators unter Null-Eingang". Machine Learning.29 (2/3): 103–137.doi:10.1023/A:1007413511361. Webb, G. I.; Boughton, J.; Wang, Z. (2005). " Nicht So Naive Bayes:Aggregating One-Dependence Estimators".Machine Learning.58 (1): 5–24.doi:10.1007/s10994-005-4258-6.Mozina, M.; Demsar, J.; Kattan, M.; Zupan, B. (2004). Nomogramme zur Visualisierung von Naive Bayesian Classifier (PDF). Proc.PKDD-2004.pp.337–348. Maron, M. E. (1961)."Automatische Indexierung:Eine experimentelle Untersuchung".Journal of the ACM.8 (3): 404–417.doi:10.1145/321075.321084.hdl:2027/uva.x030748531 S2CID 6692916.Minsky, M. (1961). Schritte in Richtung Künstliche Intelligenz.Proc.IRE.49.pp.8–30 Externe Links Buch Kapitel: Naive Bayes Textklassifikation, Einführung in Informationen Retrieval Naive Bayes für Textklassifikation mit Unbalanced Classes Benchmark Ergebnisse der Naive Bayes Implementierungen Hierarchical Naive Bayes Klassifikatoren für unsichere Daten (eine Erweiterung des Naive Bayes Klassifikators). SoftwareNaive Bayes-Klassifikatoren sind in vielen allgemeinen maschinellen Lernen und NLP-Paketen, einschließlich Apache Mahout, Mallet, NLTK, Orange, scikit-learn und Weka. IMSL Numerische Bibliotheken Sammlungen von Mathematik und statistischen Algorithmen in C/C, + Fortran, Java und C#/. NET. Die Data Mining-Routinen in den IMSL-Bibliotheken umfassen einen Naive Bayes-Klassifikator. Eine interaktive Microsoft Excel Tabellenkalkulation Naive Bayes Implementierung mit VBA (erfordert aktivierte Makros) mit sichtbarem Quellcode. jBNC - Bayesian Network Classifier Toolbox Statistische Mustererkennung Toolbox für Matlab.ifile - die erste frei verfügbare (Naive)Bayesische Mail/Spam-Filter NClassifier - NClassifier ist eine .NET-Bibliothek, die Textklassifikation und Textzusammenfassung unterstützt. Es ist ein Port von Classifier4J. Classifier4J - Classifier4J ist eine Java-Bibliothek, die für die Textklassifizierung konzipiert ist. Es kommt mit einer Implementierung eines Bayesischen Klassifikators. JNBC Naive Bayes Classifier läuft in-memory oder mit schnellen Key-value-Stores (MapDB, LevelDB oder RocksDB). Blayze - Blayze ist eine minimale JVM-Bibliothek für Naive Bayes Klassifizierung geschrieben in Kotlin.