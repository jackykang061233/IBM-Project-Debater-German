Die Ethik der künstlichen Intelligenz ist der Zweig der Ethik der Technologie, die für künstlich intelligente Systeme spezifisch ist. Es ist manchmal in eine Sorge mit dem moralischen Verhalten des Menschen, wie sie entwerfen, machen, verwenden und behandeln künstlich intelligente Systeme, und eine Sorge mit dem Verhalten von Maschinen, in der Maschinenethik. Es umfasst auch die Frage einer möglichen Singularität durch superintelligente KI. Ethikfelder Ansätze Roboterethik Der Begriff "Roboterethik" (manchmal roboethics) bezieht sich auf die Moral, wie Menschen Roboter entwerfen, konstruieren, verwenden und behandeln. Roboterethik kreuzen sich mit der Ethik der KI. Roboter sind physische Maschinen, während KI nur Software sein kann. Nicht alle Roboter funktionieren über KI-Systeme und nicht alle KI-Systeme sind Roboter. Roboterethik betrachtet, wie Maschinen zum Schaden oder Nutzen von Menschen verwendet werden können, ihre Auswirkungen auf die individuelle Autonomie und ihre Auswirkungen auf die soziale Gerechtigkeit. Maschinenethik Maschinenethik (oder Maschinenmoral) ist der Bereich der Forschung, der sich mit der Gestaltung von künstlichen Moral Agenten (AMAs,) Roboter oder künstlich intelligente Computer befasst, die moralisch oder moralisch verhalten. Um die Natur dieser Agenten zu berücksichtigen, wurde vorgeschlagen, bestimmte philosophische Ideen, wie die Standardcharakterisierungen von Agentur, rationale Agentur, moralische Agentur und künstliche Agentur, die mit dem Konzept von AMAs verbunden sind zu betrachten. Isaac Asimov betrachtete das Problem in den 1950er Jahren in seinem I, Robot. Auf der Beharrlichkeit seines Herausgebers John W. Campbell Jr. schlug er die drei Gesetze der Robotik vor, künstlich intelligente Systeme zu regieren. Ein Großteil seiner Arbeit wurde dann damit verbracht, die Grenzen seiner drei Gesetze zu testen, um zu sehen, wo sie abbrechen würden, oder wo sie paradoxes oder unvorhergesehenes Verhalten schaffen würden. Seine Arbeit deutet darauf hin, dass kein Satz fester Gesetze alle möglichen Umstände ausreichend voraussehen kann. Vor kurzem haben Wissenschaftler und viele Regierungen die Idee herausgefordert, dass KI selbst verantwortlich gemacht werden kann. Ein Gremium, das 2010 vom Vereinigten Königreich einberufen wurde, überarbeitete Asimovs Gesetze, um zu klären, dass KI die Verantwortung der Hersteller oder des Eigentümers/Operators ist. Während eines Experiments im Labor für Intelligente Systeme in der Ecole Polytechnique Fédérale von Lausanne in der Schweiz haben Roboter, die programmiert wurden, miteinander zu kooperieren (bei der Suche nach einer nützlichen Ressource und der Vermeidung einer giftigen), schließlich gelernt, in einem Versuch, die nützliche Ressource zu hoardieren, miteinander zu liegen. Einige Experten und Akademiker haben die Verwendung von Robotern für den militärischen Kampf in Frage gestellt, vor allem, wenn solche Roboter ein gewisses Maß an autonomen Funktionen erhalten. Die US-Marine hat einen Bericht finanziert, der darauf hindeutet, dass es, wenn militärische Roboter komplexer werden, größere Aufmerksamkeit auf Auswirkungen ihrer Fähigkeit, autonome Entscheidungen zu treffen geben sollte. Der Präsident des Vereins zur Förderung der Künstlichen Intelligenz hat eine Studie in Auftrag gegeben, um diese Frage zu betrachten. Sie weisen auf Programme wie das Language Acquisition Device hin, das menschliche Interaktion emulieren kann. Vernor Vinge hat vorgeschlagen, dass ein Moment kommen kann, wenn einige Computer intelligenter sind als Menschen. Er nennt diese "die Singularität". Er schlägt vor, dass es für Menschen etwas oder möglicherweise sehr gefährlich sein kann. Dies wird von einer Philosophie namens Singularitarismus diskutiert. Die Intelligenz der Maschine Das Forschungsinstitut hat vorgeschlagen, "Friendly AI" zu bauen, was bedeutet, dass die bereits mit KI auftretenden Fortschritte auch einen Aufwand beinhalten sollten, KI intrinsisch freundlich und human zu machen. Es gibt Diskussionen über die Erstellung von Tests, um zu sehen, ob eine KI in der Lage ist, ethische Entscheidungen zu treffen. Alan Winfield kommt zu dem Schluss, dass der Turing-Test fehlerhaft ist und die Anforderung einer AI, den Test zu passieren, zu gering ist. Ein vorgeschlagener alternativer Test ist der sogenannte Ethical Turing Test, der den aktuellen Test verbessern würde, indem mehrere Richter entscheiden, ob die Entscheidung der KI ethische oder unethisch ist. Im Jahr 2009 besuchten Akademiker und Fachexperten eine von der Vereinigung für die Förderung der Künstlichen Intelligenz organisierte Konferenz, um die potenziellen Auswirkungen von Robotern und Computern und die Auswirkungen der hypothetischen Möglichkeit zu diskutieren, dass sie selbstbewusst werden und ihre eigenen Entscheidungen treffen können. Sie diskutierten die Möglichkeit und das Ausmaß, in dem Computer und Roboter in der Lage sein könnten, irgendeine Autonomie zu erwerben, und in welchem Maße sie solche Fähigkeiten nutzen könnten, um möglicherweise jede Bedrohung oder Gefahr zu stellen. Sie stellten fest, dass einige Maschinen verschiedene Formen der Halbautonomie erworben haben, einschließlich in der Lage, Energiequellen auf eigene Faust zu finden und in der Lage zu sein, Ziele zu wählen, die mit Waffen angreifen.Sie stellten auch fest, dass einige Computerviren der Beseitigung entgehen können und "Cockroach Intelligence erreicht haben. " Sie stellten fest, dass Selbstbewusstsein, wie in Science-Fiction dargestellt, wahrscheinlich unwahrscheinlich ist, aber dass es andere mögliche Gefahren und Fallstricke gab. Es gibt jedoch eine Technologie, die wirklich die Möglichkeit von Robotern mit moralischer Kompetenz in die Realität bringen könnte. In einem Papier über den Erwerb von moralischen Werten von Robotern erwähnt Nayef Al-Rodhan den Fall neuromorpher Chips, die darauf abzielen, Informationen ähnlich wie Menschen, nichtlinear und mit Millionen von miteinander verbundenen künstlichen Neuronen zu verarbeiten. Roboter, die mit neuromorpher Technologie eingebettet sind, könnten Wissen auf einzigartige menschliche Weise lernen und entwickeln. Unvermeidlich stellt dies die Frage der Umwelt, in der solche Roboter über die Welt erfahren würden und deren Moral sie vererben würden – oder wenn sie auch die Entwicklung menschlicher Schwächen beenden: Egoismus, eine pro-survivale Haltung, Hesitation etc. In Moralmaschinen: Lehrroboter Rechts von Wrong, Wendell Wallach und Colin Allen schlussfolgern, dass Versuche, Roboter von falscher Seite zu lehren, wahrscheinlich das Verständnis der menschlichen Ethik vorantreiben, indem Menschen dazu motiviert werden, Lücken in der modernen normativen Theorie anzusprechen und eine Plattform für experimentelle Untersuchungen bereitzustellen. Als ein Beispiel hat sie normative Ethikisten in die umstrittene Frage eingeführt, welche spezifischen Lernalgorithmen in Maschinen zu verwenden sind. Nick Bostrom und Eliezer Yudkowsky haben für Entscheidungsbäume (z.B. ID3) über neuronale Netzwerke und genetische Algorithmen argumentiert, dass Entscheidungsbäume modernen sozialen Normen von Transparenz und Vorhersagbarkeit (z.B. Stare decisis) gehorchen, während Chris Santos-Lang in der entgegengesetzten Richtung argumentierte, dass die Normen von jedem Alter geändert werden dürfen und dass natürliche Versagen, diese spezifischen menschlichen Normen vollständig zu befriedigen, weniger anfällig. Nach einem Bericht von 2019 vom Center for the Governance of AI an der University of Oxford glauben 82 % der Amerikaner, dass Roboter und KI sorgfältig verwaltet werden sollten. Die genannten Bedenken reichten von der Verwendung von KI bei der Überwachung und Verbreitung gefälschter Inhalte online (bekannt als tiefe Fälschungen, wenn sie promovierte Videobilder und Audio mit Hilfe von KI erzeugt) zu Cyberangriffen, Verletzungen von Daten Privatsphäre, Einstellung von Bias, autonomen Fahrzeugen und Drohnen, die keinen menschlichen Controller benötigen. Ethikprinzipien der künstlichen Intelligenz In der Überarbeitung von 84 Ethikrichtlinien für KI 11 wurden Prinzipiencluster gefunden: Transparenz, Gerechtigkeit und Fairness, Nicht-Malefizienz, Verantwortung, Privatsphäre, Benefizienz, Freiheit und Autonomie, Vertrauen, Nachhaltigkeit, Würde, Solidarität. Luciano Floridi und Josh Cowls schafften einen ethischen Rahmen von KI-Prinzipien, der durch vier Prinzipien der Bioethik (Benefizienz, Nicht-Malefizienz, Autonomie und Gerechtigkeit) und ein zusätzliches KI-Ermöglichungsprinzip – Explicability gesetzt wurde. Transparenz, Rechenschaftspflicht und Open Source Bill Hibbard argumentiert, dass KI-Entwickler, weil KI eine so tiefgreifende Wirkung auf die Menschheit haben wird, Vertreter der zukünftigen Menschheit sind und somit eine ethische Verpflichtung haben, in ihren Bemühungen transparent zu sein. Ben Goertzel und David Hart haben OpenCog als Open Source-Rahmen für die KI-Entwicklung geschaffen. Öffnen KI ist ein gemeinnütziges KI-Forschungsunternehmen, das von Elon Musk, Sam Altman und anderen gegründet wurde, um für die Menschheit eine offene KI zu entwickeln. Es gibt zahlreiche andere Open-Source-KI-Entwicklungen. Leider macht die Erstellung von Code Open Source es nicht verständlich, was durch viele Definitionen bedeutet, dass der AI-Code nicht transparent ist. Der IEEE hat einen Standardisierungsaufwand für die KI-Transparenz. Der IEEE-Ansatz identifiziert mehrere Maßstäbe der Transparenz für verschiedene Benutzer. Darüber hinaus besteht Besorgnis darüber, dass die Freigabe der vollen Kapazität der gegenwärtigen KI für einige Organisationen ein öffentliches Übel sein kann, das heißt, mehr Schaden als gut tun. Microsoft hat zum Beispiel Bedenken geäußert, universellen Zugriff auf seine Gesichtserkennungssoftware zu ermöglichen, auch für diejenigen, die dafür bezahlen können. Microsoft veröffentlichte einen außergewöhnlichen Blog zu diesem Thema, bat um Regierungsverordnung zu helfen, die richtige Sache zu bestimmen. Nicht nur Unternehmen, sondern viele andere Forschende und Bürgerbefürworter empfehlen die Regierungsregelung als Mittel zur Gewährleistung der Transparenz und damit der menschlichen Rechenschaftspflicht. Diese Strategie hat sich als kontrovers erwiesen, da einige sorgen, dass sie die Innovationsrate verlangsamen wird. Andere argumentieren, dass Regulierung zu einer systemischen Stabilität führt, die langfristig Innovationen unterstützen kann.Die OECD, die Vereinten Nationen, die EU und viele Länder arbeiten derzeit an Strategien zur Regulierung von KI und finden entsprechende Rechtsrahmen. Am 26. Juni 2019 veröffentlichte die European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) ihre „Policy and Investment Empfehlungen for Trustworthy Artificial Intelligence“. Dies ist die zweite Lieferung von AI HLEG nach der Veröffentlichung der "Ethics Guidelines for Trustworthy AI" im April 2019. Die Empfehlungen von Juni AI HLEG umfassen vier Hauptthemen: Menschen und Gesellschaft in der Groß-, Forschungs- und Wissenschafts-, Privatwirtschaft und im öffentlichen Sektor. Die Europäische Kommission behauptet, dass "die Empfehlungen vonHLEG sowohl die Möglichkeiten von KI-Technologien zur Förderung des Wirtschaftswachstums, des Wohlstands und der Innovation als auch die potenziellen Risiken widerspiegeln" und erklärt, dass die EU die Politik der KI international vorantreiben will. Ethische Herausforderungen Biases in KI-Systemen KI ist zunehmend in den Gesichts- und Spracherkennungssystemen verankert. Einige dieser Systeme haben echte Business-Anwendungen und beeinflussen direkt Menschen. Diese Systeme sind anfällig für Vorurteile und Fehler, die von seinen menschlichen Schöpfern eingeführt werden. Auch die Daten, die zur Ausbildung dieser KI-Systeme verwendet werden, können Vorurteile haben. Zum Beispiel hatten Gesichtserkennungsalgorithmen von Microsoft, IBM und Face+ alle Vorurteile, wenn es darum ging, das Geschlecht der Menschen zu erkennen; Diese KI-Systeme konnten Geschlecht von weißen Männern genauer erkennen als Geschlecht von dunkleren Hautmännern. Darüber hinaus fand eine Studie von 2020 Spracherkennungssysteme von Amazon, Apple, Google, IBM und Microsoft heraus, dass sie höhere Fehlerquoten haben, wenn sie schwarze Stimmen als weiße Menschen transkribieren. Darüber hinaus beendete Amazon ihre Verwendung von KI Einstellung und Rekrutierung, weil der Algorithmus begünstigt männliche Kandidaten über weibliche. Dies war, weil Amazon's System mit Daten, die über 10-Jahres-Periode gesammelt wurden, trainiert wurde, die meisten von männlichen Kandidaten kam. Bias kann in vielerlei Hinsicht in Algorithmen eintauchen. Zum Beispiel identifizieren Friedman und Nissenbaum drei Kategorien von Vorurteilen in Computersystemen: bestehende Vorspannung, technische Vorspannung und ergebnislose Vorspannung. Bei der natürlichen Sprachverarbeitung können Probleme aus dem Text corpus entstehen - das Quellmaterial, das der Algorithmus verwendet, um über die Beziehungen zwischen verschiedenen Wörtern zu lernen. Große Unternehmen wie IBM, Google, etc. haben sich bemüht, diese Vorurteile zu erforschen und anzugehen. Eine Lösung für die Adressierung von Vorurteilen ist die Erstellung von Dokumentationen für die Daten, die zur Ausbildung von KI-Systemen verwendet werden. Das Problem der Vorurteile beim maschinellen Lernen wird wahrscheinlich noch deutlicher, da die Technologie sich auf kritische Bereiche wie Medizin und Recht ausbreitet, und da mehr Menschen ohne ein tiefes technisches Verständnis mit der Bereitstellung beauftragt werden. Einige Experten warnen, dass algorithmische Vorurteile bereits in vielen Branchen pervasiv sind und dass fast niemand versucht, sie zu identifizieren oder zu korrigieren. Es gibt einige Open-Source-Tools von Zivilgesellschaften, die mehr Bewusstsein für voreingenommene KI bringen wollen. Bedrohung der Menschenwürde Joseph Weizenbaum argumentierte 1976, dass KI-Technologie nicht verwendet werden sollte, um Menschen in Positionen zu ersetzen, die Respekt und Sorgfalt erfordern, wie: Ein Kundendienstvertreter (KI-Technologie wird heute bereits für telefonische interaktive Sprachantwortsysteme verwendet) Ein Therapeut (wie von Kenneth Colby in den 1970er Jahren vorgeschlagen) Ein Kindermädchen für ältere Menschen (wie von Pamela McCorduck in ihrem Buch The Fifth Generation) Ein Soldat Ein Richter Ein Polizist Weizenbaum erklärt, dass wir authentische Gefühle von Empathie von Menschen in diesen Positionen erfordern. Wenn Maschinen sie ersetzen, werden wir uns entfremdet, entwertet und frustriert finden, denn das künstlich intelligente System wäre nicht in der Lage, Empathie zu simulieren. Künstliche Intelligenz, wenn sie auf diese Weise verwendet wird, stellt eine Bedrohung für die Menschenwürde dar. Weizenbaum argumentiert, dass die Tatsache, dass wir die Möglichkeit der Maschinen in diesen Positionen unterhaltsam sind, nahelegt, dass wir eine "Atrophie des menschlichen Geistes erlebt haben, die vom Denken an uns als Computer kommt." PamelaMcCorduck spricht für Frauen und Minderheiten "Ich nehme lieber meine Chancen mit einem unparteiischen Computer", betont, dass es Bedingungen gibt, in denen wir es vorziehen würden, automatisierte Richter und Polizei zu haben, die überhaupt keine persönliche Agenda haben. Kaplan und Haenlein betonen jedoch, dass KI-Systeme nur so intelligent sind wie die Daten, die verwendet werden, um sie zu trainieren, da sie in ihrer Essenz nichts mehr sind als Phantasie-Kurven-Fitting-Maschinen; Mit KI ein Gericht zu unterstützen kann sehr problematisch sein, wenn vergangene Urteile Voreingenommenheit gegenüber bestimmten Gruppen zeigen, da diese Voreingenommenen formalisiert und engagiert werden, was sie noch schwieriger zu erkennen und zu bekämpfen macht.KI-Gründer John McCarthy stellt den moralischen Ton von Weizenbaums Kritik dar. " Wenn die Moral sowohl vehement als auch vage ist, lädt sie autoritären Missbrauch ein", schreibt er. Bill Hibbard schreibt: "Die Menschenwürde erfordert, dass wir uns bemühen, unsere Unwissenheit über die Natur der Existenz zu entfernen, und KI ist für dieses Streben notwendig." Haftung für selbstfahrende Autos Da der weit verbreitete Einsatz autonomer Autos zunehmend an Bedeutung gewinnt, müssen neue Herausforderungen angegangen werden, die von vollautonomen Fahrzeugen aufgeworfen werden. Vor kurzem wurde über die rechtliche Haftung der verantwortlichen Partei diskutiert, wenn diese Autos in Unfälle geraten. In einem Bericht, wo ein fahrerloses Auto einen Fußgänger traf, war der Fahrer im Auto, aber die Kontrollen waren voll in der Hand von Computern. Dies führte zu einem Dilemma, das für den Unfall schuld war. In einem weiteren Vorfall am 19. März 2018 wurde ein Elaine Herzberg von einem selbstfahrenden Uber in Arizona getroffen und getötet. In diesem Fall war das automatisierte Auto in der Lage, Autos und bestimmte Hindernisse zu erkennen, um die Straße autonom zu navigieren, aber es konnte keinen Fußgänger in der Mitte der Straße erwarten. Dies stellte die Frage, ob der Fahrer, Fußgänger, das Autounternehmen oder die Regierung für ihren Tod verantwortlich gemacht werden sollten. Gegenwärtig werden selbstfahrende Autos als halbautonom betrachtet, die den Fahrer dazu verpflichten, Aufmerksamkeit zu schenken und bei Bedarf bereit zu sein, die Kontrolle zu übernehmen. So fällt es auf die Regierungen, den Fahrer zu regulieren, der sich auf autonome Funktionen verlässt. und sie auch erziehen, dass es nur Technologien sind, die, während bequem, keine vollständige Ersatz sind. Bevor autonome Autos weit verbreitet werden, müssen diese Probleme durch neue Politiken angegangen werden. Weaponization of künstliche Intelligenz Einige Experten und Wissenschaftler haben die Verwendung von Robotern für militärische Kämpfe in Frage gestellt, vor allem, wenn solche Roboter ein gewisses Maß an Autonomie erhalten. Am 31. Oktober 2019 veröffentlichte das United States Department of Defense's Defense Innovation Board den Entwurf eines Berichts, der Prinzipien für die ethische Nutzung künstlicher Intelligenz durch das Verteidigungsministerium empfiehlt, die sicherstellen würden, dass ein menschlicher Bediener immer in der Lage wäre, in die "schwarze Box" zu schauen und den Kill-chain-Prozess zu verstehen. Es geht jedoch um die Umsetzung des Berichts. Die US-Marine hat einen Bericht finanziert, der darauf hindeutet, dass es, wenn militärische Roboter komplexer werden, größere Aufmerksamkeit auf Auswirkungen ihrer Fähigkeit, autonome Entscheidungen zu treffen geben sollte. Einige Forscher behaupten, dass autonome Roboter humaner sein könnten, da sie Entscheidungen effektiver treffen könnten. In diesem letzten Jahrzehnt hat es eine intensive Forschung an autonomer Macht mit der Fähigkeit gegebene moralische Verantwortung zu lernen. " Die Ergebnisse können bei der Gestaltung zukünftiger militärischer Roboter verwendet werden, um unerwünschte Tendenzen zu kontrollieren, um den Robotern die Verantwortung zu übertragen." Aus konsequentialistischer Sicht besteht die Chance, dass Roboter die Fähigkeit entwickeln, ihre eigenen logischen Entscheidungen zu treffen, über die sie töten sollen, und deshalb sollte es einen bestimmten moralischen Rahmen geben, den die KI nicht überschreiben kann. In Bezug auf die Konstruktion von künstlichen Geheimdienstwaffen gab es einen jüngsten Umbruch, der Ideen einer Roboterübernahme der Menschheit einbezieht. KI-Waffen stellen eine Art von Gefahr dar, die sich von der von Menschen kontrollierten Waffen unterscheidet. Viele Regierungen haben begonnen, Programme zur Entwicklung von KI Waffen zu finanzieren. Die Vereinigten Staaten Navy kündigte kürzlich Pläne zur Entwicklung autonomer Drohnenwaffen an und parallele ähnliche Ankündigungen von Russland und Korea. Aufgrund des Potenzials von KI-Waffen, die gefährlicher als menschliche Waffen werden, haben Stephen Hawking und Max Tegmark eine Petition zum Verbot von KI-Waffen unterschrieben. Die Botschaft von Hawking und Tegmark besagt, dass KI-Waffen eine unmittelbare Gefahr darstellen und dass Maßnahmen erforderlich sind, um Katastrophen in naher Zukunft zu vermeiden. " Wenn eine große militärische Macht mit der KI-Waffenentwicklung vorantreibt, ist ein globales Waffenrennen praktisch unvermeidlich, und der Endpunkt dieser technologischen Trajektorie ist offensichtlich: autonome Waffen werden die Kalashnikovs von morgen", sagt die Petition, die Skype-Mitbegründer Jaan Tallinn und MIT-Professor der Linguistik Noam Chomsky als zusätzliche Unterstützer gegen KI-Waffen umfasst. Physiker und Astronom Royal Sir Martin Rees hat von katastrophalen Instanzen gewarnt, wie "Dämme Roboter gehen rogue oder ein Netzwerk, das einen Geist von sich entwickelt. " Huw Price, ein Kollege von Rees in Cambridge, hat eine ähnliche Warnung ausgesprochen, dass Menschen vielleicht nicht überleben, wenn Intelligenz "entdeckt die Zwänge der Biologie." Diese beiden Professoren schufen das Zentrum für die Studie des bestehenden Risikos an der Universität Cambridge in der Hoffnung, diese Bedrohung für die menschliche Existenz zu vermeiden. In Bezug auf das Potenzial für intelligentere als menschliche Systeme militärisch eingesetzt werden, schreibt das Open Philanthropy-Projekt, dass diese Szenarien "so wichtig wie die Risiken im Zusammenhang mit dem Verlust der Kontrolle" sind, aber die Forschung, die die langfristigen sozialen Auswirkungen von AI untersuchen, relativ wenig Zeit auf diese Sorge verbracht haben: "Diese Klasse von Szenarien hat für die Organisationen, die in diesem Raum am meisten aktiv gewesen sind, wie das Machine Intelligence Institute of Opaque Algorithmen Ansätze wie maschinelles Lernen mit neuronalen Netzwerken können dazu führen, dass Computer Entscheidungen treffen, die sie und die Menschen, die sie programmiert haben, nicht erklären können. Es ist schwierig für die Menschen festzustellen, ob solche Entscheidungen fair und vertrauenswürdig sind, was potenziell zu Vorurteilen in KI-Systemen führt, die unentdeckt werden, oder Menschen, die die Verwendung solcher Systeme ablehnen. Dies hat zu einer Befürchtung und in einigen Rechtsordnungen gesetzliche Anforderungen an eine erklärende künstliche Intelligenz geführt. Singular Viele Forscher haben argumentiert, dass durch eine "Intelligenzexplosion" eine selbstverbessernde KI so mächtig werden könnte, dass die Menschen sie nicht davon abhalten könnten, ihre Ziele zu erreichen. In seiner Zeitung "Ethical Issues in Advanced Artificial Intelligence" und anschließendem Buch Superintelligence: Paths, Dangers, Strategien, Philosoph Nick Bostrom argumentiert, dass künstliche Intelligenz die Fähigkeit hat, menschliche Auslöschung zu bewirken. Er behauptet, dass allgemeine Superintelligenz in der Lage wäre, unabhängige Initiative und eigene Pläne zu machen, und kann daher besser als autonomer Agent betrachtet werden. Da künstliche Intellekte unsere menschlichen Motivationstendenzen nicht teilen müssen, wäre es an den Designern der Superintelligenz, ihre ursprünglichen Motivationen festzulegen. Da eine superintelligente KI in der Lage wäre, fast jedes mögliche Ergebnis zu bringen und jeden Versuch zu vereiteln, die Umsetzung ihrer Ziele zu verhindern, könnten viele unkontrollierte unbeabsichtigte Konsequenzen entstehen. Es könnte alle anderen Agenten töten, sie dazu überreden, ihr Verhalten zu ändern oder ihre Interferenzversuche zu blockieren. Anstatt die menschliche Rasse zu überwältigen und zu unserer Zerstörung zu führen, hat Bostrom aber auch behauptet, dass Superintelligenz uns helfen kann, viele schwierige Probleme wie Krankheit, Armut und Umweltzerstörung zu lösen, und könnte uns helfen, uns selbst zu „Hilfe“ zu helfen. Die schiere Komplexität menschlicher Wertsysteme macht es sehr schwierig, die Motivationen von AI menschlich zu gestalten. Es sei denn, moralische Philosophie bietet uns eine makellose ethische Theorie, eine KI-Dienstleistung könnte viele potenziell schädliche Szenarien erlauben, die mit einem gegebenen ethischen Rahmen, aber nicht "gemeinsamen Sinn" übereinstimmen. Nach Eliezer Yudkowsky gibt es wenig Grund zu vermuten, dass ein künstlich gestalteter Geist eine solche Anpassung hätte. KI-Forscher wie Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello und Luciano Floridi haben Designstrategien für die Entwicklung nützlicher Maschinen vorgeschlagen. Schauspieler in der KI-Ethik Es gibt viele Organisationen, die sich mit KI-Ethik und Politik, öffentlichen und staatlichen sowie mit Unternehmen und Gesellschaft befassen. Amazon, Google, Facebook, IBM und Microsoft haben eine gemeinnützige, The Partnership on AI to Benefit People and Society, Best Practices auf künstliche Intelligenz Technologien zu formulieren, das Verständnis der Öffentlichkeit voranzutreiben und als Plattform für künstliche Intelligenz zu dienen. Apple trat im Januar 2017 bei. Die Unternehmensmitglieder werden Finanz- und Forschungsbeiträge an die Gruppe leisten, während sie sich mit der wissenschaftlichen Gemeinschaft engagieren, um Akademiker auf den Vorstand zu bringen. Die IEEE hat eine Globale Initiative zur Ethik von autonomen und intelligenten Systemen zusammengefügt, die mit Hilfe des öffentlichen Inputs Richtlinien erstellt und überarbeitet hat und als Mitglieder viele Fachleute von innen und ohne ihre Organisation akzeptiert. Traditionell wurde die Regierung von Gesellschaften verwendet, um sicherzustellen, dass Ethik durch Gesetzgebung und Polizei beobachtet wird. Es gibt jetzt viele Anstrengungen der nationalen Regierungen sowie transnationale Regierungs- und Nichtregierungsorganisationen, um sicherzustellen, dass KI ethisch angewendet wird. Regierungsinitiativen: Die Europäische Kommission hat eine hochrangige Expertengruppe über künstliche Intelligenz.Am 8. April 2019 veröffentlichte dies ihre "Ethics Guidelines for Trustworthy Artificial Intelligence". Die Europäische Kommission verfügt außerdem über eine Robotik- und Künstliche Intelligenz-Innovations- und Exzellenzeinheit, die am 19. Februar 2020 ein Weißbuch über Exzellenz und Vertrauen in die Innovation in der künstlichen Intelligenz veröffentlichte. Die OECD hat ein Beobachtungsnetz für die AI-Politik der OECD eingerichtet. Regierungsinitiativen: In den Vereinigten Staaten hat die Obama-Administration einen Roadmap für KI-Politik erstellt. Die Obama-Administration veröffentlichte zwei prominente Whitepaper über die Zukunft und Auswirkungen von AI. Im Jahr 2019 hat das Weiße Haus durch eine Executive Memo, die als "American AI Initiative" bekannt ist, NIST das (National Institute of Standards and Technology) beauftragt, die Arbeit an der Federal Engagement of AI Standards (Februar 2019) zu beginnen. Im Januar 2020 veröffentlichte die Trump-Administration in den Vereinigten Staaten einen vom Amt für Management und Budget (OMB) herausgegebenen Entwurf für eine „Leitfaden für die Regulierung von künstlichen Intelligenzanwendungen“ („OMB AI Memorandum“). Die Ordnung betont die Notwendigkeit, in KI-Anwendungen zu investieren, das öffentliche Vertrauen in KI zu stärken, Barrieren für den Einsatz von KI zu reduzieren und die amerikanische KI-Technologie in einem globalen Markt wettbewerbsfähig zu halten. Es gibt einen Hinweis auf die Notwendigkeit von Datenschutzbedenken, aber kein weiteres Detail zur Durchsetzung. Die Fortschritte der amerikanischen KI-Technologie scheinen der Fokus und die Priorität zu sein. Darüber hinaus werden föderale Organisationen sogar dazu ermutigt, den Auftrag zu nutzen, um alle staatlichen Gesetze und Vorschriften zu umgehen, die ein Markt als zu beleidigend zu erfüllen sehen könnte. Das Computing Community Consortium (CCC) wiegt mit einem 100-plus-Seiten-Entwurf-Bericht ein – Eine 20-Jahre-Community Roadmap für Künstliche Intelligenzforschung in den USADas Center for Security and Emerging Technology berät US-Politiker über die Auswirkungen der Sicherheit auf neue Technologien wie AI. Wissenschaftliche Initiativen: Es gibt drei Forschungsinstitute an der University of Oxford, die zentral auf die AI-Ethik ausgerichtet sind. Das Future of Humanity Institute konzentriert sich sowohl auf die KI-Sicherheit als auch auf die Governance von KI. Das Institut für Ethik in KI unter der Leitung von John Tasioulas, dessen Hauptziel unter anderem darin besteht, die KI-Ethik als ein im Vergleich zu verwandten angewandten Ethikfeldern geeignetes Feld zu fördern. Das Oxford Internet Institute, geleitet von Luciano Floridi, konzentriert sich auf die Ethik der kurzfristigen KI-Technologien und IKT. Das AI Now Institute in NYU ist ein Forschungsinstitut, das die sozialen Auswirkungen künstlicher Intelligenz untersucht. Die interdisziplinäre Forschung konzentriert sich auf die Themen Bias und Inklusion, Arbeit und Automatisierung, Rechte und Freiheiten sowie Sicherheit und zivile Infrastruktur. Das Institut für Ethik und Emerging Technologies (IEET) erforscht die Auswirkungen von KI auf Arbeitslosigkeit und Politik. Das Institut für Ethik in Künstliche Intelligenz (IEAI) der Technischen Universität München unter der Leitung von Christoph Lütge forscht in verschiedenen Bereichen wie Mobilität, Beschäftigung, Gesundheit und Nachhaltigkeit. Die Rolle der Fiktion in der KI-Ethik Die Rolle der Fiktion in der KI-Ethik war eine komplexe. Man kann drei Ebenen unterscheiden, auf denen Fiktion die Entwicklung von künstlicher Intelligenz und Robotik beeinflusst hat: Historisch hat Fiktion gemeinsame Tropes vorgebildet, die nicht nur Ziele und Visionen für KI beeinflusst haben, sondern auch ethische Fragen und gemeinsame Ängste, die damit verbunden sind, skizziert haben. In der zweiten Hälfte des zwanzigsten und ersten Jahrzehnte des 21. Jahrhunderts haben die populäre Kultur, insbesondere Filme, TV-Serien und Videospiele oft Präoccupationen und dystopische Projektionen über ethische Fragen im Zusammenhang mit KI und Robotik angestoßen. In letzter Zeit wurden diese Themen auch in der Literatur über das Reich der Science-Fiction zunehmend behandelt. Und, wie Carme Torras, Forschungsprofessor am Institut de Robòtica i Informàtica Industrial (Institut für Robotik und Industrial Computing) an der Technischen Universität Katalonien, in der Hochschulbildung, wird die Science Fiction auch zunehmend für die Lehre technologiebezogene ethische Fragen in technologischen Grad verwendet. Historisch gesehen geht die Untersuchung moralischer und ethischer Implikationen von „Dinkmaschinen“ zumindest auf die Aufklärung zurück: Leibniz stellt bereits die Frage, ob wir Intelligenz auf einen Mechanismus, der sich verhält, als wäre es ein fühlendes Wesen, und so auch Descartes, der beschreibt, was als eine frühe Version des Turing-Tests angesehen werden könnte. Die romantische Periode hat mehrere Male künstliche Kreaturen gedacht, die der Kontrolle ihres Schöpfers entkommen mit schmutzigen Folgen, am berühmtesten in Mary Shelleys Frankenstein.Die weit verbreitete Belegung mit Industrialisierung und Mechanisierung im 19. und Anfang des 20. Jahrhunderts führte jedoch zu ethischen Implikationen unangetasteter technischer Entwicklungen an die Spitze der Fiktion: R.U.R – Rossum’s Universal Robots, Karel Čapeks Spiel der mit Emotionen, die als Sklavenarbeit genutzt werden, ausgestatteten Roboter wird nicht nur mit der Erfindung des Begriffs „Robot“ (aus dem tschechischen Wort für Zwangsarbeit, Robota) gewürdigt, sondern war auch ein internationaler Erfolg nach seiner Premiere 1921. George Bernard Shaws Spiel Zurück zu Metuselah, veröffentlicht 1921, stellt an einem Punkt die Gültigkeit von Denkmaschinen, die wie Menschen wirken; Fritz Langs 1927 Film Metropolis zeigt einen Androiden, der den Aufstand der ausgenutzten Massen gegen das unterdrückte Regime einer technokratischen Gesellschaft führt. Auswirkungen der Fiktion auf die technologische Entwicklung Während die Vorfreude einer Zukunft, die von potentiell indomitabler Technologie dominiert wird, die Vorstellung von Schriftstellern und Filmemachern seit langem beflügelt hat, wurde eine Frage weniger häufig analysiert, nämlich in welchem Ausmaß Fiktion eine Rolle bei der Bereitstellung von Inspirationen für die technologische Entwicklung gespielt hat. So wurde zum Beispiel dokumentiert, dass der junge Alan Turing 1933 G.B Shaws Spiel Back to Metuselah (nur 3 Jahre vor der Veröffentlichung seines ersten Halbzeitpapiers, das den Grundstein für den digitalen Computer gelegt hat) sah und er sich wahrscheinlich zumindest über Theaterstücke wie R.U.R, die ein internationaler Erfolg war und in viele Sprachen übersetzt worden wären, informierte. Man könnte auch die Frage stellen, welche Rolle die Science-Fiction bei der Festlegung der Tenets und ethischen Implikationen der KI-Entwicklung gespielt hat: Isaac Asimov begriff seine Drei Gesetze der Robotik in der Kurzgeschichte „Runaround“, Teil der Kurzgeschichtensammlung I, Robot; Arthur C. Clarkes Kurzfilm „The sentinel“, auf dem Stanley Kubricks Film 2001:A Space Odyssey basiert, wurde 1948 geschrieben und 1952 veröffentlicht. Ein weiteres Beispiel (unter vielen anderen) wäre Philip K. Dicks zahlreiche Kurzgeschichten und Romane – vor allem Do Androids Dream of Electric Sheep?, 1968 veröffentlicht, und mit einer eigenen Version eines Turing Test, der Voight-Kampff Test, emotionale Reaktionen von Androiden, die von Menschen nicht zu unterscheiden. Der Roman wurde später die Basis des einflussreichen Films Blade Runner von Ridley Scott. Die Science Fiction hat seit Jahrzehnten mit ethischen Implikationen von KI-Entwicklungen geschärft und hat damit einen Blaupause für ethische Fragen geschaffen, die nach Erreichen einer allgemeinen künstlichen Intelligenz entstehen könnten: Spike Jonzes 2013 Film Sie zeigt, was passieren kann, wenn ein Benutzer sich in die verführerische Stimme seines Smartphone-Betriebssystems verliebt; Ex Machina hingegen stellt eine schwierigere Frage: wenn mit einer deutlich erkennbaren Maschine konfrontiert, nur von einem Gesicht und einer empästhetischen und sinnlichen Stimme gemacht, würden wir immer noch in der Lage sein, eine emotionale Verbindung herzustellen, die von ihr noch verführt wird? (Der Film hallt ein Thema an, das bereits zwei Jahrhunderte früher anwesend ist, in der 1817 kurzen Geschichte “The Sandmann” von E.T.A Hoffmann.) Das Thema der Koexistenz mit künstlichen Lebewesen ist auch das Thema von zwei neuen Romanen: Maschinen wie ich von Ian McEwan, veröffentlicht im Jahr 2019, beinhaltet (unter vielen anderen Dingen) ein Liebestriangle mit einer künstlichen Person sowie ein menschliches Paar. Klara und der im Jahr 2021 veröffentlichte Sun by Nobelpreisträger Kazuo Ishiguro ist das erste Personenkonto von Klara, einem „AF“ (künstlichen Freund), der auf eigene Weise versucht, dem Mädchen, mit dem sie lebt, zu helfen, der, nachdem er „geliftet“ wurde (d.h. genetischen Verbesserungen unterzogen worden war), an einer seltsamen Krankheit leidet. TV Serie Während ethische Fragen im Zusammenhang mit KI seit Jahrzehnten in der Science-Fiction-Literatur und Spielfilmen vorgestellt wurden, führt die Entstehung der TV-Serie als Genre, das längere und komplexere Story-Lines und Charakterentwicklung ermöglicht, zu einigen bedeutenden Beiträgen, die sich mit ethischen Implikationen der Technologie beschäftigen. Die schwedische Serie Real Humans (2012–2013) befasste sich mit den komplexen ethischen und sozialen Folgen, die mit der Integration künstlicher Lebewesen in der Gesellschaft verbunden sind. Die britische dystopische Science-Fiction-Anthologie-Serie Black Mirror (2013–2019) war besonders für Experimente mit dystopischen fiktiven Entwicklungen im Zusammenhang mit einer Vielzahl von resent-technologischen Entwicklungen bemerkenswert. Sowohl die französische Serie Osmosis (2020) als auch die britische Serie Der Eine behandelt die Frage, was passieren kann, wenn die Technologie versucht, den idealen Partner für eine Person zu finden.Zukunftsvisionen in Fiction und Games Der Film The Thirteenth Floor schlägt eine Zukunft vor, in der simulierte Welten mit fühlenden Bewohnern von Computerspielkonsolen zum Zwecke der Unterhaltung erstellt werden. Der Film Die Matrix schlägt eine Zukunft vor, in der die dominante Spezies auf dem Planeten Erde fühlende Maschinen und die Menschheit mit größtem Speciesismus behandelt wird. Die Kurzgeschichte "The Planck Dive" schlägt eine Zukunft vor, in der sich die Menschheit in Software verwandelt hat, die dupliziert und optimiert werden kann und die relevante Unterscheidung zwischen Software-Typen empfänglich und nicht-sentient ist. Die gleiche Idee findet sich im Emergency Medical Hologram of Starship Voyager, das eine scheinbar geschickte Kopie einer reduzierten Teilmenge des Bewusstseins seines Schöpfers ist, Dr. Zimmerman, der für die besten Motive das System geschaffen hat, um medizinische Hilfe bei Notfällen zu geben. Die Filme Bicentennial Man und A.I beschäftigen sich mit der Möglichkeit von sentient Robotern, die lieben könnten. Ich, Robot untersuchte einige Aspekte von Asimovs drei Gesetzen. All diese Szenarien versuchen, möglicherweise unethische Konsequenzen der Schaffung von dienstlichen Computern zu sehen. Die Ethik der künstlichen Intelligenz ist eines von mehreren Kernthemen in BioWare's Masseneffekt Serie von Spielen. Es erforscht das Szenario einer Zivilisation zufällig KI durch eine rasche Zunahme der Rechenleistung durch ein globales neuronales Netz zu schaffen. Dieses Ereignis verursachte einen ethischen Schiss zwischen denen, die sich für die Gewährung von organischen Rechten auf die neu gesandte Geth hielten, war angemessen und diejenigen, die sie weiterhin als Einwegmaschinen sahen und kämpften, um sie zu zerstören. Über den anfänglichen Konflikt hinaus ist die Komplexität der Beziehung zwischen den Maschinen und ihren Schöpfern ein weiteres laufendes Thema in der Geschichte. Im Laufe der Zeit konzentrierten sich die Debatten immer weniger auf die Möglichkeit und mehr auf die Verzweiflung, wie in den von Hugo de Garis und Kevin Warwick initiierten Cosmist- und Terran-Debatten betont wurde. Ein Kosmist, nach Hugo de Garis, versucht tatsächlich, intelligentere Nachfolger für die menschliche Spezies zu bauen. Experten an der Universität von Cambridge haben argumentiert, dass KI in Fiktion und Nonfiction überwältigend als rassisch Weiß dargestellt wird, in Weisen, die Wahrnehmungen seiner Risiken und Vorteile verfälschen. Siehe auch Hinweise Externe Links Ethik der Künstlichen Intelligenz im Internet Enzyklopädie der Philosophie Ethik der Künstlichen Intelligenz und Robotik in der Stanford Enzyklopädie der Philosophie Russell, S;. Hauert, S;. Altman, R;. Veloso, M. (Mai 2015). "Robotik: Ethik der künstlichen Intelligenz".Nature.521 (7553:) 415–418.Bibcode:2015Natur.521..415 doi:10.1038/521415a.PMID 26017428.S2CID 4452826.BBC Nachrichten: Spiele, um ein Leben ihres eigenen Wer ist Angst vor Robotern?, ein Artikel über die Angst der Menschheit vor künstlicher Intelligenz. Eine kurze Geschichte der Computerethik AI Ethics Guidelines Global Inventory von Algorithmwatch Hagendorff, Thilo (März 2020). "Die Ethik der KI-Ethik: Eine Bewertung der Leitlinien". Minds and Machines.30 (1:) 99–120.doi:10.1007/s11023-020-09517-8.S2CID 72940833.