Bestätigungs-Bias ist die Tendenz, nach Informationen zu suchen, zu interpretieren, zu bevorzugen und zu erinnern, in einer Weise, die die vorherigen Überzeugungen oder Werte bestätigt oder unterstützt. Die Menschen zeigen diese Vorurteile, wenn sie Informationen auswählen, die ihre Ansichten unterstützen, gegensätzliche Informationen ignorieren oder wenn sie mehrdeutige Beweise als Unterstützung ihrer bestehenden Einstellungen interpretieren. Die Wirkung ist am stärksten für die gewünschten Ergebnisse, für emotional aufgeladene Fragen und für tief verwurzelte Überzeugungen. Konfirmationsvorspannungen können nicht vollständig beseitigt werden, sondern können beispielsweise durch Bildung und Ausbildung in kritischen Denkfähigkeiten verwaltet werden. Bestätigungsvorspannung ist ein breites Konstrukt, das eine Reihe von Erklärungen abdeckt. Die Suche nach Informationen, die voreingenommene Interpretation dieser Informationen und die voreingenommene Gedächtnisrückrufe wurden aufgerufen, vier spezifische Effekte zu erklären: 1) Haltungspolarisation (wenn eine Meinungsverschiedenheit extremer wird, auch wenn die verschiedenen Parteien den gleichen Beweisen ausgesetzt sind;) 2) Überzeugungsbeharrlichkeit (wenn die Überzeugungen nach den Beweisen für sie bestehen, als falsch dargestellt wird); 3) die irrationale Primacy-Effekt (eine größere Abhängigkeit von Informationen, die früh in einer Reihe aufgetreten sind); und 4) illusorische Korrelation (wenn die Menschen falsch einen Zusammenhang zwischen zwei Ereignissen oder Situationen wahrnehmen). Eine Reihe von psychologischen Experimenten in den 1960er Jahren schlug vor, dass die Menschen dazu neigen, ihre bestehenden Überzeugungen zu bestätigen. Spätere Arbeiten interpretierten diese Ergebnisse als eine Tendenz, Ideen auf einseitige Weise zu testen, konzentrierten sich auf eine Möglichkeit und ignorieren Alternativen (Myside Bias, ein alternativer Name für Bestätigung Bias). In der Regel zeigen aktuelle Erläuterungen zu den beobachteten Vorurteilen die begrenzte menschliche Fähigkeit, die gesamte Menge an verfügbaren Informationen zu verarbeiten, was zu einem Ausfall der neutralen, wissenschaftlichen Untersuchung führt. Geflogene Entscheidungen aufgrund von Bestätigungsversagen wurden in politischen, organisatorischen, finanziellen und wissenschaftlichen Kontexten gefunden. Diese Vorurteile tragen dazu bei, den persönlichen Überzeugungen zu widersprechen und können den Glauben an das Angesicht der widersprüchlichen Beweise bewahren oder stärken. Zum Beispiel führt die Bestätigungsvoreinschätzung zu systematischen Fehlern in der wissenschaftlichen Forschung auf der Grundlage induktiver Argumentation (die allmähliche Anhäufung von unterstützenden Beweisen). In ähnlicher Weise kann ein Polizeidetektiv einen Verdächtigen frühzeitig in einer Untersuchung identifizieren, aber dann nur nach Bestätigung suchen, anstatt Beweise zu entschärfen. Ein ärztlicher Praktizierender kann sich frühzeitig auf eine bestimmte Störung in einer diagnostischen Sitzung konzentrieren und dann nur nach Bestätigung der Beweise suchen. In sozialen Medien wird die Bestätigungsvoreinstufung durch die Verwendung von Filterblasen oder "algorithmische Bearbeitung" verstärkt, die den Individuen nur Informationen anzeigen, mit denen sie einverstanden sind, während sie gegensätzliche Ansichten ausschließen. Definition und Kontext Konfirmation Bias, eine Phrase von englischer Psychologe Peter Wason, ist die Tendenz der Menschen, Informationen zu bevorzugen, die ihre Überzeugungen oder Werte bestätigt oder stärkt, und ist schwierig zu dislodge einmal bestätigt. Bestätigungsvorspannung ist ein Beispiel einer kognitiven Vorspannung. Die Bestätigungsvorspannung (oder die Bestätigungsvorspannung) wurde auch als myside Vorspannung bezeichnet." Bestätigungsverzerrungen sind Effekte in der Informationsverarbeitung. Sie unterscheiden sich von dem, was manchmal als Verhaltensbestätigungseffekt bezeichnet wird, allgemein als selbstfüllende Prophezeiung bezeichnet, in dem die Erwartungen einer Person ihr eigenes Verhalten beeinflussen und das erwartete Ergebnis bewirken. Einige Psychologen beschränken den Begriff "Bestätigung Vorurteil" auf selektive Sammlung von Beweisen, die das, was man bereits glaubt, unterstützt, während ignorieren oder ablehnende Beweise, die eine andere Schlussfolgerung unterstützt. Andere wenden den Begriff breiter auf die Tendenz, seine bestehenden Überzeugungen bei der Suche nach Beweisen zu bewahren, zu interpretieren oder ihn aus Erinnerung zu erinnern. Die Bestätigungsvorspannung ist ein Ergebnis automatischer, unbeabsichtigter Strategien und nicht bewusster Täuschung. Konfirmation Bias kann nicht vollständig vermieden oder beseitigt werden, sondern nur durch die Verbesserung der Bildung und kritischen Denken Fähigkeiten verwaltet. Die Bestätigungsvorspannung ist ein breites Konstrukt, das eine Reihe von möglichen Erklärungen hat, nämlich: Hypothesentesting durch Fälschung, Hypothesentests durch positive Teststrategie und Erläuterungen zur Informationsverarbeitung. Arten der Bestätigung Biased Suche nach Informationen Experimente haben wiederholt festgestellt, dass Menschen neigen, Hypothesen einseitig zu testen, indem sie nach Beweisen im Einklang mit ihrer aktuellen Hypothese suchen. Anstatt alle relevanten Beweise zu durchsuchen, stellen sie Fragen, um eine bejahende Antwort zu erhalten, die ihre Theorie unterstützt. Sie suchen nach den Konsequenzen, die sie erwarten würden, wenn ihre Hypothese wahr wäre, anstatt was passieren würde, wenn sie falsch wären. Zum Beispiel, jemand mit ja / keine Fragen, um eine Nummer zu finden, die sie vermuten, die Nummer 3 könnte fragen, "Ist es eine seltsame Nummer?" Menschen bevorzugen diese Art von Frage, genannt ein "positiver Test", auch wenn ein negativer Test wie "Ist es eine gerade Zahl?" würde genau die gleichen Informationen liefern. Dies bedeutet jedoch nicht, dass Menschen Tests suchen, die eine positive Antwort garantieren. In Studien, in denen Subjekte entweder solche Pseudotests oder wirklich diagnostische auswählen könnten, begünstigten sie die wirklich diagnostische. Die Vorliebe für positive Tests an sich ist keine Vorspannung, da positive Tests sehr informativ sein können. In Kombination mit anderen Effekten kann diese Strategie jedoch bestehende Überzeugungen oder Annahmen bestätigen, unabhängig davon, ob sie wahr sind. In realen Situationen ist der Beweis oft komplex und gemischt. Beispielsweise könnten verschiedene widersprüchliche Ideen über jemanden unterstützt werden, indem sie sich auf einen Aspekt seines oder ihres Verhaltens konzentrieren. So ist jede Suche nach Beweisen zugunsten einer Hypothese wahrscheinlich erfolgreich. Eine Darstellung davon ist die Art und Weise, wie die Phrasierung einer Frage die Antwort deutlich ändern kann. Zum Beispiel, Menschen, die gefragt werden, "Sind Sie glücklich mit Ihrem sozialen Leben?" berichten mehr Zufriedenheit als die gefragt, "Sind Sie unglücklich mit Ihrem sozialen Leben? " Selbst eine kleine Änderung der Formulierung einer Frage kann beeinflussen, wie Menschen durch verfügbare Informationen suchen, und damit die Schlussfolgerungen, die sie erreichen. Dies wurde mit einem fiktiven Kinderhaftungsfall gezeigt. Teilnehmer lesen, dass Eltern A war mäßig geeignet, der Wächter auf vielfältige Weise zu sein. Elternteil B hatte eine Mischung aus salient positiven und negativen Qualitäten: eine enge Beziehung zum Kind, aber eine Arbeit, die sie für lange Zeit wegnehmen würde. Als er fragte: "Welche Eltern das Sorgerecht haben sollten?" wählte die Mehrheit der Teilnehmer den Elternteil B, der hauptsächlich nach positiven Attributen sucht. Als sie jedoch fragten: "Welche Eltern sollten das Sorgerecht des Kindes verweigert werden?" suchten sie negative Attribute und die Mehrheit antwortete, dass Elternteil B verleugnet werden sollte, was bedeutete, dass Elternteil A Gewahrsam haben sollte. Ähnliche Studien haben gezeigt, wie die Menschen eine voreingenommene Suche nach Informationen machen, aber auch, dass dieses Phänomen durch eine Vorliebe für echte Diagnosetests begrenzt werden kann. In einem ersten Experiment bewerteten die Teilnehmer auf der Grundlage eines Interviews eine andere Person auf der Introversion-Extroversion Persönlichkeitsdimension. Sie wählten die Interview-Fragen aus einer bestimmten Liste. Als der Interviewte als Introvert eingeführt wurde, wählten die Teilnehmer Fragen, die eine Introversion vermuteten, wie: "Was finden Sie unangenehm über laute Parteien?" Als der Interviewte als extrovertiert beschrieben wurde, fast alle Fragen vermuteten extroversion, wie: "Was würden Sie tun, um eine langweilige Partei zu leben? " Diese geladenen Fragen gaben den Befragten wenig oder keine Gelegenheit, die Hypothese über sie zu verfälschen. Eine spätere Version des Experiments gab den Teilnehmern weniger presumptive Fragen zur Auswahl, wie zum Beispiel: "Scheuen Sie sich von sozialen Interaktionen weg?" Die Teilnehmer bevorzugt, um diese diagnostischen Fragen zu stellen, zeigen nur eine schwache Vorspannung gegenüber positiven Tests. Dieses Muster, das für diagnostische Tests und eine schwächere Vorliebe für positive Tests vorkommt, wurde in anderen Studien repliziert. Persönlichkeitsmerkmale beeinflussen und interagieren mit vorgespannten Suchvorgängen. Individuen unterscheiden sich in ihren Fähigkeiten, ihre Einstellungen von externen Angriffen in Bezug auf selektive Exposition zu verteidigen. Selektive Exposition tritt auf, wenn Personen nach Informationen suchen, die nicht inkonsistent sind, mit ihren persönlichen Überzeugungen. Ein Experiment untersuchte, inwieweit Einzelpersonen Argumente widerlegen konnten, die ihren persönlichen Überzeugungen widersprachen. Menschen mit hohem Vertrauensniveau suchen eher widersprüchliche Informationen zu ihrer persönlichen Position, um ein Argument zu bilden. Personen mit geringem Vertrauen suchen keine widersprüchlichen Informationen und bevorzugen Informationen, die ihre persönliche Position unterstützen. Die Menschen erzeugen und bewerten Beweise in Argumenten, die auf ihre eigenen Überzeugungen und Meinungen hin vorgespannt sind. Höhere Vertrauensniveaus verringern die Präferenz für Informationen, die die persönlichen Überzeugungen des Einzelnen unterstützen. Ein weiteres Experiment gab den Teilnehmern eine komplexe Regel-Entdeckungsaufgabe, die sich mit bewegten Objekten, die von einem Computer simuliert wurden, beschäftigte. Objekte auf dem Computerbildschirm folgten bestimmten Gesetzen, die die Teilnehmer herausfinden mussten. So konnten die Teilnehmer Objekte über den Bildschirm feuern, um ihre Hypothesen zu testen. Trotz zahlreicher Versuche über eine zehnstündige Sitzung stellte keiner der Teilnehmer die Regeln des Systems fest. Sie versuchten in der Regel zu bestätigen, anstatt ihre Hypothesen zu verfälschen, und waren zögerlich, Alternativen zu berücksichtigen. Auch nachdem sie objektive Beweise sahen, die ihre Arbeitshypothesen widerlegten, führten sie häufig dieselben Tests durch. Einige der Teilnehmer wurden ordnungsgemäße Hypothese-Testing gelehrt, aber diese Anweisungen hatten fast keine Wirkung. Bestimmte Interpretation von Informationen Bestätigungsversagen sind nicht auf die Sammlung von Beweisen beschränkt. Auch wenn zwei Personen die gleichen Informationen haben, kann die Art, wie sie interpretieren es voreingenommen werden. Ein Team der Stanford University führte ein Experiment mit Teilnehmern, die sich stark über die Todesstrafe fühlten, mit der Hälfte zu Gunsten und der Hälfte gegen sie. Jeder Teilnehmer liest Beschreibungen von zwei Studien: einen Vergleich der US-Staaten mit und ohne die Todesstrafe und einen Vergleich der Mordquoten in einem Staat vor und nach der Einführung der Todesstrafe. Nach einer schnellen Beschreibung jeder Studie wurden die Teilnehmer gefragt, ob sich ihre Meinungen geändert hatten. Dann lesen sie einen detaillierteren Überblick über die Prozedur jeder Studie und mussten bewerten, ob die Forschung gut umgesetzt und überzeugend war. Tatsächlich waren die Studien fiktiv. Die Hälfte der Teilnehmer wurde erzählt, dass eine Art Studie die abschreckende Wirkung unterstützte und die andere sie untergraben, während für andere Teilnehmer die Schlussfolgerungen getauscht wurden. Die Teilnehmer, ob Unterstützer oder Gegner, berichteten, ihre Einstellungen leicht in Richtung der ersten Studie zu verschieben, die sie lesen. Nachdem sie die ausführlicheren Beschreibungen der beiden Studien gelesen haben, kehrten sie fast alle in ihren ursprünglichen Glauben zurück, unabhängig von den vorgelegten Beweisen, indem sie auf Details hinwiesen, die ihren Standpunkt unterstützten und nichts Gegenteils missachteten. Die Teilnehmer beschrieb Studien, die ihre vorbestehende Sicht als überlegen gegenüber denen, die widersprüchlich, in detaillierten und spezifischen Weisen. Über eine Studie zu schreiben, die den Abschreckungseffekt untergraben schien, schrieb ein Befürworter der Todesstrafe: "Die Forschung deckte keine lange genug Zeit ab", während ein Kommentar eines Gegners zu derselben Studie sagte: "Keine starken Beweise gegen die Forscher wurden vorgelegt." Die Ergebnisse zeigten, dass die Menschen höhere Nachweisstandards für Hypothesen setzen, die gegen ihre aktuellen Erwartungen gehen. Dieser Effekt, bekannt als "Desconfirmation Bias", wurde durch andere Experimente unterstützt. Eine weitere Studie der voreingenommenen Interpretation trat während der Präsidentschaftswahl 2004 und beteiligten Teilnehmern auf, die mit starken Gefühlen über die Kandidaten berichteten. Sie wurden scheinbar widersprüchliche Aussagenpaare gezeigt, entweder aus republikanischem Kandidat George W. Bush, demokratischem Kandidat John Kerry oder einer politisch neutralen öffentlichen Figur. Sie erhielten auch weitere Aussagen, die den scheinbaren Widerspruch angemessen erscheinen ließen. Aus diesen drei Informationen mussten sie entscheiden, ob die Aussagen jedes einzelnen unvereinbar waren. Es gab starke Unterschiede in diesen Bewertungen, mit den Teilnehmern viel wahrscheinlicher, Aussagen von dem Kandidaten zu interpretieren, den sie als widersprüchlich widersprechen. In diesem Experiment haben die Teilnehmer ihre Urteile getroffen, während sie in einem Magnetresonanz-Imaging (MRI)-Scanner ihre Gehirnaktivität überwachten. Da die Teilnehmer widersprüchliche Aussagen ihres bevorzugten Kandidaten bewerteten, wurden emotionale Zentren ihres Gehirns erregt. Dies geschah nicht mit den Aussagen der anderen Zahlen. Die Experimente ergaben, dass die unterschiedlichen Reaktionen auf die Aussagen nicht auf passive Argumentationsfehler zurückzuführen waren. Stattdessen reduzierten die Teilnehmer aktiv die kognitive Dissonanz, die durch das Lesen über das irrationale oder hypokritische Verhalten ihres begünstigten Kandidaten hervorgerufen wurde. Biasen in der Glaubensinterpretation sind persistent, unabhängig von der Intelligenz Ebene. Die Teilnehmer eines Experiments nahmen den SAT-Test (ein College-Zulassungstest, der in den Vereinigten Staaten verwendet wurde), um ihre Intelligenz zu bewerten. Sie lesen dann Informationen über Sicherheitsbedenken für Fahrzeuge, und die Experimentatoren manipulierten den nationalen Ursprung des Autos. Amerikanische Teilnehmer gaben ihre Meinung, wenn das Auto auf einer sechs-Punkte-Skala verboten werden sollte, wo man "definite ja" und sechs angedeutet "definitely no". Die Teilnehmer bewerteten zunächst, ob sie ein gefährliches deutsches Auto auf amerikanischen Straßen und ein gefährliches amerikanisches Auto auf deutschen Straßen zulassen würden. Die Teilnehmer glaubten, dass das gefährliche deutsche Auto auf amerikanischen Straßen schneller verboten werden sollte als das gefährliche amerikanische Auto auf deutschen Straßen. Es gab keinen Unterschied zwischen den Geheimdiensten bei der Rate Teilnehmer würde ein Auto verbieten. Biased Interpretation ist nicht auf emotional bedeutsame Themen beschränkt. In einem anderen Experiment wurden die Teilnehmer über einen Diebstahl erzählt. Sie mussten die offensichtliche Bedeutung von Aussagen bewerten, die entweder für oder gegen einen bestimmten Charakter verantwortlich sind. Als sie die Schuld dieses Charakters unterschätzten, bewerteten sie Aussagen, die diese Hypothese als wichtiger als widersprüchliche Aussagen unterstützen. Erinnerungen an Informationen Die Menschen können sich an Beweise erinnern, die selektiv ihre Erwartungen zu verstärken, auch wenn sie Beweise in neutraler Weise sammeln und interpretieren. Dieser Effekt wird als "selektiver Rückruf", "Bestätigungsspeicher" oder "Zugriffsspeicher" bezeichnet. Psychologische Theorien unterscheiden sich in ihren Vorhersagen über selektive Erinnerung. Schema-Theorie sagt voraus, dass Informationen, die den vorherigen Erwartungen entsprechen, leichter gespeichert und zurückgerufen werden als Informationen, die nicht übereinstimmen. Einige alternative Ansätze sagen, dass überraschende Informationen herausragen und so unvergesslich ist. Prädiktionen aus diesen beiden Theorien wurden in verschiedenen experimentellen Kontexten bestätigt, ohne dass die Theorie klar herauskommt. In einer Studie lesen die Teilnehmer ein Profil einer Frau, die eine Mischung aus introvertierten und extrovertierten Verhaltensweisen beschreibt. Später mussten sie an Beispiele ihrer Introversion und Extroversion erinnern. Eine Gruppe wurde gesagt, dass dies die Frau für einen Job als Bibliothekar bewerten sollte, während eine zweite Gruppe gesagt wurde, dass es für einen Job in Immobilienverkäufen war. Es gab einen signifikanten Unterschied zwischen dem, was diese beiden Gruppen erinnerten, mit der librären Gruppe, die mehr Beispiele der Introversion und die Vertriebsgruppen erinnerte, die mehr extrovertiertes Verhalten erinnerte. Ein selektiver Memory-Effekt wurde auch in Experimenten gezeigt, die die Desirabilität von Persönlichkeitstypen manipulieren. In einem davon wurde eine Gruppe von Teilnehmern bewiesen, dass extrovertierte Menschen erfolgreicher sind als introvertiert. Eine andere Gruppe wurde dem Gegenteil erzählt. In einer anschließenden, scheinbar unabhängigen Studie wurden die Teilnehmer gebeten, sich an Ereignisse aus ihrem Leben zu erinnern, in denen sie entweder introvertiert oder extrovertiert worden waren. Jede Gruppe von Teilnehmern gab mehr Erinnerungen, die sich mit dem wünschenswerteren Persönlichkeitstyp verbinden, und erinnerte diese Erinnerungen schneller. Veränderungen in emotionalen Zuständen können auch die Erinnerung beeinflussen. Die Teilnehmer bewerteten, wie sie sich fühlten, als sie zuerst erfuhren, dass O. J. Simpson von Mordanfällen befreit worden war. Sie beschreiben ihre emotionalen Reaktionen und das Vertrauen in das Urteil eine Woche, zwei Monate und ein Jahr nach dem Prozess. Die Ergebnisse zeigten, dass sich die Einschätzungen der Teilnehmer für Simpsons Schuld im Laufe der Zeit veränderten. Je mehr sich die Meinung der Teilnehmer zum Urteil verändert hatte, desto weniger stabil waren die Erinnerungen des Teilnehmers an ihre anfänglichen emotionalen Reaktionen. Als die Teilnehmer ihre anfänglichen emotionalen Reaktionen zwei Monate und ein Jahr später erinnerten, ähnelten vergangene Begutachtungen den aktuellen Emotionen. Die Menschen demonstrieren bei der Diskussion über ihre Meinungen zu kontroversen Themen. Erinnerungsrückruf und Aufbau von Erfahrungen werden in Bezug auf entsprechende emotionale Zustände überarbeitet. Myside-Bias hat sich gezeigt, um die Genauigkeit der Erinnerung zu beeinflussen. In einem Experiment bewerteten Witwen und Witwer die Intensität ihrer erfahrenen Trauer sechs Monate und fünf Jahre nach dem Tod ihrer Ehepartner. Die Teilnehmer stellten eine höhere Trauererfahrung bei sechs Monaten statt bei fünf Jahren fest. Doch als die Teilnehmer nach fünf Jahren gefragt wurden, wie sie sich sechs Monate nach dem Tod ihres bedeutenden Anderen gefühlt hatten, war die Intensität der Trauerteilnehmer sehr mit ihrem aktuellen Maß an Trauer verbunden. Individuen scheinen ihre aktuellen emotionalen Zustände zu analysieren, wie sie fühlen müssen, wenn sie vergangene Ereignisse erlebt haben. Emotionale Erinnerungen werden durch aktuelle emotionale Zustände rekonstruiert. Eine Studie zeigte, wie selektives Gedächtnis den Glauben an extrasensorische Wahrnehmung (ESP) bewahren kann. Gläubige und Ungläubige wurden jeweils Beschreibungen von ESP-Experimenten gezeigt. Die Hälfte jeder Gruppe wurde erzählt, dass die experimentellen Ergebnisse die Existenz von ESP unterstützten, während die anderen gesagt wurden, dass sie es nicht taten. In einem anschließenden Test erinnerten die Teilnehmer an das Material, abgesehen von Gläubigen, die die nicht unterstützenden Beweise gelesen hatten. Diese Gruppe erinnerte sich deutlich weniger Informationen und einige von ihnen falsch erinnert die Ergebnisse als Unterstützung ESP. Individuelle Unterschiede Myside-Bias wurde einmal angenommen, mit Intelligenz korreliert zu werden; Studien haben jedoch gezeigt, dass Myside-Bias durch Fähigkeit, rational zu denken, im Gegensatz zu Intelligenz. Myside-Bias kann eine Unfähigkeit verursachen, die entgegengesetzte Seite eines Arguments effektiv und logisch auszuwerten. Studien haben festgestellt, dass Myside-Bias eine Abwesenheit von "aktiver Offenheit" ist, was die aktive Suche nach, warum eine erste Idee falsch sein kann. Typischerweise wird Myside-Bias in empirischen Studien als die Menge der Beweise, die zur Unterstützung ihrer Seite im Vergleich zu der gegenüberliegenden Seite verwendet. Eine Studie hat individuelle Unterschiede in der Myside-Bias gefunden. Diese Studie untersucht individuelle Unterschiede, die durch das Lernen in einem kulturellen Kontext erworben und mutierbar sind. Der Forscher fand einen wichtigen individuellen Unterschied in der Argumentation. Studien haben vorgeschlagen, dass einzelne Unterschiede wie deduktive Vernunftsfähigkeit, Fähigkeit, Glauben Bias zu überwinden, epistemologisches Verständnis und Denken Disposition sind signifikante Vorhersagen der Argumentation und Generierung Argumente, Gegenargumente und Rebuttale. Eine Studie von Christopher Wolfe und Anne Britt untersuchte auch, wie die Meinung der Teilnehmer zu "was ein gutes Argument macht?" eine Quelle von Myside-Bias sein kann, die die Art beeinflusst, wie eine Person ihre eigenen Argumente formuliert. Die Studie untersuchte individuelle Unterschiede des Argumentationsschemas und bat die Teilnehmer, Essays zu schreiben. Die Teilnehmer wurden zufällig zu schreiben Essays entweder für oder gegen ihre bevorzugte Seite eines Arguments zugeordnet und erhielten Forschungsanweisungen, die entweder einen ausgewogenen oder einen uneingeschränkten Ansatz einnahmen.Die ausbalancierten Forschungsanweisungen richteten die Teilnehmer an die Schaffung eines ausgewogenen Arguments, d.h. sowohl Pros als auch Cons; die uneingeschränkten Forschungsanweisungen enthielten nichts darüber, wie das Argument geschaffen werden soll. Insgesamt ergaben die Ergebnisse, dass die ausbalancierten Forschungsanweisungen das Auftreten von Gegeninformationen in Argumenten deutlich erhöht haben. Diese Daten zeigen auch, dass der persönliche Glaube keine Quelle von Myside-Bias ist; jedoch, dass diejenigen Teilnehmer, die glauben, dass ein gutes Argument auf Fakten basiert, eher myside-Bias als andere Teilnehmer zeigen. Diese Beweise stimmen mit den in Barons Artikel vorgeschlagenen Behauptungen überein, dass die Meinungen der Menschen über das, was gutes Denken macht, beeinflussen können, wie Argumente erzeugt werden. Discovery Informelle Beobachtungen Vor der psychologischen Forschung über Bestätigungsvoreingenommenheit wurde das Phänomen in der gesamten Geschichte beobachtet. Beginnend mit dem griechischen Historiker Thucydides (ca. 460 v. Chr. – ca. 395 v. Chr.), der im Peloponnesischen Krieg aus fehlgeleiteter Vernunft schrieb; "... denn es ist eine Gewohnheit der Menschheit, der unachtsamen Hoffnung zu vertrauen, wofür sie sich sehnen, und souveräne Vernunft zu verwenden, was sie nicht mögen". Der italienische Dichter Dante Alighieri (1265–1321) bemerkte es in der Göttlichen Komödie, in der der hl. Thomas Aquinas Dante auf das Treffen im Paradies aufmerksam machte, "Opinion-hasty-often kann auf die falsche Seite neigen, und dann Zuneigung für seine eigene Meinung bindet, beschränkt den Verstand". Ibn Khaldun bemerkte den gleichen Effekt in seinem Muqaddimah:Untruth trifft natürlich historische Informationen. Es gibt verschiedene Gründe, die dies unvermeidlich machen. Eines von ihnen ist Partisane für Meinungen und Schulen.][Wenn die Seele mit Partisane für eine bestimmte Meinung oder Sekte infiziert ist, akzeptiert sie ohne einen Moment zögern die Informationen, die ihm zuzustimmen sind. Vorurteile und Partisanenschaft bedecken die kritische Fakultät und schließen kritische Untersuchungen aus. Das Ergebnis ist, dass Falschheiten akzeptiert und übertragen werden. Im Novum Organum merkte der englische Philosoph und Wissenschaftler Francis Bacon (1561–1626) an, dass die voreingenommene Beurteilung von Beweisen "alle Aberglauben, sei es in Astrologie, Träume, Omen, göttliche Urteile oder dergleichen". Er schrieb: Das menschliche Verständnis, wenn es einmal eine Meinung angenommen hat... zieht alles andere, um zu unterstützen und damit einverstanden. Und obwohl es eine größere Anzahl und Gewicht von Fällen auf der anderen Seite zu finden, aber diese es entweder vernachlässigt oder verachtet, oder auch durch irgendeine Unterscheidung setzt beiseite oder Ablehnungen[.] Im zweiten Band seiner Welt als Will und Repräsentation (1844) beobachtete der deutsche Philosoph Arthur Schopenhauer, dass "Eine angenommene Hypothese uns Luchsaugen für alles gibt, was sie bestätigt und uns blind macht für alles, was ihm widerspricht. " In seinem Essay (1897) "Was ist Kunst?" Der russische Schriftsteller Leo Tolstoy schrieb: Ich weiß, dass die meisten Menschen – nicht nur die als clever betrachteten, sondern auch diejenigen, die sehr clever sind, und in der Lage sind, schwierigste wissenschaftliche, mathematische oder philosophische Probleme zu verstehen – selten sogar die einfachste und offensichtlichste Wahrheit erkennen können, wenn es darum geht, sie dazu zu verpflichten, die Fälschung von Schlussfolgerungen zuzulassen, die sie entstanden haben, vielleicht mit großer Schwierigkeit – Ausschlüsse, von denen sie stolz sind, die sie anderen gelehrt haben, und auf die sie gebaut haben. In seinem Aufsatz (1894) "Das Reich Gottes ist in dir", schrieb der russische Roman Leo Tolstoy früher: Die schwierigsten Probanden können dem langsamsten Gesagten erklärt werden, wenn er keine Ahnung von ihnen bereits gebildet hat; aber das einfachste kann dem intelligentesten Mann nicht klar gemacht werden, wenn er fest davon überzeugt ist, dass er schon ohne Zweifel weiß, was vor ihm gelegt ist. Hypothese-Testing (Verfälschung) Erklärung (Wason) In Peter Wasons Anfangsexperiment, das 1960 veröffentlicht wurde (was nicht den Begriff "Bestätigungsvorspannung" erwähnt), forderte er wiederholt die Teilnehmer auf, eine Regel zu identifizieren, die sich auf Dreifachzahlen bezieht. Man sagte ihnen, dass (2,4,6) der Regel entspricht. Sie erzeugten Dreifache, und der Experimentator sagte ihnen, ob jedes Dreifache der Regel entspricht. Die tatsächliche Regel war einfach "je eine aufsteigende Sequenz", aber die Teilnehmer hatten große Schwierigkeiten, es zu finden, oft Ankündigung von Regeln, die viel spezifischer waren, wie "die mittlere Zahl ist der Durchschnitt der ersten und letzten". Die Teilnehmer schienen nur positive Beispiele zu testen – Streifen, die ihrer hypothetischen Herrschaft gehorchten. Zum Beispiel, wenn sie dachte, die Regel sei: "Jede Zahl ist zwei größer als ihr Vorgänger", sie würden ein Dreifach anbieten, das diese Regel, wie (11,13,15) angebracht (bestätigt) anstatt ein Dreifaches, das sie verletzte (verfälscht), wie (11,12,19). Wason interpretierte seine Ergebnisse als eine Vorliebe für die Bestätigung über die Fälschung, weshalb er den Begriff "Bestätigungsvorspannung" prägte. Wason nutzte auch die Bestätigungsvoreinschätzung, um die Ergebnisse seines Auswahlaufgabeexperiments zu erklären. In den meisten Fällen ignorierte man Informationen, die die angegebene Regel möglicherweise widerlegen könnten (falsify). Hypothesentests (positive Teststrategie) Erklärung (Klayman und Ha) Klayman und Ha's 1987 Papier argumentieren, dass die Wason-Experimente eigentlich keine Vorurteile auf Bestätigung zeigen, sondern eine Tendenz, Tests im Einklang mit der Arbeitshypothese zu machen. Sie nannten dies die "positive Teststrategie". Diese Strategie ist ein Beispiel für eine heuristische: eine aussagekräftige Shortcut, die unvollkommen, aber einfach zu berechnen ist. Klayman und Ha nutzten Bayesische Wahrscheinlichkeits- und Informationstheorie als Standard der Hypothese-Testing, anstatt der von Wason verwendeten Verfälschung. Nach diesen Vorstellungen gibt jede Antwort auf eine Frage eine andere Menge an Informationen, die von den vorherigen Überzeugungen der Person abhängt. So ist ein wissenschaftlicher Test einer Hypothese eine, die erwartet wird, die meisten Informationen zu produzieren. Da der Informationsinhalt von Anfangswahrscheinlichkeiten abhängt, kann ein positiver Test entweder hochinformativ oder uninformativ sein. Klayman und Ha argumentierten, dass, wenn die Menschen an realistische Probleme denken, sie nach einer bestimmten Antwort mit einer kleinen anfänglichen Wahrscheinlichkeit suchen. In diesem Fall sind positive Tests meist informativer als negative Tests. Allerdings ist die Antwort – drei Zahlen in aufsteigender Reihenfolge – in der Regel Entdeckungsaufgabe von Wason sehr breit, so dass positive Tests unwahrscheinlich sind, informative Antworten zu liefern. Klayman und Ha unterstützten ihre Analyse, indem sie ein Experiment zitierten, das die Labels DAX und MED anstelle von "fits the rule" verwendet und "passt nicht an die Regel". Dadurch wurde vermieden, dass das Ziel darin bestand, eine schutzarme Regel zu finden. Die Teilnehmer hatten viel mehr Erfolg mit dieser Version des Experiments. Angesichts dieser und anderer Kritiken ging der Forschungsschwerpunkt von der Bestätigung gegen die Verfälschung einer Hypothese weg, um zu prüfen, ob die Menschen Hypothesen auf informative Weise testen, oder eine uninformative, aber positive Weise. Die Suche nach einer echten Bestätigungs-Bias führte Psychologen zu einem breiteren Spektrum von Effekten, wie Menschen Informationen verarbeiten. Erläuterungen zur Informationsverarbeitung Derzeit gibt es drei Hauptinformationen, die die Erklärung der Bestätigungsverzerrung und eine jüngste Ergänzung enthalten. Kognitive gegen Motivation Laut Robert MacCoun erfolgt die voreingenommene Beweisverarbeitung durch eine Kombination aus kalten (kognitiven) und heißen (motivierten) Mechanismen. Kognitive Erklärungen zur Bestätigungsvoreinstufung basieren auf Einschränkungen in der Fähigkeit der Menschen, komplexe Aufgaben zu bewältigen, und die Verknüpfungen, genannt Heuristik, dass sie verwenden. Zum Beispiel können die Menschen die Zuverlässigkeit von Beweisen beurteilen, indem sie die Verfügbarkeit heuristic, das ist, wie leicht eine bestimmte Idee kommt zu berücksichtigen. Es ist auch möglich, dass die Menschen sich nur auf einen Gedanken zu einer Zeit konzentrieren können, so dass es schwierig ist, alternative Hypothesen parallel zu testen. Eine weitere heuristische Strategie ist die positive Teststrategie von Klayman und Ha, in der die Menschen eine Hypothese testen, indem sie Fälle untersuchen, in denen sie erwarten, dass eine Eigenschaft oder ein Ereignis auftritt. Diese Heuristik vermeidet die schwierige oder unmögliche Aufgabe, herauszufinden, wie diagnostisch jede mögliche Frage sein wird. Es ist jedoch nicht universell zuverlässig, so dass die Menschen die Herausforderungen ihrer bestehenden Überzeugungen übersehen können. Motivationserklärungen beinhalten einen Effekt des Verlangens auf den Glauben. Es ist bekannt, dass die Menschen positive Gedanken über negative auf eine Reihe von Wegen bevorzugen: Dies nennt man das "Pollyanna-Prinzip". Angeführt auf Argumente oder Beweisquellen, könnte dies erklären, warum die gewünschten Schlussfolgerungen wahrscheinlicher angenommen werden. Nach Experimenten, die die Verzweiflung des Fazits manipulieren, fordern die Menschen einen hohen Beweisstandard für unpalatable Ideen und einen niedrigen Standard für bevorzugte Ideen. Mit anderen Worten, sie fragen: "Kann ich das glauben?"für einige Vorschläge und, "Muss ich das glauben?" für andere. Obwohl Konsistenz ein wünschenswertes Merkmal von Einstellungen ist, ist ein übermäßiger Antrieb für Konsistenz eine andere potentielle Quelle von Vorurteilen, weil sie verhindern kann, dass die Menschen neue, überraschende Informationen neutral bewerten. Sozialpsychologe Ziva Kunda kombiniert die kognitiven und motivierenden Theorien, argumentiert, dass Motivation die Vorspannung schafft, aber kognitive Faktoren bestimmen die Größe der Wirkung. Kosten-Nutzen-Erläuterungen in Bezug auf Kosten-Nutzen-Analysen gehen davon aus, dass die Menschen nicht nur Hypothesen disinteressiert testen, sondern die Kosten verschiedener Fehler bewerten. Mit Ideen aus der evolutionären Psychologie schlägt James Friedrich vor, dass die Menschen nicht in erster Linie auf die Wahrheit bei der Prüfung von Hypothesen zielen, sondern versuchen, die teuersten Fehler zu vermeiden. Zum Beispiel können Arbeitgeber einseitige Fragen in Bewerbungsgesprächen stellen, weil sie sich auf das Ausscheiden von ungeeigneten Kandidaten konzentrieren. Yaacov Trope und Akiva Libermans Verfeinerung dieser Theorie geht davon aus, dass die Menschen die beiden verschiedenen Arten von Fehlern vergleichen: eine falsche Hypothese akzeptieren oder eine wahre Hypothese ablehnen. Zum Beispiel könnte jemand, der die Ehrlichkeit eines Freundes unterschätzt, ihn oder sie verdächtig behandeln und so die Freundschaft untergraben. Die Überschätzung der Ehrlichkeit des Freundes kann auch teuer sein, aber weniger. In diesem Fall wäre es vernünftig, Beweise für ihre Ehrlichkeit in einer voreingenommenen Weise zu suchen, auszuwerten oder zu erinnern. Wenn jemand einen ersten Eindruck davon gibt, introvertiert oder extrovertiert zu werden, kommen Fragen, die diesem Eindruck entsprechen, als empathisch empathisch. Dies deutet darauf hin, dass es ein Zeichen besserer sozialer Fähigkeiten ist, wenn man mit jemandem spricht, der ein Introvert zu sein scheint, zu fragen: " Fühlen Sie sich in sozialen Situationen unangenehm?" anstatt: "Gefällt Ihnen laute Parteien? " Die Verbindung zwischen Bestätigungs-Bias und sozialen Fähigkeiten wurde durch eine Studie, wie College-Studenten lernen, andere Menschen korrodiert. Hochgradig selbstüberwachende Studenten, die empfindlicher auf ihre Umwelt und auf soziale Normen sind, stellten bei der Interviewung eines hochrangigen Mitarbeiters mehr passende Fragen als bei der Lernenden. Exploratory gegensus bestätigende Psychologen Jennifer Lerner und Philip Tetlock unterscheiden zwei verschiedene Arten von Denken Prozess. Exploratory Though neutral betrachtet mehrere Standpunkte und versucht, alle möglichen Einwände gegen eine bestimmte Position zu antizipieren, während der bestätigende Gedanke versucht, einen bestimmten Standpunkt zu rechtfertigen. Lerner und Tetlock sagen, dass, wenn die Menschen erwarten, ihre Position zu anderen zu rechtfertigen, deren Ansichten sie bereits kennen, sie dazu neigen, eine ähnliche Position zu diesen Menschen zu übernehmen, und dann bestätigenden Gedanken, ihre eigene Glaubwürdigkeit zu stärken. Wenn die externen Parteien jedoch übermäßig aggressiv oder kritisch sind, werden sich die Menschen völlig vom Gedanken lösen und ihre persönlichen Meinungen ohne Begründung einfach behaupten. Lerner und Tetlock sagen, dass die Menschen sich nur drängen, kritisch und logisch zu denken, wenn sie im Voraus wissen, müssen sie sich anderen erklären, die gut informiert sind, wirklich interessiert an der Wahrheit sind, und deren Ansichten sie nicht schon wissen. Weil diese Bedingungen selten existieren, argumentieren sie, die meisten Menschen verwenden bestätigenden Gedanken die meiste Zeit. Make-believe Entwicklungspsychologe Eve Whitmore hat argumentiert, dass Überzeugungen und Bias, die an der Bestätigungsverurteilung beteiligt sind, ihre Wurzeln in der Kindheit haben, die durch Make-Believe zu bewältigen, was "die Grundlage für komplexere Formen der Selbsttäuschung und Illusion in das Erwachsenenalter wird. " Die Reibung, die durch die Befragung als Jugendlicher mit der Entwicklung kritischen Denkens hervorgerufen wird, kann zur Rationalisierung falscher Überzeugungen führen, und die Gewohnheit einer solchen Rationalisierung kann im Laufe der Jahre unbewusst werden. Real-Welt-Effekte Soziale Medien In den sozialen Medien wird die Bestätigungsvoreinschätzung durch die Verwendung von Filterblasen oder "algorithmische Bearbeitung" verstärkt, die den Individuen nur Informationen zeigt, mit denen sie einverstanden sind, während sie gegensätzliche Ansichten ausschließen. Einige haben argumentiert, dass Bestätigung Bias der Grund ist, warum die Gesellschaft nie aus Filterblasen entkommen kann, weil Menschen psychologisch hart verdrahtet sind, um Informationen zu suchen, die mit ihren vorhandenen Werten und Überzeugungen übereinstimmen. Andere haben weiter argumentiert, dass die Mischung der beiden die Demokratie abbaut – indem sie behauptet, dass diese "algorithmische Bearbeitung" verschiedene Standpunkte und Informationen entfernt – und dass es sei denn, Filterblasenalgorithmen werden entfernt, Wähler nicht in der Lage sein, vollständig informierte politische Entscheidungen zu treffen. Der Aufstieg der sozialen Medien hat stark zur schnellen Verbreitung von gefälschten Nachrichten beigetragen, d.h. falsche und irreführende Informationen, die als glaubwürdige Nachrichten aus einer scheinbar zuverlässigen Quelle dargestellt werden. Die Bestätigungsvoreingenommenheit (Auswahl oder Neuinterpretation von Beweisen zur Unterstützung der eigenen Überzeugungen) ist eine von drei wichtigsten Hürden, die angeführt werden, warum kritisches Denken unter diesen Umständen verrät. Die anderen beiden sind kurz geschnittene Heuristiken (wenn überwältigt oder kurz Zeit, Menschen verlassen sich auf einfache Regeln wie Gruppenkonsensität oder Vertrauen eines Experten oder Rollenmodells) und soziale Ziele (Sozialmotivation oder Peer Pressure kann die objektive Analyse von Fakten zur Hand stören). Bei der Bekämpfung der Verbreitung von gefälschten Nachrichten haben Social Media-Seiten als "digital nudging" betrachtet. Dies kann derzeit in zwei verschiedenen Formen der Nudging erfolgen. Dazu gehören die Nudging von Informationen und die Nudging von Präsentationen. Die Vervielfältigung von Informationen führt dazu, dass Social-Media-Seiten einen Disclaimer oder Label-Frageing oder Warn-Nutzer der Gültigkeit der Quelle zur Verfügung stellen, während die Verkeilung der Präsentation beinhaltet, dass die Benutzer neue Informationen, die sie möglicherweise nicht herausgesucht haben, ausstellen, aber sie zu Standpunkten, die ihre eigenen Bestätigungsvoreinschätzungen bekämpfen können. Wissenschaft und wissenschaftliche ForschungA Unterscheidungsmerkmal des wissenschaftlichen Denkens ist die Suche nach bestätigenden oder unterstützenden Beweisen (induktive Argumentation) sowie gefälschte Beweise (deduktive Argumentation). Insbesondere die induktive Forschung kann ein ernstes Problem mit der Bestätigungsvorspannung haben. Viele Male in der Wissenschaftsgeschichte haben sich Wissenschaftler neuen Entdeckungen widersetzt, indem sie unvorteilbare Daten selektiv interpretieren oder ignorieren. Die Beurteilung der Qualität der wissenschaftlichen Studien scheint besonders verletzlich zu sein, um die Bestätigungsvoreingenommenheit zu bestätigen. Mehrere Studien haben gezeigt, dass Wissenschaftler Studien bewerten, die Befunde im Einklang mit ihren früheren Überzeugungen günstiger berichten als Studien, die Befunde im Einklang mit ihren früheren Überzeugungen berichten. Unter der Annahme, dass die Forschungsfrage relevant ist, sollte das experimentelle Design angemessen und die Daten klar und umfassend beschrieben werden, die empirischen Daten sollten für die wissenschaftliche Gemeinschaft von Bedeutung sein und sollten nicht vorgerichtlich betrachtet werden, unabhängig davon, ob sie den aktuellen theoretischen Vorhersagen entsprechen. In der Praxis können Forscher sie missverstanden, missinterpret oder nicht in allen Studien lesen, die ihren Vorurteilen widersprechen, oder sie sowieso falsch zitieren, als ob sie tatsächlich ihre Behauptungen unterstützten. Darüber hinaus können Bestätigungsversagen wissenschaftliche Theorien oder Forschungsprogramme angesichts unzureichender oder sogar widersprüchlicher Beweise unterstützen. Die Disziplin der Parapsychologie wird oft als Beispiel im Kontext genannt, ob es sich um eine Protowissenschaft oder um eine Pseudowissenschaft handelt. Die Bestätigungsvorspannung eines Experimenters kann möglicherweise beeinflussen, welche Daten gemeldet werden. Daten, die mit den Erwartungen des Experimenters in Konflikt stehen, können leichter als unzuverlässig abgeworfen werden, wodurch der sogenannte Dateischubeffekt entsteht. Um diese Tendenz zu bekämpfen, lehrt die wissenschaftliche Ausbildung Möglichkeiten, Bias zu verhindern. So zielt beispielsweise die experimentelle Gestaltung von randomisierten kontrollierten Versuchen (gekoppelt mit ihrer systematischen Überprüfung) darauf ab, Vorurteilsquellen zu minimieren. Der soziale Prozess der Peer Review zielt darauf ab, die Wirkung der einzelnen Wissenschaftler Vorurteile zu mildern, obwohl der Peer Review-Prozess selbst anfällig für solche Vorurteile sein kann. Wissenschaftliche Innovatoren treffen sich oft mit Widerstand von der wissenschaftlichen Gemeinschaft, und Forschung, die kontroverse Ergebnisse präsentieren, erhält häufig eine harte Peer Review. Medien- und Fact-Checking Sensationalistische Zeitungen in den 1850er Jahren und später führen zu einem allmählichen Bedarf an mehr faktischen Medien. Colin Dickey hat die nachfolgende Evolution der Fact-Checking beschrieben. Die wichtigsten Elemente waren die Gründung der Associated Press in den 1850er Jahren (kurze Sachkunde erforderlich,) Ralph Pulitzer von der New York World (sein Bureau of Accuracy and Fair Play, 1912,) Henry Luce and Time Magazine (original working title: Facts,) und die berühmte Fact-Checking-Abteilung von The New Yorker. In letzter Zeit sind die Mainstream-Medien von Online-Startups unter einer starken wirtschaftlichen Bedrohung. Darüber hinaus kriecht die rasche Verbreitung von Fehlinformationen und Verschwörungstheorien über soziale Medien langsam in Mainstream-Medien. Eine Lösung ist, dass mehr Medienmitarbeiter eine Fact-Checking-Rolle zugewiesen werden, wie zum Beispiel The Washington Post. Auch unabhängige Fact-Checking-Organisationen sind prominent geworden, wie Politifact. Die Tatsachenüberprüfung von Medienberichten und -untersuchungen unterliegt jedoch dem gleichen Bestätigungsversagen wie bei der Peer-Überprüfung der wissenschaftlichen Forschung. Diese Vorurteile wurden bisher wenig untersucht. Zum Beispiel könnte ein Tatsachenchecker mit progressiven politischen Ansichten kritischer sein als notwendig eines faktischen Berichts eines konservativen Kommentators. Ein weiteres Beispiel ist, dass Tatsachen oft mit mehrdeutigen Worten erklärt werden, so dass Progressive und Konservative die Worte entsprechend ihrem eigenen Glauben anders interpretieren können. Finance Confirmation Bias kann Investoren zu überwältigen, ignorieren Beweise, dass ihre Strategien werden verlieren Geld. In Studien über die politischen Aktienmärkte haben Investoren mehr Gewinn erzielt, als sie sich gegen Vorurteile wehren. Zum Beispiel, Teilnehmer, die die Diskussionsleistung eines Kandidaten neutral und nicht parteiisch interpretierten, waren eher profitabel. Um die Wirkung der Bestätigungsvoreingenommenheit zu bekämpfen, können Investoren versuchen, einen gegensätzlichen Standpunkt "zum Zwecke der Argumentation" anzunehmen. In einer Technik stellen sie sich vor, dass ihre Investitionen zusammengebrochen sind und sich fragen, warum dies geschehen könnte. Medizin und Gesundheit Kognitive Vorurteile sind wichtige Variablen in der klinischen Entscheidungsfindung durch medizinische Allgemeinmediziner (GPs) und medizinische Spezialisten. Zwei wichtige sind Bestätigungs-Bias und die überlappende Verfügbarkeits-Bias. Ein GP kann während einer Prüfung eine Diagnose früh durchführen und dann nach Bestätigungsnachweisen suchen, anstatt Beweise zu verfälschen. Dieser kognitive Fehler wird teilweise durch die Verfügbarkeit von Beweisen über die angebliche Störung verursacht, die diagnostiziert wird. Beispielsweise kann der Client die Störung erwähnt haben, oder die GP kann kürzlich ein viel diskutiertes Papier über die Störung gelesen haben. Die Grundlage dieser kognitiven Verknüpfung oder heuristischen (terminierten Verankerung) ist, dass der Arzt nicht mehrere Möglichkeiten auf der Grundlage von Beweisen betrachtet, sondern vorzeitig auf (oder Anker) eine einzige Ursache verrastet. In der Notfallmedizin gibt es aufgrund des Zeitdrucks eine hohe Entscheidungsdichte und es werden häufig Abkürzungen angewendet. Die potenzielle Ausfallrate dieser kognitiven Entscheidungen muss durch Bildung über die 30 oder mehr kognitiven Vorurteile, die auftreten können, verwaltet werden, um richtige Debiasing-Strategien zu setzen.Die Bestätigungsvorspannung kann auch dazu führen, dass Ärzte unnötige medizinische Prozeduren durch Druck von beschädigenden Patienten durchführen. Raymond Nickerson, Psychologe, schuldet Bestätigungsverweigerung für die unwirksamen medizinischen Verfahren, die seit Jahrhunderten vor der Ankunft der wissenschaftlichen Medizin verwendet wurden. Wenn sich ein Patient erholte, zählten die medizinischen Behörden die Behandlung als erfolgreich, anstatt nach alternativen Erläuterungen zu suchen, wie zum Beispiel, dass die Krankheit ihren natürlichen Kurs durchgeführt hatte. Biased Assimilation ist ein Faktor in der modernen Attraktivität der alternativen Medizin, deren Proponenten durch positive anekdotale Beweise, aber behandeln wissenschaftliche Beweise hyperkritisch. Kognitive Therapie wurde von Aaron T entwickelt. Beck in den frühen 1960er Jahren und ist zu einem beliebten Ansatz geworden. Laut Beck ist die voreingestellte Informationsverarbeitung ein Faktor in der Depression. Sein Ansatz lehrt Menschen, Beweise unparteiisch zu behandeln, anstatt selektiv negative Aussichten zu verstärken. Phobias und Hypochondria sind auch gezeigt worden, um Bestätigungsversagen für drohende Informationen zu beinhalten. Politik, Recht und Polizei Nickerson argumentiert, dass die Argumentation in rechtlichen und politischen Kontexten manchmal unterbewusst voreingenommen wird, was Schlussfolgerungen begünstigt, die Richter, Richter oder Regierungen bereits begangen haben. Da die Beweise in einem Jury-Studiengang komplex sein können und Juroren oft Entscheidungen über das Urteil früh treffen, ist es sinnvoll, eine Einstellung Polarisationswirkung zu erwarten. Die Vorhersage, dass Juroren in ihren Ansichten extremer werden, da sie mehr Beweise sehen, wurde in Experimenten mit Mock-Tests geboren. Sowohl inquisitorische als auch adversariale Strafjustizsysteme werden durch Bestätigungsversagen beeinflusst. Die Bestätigungsvoreingenommenheit kann ein Faktor bei der Schaffung oder Erweiterung von Konflikten sein, von emotional angeklagten Debatten bis zu Kriegen: Durch die Interpretation der Beweise zu ihren Gunsten kann jede gegnerische Partei überwältigt werden, dass sie in der stärkeren Position ist. Auf der anderen Seite kann eine Bestätigungsverwerfung dazu führen, dass Menschen die Zeichen eines bevorstehenden oder anfänglichen Konflikts ignorieren oder missinterpretieren. Zum Beispiel, Psychologen Stuart Sutherland und Thomas Kida haben argumentiert, dass US Navy Admiral Husband E. Kimmel zeigte Bestätigung Bias, wenn die ersten Anzeichen des japanischen Angriffs auf Pearl Harbor. Eine zweitniedrige Studie von politischen Pundits von Philip E. Tetlock fand, dass insgesamt ihre Vorhersagen nicht viel besser waren als Zufall. Tetlock teilte Experten in Füchse, die mehrere Hypothesen aufrechterhalten, und Hedgehogs, die dogmatischer waren. Im Allgemeinen waren die Hedgehogs viel weniger genau. Tetlock gab ihr Versagen auf Bestätigungsversagen und vor allem auf ihre Unfähigkeit, neue Informationen zu nutzen, die ihren bestehenden Theorien widersprechen. Bei Polizeiuntersuchungen kann ein Detektiv einen Verdächtigen frühzeitig in einer Untersuchung identifizieren, aber manchmal weitgehend versuchen, Beweise zu unterstützen oder zu bestätigen, ignorieren oder herunterzuspielen gefälschte Beweise. Sozialpsychologie Sozialpsychologen haben zwei Tendenzen in der Art identifiziert, wie Menschen Informationen über sich selbst suchen oder interpretieren. Selbstverifikation ist der Antrieb, um das bestehende Selbstbild zu stärken und Selbsteinschätzung ist der Antrieb, positives Feedback zu suchen. Beide werden von Bestätigungsversagen bedient. In Experimenten, in denen Menschen Feedback gegeben werden, die mit ihrem Selbstbild in Konflikt stehen, sind sie weniger wahrscheinlich, sich daran zu beteiligen oder sich daran zu erinnern, als wenn sie selbstverifizierendes Feedback geben. Sie reduzieren die Auswirkungen solcher Informationen, indem sie sie als unzuverlässig interpretieren. Ähnliche Experimente haben eine Vorliebe für positives Feedback gefunden, und die Menschen, die es geben, über negatives Feedback. Massentäuschungen Bestätigung Bias kann eine zentrale Rolle bei der Verbreitung von Massentäuschungen spielen. Hexenversuche werden häufig als Beispiel genannt. Ein weiteres Beispiel, in der Seattle-Windschutzschild-Pitting-Epidemie, schien es eine "Pitting Epidemie", in der Windschutzscheiben wegen einer unbekannten Ursache beschädigt wurden. Als sich die Nachricht über die scheinbare Schadenswelle verbreitete, haben immer mehr Menschen ihre Windschutzscheiben überprüft, entdeckt, dass auch ihre Windschutzscheiben beschädigt wurden und damit den Glauben an die angebliche Epidemie bestätigten. Tatsächlich wurden die Windschutzscheiben zuvor geschädigt, aber der Schaden ging unbemerkt, bis die Menschen ihre Windschutzscheiben überprüften, wie sich die Täuschung verbreitete. Paranormale Überzeugungen Ein Faktor im Appell der angeblichen psychischen Lesungen ist, dass die Zuhörer eine Bestätigungsverurteilung anwenden, die den Aussagen der psychischen Person zu ihrem eigenen Leben passt. Durch eine große Anzahl von mehrdeutigen Aussagen in jeder Sitzung gibt der Psyche dem Kunden mehr Möglichkeiten, ein Spiel zu finden. Dies ist eine der Techniken des Kalten Lesens, mit denen eine Psyche eine subjektiv beeindruckende Lesung ohne vorherige Informationen über den Kunden liefern kann. Der Ermittler James Randi vergleicht die Transkription einer Lesung mit dem Bericht des Klienten über das, was die psychische gesagt hatte, und fand heraus, dass der Mandant eine starke selektive Erinnerung an die Treffer zeigte". Als auffällige Darstellung der Bestätigungsverschiedenheit in der realen Welt erwähnt Nickerson die numerologische Pyramidologie: die Praxis der Bedeutungsfindung in den Proportionen der ägyptischen Pyramiden. Es gibt viele verschiedene Längenmessungen, die beispielsweise aus der Großen Pyramide von Giza und vielen Möglichkeiten, sie zu kombinieren oder zu manipulieren gemacht werden können. Daher ist es fast unvermeidlich, dass Menschen, die diese Zahlen selektiv betrachten, oberflächlich beeindruckende Korrespondenzen finden, beispielsweise mit den Dimensionen der Erde. Rekrutierung und Auswahl Unbewusste kognitive Vorurteile (einschließlich Bestätigungsversagen) bei der Einstellung von Entscheidungen und kann möglicherweise einen vielfältigen und integrativen Arbeitsplatz verbieten. Es gibt eine Vielzahl von unbewussten Vorurteilen, die Rekrutierungsentscheidungen beeinflussen, aber die Bestätigungsvoreinschätzung ist eine der wichtigsten, vor allem während des Interviews. Der Interviewer wird oft einen Kandidaten auswählen, der seine eigenen Überzeugungen bestätigt, auch wenn andere Kandidaten gleich oder besser qualifiziert sind. Assoziierte Effekte und Ergebnisse Polarisierung der Meinung Wenn Menschen mit entgegengesetzten Ansichten neue Informationen in einer voreingenommenen Weise interpretieren, können ihre Ansichten noch weiter auseinander gehen. Das nennt man "Attitude Polarisation". Der Effekt zeigte sich durch ein Experiment, das eine Reihe von roten und schwarzen Kugeln aus einem von zwei verdeckten "Bingo-Korben" zog. Die Teilnehmer wussten, dass ein Korb 60 Prozent schwarz und 40 Prozent rote Kugeln enthielt; die anderen, 40 Prozent schwarz und 60 Prozent rot. Die Experimentatoren sahen sich an, was passierte, als Kugeln von alternierenden Farbe wiederum gezogen wurden, eine Sequenz, die weder Korb begünstigt. Nachdem jeder Ball gezogen wurde, wurden die Teilnehmer an einer Gruppe gebeten, laut ihre Urteile der Wahrscheinlichkeit, dass die Kugeln aus einem oder dem anderen Korb gezogen wurden, zu erklären. Diese Teilnehmer neigen dazu, mit jeder aufeinanderfolgenden Ziehung zuversichtlicher zu wachsen – ob sie zunächst dachten, dass der Korb mit 60 Prozent schwarzen Kugeln oder die mit 60 Prozent roten Kugeln die wahrscheinlichere Quelle war, ihre Schätzung der Wahrscheinlichkeit stieg. Eine andere Gruppe von Teilnehmern wurde gebeten, die Wahrscheinlichkeitsschätzungen nur am Ende einer Folge von gezogenen Kugeln, anstatt nach jedem Ball zu nennen. Sie zeigten den Polarisationseffekt nicht, was darauf hindeutete, dass es nicht notwendigerweise auftritt, wenn die Menschen einfach entgegengesetzte Positionen halten, sondern wenn sie offen zu ihnen verpflichten. Eine weniger abstrakte Studie war das Stanford-Interpretationsexperiment, in dem Teilnehmer mit starken Meinungen über die Todesstrafe über gemischte experimentelle Beweise lesen. Zweiundzwanzig Prozent der Teilnehmer berichteten, dass ihre Ansichten extremer geworden seien, und diese selbst gemeldete Verschiebung korrelierte stark mit ihren anfänglichen Einstellungen. In späteren Experimenten berichteten die Teilnehmer auch, dass ihre Meinungen als Reaktion auf mehrdeutige Informationen extremer wurden. Vergleiche ihrer Einstellungen vor und nach den neuen Beweisen zeigten jedoch keine signifikante Veränderung, was darauf hindeutete, dass die selbst gemeldeten Veränderungen nicht real sein könnten. Auf der Grundlage dieser Experimente kamen Deanna Kuhn und Joseph Lao zu dem Schluss, dass die Polarisation ein echtes Phänomen ist, aber weit davon entfernt ist, nur in einer kleinen Minderheit von Fällen zu passieren, und es wurde nicht nur durch die Betrachtung gemischter Beweise, sondern durch bloßes Denken über das Thema ausgelöst. Charles Taber und Milton Lodge argumentierten, das Ergebnis des Stanford-Teams sei schwer zu replizieren gewesen, weil die Argumente in späteren Experimenten zu abstrakt oder verwirrend waren, eine emotionale Reaktion hervorzurufen. Die Taber und Lodge-Studie nutzten die emotional geladenen Themen der Waffenkontrolle und der bestätigenden Handlung. Sie haben die Einstellungen ihrer Teilnehmer zu diesen Themen vor und nach der Auslesung von Argumenten auf jeder Seite der Debatte gemessen. Zwei Gruppen von Teilnehmern zeigten die Einstellungspolarisation: diejenigen mit starken Vorurteilen und diejenigen, die politisch kenntnisreich waren. In einem Teil dieser Studie wählten die Teilnehmer, welche Informationsquellen zu lesen sind, aus einer von den Experimentatoren erstellten Liste. Zum Beispiel konnten sie die Argumente der National Rifle Association und der Brady Anti-Handgun Coalition zur Waffenkontrolle lesen. Auch wenn man bewiesen hat, sogar gehandhabt zu werden, waren die Teilnehmer eher Argumente zu lesen, die ihre bestehenden Einstellungen unterstützten als Argumente, die es nicht taten. Diese voreingenommene Suche nach Informationen korrelierte gut mit dem Polarisationseffekt. Der Backfire-Effekt ist ein Name für die Feststellung, dass die Menschen die Beweise ablehnen und noch stärker glauben können. Der Satz wurde 2010 von Brendan Nyhan und Jason Reifler geprägt. Die anschließende Forschung hat jedoch seither keine Erkenntnisse zur Unterstützung des Backfire-Effekts nachgebildet. Eine Studie, die von der Ohio State University und George Washington University durchgeführt wurde, untersuchte 10.100 Teilnehmer mit 52 verschiedenen Problemen, die einen Rückfeuereffekt auslösen würden. Während die Befunde zu dem Schluss gelangten, dass Individuen widerwillig sind, Tatsachen zu akzeptieren, die ihrer bereits gehaltenen Ideologie widersprechen, wurden keine Fälle von Rückfeuer festgestellt. Der Backfire-Effekt wurde seither als ein seltenes Phänomen und nicht als ein gemeinsames Ereignis bezeichnet (vergleiche den Bumerang-Effekt). Beharrlichkeit der diskreditierten Überzeugungen Bestätigungsbräser geben eine plausible Erklärung für die Beharrlichkeit der Überzeugungen, wenn die anfänglichen Beweise für sie entfernt werden oder wenn sie scharf widersprochen wurden. Dieser Glaubensbeharrlichkeitseffekt wurde erstmals experimentell von Festinger, Riecken und Schachter demonstriert. Diese Psychologen verbrachten Zeit mit einem Kult, dessen Mitglieder überzeugt waren, dass die Welt am 21. Dezember 1954 enden würde. Nachdem die Vorhersage versagte, klammerten sich die meisten Gläubigen noch an ihren Glauben. Ihr Buch, das diese Forschung beschreibt, ist geeignet benannt, wenn Prophecy Fails. Der Begriff "Beglaubsbeharrlichkeit" wurde jedoch in einer Reihe von Experimenten mit dem sogenannten "debriefing paradigm" geprägt: Teilnehmer lesen gefälschte Beweise für eine Hypothese, ihre Haltungsänderung wird gemessen, dann wird die Fälschung im Detail exponiert. Ihre Einstellungen werden dann noch einmal gemessen, um zu sehen, ob ihr Glaube wieder auf seine frühere Ebene zurückkehrt. Eine gemeinsame Erkenntnis ist, dass zumindest einige der ersten Überzeugungen auch nach einer vollständigen Befragung bestehen bleiben. In einem Experiment mussten die Teilnehmer zwischen echten und gefälschten Selbstmordnoten unterscheiden. Das Feedback war zufällig: Einige sagten, sie hätten gut gemacht, während andere sagten, sie hätten schlecht durchgeführt. Auch nach vollständiger Befragung wurden die Teilnehmer noch durch das Feedback beeinflusst. Sie dachten immer noch, sie seien besser oder schlechter als durchschnittliche bei dieser Art von Aufgabe, je nachdem, was sie ursprünglich gesagt wurden. In einer anderen Studie lesen die Teilnehmer Stellenleistungen von zwei Feuerwehrleuten, zusammen mit ihren Antworten auf einen Risiko-Aversion-Test. Diese fiktiven Daten wurden arrangiert, um entweder einen negativen oder positiven Verein zu zeigen: Einige Teilnehmer sagten, dass ein risikobehafteter Feuerwehrmann besser war, während andere gesagt wurden, dass sie weniger gut waren als ein risikobehafteter Kollege. Selbst wenn diese beiden Fallstudien wahr wären, wären sie wissenschaftlich schlecht für eine Schlussfolgerung über Feuerwehrleute im Allgemeinen gewesen. Die Teilnehmer fanden sie jedoch subjektiv überzeugend. Als die Fallstudien als fiktiv erwiesen wurden, verringerte sich der Glaube der Teilnehmer an eine Verbindung, aber rund die Hälfte der ursprünglichen Wirkung blieb. Folgeinterviews stellten fest, dass die Teilnehmer die Befragung verstanden und ernst genommen hatten. Die Teilnehmer schienen der Befragung zu vertrauen, betrachteten aber die diskreditierten Informationen als irrelevant für ihren persönlichen Glauben. Der anhaltende Einfluss ist die Tendenz, zuvor gelernte Fehlinformationen auch nach Korrektur zu glauben. Misinformation kann immer noch Inferenzen beeinflussen, die man nach einer Korrektur erzeugt. Vorlieben für frühe Informationen Experimente haben gezeigt, dass Informationen stärker gewichtet werden, wenn es in einer Reihe früh erscheint, auch wenn die Bestellung unwichtig ist. Zum Beispiel, Menschen bilden einen positiveren Eindruck von jemand beschrieben als "intelligent, fleißig, impulsiv, kritisch, stur, envious" als wenn sie in umgekehrter Reihenfolge dieselben Worte gegeben werden. Dieser irrationale Primat-Effekt ist unabhängig vom Primat-Effekt im Speicher, in dem die früheren Elemente in einer Serie eine stärkere Speicherspur hinterlassen. Biased Interpretation bietet eine Erklärung für diesen Effekt: die ersten Beweise zu sehen, Menschen bilden eine Arbeitshypothese, die beeinflusst, wie sie den Rest der Informationen interpretieren. Eine Demonstration von irrationalen Primacy verwendet farbige Chips angeblich aus zwei Urnen gezogen. Den Teilnehmern wurden die Farbverteilungen der Urnen mitgeteilt und die Wahrscheinlichkeit eines von ihnen gezogenen Chips abgeschätzt. Tatsächlich erschienen die Farben in einer vorbestellten Reihenfolge. Die ersten dreißig Ziehungen begünstigten einen Urn und die nächsten dreißig begünstigte den anderen. Die Serie insgesamt war neutral, so rational, die beiden Urnen waren ebenso wahrscheinlich. Jedoch, nach sechzig Ziehungen, die Teilnehmer begünstigte die Urn vorgeschlagen von der ersten dreißig. Ein weiteres Experiment beinhaltete eine Diashow eines einzigen Objekts, gesehen als nur ein Unschärfer zuerst und in etwas besserem Fokus mit jedem nachfolgenden Objekt. Nach jedem Dia mussten die Teilnehmer ihre beste Vermutung darüber geben, was das Objekt war. Die Teilnehmer, deren frühe Vermutungen falsch waren, blieben mit diesen Vermutungen fort, selbst wenn das Bild ausreichend im Fokus lag, dass das Objekt für andere Menschen leicht erkennbar war. Illusory Assoziation zwischen Ereignissen Illusory Korrelation ist die Tendenz, nicht vorhandene Korrelationen in einer Reihe von Daten zu sehen. Diese Tendenz wurde erstmals in einer Reihe von Experimenten in den späten 1960er Jahren gezeigt. In einem Experiment lesen die Teilnehmer eine Reihe von psychiatrischen Fallstudien, einschließlich Reaktionen auf den Rorschach Tintenblastest. Die Teilnehmer berichteten, dass die homosexuellen Männer im Set wahrscheinlicher waren, zu sehen, wie Gesäß, Anuss oder sexuell mehrdeutige Figuren in den Tintenblumen. In der Tat waren die fiktiven Fallstudien so konstruiert worden, dass die homosexuellen Männer nicht wahrscheinlicher waren, diese Bildsprache zu berichten oder in einer Version des Experiments weniger wahrscheinlich waren, es als heterosexuelle Männer zu melden. In einer Umfrage berichtete eine Gruppe von erfahrenen Psychoanalytikern die gleiche Gruppe von illusorischen Assoziationen mit Homosexualität. Eine weitere Studie erfasste die Symptome von arthritischen Patienten, zusammen mit Wetterbedingungen über einen 15-monatigen Zeitraum. Fast alle Patienten berichteten, dass ihre Schmerzen mit Wetterbedingungen korrelierten, obwohl die reale Korrelation Null war. Dieser Effekt ist eine Art voreingenommene Interpretation, in der objektiv neutrale oder ungünstige Beweise ausgelegt werden, um bestehende Überzeugungen zu unterstützen. Es ist auch mit Bias im Hypothese-Testing-Verhalten verbunden. Bei der Beurteilung, ob zwei Ereignisse, wie Krankheit und schlechtes Wetter, zusammenhängen, verlassen sich die Menschen stark auf die Anzahl der positiven Fälle: in diesem Beispiel, Fälle von Schmerz und schlechtem Wetter. Sie achten relativ wenig auf die anderen Arten der Beobachtung (ohne Schmerzen und/oder gutes Wetter). Dies führt dazu, dass positive Tests bei Hypothesentests eingehalten werden. Es kann auch ein selektiver Rückruf widerspiegeln, in dem die Menschen möglicherweise einen Sinn haben, dass zwei Ereignisse zusammenhängen, weil es einfacher ist, Zeiten zu erinnern, wenn sie zusammen passierten. Siehe auch Anmerkungen Referenzen Zitate Quellen Weiter lesen Keohane, Joe (11. Juli 2010,) "Wie Fakten Backfire: Forscher entdecken eine überraschende Bedrohung für Demokratie: unser Gehirn", Boston Globe, The New York Times Leavitt, Fred (2015,) Tanzen mit Absurdität: Ihre meist geschätzten Überzeugungen (und all Ihre anderen) sind wahrscheinlich falsch, Peter Lang Publishers Stanovich, Keith (2009,) Welche Intelligenztests vermissen: Die Psychologie des rationalen Denkens, New Haven (CT:) Yale University Presse, ISBN 978-0-300-12385-2, Laienübersicht (PDF) (21. November 2010)Westen, Drew (2007,) Das politische Gehirn: Die Rolle der Emotion bei der Entscheidung des Schicksals der Nation, PublicAffairs, ISBN 978-1-58648-425-5, OCLC 86117725 Externe Links Skeptic Wörterbuch: Bestätigung Voreingenommen – Robert T. Carroll Lehren über Bestätigung Voreingenommenheit – Klassen-Handout und Instruktor Notizen von K.H Grobman Confirmation Voreingenommenheit bei You Are Not So Smart Confirmation Voreingenommenheit Lernobjekt – interaktive Nummer Triples Übung von Rod McFarland für Simon Fraser University Kurzfassung der 1979 Stanford Assimilation Voreingenommen Studie – Keith Rollag, Babson College