Tensor Processing Unit (TPU) ist eine KI-Beschleuniger-Anwendungsspezifische integrierte Schaltung (ASIC), die von Google speziell für das neuronale Netzwerk-Maschinenlernen entwickelt wurde, insbesondere unter Verwendung der eigenen TensorFlow-Software von Google. Google begann mit der Nutzung von TPUs im Jahr 2015, und im Jahr 2018 stellte sie für die Nutzung von Drittanbietern zur Verfügung, sowohl im Rahmen seiner Cloud-Infrastruktur als auch durch die Bereitstellung einer kleineren Version des Chips zum Verkauf. Überblick Die Tensor-Verarbeitungseinheit wurde im Mai 2016 bei Google I/O bekannt gegeben, als das Unternehmen sagte, dass die TPU bereits seit über einem Jahr in ihren Rechenzentren verwendet worden sei. Der Chip wurde speziell für das Framework von Google TensorFlow entwickelt, eine symbolische Mathe-Bibliothek, die für maschinelle Lernanwendungen wie neuronale Netzwerke verwendet wird. Doch seit 2017 verwendet Google noch CPUs und GPUs für andere Arten von maschinellem Lernen. Andere AI-Beschleuniger-Designs erscheinen auch von anderen Anbietern und sind auf Embedded- und Robotik-Märkte ausgerichtet. Googles TPUs sind proprietär. Einige Modelle sind kommerziell verfügbar, und am 12. Februar 2018, The New York Times berichtet, dass Google "es anderen Unternehmen erlauben würde, Zugang zu diesen Chips durch seinen Cloud-Computing-Service zu kaufen." Google hat gesagt, dass sie in der AlphaGo versus Lee Sedol Serie von Mensch-Maschine Go-Spiele verwendet wurden, sowie im AlphaZero-System, das Chess, Shogi und Go Spielprogramme aus den Spielregeln alleine produziert und ging auf die führenden Programme in diesen Spielen zu schlagen. Google hat auch TPUs für Google Street verwendet Zeige die Textverarbeitung an und konnte den ganzen Text in der Datenbank Street View in weniger als fünf Tagen finden. In Google Photos kann eine individuelle TPU pro Tag über 100 Millionen Fotos bearbeiten. Es wird auch in RankBrain verwendet, die Google verwendet, um Suchergebnisse bereitzustellen. Im Vergleich zu einer Grafik-Verarbeitungseinheit ist sie für ein hohes Volumen an niedriger Präzisions-Rechnung (z.B. bis zu 8-Bit-Präzision) mit mehr Input/Output-Operationen pro Joule und ohne Hardware zur Rasterung/Textur-Mapping ausgelegt. Die TPU ASICs sind in einem Kühlkörper montiert, der in einen Festplattenschlitz innerhalb eines Rechenzentrumsregals passen kann, so Norman Jouppi. Verschiedene Arten von Prozessoren sind für verschiedene Arten von maschinellen Lernmodellen geeignet, TPUs sind für CNNs gut geeignet, während GPUs Vorteile für einige vollständig vernetzte neuronale Netzwerke haben, und CPUs können Vorteile für RNNs haben. Google bietet Dritten Zugang zu TPUs durch seinen Cloud TPU-Service im Rahmen der Google Cloud Platform und durch seine Notebook-basierten Services Kaggle und Colaboratory. Produkte Erste Generation TPUDie erste Generation TPU ist eine 8-Bit-Matrix-Multiplikationsmaschine, die mit CISC-Anweisungen des Host-Prozessors über einen PCIe 3.0-Bus betrieben wird. Es wird auf einem 28 nm-Verfahren mit einer Düsengröße ≤ 331 mm2 hergestellt. Die Taktgeschwindigkeit beträgt 700 MHz und hat eine thermische Designleistung von 28–40 W. Es hat 28 MiB auf dem Chip-Speicher, und 4 MiB von 32-Bit-Akkumulatoren, die die Ergebnisse einer 256×256 systolic Array von 8-Bit-Multiplikatoren. Innerhalb des TPU-Pakets ist 8 GiB von zweikanaligen 2133 MHz DDR3 SDRAM mit 34 GB/s Bandbreite. Befehle übertragen Daten auf oder vom Host, führen Matrixmultiplikationen oder -Konvolutionen aus und wenden Aktivierungsfunktionen an. TPU der zweiten Generation Die TPU der zweiten Generation wurde im Mai 2017 bekannt gegeben. Google erklärte, das TPU-Design der ersten Generation sei durch Speicherbandbreite limitiert und mit 16 GB High Bandwidth Memory im Design der zweiten Generation erhöhte Bandbreite auf 600 GB/s und Leistung auf 45 teraFLOPS. Die TPUs werden dann in Vier-Chip-Module mit einer Leistung von 180 teraFLOPS angeordnet. Dann werden 64 dieser Module in 256-Chip-Pods mit 11,5 petaFLOPS der Leistung montiert. Insbesondere während die TPUs der ersten Generation auf ganze Zahlen beschränkt waren, können die TPUs der zweiten Generation auch im Floating Point berechnet werden. Dies macht die TPUs der zweiten Generation sowohl für die Ausbildung als auch für die Inferenz von maschinellen Lernmodellen nützlich. Google hat erklärt, dass diese TPUs der zweiten Generation auf der Google Compute Engine für den Einsatz in TensorFlow-Anwendungen zur Verfügung stehen. TPU der dritten Generation Die TPU der dritten Generation wurde am 8. Mai 2018 bekannt gegeben. Google kündigte an, dass Prozessoren selbst doppelt so leistungsstark sind wie die TPUs der zweiten Generation, und würde in Pods mit vier Mal so viele Chips wie die vorige Generation eingesetzt werden. Dies führt zu einer 8-fachen Leistungssteigerung pro Pod (mit bis zu 1.024 Chips pro Pod) im Vergleich zur TPU-Bereitstellung der zweiten Generation. TPU der vierten Generation Am 18. Mai 2021, Google CEO Sundar Pichai sprach über TPU v4 Tensor Processing Units während seiner Keynote auf der virtuellen Konferenz von Google I/O.TPU v4 verbesserte Leistung um mehr als 2x über TPU v3 Chips. Pichai sagte: "Ein einzelnes v4 Pod enthält 4,096 v4 Chips, und jedes Pod hat 10x die Interconnect-Bandbreite pro Chip im Maßstab, im Vergleich zu jeder anderen Netzwerktechnologie." Edge TPUIm Juli 2018 kündigte Google die Edge TPU an. Die Edge TPU ist der zweckgebundene ASIC-Chip von Google, der entwickelt wurde, um Machine Learning (ML)-Modelle für Edge Computing zu betreiben, was bedeutet, dass es viel kleiner ist und viel weniger Strom verbraucht als die in Google Rechenzentren gehosteten TPUs (auch bekannt als Cloud TPUs). Im Januar 2019 stellte Google die Edge TPU für Entwickler mit einer Produktlinie unter der Marke Coral zur Verfügung. Die Edge TPU ist in der Lage, 4 Trillion Operationen pro Sekunde unter Verwendung von 2W. Das Produktangebot umfasst einen Einplatinen-Computer (SBC), ein System auf Modul (SoM,) ein USB-Zubehör, eine Mini-PCI-e-Karte und eine M.2 Karte. Das SBC Coral Dev Board und Coral SoM führen beide Mendel Linux OS – ein Derivat von Debian. Die USB-, PCI-e- und M.2-Produkte funktionieren als Add-Ons zu bestehenden Computersystemen und unterstützen Debian-basierte Linux-Systeme auf x86-64 und ARM64 Hosts (einschließlich Raspberry Pi). Die für die Ausführung von Modellen auf der Edge TPU verwendete Machine Learning-Laufzeit basiert auf TensorFlow Lite. Die Edge TPU ist nur in der Lage, Vorwärts-Pass-Operationen zu beschleunigen, was bedeutet, dass es vor allem für die Durchführung von Inferenzen nützlich ist (obwohl es möglich ist, leichte Transfer-Lernen auf der Edge TPU durchzuführen). Die Edge TPU unterstützt auch nur 8-Bit-Math, was bedeutet, dass für ein Netzwerk, das mit der Edge TPU kompatibel ist, es entweder mit der TensorFlow-Quantisierungs-Aware-Trainingstechnik trainiert werden muss, oder seit Ende 2019 ist es auch möglich, Post-Training-Quantisierung zu verwenden. Am 12. November 2019 kündigte Asus ein Paar Single-Board-Computer (SBCs) mit der Edge TPU an. Das Asus Tinker Edge T und Tinker Edge R Board wurde für IoT und Edge AI entwickelt. Die SBCs unterstützen offiziell Android- und Debian-Betriebssysteme. ASUS hat auch einen Mini-PC namens Asus PN60T mit der Edge TPU gezeigt. Am 2. Januar 2020 kündigte Google das Coral Accelerator Module und Coral Dev Board Mini an, das später im selben Monat auf der CES 2020 gezeigt werden soll. Der Korallenbeschleuniger Modul ist ein Multichip-Modul mit den Edge TPU-, PCIe- und USB-Schnittstellen zur einfacheren Integration. Das Coral Dev Board Mini ist ein kleinerer SBC mit dem Coral Accelerator Module und MediaTek 8167s SoC. Pixel Neural CoreAm 15. Oktober 2019 gab Google das Pixel 4 Smartphone bekannt, das eine Edge TPU namens Pixel Neural Core enthält. Siehe auch Vision-Verarbeitungseinheit ein ähnliches Gerät, das sich auf die Bildverarbeitung spezialisiert hat. TrueNorth ein ähnliches Gerät simuliert spiking Neuronen anstelle von niedrigen Präzisions Tensors. Neural Processing Unit Cognitive Computer Tensor Core, eine ähnliche Architektur vorgeschlagen von Nvidia Referenzen Externe Links Cloud Tensor Processing Units (TPUs) (Dokumentation von Google Cloud) Foto von Googles TPU Chip und Board Foto von Googles TPU v2 Board Foto von Googles TPU v3 Board Foto von Googles TPU v2pod