Die Computersysteme der Fünften Generation (FGCS) war eine Initiative des japanischen Ministeriums für internationalen Handel und Industrie (MITI), das 1982 begonnen wurde, um Computer mit massiv parallelen Computer- und Logikprogrammierung zu erstellen. Es sollte das Ergebnis eines Forschungsprojekts der Regierung/Industrie in Japan in den 1980er Jahren sein. Sie zielte darauf ab, einen "epoch-making computer" mit supercomputerähnlicher Leistung zu schaffen und eine Plattform für zukünftige Entwicklungen in der künstlichen Intelligenz zu schaffen. Es gab auch ein unabhängiges russisches Projekt, das auch als Computer der fünften Generation benannt wurde (siehe Kronos (Computer). Ehud Shapiro, in seinem "Trip Report"-Papier (der das FGCS-Projekt auf die gleichzeitige logische Programmierung als Software-Stiftung für das Projekt konzentrierte) erfasste die Rationalität und Motivationen, die dieses Projekt vorantreiben: "Als Teil der Bemühungen Japans, in der Computerindustrie zu führen, hat das Institut für Neue Generation Computertechnologie einen revolutionären zehnjährigen Plan für die Entwicklung großer Computersysteme gestartet, die auf Wissensinformationssysteme anwendbar sein werden. Diese Computer der Fünften Generation werden um die Konzepte der Logik-Programmierung gebaut. Um die Anschuldigung zu widerlegen, dass Japan das Wissen aus dem Ausland ausnutzt, ohne selbst beizutragen, wird dieses Projekt die ursprüngliche Forschung anregen und seine Ergebnisse der internationalen Forschungsgemeinschaft zur Verfügung stellen." Der Begriff "fünfte Generation" soll das System als Fortgeschrittenes vermitteln. In der Geschichte der Computerhardware wurden Computer mit Vakuumröhren als erste Generation bezeichnet; Transistoren und Dioden, die zweite; integrierte Schaltungen, die dritte; und die mit Mikroprozessoren, die vierte. Während frühere Computer-Generationen auf die Erhöhung der Anzahl der logischen Elemente in einer einzigen CPU, die fünfte Generation, es wurde damals weit geglaubt, würde stattdessen zu massiven Anzahl von CPUs für zusätzliche Leistung wenden. Das Projekt bestand darin, den Computer über einen Zeitraum von zehn Jahren zu erstellen, wonach er als beendet betrachtet wurde und Investitionen in ein neues Projekt der sechsten Generation beginnen würden. Die Meinungen über sein Ergebnis sind geteilt: entweder war es ein Misserfolg, oder es war vor seiner Zeit. Informationen In den späten 1965er Jahren war es eine der am häufigsten verwendeten bis Anfang der 1970er Jahre, es gab viel über Generationen von Computerhardware – in der Regel "drei Generationen". Erste Generation: Thermische Vakuumröhren. Mitte der 1940er Jahre.IBM hat die Anordnung von Vakuumrohren in steckbaren Modulen vorangetrieben. Der IBM 650 war ein Computer der ersten Generation. Zweite Generation: Transistoren. 1956.Die Ära der Miniaturisierung beginnt. Transistoren sind viel kleiner als Vakuumröhren, zeichnen weniger Leistung und erzeugen weniger Wärme. Diskrete Transistoren werden mit Leiterplatten verlötet, wobei auf der Rückseite durch schablonengeschirmte Leitermuster miteinander verbunden sind. Der IBM 7090 war ein Computer der zweiten Generation. Dritte Generation: Integrierte Schaltungen (Siliziumchips mit mehreren Transistoren).1964. Ein wegweisendes Beispiel ist das im IBM 360/91 verwendete ACPX-Modul, das durch Stapeln von Schichten aus Silizium auf einem keramischen Substrat über 20 Transistoren pro Chip untergebracht werden kann; die Chips könnten zusammen auf eine Leiterplatte verpackt werden, um beispiellose Logikdichten zu erreichen. Der IBM 360/91 war ein hybrider Computer der zweiten und dritten Generation. Aus dieser Taxonomie ist der Computer der nullten Generation auf Metallrädern (z.B. IBM 407) oder mechanische Relais (z.B. Mark I) und die Computer der nach der dritten Generation auf Basis von Very Large Scale Integrated (VLSI) Schaltungen. Es gab auch eine parallele Reihe von Generationen für Software: Erste Generation: Maschinensprache. Zweite Generation: Programmiersprachen auf niedrigem Niveau wie Montagesprache. Dritte Generation: Strukturierte hochrangige Programmiersprachen wie C, COBOL und FORTRAN. Vierte Generation: Nichtprozedurale hochrangige Programmiersprachen (wie objektorientierte Sprachen) Während dieser mehreren Generationen bis in die 1970er Jahre baute Japan Computer nach US- und britischen Leads. Mitte der 70er Jahre hielt das Ministerium für internationalen Handel und Industrie nach westlichen Leads auf und begann, in die Zukunft des Computings auf kleinem Maßstab zu suchen. Sie forderten das Japan Information Processing Development Center (JIPDEC) auf, eine Reihe zukünftiger Richtungen anzuzeigen, und im Jahr 1979 bot ein dreijähriger Vertrag, um eingehendere Studien zusammen mit Industrie und Wissenschaft durchzuführen. Während dieser Zeit begann der Begriff "Computer der fünften Generation" zu verwenden. Vor den 1970er Jahren hatte die MITI-Anleitung Erfolge wie eine verbesserte Stahlindustrie, die Schaffung des Öl-Supertankers, der Automobilindustrie, der Unterhaltungselektronik und des Computerspeichers.MITI entschied, dass die Zukunft die Informationstechnologie sein würde. Die japanische Sprache, sowohl in schriftlicher als auch gesprochener Form, stellt jedoch Hindernisse für Computer vor. Durch diese Hürden veranstaltete MITI eine Konferenz, um Experten zu unterstützen. Die wichtigsten Untersuchungsgebiete dieses ersten Projekts waren: Inferenz-Computertechnologien zur Wissensverarbeitung Computertechnologien zur Verarbeitung von großformatigen Datenbanken und Wissensbasen Hochleistungsarbeitsplätze verteilte funktionelle Computertechnologien Supercomputer zur wissenschaftlichen Berechnung Das Projekt stellte sich einen "epoch-making computer" mit supercomputerähnlicher Leistung vor, der massiv parallel Computing/Processing verwendet wurde. Ziel war es, parallele Computer für künstliche Intelligenz-Anwendungen unter gleichzeitiger Logik-Programmierung aufzubauen. Das FGCS-Projekt und seine umfangreichen Erkenntnisse trugen maßgeblich zur Entwicklung des gleichzeitigen Logikprogrammierungsfelds bei. Ziel des FGCS-Projekts war es, "Knowledge Information Processing Systems" (gerade Bedeutung, angewandte künstliche Intelligenz) zu entwickeln. Das gewählte Tool, um dieses Ziel umzusetzen, war die logische Programmierung. Logischer Programmieransatz, der von Maarten Van Emden – einem seiner Gründer – charakterisiert wurde, wie: Die Verwendung von Logik, um Informationen in einem Computer auszudrücken. Die Verwendung von Logik, um Probleme einem Computer zu präsentieren. Die Verwendung logischer Inferenz, um diese Probleme zu lösen. Technisch gesehen kann es in zwei Gleichungen zusammengefasst werden: Programm=Set von Axioms. Computation = Beweis einer Aussage von Axiomen. Die üblicherweise verwendeten Axiome sind universelle Axiome einer eingeschränkten Form, genannt Horn-Klauen oder definite-Klausen. Die in einer Berechnung nachgewiesene Aussage ist eine existentielle Aussage. Der Nachweis ist konstruktiv und liefert Werte für die existentiell quantifizierten Größen: Diese Werte stellen den Ausgang der Berechnung dar. Logische Programmierung wurde als etwas gedacht, das verschiedene Gradienten der Informatik (Software Engineering, Datenbanken, Computerarchitektur und künstliche Intelligenz) vereinigt. Es schien, dass die Logik-Programmierung eine wesentliche fehlende Verbindung zwischen Wissenstechnik und parallelen Computer-Architekturen war. Das Projekt stellte sich einen parallelen Verarbeitungsrechner vor, der über große Datenbanken (im Gegensatz zu einem herkömmlichen Dateisystem) mit einer logischen Programmiersprache läuft, um die Daten zu definieren und zuzugreifen. Sie planten, eine Prototyp-Maschine mit Leistung zwischen 100M und 1G LIPS zu bauen, wo ein LIPS eine logische Inferenz pro Sekunde ist. Zu der Zeit waren typische Arbeitsplatzmaschinen in der Lage, etwa 100k LIPS. Sie schlugen vor, diese Maschine über einen Zeitraum von zehn Jahren zu bauen, 3 Jahre für erste FuE, 4 Jahre für den Bau verschiedener Teilsysteme, und ein letztes 3 Jahre, um ein funktionierendes Prototypsystem abzuschließen. 1982 beschloss die Regierung, das Projekt voranzutreiben und das Institut für Informatik der Neuen Generation (ICOT) durch gemeinsame Investitionen mit verschiedenen japanischen Computerfirmen zu etablieren. Im selben Jahr erfand Ehud Shapiro bei einem Besuch des ICOT Concurrent Prolog eine neue, gleichzeitige Programmiersprache, die die logische Programmierung und gleichzeitige Programmierung integriert. Concurrent Prolog ist eine logische Programmiersprache für die gleichzeitige Programmierung und parallele Ausführung. Es handelt sich um eine prozessorientierte Sprache, die die Datenflusssynchronisation und die befehlsbewährte Unbestimmtheit als grundlegende Kontrollmechanismen verkörpert. Shapiro beschreibt die Sprache in einem Bericht, der als ICOT Technical Report 003 gekennzeichnet ist, der einen in Prolog geschriebenen Concurrent Prolog-Interpreter präsentierte. Shapiros Arbeit an Concurrent Prolog inspirierte eine Änderung in Richtung FGCS von der Fokussierung auf die parallele Implementierung von Prolog bis zum Fokus auf die gleichzeitige Logikprogrammierung als Software-Stiftung für das Projekt. Es inspirierte auch die gleichzeitige logische Programmiersprache Guarded Horn Clauses (GHC) von Ueda, die die Basis von KL1 war, die Programmiersprache, die schließlich vom FGCS-Projekt als zentrale Programmiersprache konzipiert und umgesetzt wurde. Durchführung Der Glaube, dass paralleles Computing die Zukunft aller Leistungsgewinne war, die durch das Fünfte-Generation-Projekt generiert wurden, erzeugte eine Welle des Begreifens im Computerfeld. Nachdem die Japaner in den 1980er Jahren in den 70er Jahren und in der Automobilwelt das Feld der Verbraucherelektronik beeinflusst hatten, entwickelten sie einen starken Ruf. So wurden in den USA parallele Projekte als Strategische Computing-Initiative und die Microelectronics and Computer Technology Corporation (MCC) in Großbritannien als Alvey und in Europa als Europäisches Strategisches Programm für Forschung in der Informationstechnologie (ESPRIT) sowie das Europäische Informatik-Industry Research Centre (ECRC) in München, eine Zusammenarbeit zwischen ICL in Großbritannien, Bull in Frankreich und Siemens in Deutschland, aufgebaut.Fünf laufende Parallel Inference Machines (PIM) wurden schließlich produziert: PIM/m, PIM/p, PIM/i, PIM/k, PIM/c.Das Projekt produzierte auch Anwendungen, um auf diesen Systemen zu laufen, wie das parallele Datenbankmanagementsystem Kappa, das Rechtsgrundsystem HELIC-II und das automatisierte Theorem Prover MGTP sowie Anwendungen für Bioinformatik. Ausfall Die FGCS Projekt nicht mit kommerziellen Erfolg aus Gründen ähnlich wie die Lisp Maschinenfirmen und Denkmaschinen. Die hochparallele Computerarchitektur wurde schließlich durch weniger spezialisierte Hardware (z.B. Sun Workstations und Intel x86 Maschinen) in der Geschwindigkeit übertroffen. Das Projekt produzierte eine neue Generation von vielversprechenden japanischen Forschern. Doch nach dem FGCS-Projekt hat MITI aufgehört, groß angelegte Computerforschungsprojekte zu finanzieren, und das Forschungsmoment, das vom FGCS-Projekt entwickelt wurde, wurde abgenommen. MITI/ICOT startete jedoch in den 1990er Jahren ein Sechstes Generationsprojekt. Ein Hauptproblem war die Wahl der gleichzeitigen Logikprogrammierung als Brücke zwischen der parallelen Computerarchitektur und der Verwendung von Logik als Wissensdarstellung und Problemlösungssprache für KI-Anwendungen. Dies geschah nie sauber; eine Reihe von Sprachen wurden entwickelt, alle mit ihren eigenen Einschränkungen. Insbesondere störte das engagierte Wahlmerkmal der gleichzeitigen konstraktiven Logikprogrammierung die logische Semantik der Sprachen. Ein weiteres Problem war, dass die vorhandene CPU-Leistung schnell durch die Barrieren geschoben wurde, die Experten in den 1980er Jahren wahrgenommen haben, und der Wert des Parallel Computings auf den Punkt, an dem sie seit einiger Zeit nur in Nischensituationen verwendet wurde. Obwohl eine Reihe von Workstations mit steigender Kapazität über die Lebensdauer des Projekts konzipiert und gebaut wurden, fanden sie sich im Allgemeinen bald überholt von "off the Regal" Einheiten kommerziell erhältlich. Auch das Projekt konnte nicht kontinuierlich wachsen. Während seiner Lebensdauer wurden GUIs Mainstream in Computern; das Internet ermöglichte lokal gespeicherten Datenbanken zu verbreiten; und sogar einfache Forschungsprojekte lieferte bessere reale Ergebnisse in der Datengewinnung. Darüber hinaus stellte das Projekt fest, dass die Versprechungen der Logikprogrammierung durch den Einsatz einer bestimmten Wahl weitgehend vernachlässigt wurden. Am Ende des zehnjährigen Zeitraums hatte das Projekt über 50 Milliarden US-Dollar (etwa 400 Millionen US-Dollar zu Wechselkursen von 1992) ausgegeben und wurde ohne seine Ziele erreicht. Die Workstations hatten keinen Appell in einem Markt, in dem allgemeine Zielsysteme sie jetzt ersetzen und überlagern könnten. Dies ist parallel zum Lisp-Maschinenmarkt, wo Regel-basierte Systeme wie CLIPS auf Universalrechnern laufen könnten, wodurch teure Lisp-Maschinen unnötig. Vor seiner Zeit hat Albeit nicht viel Erfolg produziert, viele der Ansätze, die im Fünften Generationsprojekt gesehen wurden, wie z.B. die logische Programmierung, wurden über massive Wissensbasen verteilt und werden nun in aktuellen Technologien neu interpretiert. Beispielsweise verwendet die Web Ontology Language (OWL) mehrere Schichten von Logik-basierten Wissensrepräsentationssystemen. Es scheint jedoch, dass diese neuen Technologien neu erfunden wurden, anstatt die unter der Fünften Generation untersuchten Ansätze zu nutzen. Anfang des 21. Jahrhunderts begannen viele Aromen von Parallel Computing zu proliferieren, darunter Multi-Core-Architekturen am Low-End und massiv parallele Verarbeitung am High-End. Als die Taktgeschwindigkeiten von CPUs begannen, in den 3–5 GHz-Bereich zu wechseln, wurden CPU-Leistungsableitung und andere Probleme wichtiger. Die Fähigkeit der Industrie, immer schnellere einzelne CPU-Systeme zu produzieren (verknüpft an Moore's Law über die periodische Verdoppelung von Transistoren zählt) begann zu bedroht. Regelmäßige Verbrauchermaschinen und Spielkonsolen begannen parallele Prozessoren wie Intel Core, AMD K10 und Cell zu haben. Grafikkarten-Unternehmen wie Nvidia und AMD begannen mit der Einführung großer paralleler Systeme wie CUDA und OpenCL.Again, aber es ist nicht klar, dass diese Entwicklungen durch das Fünfte-Generation-Projekt in irgendeiner Weise erleichtert wurden. und alle seine Art Zusammenfassend wird argumentiert, dass das Fünfte-Generation-Projekt revolutionär war, aber immer noch Bereiche des Untergangs hatte. = Referenzen ==