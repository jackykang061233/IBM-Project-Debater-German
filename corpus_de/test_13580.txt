Wissensvertretung und -vernunft (KR2, KR&R) ist das Gebiet der künstlichen Intelligenz (KI), das sich der Darstellung von Informationen über die Welt in einer Form widmet, die ein Computersystem zur Lösung komplexer Aufgaben wie der Diagnose eines medizinischen Zustands oder des Dialogs in einer natürlichen Sprache nutzen kann. Wissensvertretung enthält Erkenntnisse aus der Psychologie, wie Menschen Probleme lösen und Wissen darstellen, um Formalismen zu entwerfen, die komplexe Systeme einfacher zu entwerfen und zu bauen. Die Wissensdarstellung und -vernunft beinhaltet auch Erkenntnisse aus der Logik, um verschiedene Arten von Argumenten zu automatisieren, wie die Anwendung von Regeln oder die Beziehungen von Sätzen und Untergruppen. Beispiele für Wissensrepräsentationsformalitäten sind semantische Netze, Systemarchitektur, Rahmen, Regeln und Ontologien. Beispiele für automatisierte Richtmotoren sind Inferenzmotoren, Theoremproversen und Klassifikatoren. Geschichte Die früheste Arbeit in der computergestützten Wissensrepräsentation konzentrierte sich auf allgemeine Problemlöser wie das von Allen Newell und Herbert A. Simon 1959 entwickelte General Problem Solver (GPS). Diese Systeme zeigten Datenstrukturen für Planung und Zersetzung. Das System würde mit einem Ziel beginnen. Es würde dann dieses Ziel in Subgoals zersetzen und dann auf Strategien aufbauen, die jedes Subgoal erreichen könnten. In diesen frühen Tagen der KI wurden auch allgemeine Suchalgorithmen wie A* entwickelt. Die amorphen Problemdefinitionen für Systeme wie GPS bedeuteten jedoch, dass sie nur für sehr eingeschränkte Spielzeugdomänen (z.B. die "Blocks-Welt") arbeiteten. Um nicht-toy-Probleme zu bewältigen, erkannten AI-Forscher wie Ed Feigenbaum und Frederick Hayes-Roth, dass es notwendig war, Systeme auf mehr eingeschränkte Probleme zu konzentrieren. Diese Bemühungen führten zur kognitiven Revolution in der Psychologie und zur Phase der KI konzentrierten sich auf die Wissensvertretung, die in den 1970er und 80er Jahren zu Expertensystemen, Produktionssystemen, Rahmensprachen usw. führte. Anstatt allgemeine Problemlöser, hat AI seinen Fokus auf Expertensysteme geändert, die menschliche Kompetenz auf eine bestimmte Aufgabe, wie die medizinische Diagnose, passen könnten. Expertensysteme gaben uns heute noch die Terminologie, in der KI-Systeme in eine Wissensbasis eingeteilt werden, mit Fakten über die Welt und Regeln, und einen Inferenzmotor, der die Regeln auf die Wissensbasis anwendet, um Fragen zu beantworten und Probleme zu lösen. In diesen frühen Systemen war die Wissensbasis eher eine ziemlich flache Struktur, im Wesentlichen Behauptungen über die Werte von Variablen, die von den Regeln verwendet werden. Neben Expertensystemen entwickelten andere Forscher Mitte der 1980er Jahre das Konzept der rahmenbasierten Sprachen. Ein Rahmen ist ähnlich einer Objektklasse: Es ist eine abstrakte Beschreibung einer Kategorie, die Dinge in der Welt, Probleme und potenzielle Lösungen beschreibt. Frames wurden ursprünglich auf Systemen verwendet, die auf menschliche Interaktion ausgerichtet sind, z.B. das Verständnis der natürlichen Sprache und der sozialen Einstellungen, in denen verschiedene Standarderwartungen wie die Bestellung von Lebensmitteln in einem Restaurant den Suchraum verengen und dem System die Wahl entsprechender Antworten auf dynamische Situationen ermöglichen. Es war nicht lange bevor die Rahmengemeinden und die regelbasierten Forscher erkannten, dass es eine Synergie zwischen ihren Ansätzen gab. Frames waren gut für die Darstellung der realen Welt, beschrieben als Klassen, Unterklassen, Slots (Datenwerte) mit verschiedenen Einschränkungen auf mögliche Werte. Regeln waren gut für die Darstellung und Nutzung komplexer Logik wie der Prozess, um eine medizinische Diagnose zu machen. Es wurden integrierte Systeme entwickelt, die Rahmen und Regeln miteinander kombinieren. Eines der leistungsfähigsten und bekanntesten war die 1983er Wissenstechnik-Umgebung (KEE) von Intellicorp. KEE hatte einen kompletten Regelmotor mit Vorwärts- und Rückwärtsketten. Es hatte auch eine komplette rahmenbasierte Wissensbasis mit Triggern, Slots (Datenwerte,) Erbschaft und Nachrichtenübermittlung. Obwohl die Weitergabe von Meldungen in der objektorientierten Gemeinschaft statt KI entstand, wurde sie von KI-Forschern sowie in Umgebungen wie KEE und in den Betriebssystemen für Lisp-Maschinen von Symbolics, Xerox und Texas Instruments schnell umgriffen. Die Integration von Frames, Regeln und objektorientierten Programmierungen wurde maßgeblich von kommerziellen Projekten wie KEE und Symbolics aus verschiedenen Forschungsprojekten angetrieben. Zur gleichen Zeit wie dies geschah, gab es einen anderen Forschungsstamm, der weniger kommerziell fokussiert war und von mathematischer Logik und automatisierten Theorien angetrieben wurde. Eine der einflussreichsten Sprachen dieser Forschung war die KL-ONE Sprache der Mitte der 80er Jahre. KL-ONE war eine Rahmensprache, die eine strenge Semantik, formale Definitionen für Konzepte wie eine Is-A-Beziehung hatte. KL-ONE und Sprachen, die von ihm beeinflusst wurden, wie Loom hatte eine automatisierte Richtmaschine, die auf formaler Logik basierte anstatt auf IF-THEN Regeln. Dieser Vernunft nennt man den Klassifikator. Ein Klassifikator kann eine Reihe von Erklärungen analysieren und neue Behauptungen mindern, zum Beispiel eine Klasse neu definieren, um eine Unterklasse oder Superklasse einer anderen Klasse zu sein, die nicht formal spezifiziert wurde. Auf diese Weise kann der Klassifikator als Inferenz-Engine fungieren und neue Fakten aus einer vorhandenen Wissensbasis ableiten. Der Klassifikator kann auch eine Konsistenzprüfung auf einer Wissensbasis (die bei KL-ONE-Sprachen auch als Ontologie bezeichnet wird) durchführen. Ein weiterer Bereich der Wissensvertretung Forschung war das Problem der gesunden Vernunft. Eine der ersten Erkenntnisse aus dem Versuch, Software zu machen, die mit menschlicher natürlicher Sprache funktionieren kann, war, dass Menschen regelmäßig auf eine umfangreiche Grundlage des Wissens über die reale Welt ziehen, die wir einfach für selbstverständlich, aber das ist überhaupt nicht offensichtlich für einen künstlichen Agenten. Grundprinzipien der Allgemeinsinnphysik, Kausalität, Intentionen usw. Ein Beispiel ist das Rahmenproblem, dass in einer ereignisgesteuerten Logik Axiome sein müssen, die die Dinge von einem Moment zum nächsten halten, es sei denn, sie werden von einer äußeren Kraft bewegt. Um einen echten künstlichen Intelligenzagenten zu machen, der mit Menschen mit natürlicher Sprache konversieren kann und grundlegende Aussagen und Fragen über die Welt verarbeiten kann, ist es unerlässlich, diese Art von Wissen zu repräsentieren. Eines der ehrgeizigsten Programme, um dieses Problem anzugehen, war das Cyc-Projekt von Doug Lenat. Cyc etablierte seine eigene Frame-Sprache und hatte eine große Anzahl von Analysten Dokument verschiedene Bereiche des gesunden Menschenverstands in dieser Sprache. Das in Cyc aufgezeichnete Wissen umfasste gemeinsame Sinnmodelle von Zeit, Kausalität, Physik, Intentionen und viele andere. Ausgangspunkt für die Wissensrepräsentation ist die Wissensrepräsentationshypothese, die 1985 von Brian C. Smith formalisiert wurde: Jeder mechanisch verkörperte intelligente Prozess wird aus strukturellen Bestandteilen bestehen, die a) wir als externe Beobachter selbstverständlich berücksichtigen, um eine propositionale Berücksichtigung des Wissens, dass der Gesamtprozess zeigt, und b) unabhängig von einer solchen externen semantischen Zuschreibung eine formale, aber ursächliche und wesentliche Rolle bei der Erzeugung des Verhaltens spielen, das dieses Wissen manifestiert. Derzeit sind eines der aktivsten Bereiche der Wissensrepräsentationsforschung Projekte im Zusammenhang mit dem Semantischen Web. Das Semantische Web versucht, eine Schicht der Semantik (Bedeutung) auf dem aktuellen Internet hinzuzufügen. Anstatt Webseiten und Seiten über Keywords zu indexieren, erstellt das Semantic Web große Onlogien von Konzepten. Die Suche nach einem Konzept wird effektiver sein als herkömmliche Text nur sucht. Frame-Sprachen und automatische Klassifizierung spielen eine große Rolle in der Vision für das zukünftige Semantic Web. Die automatische Klassifizierung gibt Entwicklern Technologie, um Bestellung auf einem sich ständig weiterentwickelnden Netzwerk von Wissen zu liefern. Die Definition von Onlogien, die statisch und unfähig sind, sich auf der Fliege zu entwickeln, wäre für internetbasierte Systeme sehr begrenzt. Die Klassifikatortechnologie bietet die Möglichkeit, sich mit der dynamischen Umgebung des Internets zu beschäftigen. Neuere Projekte, die vor allem von der Defense Advanced Research Projects Agency (DARPA) finanziert werden, haben Rahmensprachen und Klassifikatoren mit Markupsprachen auf Basis von XML integriert. Das Resource Description Framework (RDF) bietet die grundlegende Fähigkeit, Klassen, Unterklassen und Eigenschaften von Objekten zu definieren. Die Web Ontology Language (OWL) bietet zusätzliche Ebenen der Semantik und ermöglicht die Integration mit Klassifikationsmotoren. Überblick Wissensrepräsentation ist ein Bereich der künstlichen Intelligenz, der sich auf die Gestaltung von Computerrepräsentationen konzentriert, die Informationen über die Welt erfassen, die zur Lösung komplexer Probleme verwendet werden können. Die Begründung für die Wissensdarstellung ist, dass der konventionelle Verfahrenscode nicht der beste Formalismus ist, um komplexe Probleme zu lösen. Die Wissensdarstellung erleichtert die Definition und Aufrechterhaltung komplexer Software als Verfahrenscode und kann in Expertensystemen eingesetzt werden. Zum Beispiel, mit Experten in Bezug auf die Geschäftsregeln zu sprechen, anstatt Code die semantische Lücke zwischen Benutzern und Entwicklern verringert und die Entwicklung komplexer Systeme praktischer macht. Die Wissensdarstellung geht Hand in Hand mit automatisierter Argumentation, weil einer der Hauptzwecke der expliziten Wissensvertretung darin besteht, in der Lage zu sein, über dieses Wissen zu verstehen, Inferenzen zu machen, neue Kenntnisse geltend zu machen usw. Nahezu alle Wissensdarstellungssprachen haben im Rahmen des Systems eine Richt- oder Inferenzmaschine. Ein entscheidender Kompromiss bei der Gestaltung eines Wissensvertretungsformalismus ist, dass zwischen Ausdruckskraft und Praxis. Die ultimative Wissensdarstellung Formalismus in Bezug auf Ausdruckskraft und Kompaktheit ist First Order Logic (FOL). Es gibt keinen mächtigeren Formalismus als der von Mathematikern verwendet, um allgemeine Vorschläge über die Welt zu definieren. FOL hat jedoch zwei Nachteile als Wissensvertretung Formalismus: einfache Bedienung und praktische Umsetzung. Erstauftragslogik kann auch für viele Softwareentwickler einschüchtern. Sprachen, die nicht die komplette formale Macht von FOL haben, können noch in der Nähe der gleichen ausdrucksstarken Macht mit einer Benutzeroberfläche, die für den durchschnittlichen Entwickler besser zu verstehen ist. Die praktische Umsetzung ist, dass FOL in gewisser Weise zu ausdrucksstark ist. Mit FOL ist es möglich, Aussagen (z.B. Quantifizierung über unendliche Sätze) zu erstellen, die dazu führen würden, dass ein System niemals endet, wenn es versuchte, sie zu überprüfen. So kann eine Teilmenge von FOL einfacher zu bedienen und praktischer zu implementieren sein. Dies war eine treibende Motivation hinter regelbasierten Expertensystemen. IF-THEN-Regeln bieten eine Untermenge von FOL, aber eine sehr nützliche, auch sehr intuitive. Die Geschichte der meisten der frühen KI-Wissensdarstellung Formalismen; von Datenbanken zu semantischen Netzen zu Theorem-Proversen und Produktionssystemen können als verschiedene Design-Entscheidungen angesehen werden, ob Ausdruckskraft oder Rechenfähigkeit und Effizienz zu betonen. In einer Schlüsselarbeit von 1993 zum Thema hat Randall Davis vom MIT fünf verschiedene Rollen zur Analyse eines Wissensrepräsentationsrahmens skizziert: Eine Wissensvertretung (KR) ist im Wesentlichen ein Surrogat, ein Ersatz für die Sache selbst, verwendet, um eine Entität zu ermöglichen, Konsequenzen zu bestimmen, indem sie nicht handeln, d.h. durch die Vernunft über die Welt, anstatt zu handeln. Es handelt sich um eine Reihe von ontologischen Verpflichtungen, d.h. eine Antwort auf die Frage: Was soll ich über die Welt denken? Es handelt sich um eine fragmentarische Theorie der intelligenten Vernunft, ausgedrückt in Bezug auf drei Komponenten: (i) die grundlegende Vorstellung der Darstellung intelligenter Vernunft; (ii) der Satz von Inferenzen der Repräsentationsanktionen; und (iii) der Satz von Inferenzen, die es empfiehlt. Es ist ein Medium für eine pragmatisch effiziente Berechnung, d.h. die rechnerische Umgebung, in der das Denken vollendet wird. Ein Beitrag zu dieser pragmatischen Effizienz wird von der Führung bereitgestellt, die eine Darstellung zur Organisation von Informationen vorsieht, um die empfohlenen Beiträge zu erleichtern. Es ist ein Medium menschlichen Ausdrucks, d.h. eine Sprache, in der wir Dinge über die Welt sagen. Wissensdarstellung und -vernunft sind eine Schlüsseltechnologie für das Semantische Web. Sprachen basierend auf dem Rahmenmodell mit automatischer Klassifikation bieten eine Schicht der Semantik auf dem vorhandenen Internet. Anstatt über Textzeichenfolgen zu suchen, wie es heute üblich ist, können logische Abfragen definiert und Seiten gefunden werden, die auf diese Abfragen abbilden. Bei diesen Systemen handelt es sich um einen als Klassifikator bekannten Motor. Klassifikatoren konzentrieren sich auf die Subsumption Relations in einer Wissensbasis anstatt auf Regeln. Ein Klassifikator kann neue Klassen erniedrigen und die Ontologie dynamisch ändern, da neue Informationen verfügbar werden. Diese Fähigkeit ist ideal für den sich ständig verändernden und weiterentwickelnden Informationsraum des Internets. Das Semantic Web integriert Konzepte aus der Wissensdarstellung und der Argumentation mit Markupsprachen basierend auf XML. Das Resource Description Framework (RDF) bietet die grundlegenden Fähigkeiten, wissensbasierte Objekte im Internet mit grundlegenden Merkmalen wie Is-A-Beziehungen und Objekteigenschaften zu definieren. Die Web Ontology Language (OWL) fügt zusätzliche Semantik hinzu und integriert sich mit automatischen Klassifikationsbegründen. Merkmale Im Jahr 1985 kategorisierte Ron Brachman die Kernfragen der Wissensvertretung wie folgt: Primitiven. Was ist der zugrunde liegende Rahmen, der verwendet wird, um Wissen zu repräsentieren? Semantische Netzwerke waren eines der ersten Wissensrepräsentationsprimitiven. Auch Datenstrukturen und Algorithmen für die allgemeine schnelle Suche. In diesem Bereich gibt es eine starke Überschneidung mit der Forschung in Datenstrukturen und Algorithmen in der Informatik. In frühen Systemen wurde die nach dem Lambda-Kalkus modelierte Programmiersprache Lisp oft als Form der funktionalen Wissensdarstellung verwendet. Frames und Regeln waren die nächste Art von primitiv. Rahmensprachen hatten verschiedene Mechanismen zum Ausdrucken und Verzicht auf Rahmendaten. Alle Daten in Frames werden in Slots gespeichert. Slots sind analog zu den Beziehungen in der Entitätsmodellierung und Objekteigenschaften in der objektorientierten Modellierung. Eine weitere Technik für Primitiven ist die Definition von Sprachen, die nach First Order Logic (FOL) modelliert werden. Das bekannteste Beispiel ist Prolog, aber es gibt auch viele spezielle Zweck-Theorem proving-Umgebungen. Diese Umgebungen können logische Modelle validieren und neue Theorien aus bestehenden Modellen ableiten. Im Wesentlichen automatisieren sie den Prozess, durch den ein Logiker ein Modell analysieren würde. Theorem-Proving-Technologie hatte einige spezifische praktische Anwendungen in den Bereichen Software-Engineering. Beispielsweise kann nachgewiesen werden, dass ein Softwareprogramm starr an einer formalen logischen Spezifikation haftet. Metadarstellung. Dies ist auch als Thema der Reflexion in der Informatik bekannt. Es bezieht sich auf die Fähigkeit eines Formalismus, Zugang zu Informationen über seinen eigenen Staat zu haben. Ein Beispiel wäre das Meta-Objekt-Protokoll in Smalltalk und CLOS, das Entwicklern einen zeitlichen Zugriff auf die Klassenobjekte gibt und diese auch zur Laufzeit die Struktur der Wissensbasis dynamisch neu definieren lässt. Meta-Repräsentation bedeutet, dass die Wissensdarstellungssprache selbst in dieser Sprache ausgedrückt wird. Zum Beispiel wären in den meisten Frame-basierten Umgebungen alle Frames Beispiele einer Frame-Klasse. Dieses Klassenobjekt kann zur Laufzeit überprüft werden, so dass das Objekt seine innere Struktur oder die Struktur anderer Teile des Modells verstehen und sogar verändern kann. In regelbasierten Umgebungen waren die Regeln in der Regel auch Instanzen von Regelklassen. Ein Teil des Meta-Protokolls für Regeln waren die Meta-Regeln, die das Regelfeuer priorisierten. Unvollkommenheit. Traditionelle Logik erfordert zusätzliche Axiome und Zwänge, um mit der realen Welt im Gegensatz zur Mathematik zu behandeln. Auch ist es oft nützlich, Grad des Vertrauens mit einer Aussage zu verbinden. Ich sage nicht einfach "Socrates is Human", sondern "Socrates is Human with trust 50%". Dies war eine der frühen Innovationen aus der Experten-Systemforschung, die auf einige kommerzielle Werkzeuge migrierte, die Fähigkeit, gewisse Faktoren mit Regeln und Schlussfolgerungen zu verbinden. Spätere Forschung in diesem Bereich ist als Fuzzy-Logik bekannt. Definitionen und Universals vs. Fakten und Standardeinstellungen. Universalen sind allgemeine Aussagen über die Welt wie "Alle Menschen sind sterblich". Fakten sind konkrete Beispiele von Universalen wie "Sokraten ist ein Mensch und daher sterblich". In logischen Begriffen sind Definitionen und Universalen über die universelle Quantifizierung, während Tatsachen und Defaults über existentielle Quantifizierungen sind. Alle Formen der Wissensdarstellung müssen sich mit diesem Aspekt befassen und dies meist mit einer Variante der Set-Theorie, der Modellierung von Universalen als Sets und Subsets und Definitionen als Elemente in diesen Sets. Nicht-monotone Argumentation. Nicht-monotone Vernunft erlaubt verschiedene Arten von hypothetischen Vernunft. Das System verbindet Fakten, die mit den Regeln und Tatsachen behauptet werden, die verwendet werden, um sie zu rechtfertigen, und da diese Tatsachen ändern auch die abhängigen Kenntnisse. In der Regel basierten Systemen ist diese Fähigkeit als Wahrheitspflegesystem bekannt. Expressive Angemessenheit. Der Standard, den Brachman und die meisten AI-Forscher verwenden, um expressive Angemessenheit zu messen, ist in der Regel First Order Logic (FOL). Theoretische Einschränkungen bedeuten, dass eine vollständige Umsetzung von FOL nicht praktisch ist. Die Forscher sollten klar sein, wie ausdrucksstark (wie viel von voller FOL Ausdruckskraft) sie ihre Vertretung beabsichtigen. Begründung der Effizienz. Dies bezieht sich auf die Laufzeiteffizienz des Systems. Die Fähigkeit der Wissensbasis, aktualisiert zu werden, und der Grund, neue Inferenzen in einer angemessenen Zeit zu entwickeln. In gewisser Weise ist dies die Flip-Seite der expressiven Angemessenheit. Im Allgemeinen, je leistungsfähiger eine Darstellung, desto mehr hat es ausdrucksstarke Angemessenheit, desto weniger effizient wird seine automatisierte Richtmaschine sein. Effizienz war oft ein Thema, insbesondere für frühe Anwendungen der Wissensrepräsentationstechnologie. Sie wurden in der Regel in interpretierten Umgebungen wie Lisp umgesetzt, die im Vergleich zu traditionellen Plattformen der Zeit langsam waren. Onkologietechnik In den frühen Jahren der wissensbasierten Systeme waren die Wissensbasis ziemlich klein. Die Wissensgrundlagen, die dazu bestimmt waren, tatsächlich reale Probleme zu lösen, anstatt Beweise für Konzeptdemonstrationen zu machen, die notwendig sind, um sich auf gut definierte Probleme zu konzentrieren. So zum Beispiel, nicht nur medizinische Diagnose als ganzes Thema, sondern medizinische Diagnose bestimmter Arten von Krankheiten. Als wissensbasierte Technologie skaliert, wurde der Bedarf an größeren Wissensbasen und für modulare Wissensbasen, die miteinander kommunizieren und ineinander integrieren können, deutlich. Dies führte zur Disziplin der Ontologietechnik, Konstruktion und Bau großer Wissensbasen, die von mehreren Projekten genutzt werden könnten. Eines der führenden Forschungsprojekte in diesem Bereich war das Cyc-Projekt. Cyc war ein Versuch, eine riesige enzyklopädische Wissensbasis aufzubauen, die nicht nur Fachwissen, sondern Wissen über den gesunden Menschenverstand enthalten würde. Bei der Gestaltung eines künstlichen Geheimdienstmitarbeiters wurde bald erkannt, dass es wichtig war, eine KI zu machen, die mit Menschen mit natürlicher Sprache interagieren könnte. Cyc sollte dieses Problem lösen. Die Sprache, die sie definierte, war CycL. Nach CycL wurden eine Reihe von Ontologiesprachen entwickelt. Die meisten sind deklarative Sprachen und sind entweder Frame-Sprachen oder basieren auf First-Ordner-Logik. Modularität – die Fähigkeit, Grenzen um bestimmte Domänen und Problemräume zu definieren – ist für diese Sprachen unerlässlich, weil, wie Tom Gruber sagte, "Jede Onlogie ist ein Vertrag - ein soziales Abkommen zwischen Menschen mit gemeinsamem Motiv im Austausch." Es gibt immer viele konkurrierende und unterschiedliche Ansichten, die einen allgemeinen Zweck Onlogie unmöglich machen. Eine allgemeine Zweck-Onlogie müsste in jeder Domäne anwendbar sein und verschiedene Wissensbereiche müssen vereinheitlicht werden. Es gibt eine lange Geschichte der Arbeit, die versucht, Ontologien für eine Vielzahl von Aufgabenbereichen aufzubauen, z.B. eine Ontologie für Flüssigkeiten, das klumpige Elementmodell weit verbreitet bei der Darstellung von elektronischen Schaltungen (z.B.) sowie Ontologien für Zeit, Glauben und sogar Programmierung selbst.Jeder dieser bietet einen Weg, um einen Teil der Welt zu sehen. Das klumpte Elementmodell z.B. deutet darauf hin, dass wir an Schaltungen in Bezug auf Komponenten mit Verbindungen zwischen ihnen denken, wobei Signale sofort entlang der Verbindungen fließen. Dies ist eine nützliche Ansicht, aber nicht die einzige mögliche. Eine andere Ontologie entsteht, wenn wir an die Elektrodynamik im Gerät teilnehmen müssen: Dabei propagieren Signale mit endlicher Geschwindigkeit und ein Objekt (wie ein Widerstand), das zuvor als einzelne Komponente mit einem I/O-Verhalten angesehen wurde, muss nun als erweitertes Medium betrachtet werden, durch das eine elektromagnetische Welle fließt. Ontologien können selbstverständlich in einer Vielzahl von Sprachen und Notationen (z.B. Logik, LISP usw.) niedergeschrieben werden; die wesentlichen Informationen sind nicht die Form dieser Sprache, sondern der Inhalt, d.h. der Satz von Konzepten, die als Denkweise an die Welt angeboten werden. Einfach gesagt, der wichtige Teil ist Begriffe wie Verbindungen und Komponenten, nicht die Wahl zwischen sie als Prädikate oder LISP Konstrukte zu schreiben. Die Verpflichtung, eine oder eine andere Ontologie zu wählen, kann eine scharfe unterschiedliche Ansicht der vorliegenden Aufgabe ergeben. Betrachten Sie den Unterschied, der bei der Auswahl der klumpten Elementansicht einer Schaltung entsteht, anstatt die elektrodynamische Ansicht derselben Vorrichtung. Als zweites Beispiel sieht die regelmäßig betrachtete medizinische Diagnose (z.B. MYCIN) wesentlich anders aus als die gleiche, in Rahmen betrachtete Aufgabe (z.B. INTERNIST). Wenn MYCIN die medizinische Welt sieht, die aus empirischen Assoziationen besteht, die Symptom für die Krankheit verbinden, sieht INTERNIST eine Reihe von Prototypen, insbesondere prototypische Krankheiten, die dem vorliegenden Fall entsprechen. Siehe auch Alphabet des menschlichen Denkens Belief Revision Chunking (Psychologie)Commonsense Wissensbasis Conceptual graph DATR, eine Sprache für die lexische Wissensdarstellung Logico-linguistische Modellierung Personal Wissensbasis Wissensdatenbank Wissensmanagement Semantische Technologie Valuation-basierte System Referenzen Weiter lesen Ronald J. Brachman; Was IS-A ist und ist nicht. Eine Analyse der Taxonomic Links in Semantic Networks; IEEE Computer, 16 (10); Oktober 1983 Ronald J. Brachman, Hector J. Levesque Knowledge Representation and Reasoning, Morgan Kaufmann, 2004 ISBN 978-1-55860-932-7 Ronald J. Brachman, Hector J. Levesque (eds) Lesungen in Wissensrepräsentation, Morgan Kaufmann, 1985, ISBN 0-934613-01-X Chein, M. Mugnier, M.-L (2009),Graph-basierte Wissensrepräsentation: Computational Foundations of Conceptual Graphs, Springer, 2009,ISBN 978-1-84800-285-2.Randall Davis, Howard Shrobe und Peter Szolovits; Was ist eine Wissensrepräsentation? AI Magazine, 14(1):17-33,1993 Ronald Fagin, Joseph Y. Halpern, Yoram Moses, Moshe Y. Vardi Wissen, MIT Press, 1995, ISBN 0-262-06162-7 Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: Umsetzungen von IS-A Relations.ER 1996: 42-57 Hermann Helbigation: Knowledge Representation and the Seman Springbook, 2006 Arthur B. Markman: Wissensrepräsentation Lawrence Erlbaum Associates, 1998 John F. Sowa: Wissensrepräsentation: Logik-, Philosophie- und Computational Foundations. Brooks/Cole: New York, 2000 Adrian Walker, Michael McCord, John F. Sowa, und Walter G. Wilson: Knowledge Systems and Prolog, Second Edition, Addison-Wesley, 1990 Mary-Anne Williams und Hans Rott: "Frontiers in Belief Revision, Kluwer", 2001. Externe Links Was ist eine Wissensrepräsentation? von Randall Davis und andere Einführung in die Wissensmodellierung von Pejman Makhfi Einführung in die Beschreibung Logics Kurs von Enrico Franconi, Fakultät für Informatik, Freie Universität von Bozen, Italien DATR Lexical Wissensrepräsentation Sprache Loom Project Home Page Principles of Knowledge Representation and Reasoning Incorporated Beschreibung Logic in Practice: A CLASSIC Application Die Regel-Markup-Initiative Nelements KOS - ein nicht-freies 3d-Wissens-Darstellungssystem