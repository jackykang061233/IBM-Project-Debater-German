Mathematik, Statistik, Finanzen, Computerwissenschaften, vor allem in der maschinellen Entwicklung und inverse Probleme, ist die Regularisierung der Prozess, um ein Problem zu lösen oder eine Überrüstung zu verhindern. Regularisierung kann auf objektive Funktionen in unangebrachten Optimierungsproblemen angewendet werden. Der Ordnungsbegriff oder die Sanktion verursachen Kosten für die Optimierungsfunktion, um die optimale Lösung einzigartig zu machen. Unabhängig von dem Problem oder Modell gibt es immer eine Datenfrist, die einer Wahrscheinlichkeit der Messung und einer Ordnungsperiode entspricht, die einem vorherigen entspricht. Indem man sowohl die Bayesischen Statistiken kombiniert, kann man ein Posterior berechnen, das sowohl die Informationsquellen als auch die Schätzungen stabilisiert. Indem man beide Ziele abschließt, wählte man sich dafür aus, die Daten stärker zu entlasten oder die Generalisierung durchzusetzen (um eine Überrüstung zu verhindern). Es gibt eine ganze Forschungsbranche, die alle möglichen Regularisierungen behandelt. In der Regel ist der Arbeitsfluss, der eine spezifische Regularisierung sucht und dann die Wahrscheinlichkeitsdichte, die dieser Regularisierung entspricht, um die Wahl zu rechtfertigen. Man kann auch körperlich motiviert sein durch gemeinsames Gefühl oder Unterweisung, was schwieriger ist. In der Maschinenausbildung entspricht der Datenbegriff den Ausbildungsdaten und die Regularisierung ist entweder die Wahl des Modells oder Änderungen des Algorithmus. Es ist immer darauf ausgerichtet, den allgemeinen Einschätzungsfehler zu verringern, d. h. die Fehlerquote mit dem ausgebildeten Modell für die Bewertung und nicht die Ausbildungsdaten. Eine der frühesten Anwendung der Regularisierung ist mit der Methode von mindestens Quadraten verbunden. Die Zahl der Wahrscheinlichkeitsdichte ist der gaussische Vertrieb, der nun unter dem Namen "Tikhonov Regularisierung" bekannt ist. Klassifikation Empirical Learning of Classifiers (aus einem Floitdatensatz) ist immer ein unbestimmtes Problem, da es versucht, eine Funktion von x {\displaystyle x} zu nutzen, nur Beispiele x 1 , x 2 , . . x n n {\displaystyle x_{1},x_{2},...x_{n } } R (oder Regularisation} F) faser Rstyle Rstyle R (f) f) f) λf Funktion (f) f) f) f) f) f) :f  1 (f)  1f)  1 (f  1f) f) f)  1f)  1  1 (f)  1f)  1f  1  1  1  1 wenn das Etikett Yendisplaystyle y} ist, wie der Verlust von Quadratmetern oder die Ableitung; und   Memestyle \lambda } ist ein Parameter, der die Bedeutung der Regularisierungsfrist kontrolliert. R ( f) Memedisplaystyle R(f)} wird in der Regel dafür entschieden, eine Sanktion gegen die Komplexität des f Memedisplaystyle f} zu verhängen, .Verlängerung der Komplexität, die verwendet wird, umfassen Beschränkungen für das reibungslose Funktionieren und die Bindung an die Norm für den Vektorraum. Eine theoretische Rechtfertigung für die Regularisierung besteht darin, dass es versucht, den Sturm von Occam auf die Lösung zu stellen (wie in der Abbildung oben dargestellt, wo die grüne Funktion, die einfachere, bevorzugt werden kann). Nach Ansicht der Bayesischen Sicht entsprechen viele Regularisierungstechniken bestimmten Vorvertriebenen auf Modellparametern. Regularisierungen können verschiedene Zwecke dienen, darunter Lern einfachere Modelle, die Einführung von Modellen, die zu klein sind, und die Einführung einer Gruppenstruktur in das Lernproblem. In vielen Bereichen der Wissenschaft wurde dieselbe Idee gefunden. Eine einfache Form der Regularisierung, die auf integrale Gleichungen (Tikhonov Regularisierung) angewendet wird, ist im Wesentlichen ein Handelsabschluß zwischen der Installation der Daten und der Verringerung einer Norm der Lösung. Kürzlich wurden nicht-lineare Regularisierungsmethoden, einschließlich der Gesamtdiversalisierung, populär. Generalisierungs-Regulierung kann als Technik zur Verbesserung der Allgemeinheit eines gelernten Modells motiviert werden. Ziel dieses Lernproblems ist es, eine Funktion zu finden, die das Ergebnis (Label) anpasst oder vorhersagen, die den erwarteten Fehler über alle möglichen Inputs und Etiketten minimieren. Der erwartete Fehler einer Funktion f n {\displaystyle f_{n} ist: I [ f ] =  Y X × Y V ( f n ( x ) , y )  ( ( x , y ) d x d y KINGstyle I[f_{n}]=\int {_X\times Y}V(f_{n}(x),y)\rho (x,y)\,dx\,dy}, wo X {\displaystyle X} und Y Memestyle Y} die Bereiche der Eingabedaten x Memestyle x} und deren Etiketten y varistyle y} sind. In der Regel in Lernproblemen sind nur ein Teil von Inputdaten und Etiketten verfügbar, gemessen an einigen Lärm. Der erwartete Fehler ist daher unumkehrbar, und die beste verfügbare Surrogate ist der empirische Fehler über die N Memestyle N} verfügbare Proben: I S [ f n ] = 1   i = 1 N V ( f n ( x ^ i) , y ^ i ) KINGstyle I_{S}[f_{n}]=1,0frac 1}{n isum i=1 iN}V(f_{n}(Spahat x}}_{i}), Lohhat y}}_{i) Ohne an die Komplexität des Funktionsbereichs gebunden (formuliert, den Einsatz von Kern Hilbert-Raum) ist ein Modell zu verstehen, dass ein Nullverlust auf dem Ersatz empirischer Fehler entsteht. Wenn Messungen (z.B. x i WELLdisplaystyle x_{i} ) mit Lärm durchgeführt wurden, kann dieses Modell unter einer Überrüstung leiden und einen schlechten erwarteten Fehler aufweisen. Regularisierung setzt eine Sanktion ein, um bestimmte Regionen des Funktionsbereichs zu erkunden, die zum Aufbau des Modells genutzt werden, was die allgemeine Ausrichtung verbessern kann. Tikhonov Regularisierung Diese Techniken werden für Andrey Nikolayevich Tikhonov benannt, die die Regularisierung auf integrale Gleichungen angewandt und in vielen anderen Bereichen wichtige Beiträge geleistet haben. Lernen Sie eine lineare Funktion f WELLdisplaystyle f} , gekennzeichnet durch einen unbekannten Vektor w {\displaystyle w}, so dass f ( x ) = w ) x {\displaystyle f(x)=w\cdot x} , eine kann die L 2 7.8displaystyle L_{2} hinzufügen -norm des Vektors w Memedisplaystyle w} zum Verlusttext, um Lösungen mit kleineren Normen vorzuziehen. Tikhonov Regularisierung ist eines der häufigsten Formen. Es ist auch bekannt als ridge Regression. Es wird ausgedrückt als: min w  i i = 1 n V ( x ^ i  w w , y ^ i ) +  2  2 2 Memestyle \min {_w _sum _i=1}^{n}V(Gethat) x{_{i icdot w, cuhat y}}_{i})+\lambda w\[10]_{2|2 , wo ( x ^ i , y ^ i ) , 1 ≤ i ≤ n , Memestyle {\hat x}}_}}_i}, Olivehat y__{i}),\,1\leqi\q n,} für die Ausbildung verwendet werden. Im Falle einer allgemeinen Funktion ist die Norm der Funktion in ihrem Umleitungskern Hilbert-Raum: min f  i i = 1 n V ( x ^ i ) , y ^ i ) +  i  f f  f H 2 {\displaystyle \min {_f}\sum _i=1}^{n}V(f(Fii}), Lohhat y__{i})+\lambda f\8.5_cal H22 Da die L 2 Memedisplaystyle L_{2} Norm unterschiedlich ist, kann das Lernen durch Bewegungsabfall vorangetrieben werden. Tikhonov-reguliert mindestens Quadrate Das Lernproblem mit der minderwertigen Verlustfunktion und der Tikhonov Regularisierung kann analytischer gelöst werden. Schriftlich in Matrixform ist die optimale w WELLdisplaystyle w} diejenige, für die die Klärigkeit der Verlustfunktion in Bezug auf w Memestyle w} beträgt 0. min w 1 n ( X ^ w − Y ) T ( X ^ w − Y ) +  w 2 2 7.8displaystyle \min _w 1frac 1}}n{\(Porthat X Xw-Y)TT}(Dok. w\|_{2}^{2  w w = 2 n X ^ T ( X ^ w − Y ) + 2 λ KINGstyle \nabla _w} 2 Xn Xhat X XT}(Gethat {X}}w-Y)+2\lambda w} 0 = X ^ T ( X ^ w − Y ) + n ) w RARstyle {X{\w-Y)+n\lambda w} (erste Voraussetzung) w = ( X ^ T X ^ +  I n I ) − 1 ( X ^ T Y ) {\displaystyle w= (Porthat X XT hat {X++\lambda nI)-1-1} (Dok. Nach Prüfung des zweiten Derivats  be w Memedisplaystyle \nabla {_ww} . Bei der Ausbildung dauert dieser Algorithmus O ( d 3 + n d 2 ) Memedisplaystyle O(d33}+nd2)2). Die Bedingungen entsprechen der Matrixumwandlung und der Berechnung von X T X X X Memestyle X XT} X . Tests dauert O (n d ) faserdisplaystyle O(nd)}. Frühzeitiger vorzeitiger Stopp kann als Regularisierung angesehen werden. Künftig wird ein Ausbildungsverfahren, wie z.B. der Bewegungsabgang, eher komplexere Funktionen mit zunehmenden Iterationsfunktionen erfahren. Durch die Regularisierung für die Zeit können die Modellierungskomplexe kontrolliert und die allgemeine Ausrichtung verbessert werden. Frühzeitige Einstellung erfolgt mit einem für die Ausbildung bestimmten Daten, einer statistisch unabhängigen Daten für die Validierung und einer anderen für die Prüfung. Das Modell wird ausgebildet, bis die Leistung auf der Validierungsform nicht mehr verbessert und dann auf den Testset angewendet wird. theoretische Motivation in mindestens Quadratmetern erwägen die finite Annäherung an die Salzburger Serie für unvertible Matrix Awhere  I I  A  A  1  1  1  1  1  1  I  I  I  I  I  I {\  I A {\ A {\  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I   i = 0 T - 1 (I - A ) i  A A − 1 {\displaystyle \sum _i=0TT-1}(I-A)^{iapproxca A-1-1} Dies kann verwendet werden, um die analytische Lösung unregelmäßiger mindestens Quadrate anzugleichen, wenn . eingeführt wird, um die Norm weniger als ein zu gewährleisten. T = ) n  i i = 0 T − 1 (I  – γ n X ^ T X ^ ) i X ^ T Y ^ KINGstyle w_{T} 7.8gamma {n}}\sum _i=0TT-1}(I-Kaffeefrac Memegamma n Xhat X XT XThat X Xi}{\hat XTT}{\hat {Y} Die genaue Lösung des unregelmäßigen Lernproblems von mindestens Quadratmetern minimiert den empirischen Fehler, kann aber scheitern. Durch die Begrenzung von T, dem einzigen freien Parameter im oben genannten Algorithmus, wird das Problem für die Zeit regelmäßig, was seine Generalisierung verbessern kann. Der vorstehende Algorithmus ist geeignet, die Zahl der Abwertungen für das empirische Risiko I s [ w ] = 1 2 n  X X ^ w − Y ^ R  2 R n 2 {\displaystyle I_{s}[w]= n{\ 2{\ | }} Y| WELL {R} n22 mit dem sprunghaften Programm: w 0 = 0 w t + 1 = (I - γ n X ^ T ^ )w t +  X n X ^ T Y ^ WELLdisplaystyle beginnt {aligned}w_{0} &=0\\w_{t+1} 7.8gamma n hat X XT}{\hat X))w_{t} 7.8gamma n Xhat X XT Yhat Y Yend{align Der Ausgangsfall ist uneinheitlich. Induktiv wird Folgendes nachgewiesen: w T = (I −  X n X ^ T X ^ )  i i = 0 T − 2 ( I −  X n X ^ T ^ X ^ ) i X ^ T Y ^ +  Y n X ^ T Y ^ T Y )  i n  i  i i = 1 T  X  X  X  X n X  X X  X X  i X  i X  i X  i X  i X  i X  i X  Y X  Y X  Y  Y  Y  Y  Y X  Y  Y  Y  Y  Y  Y  Y  Y  Y X  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y X  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y 7.8gamma n Xhat X XT Xhat X Xhat X}} 7.8gamma {n}}\sum _i=0TT-2}(I-Kaffeefrac Memegamma n Xhat X XThat{X}}{hat{i{hat{X}}^{T}{\hat Y++SSOfrac WELLgamma n hat X}}^{T Yhat Y}}\\ &=ggiofrac 7.8gamma {n)sum _i=1}^{T-1}(I-Portfrac n{\hat XTT Xhat X XT Xhat X Xi}{\hat XTT}{\hat Y++SSOfrac WELLgamma n hat X}}^{T Yhat Y}}\\ &=ggiofrac 7.8gamma {n}}\sum _i=0TT-1}(I-Kaffeefrac Memegamma n Xhat X XT XThat X Xi}{\hat XTT Yhat Y}}\end Regularizer für sparsity Assume, dass ein Wörterbuch  j j {\displaystyle \phi {_j} mit Dimension p {\displaystyle p} so gegeben ist, dass eine Funktion im Funktionsbereich als: f ( x ) =  p j = 1 p φ j  j j  j j ( x ) KINGstyle f(x)=\sum _j=1pp jphi j}(x)w_{j) Mehr Auslegungsschwierigkeiten auf dem w Memestyle w} können zu einfacheren und verwerteten Modellen führen. Dies ist in vielen Echtzeitanwendungen wie der Informatik sinnvoll. Ein Beispiel ist die Entwicklung eines einfachen prädiktiven Tests für eine Krankheit, um die Kosten für die Durchführung medizinischer Tests zu minimieren und gleichzeitig die prädiktive Leistung zu maximieren. Ein sinnvoller Spardruck ist der L 0 Coladisplaystyle L_{0} Standard  w 0 {\displaystyle w\|0}, definiert als Anzahl der nichtzero Elemente im w {\displaystyle w}. Lösung eines L 0 WELLdisplaystyle L_{0} regulärisiertes Lernproblem wurde jedoch als NP-hard nachgewiesen. L 1 Memedisplaystyle L_{1} Norm (siehe auch Norms) kann verwendet werden, um die optimale L 0 {\displaystyle L_{0} Norm über die Konvex Lockerung anzugleichen. Es kann gezeigt werden, dass die L 1 WELLdisplaystyle L_{1} norm induktiv ist. Im Falle von mindestens Quadratmetern ist dieses Problem als LASSO in der Statistik und als Grundlage für die Verfolgung der Signalverarbeitung bekannt. min   R p 1 n  X X ^ w − Y ^   2 +  w  1 1 RARstyle \min {_w\in \bb {R} ^p}}{\frac 1}}\n|[8] X}}w-Porthat Y|||2}+\lambda w\|_{1} L 1 {\displaystyle L_{1} Regularisierung kann gelegentlich nicht-uniqueliche Lösungen hervorbringen. Ein einfaches Beispiel ist in der Zahl, in der der Raum möglicher Lösungen auf einer 45-Grad-Linie liegt. Dies kann für bestimmte Anwendungen problematisch sein und wird durch die Kombination von L 1 {\displaystyle L_{1} mit L 2 {\displaystyle L_{2} Regularisierung in elastischer Netz Regularisierung, die folgende Form umfasst: min w  R R p 1 ‖ X ^ w  Y 2 +  Y 2 + ing (α  1 1 + 1  1 α 2 α 2  2 2  2 2  2  2 2  [ 2  [  [  [  0  0  0 0  0 0,  1 1  1  1  1  1  1  1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 {R} ^p}}{\frac 1}}\n|[8] X}}w-Porthat Y|||2}+\lambda (\alpha w\|_{1}+(1-\alpha w\\|2}^{2}),\alpha \in [0,1] Flexibilisierung wirkt sich tendenziell aus, wo korrelierte Eingangseigenschaften gleichgewichtig zugewiesen werden. LED-Netzreinigung wird in der Praxis üblich und in vielen maschinenlesbaren Lernbibliotheken umgesetzt. Proximal-Methoden Obwohl die L 1 Memedisplaystyle L_{1} Norm nicht zu einem NF-harten Problem führt, ist die L 1 {\displaystyle L_{1} Standard konvex, aber ist aufgrund der kink bei x = 0. Sub-Methoden, die sich auf den Subderivativen stützen, nicht unbedingt unterschiedlich, um L 1 Memestyle L_{1} reguläre Lernprobleme zu lösen. Mehr Konvergenz kann jedoch durch Proximal-Methoden erreicht werden. K ) H F ( w ) + R ( w ) Memestyle \min {_w\in H}F(w)+R(w, so dass F {\displaystyle F} konvex, kontinuierlich, differenzierbar ist, mit Lipschitz ständiger Differenzierung (z.B. die wenigsten Quadrate Verlustfunktion), und R RICHstyle R} ist konvex, kontinuierlich und richtig, dann ist die proximal Methode zur Lösung des Problems wie folgt. Erstens definiert der Proximal-Betreiber prox R ) ( v ) = argmin w  D R D { R ( w ) + 1 2 ‖ , 2 } , {\displaystyle \operatorname {prox} {_R}(v)=\operatorname {argmin} \limit_w\in \th\bb {R} ^D)R(w)+Währungfrac 1 12|||v2 und anschließender Tinte + 1 = Prox ,, R , · k −   F ( w k ) w_{k+1}=\operatorname {prox} \limits ggio_gamma ,R}(w_{k}-\gamma \nabla F(w_{k)} Die Proximal-Methode, die eserativ führt, führt zu einem Gefälle und führt anschließend das Ergebnis in den Raum ein, der von R varistyle R} . Wenn R TONstyle R} der L 1 RARstyle L_{1} Regularizer, entspricht der Proximal-Betreiber dem Soft-thres-Betreiber, S  ( ( v ) f } { v } }  v,  i 0  i 0  i  i  i  S  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i -\[lambda ,\lambda ]v_{i}+\lambda , &\text{if v_{i}<-\lambda \end{cases} Dies ermöglicht eine effiziente Berechnung. Gruppensparsamkeit ohne Überschneidungen von Merkmalen kann durch einen geringeren Aufwand regelrechtisiert werden, der für die Ausschöpfung bestimmter vorheriger Kenntnisse in ein Optimierungsproblem nützlich sein kann. Im Falle eines linearen Modells mit nicht überschneidenden bekannten Gruppen kann ein Regularizer definiert werden: R ( w ) =  of g = 1 G  w 2 , Memedisplaystyle R(w)=1|G|w_{g||_2, wo  w w g  2 2 =  j j = 1 [ G(w)]KINGstyle w_{g|_{2} 7.8sum j=1||G_{g}(w_{g}^{j})^{2 Man kann ein Regularisierer über die L 2 {\displaystyle L_{2} Standard über Mitglieder jeder Gruppe, gefolgt von einer L 1 Memestyle L_{1} Norm über Gruppen. Dies kann durch die Proximal-Methode gelöst werden, bei der der Proximal-Betreiber eine Blockierungsfunktion ist: Prox λ, R , g ) ( w g ) = { ( 1 λ  w  w  2 2 ) w g , wenn  w w g  2 2 - 0 ,, wenn  if 2  w 2  w 2 λ 2 λ λ {\ {\ {\ {\ {\ {\ {\ {\ } pro } } } } } }                                            2                2      2                        2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  R,g} (w_{g})=Barbegin{cases}(1-Getfrac ggiolambda) w_{g|_{2}}})w_{g}, w_{g|_{2}\lambda 0, & Logtext{if w_{g||_{2}\leq \lambda \end{cases} Gruppensparsamkeit mit Überschneidungen Der Algorithmus, der für Gruppensparsamkeit ohne Überschneidungen beschrieben ist, kann auf den Fall angewendet werden, in dem sich Gruppen überlappen, in bestimmten Situationen. Es wird wahrscheinlich zu Gruppen mit allen Nullelementen und anderen Gruppen mit einigen Nichtzero und einigen Nullelementen führen. Wenn es wünschenswert ist, die Gruppenstruktur zu erhalten, kann ein neuer Regularizer definiert werden: R ( w ) = inf {∑ g = 1 G  w 2 : w = ∑ g = 1 G w } g } WELLdisplaystyle R(w)=\inf links\{\sum g=1}^{ G||w_{g|_{2}:w=\sum _g=1}^{G}{\bar W__{g}\ W  w g {\displaystyle w_{g} , w ̄ g {\displaystyle 7.8bar w__{g wird definiert als Vektor, so dass die Einschränkung von w ̄ g {\displaystyle HANAbar w__{g der Gruppe g Memedisplaystyle g} gleich w {\displaystyle w_{g} und alle anderen Einträge von w . g {\ g {\ HANAstyle 7.8bar w}}_g sind null. Der Regularizer findet die optimale Zersplitterung des w Memestyle w} in Teile. Man kann alle Elemente, die in mehreren Gruppen bestehen, als duplizieren. Lernprobleme mit diesem Regularizer können auch mit der Proximal-Methode gelöst werden. Der Proximal-Betreiber kann nicht in geschlossener Form berechnet werden, sondern kann auch effektiv gelöst werden, indem er innerhalb der proximalen Methode, die eserierung vorsieht, eine innere Ersetzung bewirkt. Regularizer für das semi-revisierte Lernen Wenn Etiketten teurer sind, als Input-Beispiele zu sammeln, kann das halbgesteuerte Lernen nützlich sein. Regularizers wurden entwickelt, um Lernalgorithmen zu leiten, um Modelle zu lernen, die die Struktur unüberwachter Ausbildungsproben respektieren. Liegt eine symmetrische Gewichtsmatrix W} vor, kann ein Regularizer definiert werden: R ( f ) =  i i, j w i , j ( x i ) − f ( x j ) ) 2 Memestyle R(f)=\sum i{ij}(fx_{i})-f(x_{j} W i j {\displaystyle W_{ij} verschlüsselt das Ergebnis einiger Entfernungsparameter für Punkte x i {\displaystyle x_{i} und x j {\displaystyle x_{j}, ist es wünschenswert, dass f ( x i) . f ( x j ) faserstyle f(x_{i})\ca f(x_{j)} } . In diesem regulären Verfahren geht es um diese Intution und entspricht: R ( f) = f  L T L f {\ {\displaystyle R(f)= cubar f fT}LEINbar {f}, wo L = D − W WELLdisplaystyle L=D-W} ist die Laplacian Matrix der Graphik, die von W displaystyle W} .Theoptimization problem min f  R R m R ( f) , m = u + l {\displaystyle \min \min {_f\in \bb {R} }}mfR(f),=ul kann analytischer Art gelöst werden, wenn der Druck ( x i) y i) y displaystyle fstyle ((x) = fstyle fstyle ((x) für alle beaufsichtigt sind. Der etikettierte Teil des Vektors f WELLdisplaystyle f} ist daher offensichtlich. Der ungeschützte Teil des f {\displaydisplaystyle f} wird gelöst durch: min f u  R R u f T L f = min f u  R R u { f l u f l T l T l u f l t f u f l u f l u  l  l l f l } {\displaystyle \min f_{u}\in \ {R} u}}fTT}Lf=\min_{f_{u}\in \ Mathematikbb {R} u}}\{f_{u}^{T}L_{uu}   f u = 2 L u f u + 2 L u l Y RARstyle \nabla f_{u==2L_{uu}f_{u}+2L_{ul} f u = L u † (L u l Y ) KINGstyle f_{u}=L_{uu}^{\dagger (L_{ul}Y) Hinweis darauf, dass die Pseudo-inverse getroffen werden kann, weil L u l {\displaystyle L_{ul} das gleiche Angebot hat wie L u HANAstyle L_{uu} . Regularizer für das Multitask Learning Im Falle eines mehrjährigen Lernens werden die Probleme von T HANAdisplaystyle T} gleichzeitig geprüft, die jeweils in irgendeiner Weise miteinander verbunden sind. Ziel ist es, die Funktionen von T HANAdisplaystyle T} zu lernen, idealerweise die Anleihekraft aus der damit verbundenen Aufgaben, die eine prädiktive Macht haben. Dies entspricht dem Erlernen der Matrix W : T × D {\displaystyle W:T\times D} . Sparse Regularizer auf Spalten R ( w ) = 1 D  W W  1 2 , 1 {\displaystyle R(w)=\sum i=1|D|||W\|_{2,1 In diesem Regularizer wird eine L2-Norm in jeder Spalte und eine L1-Norm für alle Spalten definiert. Es kann durch Proximal-Methoden gelöst werden. Kernnorm Regularisierung R ( w ) =    1  W (W )  1 1 {\displaystyle R(w)=\\\\sigma (W)\· · |1}, wo  W ( W ) Memestyle \sigma (W)} die Grundwerte in der einmaligen Wertablagerung von W {\displaystyle W} ist. Mean-Konstruktion R (f 1  f f T ) = 1 T ‖ f t − 1 T ed s = 1 T f s  H H k 2 RARstyle R(f_{1}\cdots) f_{T})=\sum t=1TT||f_{t}-Spafrac 1TT sum s=1}^{T}f_{s}\_{H_{k}}^{2 In diesem regulären Verfahren werden die Funktionen, die für jede Aufgabe gelernt werden, dem Gesamtdurchschnitt der Funktionen in allen Aufgaben ähnlich sein. Es ist sinnvoll, vorherige Informationen zu übermitteln, die jede Aufgabe voraussichtlich mit jeder anderen Aufgabe teilen wird. Ein Beispiel ist die Vorhersage von Bluteisenwerten, die in verschiedenen Zeiten des Tages gemessen werden, wo jede Aufgabe individuell ist. Clustered -  I  f  1  I  I I ( r ) f s ) H k 2 Glühbirne R(f_{1}\cdots f_{T})=\sum _r=1CC sum {_t\in I(r)|f_{t}-Kaffeefrac 1 1I(r) sum {_s\in I(r)}f_{s|||_{H_{H_{k where2, wo ich (r) Memestyle I(r)} eine Reihe von Aufgaben ist. Dieser Regularizer ähnelt dem durchschnittlichen regulärisierbaren Regularizer, stellt jedoch eine Ähnlichkeit zwischen den Aufgaben innerhalb desselben Clusters her. Dies kann komplexere vorherige Informationen erfassen. Diese Technik wurde verwendet, um Netflix-Empfehlungen vorherzusagen. Ein Cluster würde einer Gruppe von Menschen entsprechen, die ähnliche Präferenzen teilen. Graphikbasierte Ähnlichkeit Mehr als oben, die Ähnlichkeit zwischen den Aufgaben kann durch eine Funktion definiert werden. Der Regularizer ermutigt das Modell, ähnliche Funktionen für ähnliche Aufgaben zu erfahren. R ( f 1  f f T ) = ) t , s = 1 , t  T s T ‖ f s s  2 2 M t s s faserstyle R(f_{1}\cdots f_{T})=\sum {_t,s=1,t\neq s|T|T|f_{t}-f_{s|||2}M_{ts für eine bestimmte symmetrische ähnliche Matrix M {\displaystyle M} . Andere Verwendungen der Regularisierung in Statistiken und Maschinenlernmethoden verwenden eine Vorbedingung, dass (normalerweise) eine geringere Wahrscheinlichkeit für komplexere Modelle gibt. Well bekannte Modellauswahltechniken umfassen das Akaike-Informationskriterium (AIC), die Mindestbeschreibungsdauer (MDL) und das Biesische Informationskriterium (BIC). Alternative Methoden zur Kontrolle der Überrüstung, die nicht mit der Regularisierung verbunden sind, umfassen die Crossvalidierung. Beispiele für Anwendungen verschiedener Methoden der Regularisierung zum linearen Modell: Siehe auch Bayesische Auslegung der Regularisierung Bias-Sortance-Regulierungsregelierung durch spektrale Filterung, die regelmäßig mindestens Quadrate Lagrange Multiplikator-Aufgaben von Neumaier, A (1998). "Solche Anfälligkeit und einzigartige lineare Systeme: ein Lehrprogramm zur Regularisierung" (PDF). SIAM-Prüfung.40 (3): 636–666.doi:10.1137/S0036144597321909.Blow-Fill-Seal (BFS)-Technologie ist eine Fertigungstechnik, die zur Herstellung kleiner (0.1mL) und großer Mengen (über 500 mL) flüssiger Container verwendet wird. Original in Europa in den 30er Jahren wurde es in den Vereinigten Staaten in den 60er Jahren eingeführt, aber in den letzten 20 Jahren ist es in der pharmazeutischen Industrie stärker verbreitet und wird mittlerweile weithin als die überlegene Form der aseptischen Verarbeitung durch verschiedene Arzneimittelregulierungsbehörden, einschließlich der USS Food and Drug Administration (FDA) bei der Verpackung von Arzneimitteln und Gesundheitsprodukten angesehen. Das grundlegende Konzept der BFS ist, dass ein Container in einem kontinuierlichen Prozess ohne menschliches Eingreifen in einem sterilen, geschlossenen Gebiet innerhalb einer Maschine gebildet, gefüllt und versiegelt wird. So kann diese Technologie verwendet werden, um sterile pharmazeutische flüssige Dosierungsformen zu produzieren. Die Materialauswahl ist in der Regel Polyethylen oder Polypropylen mit der spezifischen Besoldungsgruppe, die dem verwendeten Produkt und dem verwendeten Verfahren entspricht. Der Prozess ist multi-steed: Erstens ist pharmazeutischer Kunststoffharz vertikal durch einen Kreislauf Hals geheizt, um ein Nadelrohr namens Parison zu bilden. Diese gepresste Tube ist dann innerhalb einer zweiseitigen Form geschlossen, und die Tube wird über der Form gesenkt. Die Form wird in die Befüllungszone übertragen, oder sterile Befüllungsfläche, wo Nadeln (Behälter) gesenkt und verwendet werden, um das Kunststoff in die Form des Behälters innerhalb der Form zu überführen.Nach der Erstellung des Containers wird der Manndrel verwendet, um den Behälter mit Flüssigkeit zu füllen. Nach der Befüllung werden die Mandrels zurückgezogen und eine sekundäre Topform, die den Container enthält. Alle Maßnahmen werden in einer sterilen zerstrebten Kammer innerhalb der Maschine durchgeführt. Das Produkt wird dann in ein nichtsteriles Gebiet für Kennzeichnung, Verpackung und Vertrieb entsorgt. Kali-Deponierungstechnik verringert das Personalintervention, was eine robustere Methode für die aseptische Zubereitung von sterilen Arzneimitteln darstellt. BFS wird für die Befüllung von Flämchen für elterliche Zubereitungen und Infusionen, Augentropfen und Inhalationsprodukte verwendet. Insgesamt sind die Kunststoffbehälter aus Polyethylen und Polypropylen hergestellt. Polypropylen wird häufiger verwendet, um Container zu bilden, die durch die Autoklave weiter sterilisiert werden, da Polypropylen eine größere Wärmestabilisierung hat. Siehe auch Präzisierung von Hinweisen Soroka, W, "Grundsätze der Verpackungstechnik", IoPP, 2002, ISBN 1-930268-25-4 Yam, K. L, "Geschichte der Verpackungstechnik", John Kuhn & Sons, 2009 ISBN @0-470-08704-6 Außenbeziehungen B-Fill-Seal Technology, Pressemitteilungen [1]