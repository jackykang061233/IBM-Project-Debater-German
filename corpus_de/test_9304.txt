Im Januar 2015 unterzeichneten Stephen Hawking, Elon Musk und Dutzende von Experten für künstliche Intelligenz einen offenen Brief über künstliche Intelligenz, der die Forschung über die gesellschaftlichen Auswirkungen von AI fordert. Der Brief bestätigte, dass die Gesellschaft große potenzielle Vorteile von künstlicher Intelligenz ernten kann, forderte aber konkrete Forschung auf, wie man bestimmte potenzielle Fallstricke verhindern kann: künstliche Intelligenz hat das Potenzial, Krankheiten und Armut auszurotten, aber Forscher dürfen nicht etwas schaffen, das nicht kontrolliert werden kann. Der Vier-Paragraphenbrief mit dem Titel "Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter" enthält detaillierte Forschungsprioritäten in einem begleitenden zwölfseitigen Dokument. Hintergrund Bis 2014 hatten sowohl Physiker Stephen Hawking als auch Business Magnate Elon Musk öffentlich die Meinung geäußert, dass übermenschliche künstliche Intelligenz unkalkulierbare Vorteile bieten könnte, aber auch die menschliche Rasse beenden könnte, wenn sie unauffällig eingesetzt wird. Damals saßen Hawking und Musk beide auf dem wissenschaftlichen Beirat für die Zukunft des Lebensinstituts, einer Organisation, die "mitigate existentielle Risiken gegenüber der Menschheit" arbeitete. Das Institut verfasste einen offenen Brief an die breitere KI-Forschungsgemeinschaft und leitete ihn am ersten Wochenende 2015 an die Teilnehmer seiner ersten Konferenz in Puerto Rico weiter. Das Schreiben wurde am 12. Januar veröffentlicht. Zweck Der Brief unterstreicht sowohl die positiven als auch negativen Auswirkungen künstlicher Intelligenz. Laut Bloomberg Business zirkulierte Professor Max Tegmark vom MIT den Brief, um einen gemeinsamen Grund zwischen Unterzeichnern zu finden, die super intelligente KI als ein signifikantes existentielles Risiko betrachten, und Unterzeichnern wie Professor Oren Etzioni, die glauben, dass das KI-Feld von einem einseitigen Medienfokus auf die angeblichen Risiken eingeprägt wurde. Der Brief sieht vor, dass: Die potenziellen Vorteile (von KI) sind riesig, da alles, was die Zivilisation zu bieten hat, ein Produkt der menschlichen Intelligenz ist; wir können nicht vorhersagen, was wir erreichen könnten, wenn diese Intelligenz durch die Werkzeuge KI vergrößert wird, aber die Beseitigung von Krankheit und Armut sind nicht unerschütterlich. Aufgrund des großen Potenzials von KI ist es wichtig, zu erforschen, wie man seine Vorteile ernten und potenzielle Fallstricke vermeiden kann. Einer der Unterzeichner, Professor Bart Selman von der Cornell University, sagte, der Zweck ist es, AI-Forscher und Entwickler, mehr Aufmerksamkeit auf die AI-Sicherheit zu bekommen. Darüber hinaus soll der Brief für politische Entscheidungsträger und die Öffentlichkeit informativ, aber nicht alarmierend sein. Ein weiteres Unterzeichner, Professor Francesca Rossi, sagte: "Ich denke, es ist sehr wichtig, dass jeder weiß, dass KI-Forscher ernsthaft über diese Bedenken und ethische Fragen nachdenken." Vom Schreiben auferweckte Fragen Die Unterzeichner fragen: Wie können Ingenieure KI-Systeme schaffen, die für die Gesellschaft von Vorteil sind und die robust sind? Menschen müssen in der Kontrolle der KI bleiben; unsere KI-Systeme müssen " tun, was wir wollen, dass sie tun". Die benötigte Forschung ist interdisziplinärer Art und Weise, die von den Bereichen Wirtschaft und Recht bis zu verschiedenen Zweigen der Informatik, wie Computersicherheit und formale Verifikation, reicht. Herausforderungen, die entstehen, sind in die Überprüfung ("Baue ich das System richtig?") Gültigkeit ("Baue ich das richtige System?") Sicherheit und Kontrolle ("OK, ich habe das System falsch gebaut, kann ich es reparieren?") Kurzfristige Bedenken Einige kurzfristige Bedenken beziehen sich auf autonome Fahrzeuge, von zivilen Drohnen und selbstfahrenden Autos. Beispielsweise kann ein selbstfahrender Wagen im Notfall zwischen einem kleinen Risiko eines schweren Unfalls und einer großen Wahrscheinlichkeit eines kleinen Unfalls entscheiden. Andere Bedenken betreffen tödlich intelligente autonome Waffen: Sollten sie verboten werden? Wenn ja, wie sollte Autonomie genau definiert werden? Wenn nicht, wie sollte schuldhaft für einen Missbrauch oder Fehlfunktion zugeteilt werden? Weitere Themen sind die Datenschutzbedenken, da KI zunehmend in der Lage ist, große Überwachungsdatensätze zu interpretieren und die wirtschaftlichen Auswirkungen von durch KI vertriebenen Arbeitsplätzen am besten zu verwalten. Langfristige Bedenken Das Dokument schließt sich an die Bedenken von Microsoft-Forschungsdirektor Eric Horvitz: Wir könnten eines Tages die Kontrolle von KI-Systemen durch den Aufstieg von Superintelligenzen verlieren, die nicht nach menschlichen Wünschen handeln – und dass solche mächtigen Systeme die Menschheit gefährden würden. Sind solche dystopischen Ergebnisse möglich? Wenn ja, wie könnten diese Situationen entstehen? . Welche Investitionen in die Forschung sollten unternommen werden, um die Möglichkeit des Aufstiegs einer gefährlichen Superintelligenz oder des Auftretens einer "Intelligenzexplosion" besser zu verstehen und anzugehen? Vorhandene Werkzeuge zum Einsatz von KI, wie z.B. Verstärkungslernen und einfache Dienstfunktionen, sind nicht ausreichend, um dies zu lösen; daher ist mehr Forschung erforderlich, um eine robuste Lösung für das "Kontrollproblem" zu finden und zu validieren. Unterzeichner sind Physiker Stephen Hawking, Business Magnate Elon Musk, die Mitbegründer von DeepMind, Vikarious, Googles Forschungsdirektor Peter Norvig, Professor Stuart J. Russell der University of California Berkeley, und andere AI-Experten, Roboterhersteller, Programmierer und Ethiker. Die ursprüngliche Unterzeichnerzahl betrug über 150 Personen, darunter Akademiker aus Cambridge, Oxford, Stanford, Harvard und MIT. Hinweise Externe Links Forschungsprioritäten für robuste und beneidenswerte Künstliche Intelligenz: Ein offener Brief