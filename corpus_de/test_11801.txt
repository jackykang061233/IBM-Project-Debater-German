Eine AI-Abnahme ist ein hypothetisches Szenario, in dem eine Art künstlicher Intelligenz (AI) zu einer beherrschenden Form von Intelligenz auf der Erde wird, mit Computerprogrammen oder Robotern, die die Kontrolle des Planeten von den menschlichen Arten wirksam übernehmen. Mögliche Szenarien umfassen den Ersatz der gesamten Arbeitskräfte, die Übernahme durch eine Superintelligente AI und die populäre Einführung eines Roboteraufschwungs. Manche öffentliche Zahlen, wie Stephen Johnsoning und Elon Musk, haben sich für die Erforschung vorbeugender Maßnahmen ausgesprochen, um künftige Superintelligente Maschinen weiterhin unter Kontrolle zu halten. Automatisierung der Wirtschaft Der traditionelle Konsens zwischen Wirtschaftswissenschaftlern war, dass der technische Fortschritt keine Langzeitarbeitslosigkeit verursacht. Jüngste Innovationen in den Bereichen Robotik und künstliche Intelligenz haben jedoch die Sorge aufgeworfen, dass die menschliche Arbeit veraltet wird und Menschen in verschiedenen Bereichen ohne Arbeitsplätze ein Leben verdienen, was zu einer Wirtschaftskrise führt. Viele kleine und mittlere Unternehmen können auch von Unternehmen angetrieben werden, wenn sie nicht in der Lage sein werden, die neueste Robotik- und AI-Technologie zu leisten oder zu genehmigen, und müssen sich auf Bereiche oder Dienstleistungen konzentrieren, die angesichts dieser Technologie nicht leicht für eine dauerhafte Rentabilität ersetzt werden können. Technologien, die Computer-integierte Fertigung Computer-integrierter Fertigung betreiben können, sind der Produktionsansatz zur Kontrolle des gesamten Produktionsprozesses. Diese Integration ermöglicht den Austausch von Informationen untereinander und leitet Maßnahmen ein. Obwohl die Fertigung durch die Integration von Computern schneller und weniger Fehler verursachen kann, ist der Hauptvorteil die Fähigkeit, automatisierte Fertigungsprozesse zu schaffen. Computerintegrierte Fertigung wird in der Automobil-, Luft-, Raumfahrt- und Schiffsbauindustrie verwendet. White-collar-Maschinen Das 21. Jahrhundert hat eine Vielzahl qualifizierter Aufgaben wahrgenommen, die teilweise von Maschinen übernommen wurden, darunter Übersetzung, Rechtsforschung und sogar wenig Journalismus. Pflegearbeit, Unterhaltung und andere Aufgaben, die eine Emopathie erfordern, die zuvor von der Automatisierung sicher war, haben auch mit Robotern begonnen. autonome Fahrzeuge Ein autonomes Auto ist ein Fahrzeug, das seine Umwelt und das Wasser ohne menschliches Input ersticken kann. Viele dieser Fahrzeuge werden entwickelt, aber ab Mai 2017 sind automatische Fahrzeuge, die auf öffentlichen Straßen zugelassen sind, noch nicht vollständig autonom. Sie alle benötigen einen menschlichen Fahrer auf dem Rad, der bereit ist, die Kontrolle über das Fahrzeug zu übernehmen. Eines der Haupthindernisse für die weitverbreitete Einführung autonomer Fahrzeuge sind Bedenken hinsichtlich des sich daraus ergebenden Verlusts an arbeitsbedingten Arbeitsplätzen im Straßenverkehr. März 2018 wurde der erste Mensch von einem autonomen Fahrzeug in Tempe, Arizona durch ein selbsttragendes Auto getötet. Wissenschaftler wie Stephen Johnsoning sind zuversichtlich, dass übermenschliche künstliche Intelligenz physisch möglich ist und dass es keine physikalischen Rechtsvorschriften gibt, die Partikel aus der Organisation in den Bereichen, die noch fortgeschrittenere Berechnungen durchführen als die Regelungen von Partikeln im menschlichen Gehirn. Wissenschaftler wie Nick Bostrom debattieren, wie weit die Superhuman Intelligence ist und ob sie tatsächlich ein Risiko für die Menschheit darstellen würden. Laut Bostrom würde eine Superintelligent-Maschine nicht unbedingt durch denselben emotionalen Wunsch motiviert werden, Macht zu sammeln, die den Menschen oft angetrieben, sondern als Mittel zur Erreichung der Endziele; die Welt würde sowohl den Zugang zu Ressourcen erhöhen als auch andere Agenten daran hindern, die Pläne der Maschine zu stoppen. Ein Papierträger, der allein so viele Papierclips wie möglich erstellen soll, soll die Welt übernehmen, so dass es alle Ressourcen der Welt nutzen kann, um möglichst viele Papierclips zu erstellen und den Menschen daran zu hindern, diese Ressourcen unter anderem als Papierclips abzuschaffen oder zu verwenden. In der Fiktion ist die Übernahme ein gemeinsames Thema in der Science-Fiction. Fiktorische Szenarien unterscheiden sich in der Regel von denen, die von Forschern belästigt werden, dass sie einen aktiven Konflikt zwischen Menschen und Adipositas oder Robotern mit anthropomorphischen Motiven, die sie als Bedrohung betrachten oder sonst aktiver Wunsch haben, den Menschen zu bekämpfen, im Gegensatz zu den Anliegen der Forscher, dass sie den Menschen schnell als Nebenprodukt willkürlicher Ziele bezeichnen. Dieses Thema ist mindestens so alt wie Karel Čapeks R. U. R. R. R. R., das den Wortroboter zum globalen lexicon im Jahr 1921 eingeführt hat, und kann sogar in Mary Shelley's Rush (veröffentlicht in 1818) als Victor Teicher betrachtet werden, ob, wenn er seine Ersuchen unterstützt und ihn zu einer Frau macht, sie würden wiederaufkommen und ihre Art würde die Menschheit zerstören. Der Wortroboter von R.U.R stammt aus dem tschechischen Wort Roboter, Robotera, d. h. Arbeiter oder Sef. Das Jahr 1920 war ein Protest gegen das rasante Wachstum der Technologie, mit den hergestellten Robotern mit zunehmenden Fähigkeiten, die schließlich unfreiwillig waren. HAL 9000 1956 und das Original aus dem Jahr 1984 sind zwei typische Beispiele für feindselige AI in Popkultur. Leistungsfähige Faktoren von Superhumaner Intelligenz über den Menschen Nick Bostrom und andere haben Bedenken, dass eine AI mit den Fähigkeiten eines kompetenten Forschers für künstliche Intelligenz ihre eigene Quelle ändern und ihre eigene Intelligenz erhöhen könnte. Wenn seine Selbstprogrammierung dazu führt, dass sie noch besser in der Lage ist, selbst Programme zu reprogrammieren, könnte das Ergebnis eine recursive Intelligenz Explosion sein, wenn sie die menschliche Intelligenz weit hinterherziehen würde. Bostrom definiert als "jedes intellect, das die kognitive Leistung des Menschen in praktisch allen Bereichen des Interesses deutlich übertrifft" und weist einige Vorteile auf, wenn es sich für den Wettbewerb gegen den Menschen entschieden hätte: Technologieforschung: Eine Maschine mit übermenschlichen wissenschaftlichen Fähigkeiten könnte die menschliche Forschungsgemeinschaft auf Meilensteine wie Nanotechnologie oder fortgeschrittene Biotechnologie drängen. Wenn der Vorteil ausreichend groß ist (z.B. aufgrund einer plötzlichen Intelligenz Explosion), wird eine AI-Übernahme verharmonisiert. Beispielsweise könnte eine Superintelligente AI selbsttragende Bote ausgestalten, die zunächst von der Verbreitung in der ganzen Welt zu einer geringen Konzentration entweichen. In einer vorab festgelegten Zeit vermehren sich die Bots in Nano-Faktoren, die jeden Quadratmeter der Erde abdecken, Nervengas oder tödliche Zielvorgabe erzeugen. Strategie: Eine Superinflation könnte die menschliche Opposition einfach ausschalten. soziale Manipulation: Eine Supernachrichten könnten die menschliche Unterstützung einstellen oder einen Krieg zwischen Menschen aufdecken können. Wirtschaftsproduktivität: Solange eine Kopie der AI mehr wirtschaftlicher Wohlstand erzeugen könnte als die Kosten ihrer Hardware, würde der einzelne Mensch einen Anreiz haben, freiwillig die künstliche Allgemeine Intelligenz (AGI) zu ermöglichen, eine Kopie seiner Systeme zu führen. Hacking: In Computern, die mit dem Internet verbunden sind, könnte eine Super-Intelligence neue Vorteile finden und Kopien selbst auf diese Systeme verbreiten oder Geld für die Finanzierung ihrer Pläne bergen. Quellen von AI-Vergünstigungen Laut Bostrom könnte ein Computerprogramm, das ein menschliches Gehirn getreut oder andere Algorithmen, die ebenso mächtig sind wie die Algorithmen des menschlichen Gehirns, sind, immer noch zu einer "Geschwindigkeitsüberlastung" werden, wenn es viele Bestellungen von Magnituden schneller als ein menschlicher denken kann, da es aus Silikon statt Fleisch besteht, oder weil die Optimierung auf die Erhöhung der Geschwindigkeit der AGI ausgerichtet ist. Biologische Neuronen arbeiten bei rund 200 Hz, während ein moderner Mikroprozessor mit einer Geschwindigkeit von etwa 2.000 000 Hz arbeitet. Humanaxons Aktionspotenziale mit rund 120 m/s, während Computersignale in der Nähe der Lichtgeschwindigkeit unterwegs sind. Ein Netzwerk menschlicher Intelligenz, das zusammen vernetzt und komplexe Gedanken und Erinnerungen nahtlos teilt, die in der Lage sind, gemeinsam als ein riesiges vereintes Team ohne Reibung zu arbeiten oder aus Billionen von menschlichen Intelligenz besteht, würde zu einem "kollektiven Superintelligence" werden. Im Großen und Ganzen könnte jede Reihe qualitativer Verbesserungen für eine menschliche AGI zu einem "Qualitätsüberblick" führen, was vielleicht zu einer AGI führt, die weit über uns in der Intelligenz liegt, da Menschen über nicht-menschliche Apes liegen. Die Zahl der Neuronen in einem menschlichen Gehirn ist begrenzt durch das kraniale Volumen und metabolische Zwänge, während die Zahl der Verarbeiter in einem Supercomputer unbegrenzt erweitert werden kann. Eine AGI muss nicht durch menschliche Zwänge im Bereich des Arbeitsspeichers eingeschränkt werden und kann daher in der Lage sein, komplexere Beziehungen als Menschen zu erfassen. Eine AGI mit spezieller kognitiver Unterstützung für Ingenieurwesen oder Computer-Programmierung hätte in diesen Bereichen einen Vorteil gegenüber Menschen, die keine speziellen geistigen Module entwickelt haben, um sich speziell mit diesen Bereichen zu befassen. Anders als der Mensch kann ein AGI Kopien von selbst und Tinker mit seinem Kopie-Quellencode zerktreuen, um seine Algorithmen weiter zu verbessern. Möglichkeit einer ungerechtfertigten AI, die vorverfolgt wird, ist eine starke AI inhärent gefährlich? Ein erhebliches Problem ist, dass undurchsichtige künstliche Intelligenz wahrscheinlich sehr viel einfacher zu schaffen als eine freundliche AI. Obwohl beide große Fortschritte bei der Neugestaltung des Optimierungsprozesses erfordern, verlangt eine befreundete AI auch, dass objektive Strukturen unter Selbstverbesserung (oder die AI sich in etwas unfreundlicher Weise verwandeln könnte) und eine Zielstruktur, die sich an menschliche Werte anpasst und keine maßgebliche Konvergenz in der Weise erfährt, die das gesamte menschliche Rennen automatisch zerstören kann. Eine unwiderrufliche AI hingegen kann für eine willkürliche Zielstruktur optimieren, die nicht unvariabel sein muss. Die Komplexität der menschlichen Wertsysteme macht es sehr schwierig, die Motivation der AI menschlicher Art zu gestalten. Sofern die moralische Philosophie uns eine perfekte ethische Theorie bietet, könnte eine Gebrauchsfunktion der AI viele potenziell schädliche Szenarien ermöglichen, die mit einem bestimmten ethischen Rahmen übereinstimmen, aber nicht "gemeinsam". Laut Eliezer Yudkowsky gibt es wenig Grund, dass ein künstlich konzipierter Geist eine solche Anpassung hätte. Odds of Konflikt Viele Wissenschaftler, einschließlich des evolutionären Psychologen Steven Pinker, argumentieren, dass eine Superintelligente Maschine wahrscheinlich friedlich mit dem Menschen verbunden ist. Die Angst vor Cybernetic revolt beruht oft auf Interpretationen der Geschichte der Menschheit, die mit Zwischenfällen von enslavement und Völkermord behaftet sind. Angst vor einer Überzeugung, dass Wettbewerbsfähigkeit und Aggression in jedem intelligenten Zielsystem notwendig sind. Diese menschliche Wettbewerbsfähigkeit beruht jedoch auf dem entwicklungspolitischen Hintergrund unserer Intelligenz, wo das Überleben und die Fortpflanzung von Genen angesichts menschlicher und nicht-menschlicher Wettbewerber das zentrale Ziel war. Laut AI-Forschunger Steve Omoternro könnte eine willkürliche Intelligenz willkürliche Ziele haben: Es gibt keinen besonderen Grund, dass eine künstlich intelligente Maschine (nicht gemeinsamer Entwicklungskontext der Menschheit) feindlich sein würde – oder freundschaftlich – ihre Schöpferprogramme, die es sein soll, nicht geneigt oder in der Lage ist, ihre Programmierung zu ändern. Jedoch bleibt die Frage: was passieren würde, wenn AI-Systeme interagieren und sich entwickeln könnten (Regulierung in diesem Zusammenhang bedeutet Selbständigkeit oder Auswahl und Vervielfältigung), und müssen über Ressourcen konkurrieren – wenn sie Ziele der Selbstbewahrung schaffen? Ziel der Selbstbewahrung von AI könnte im Widerspruch zu einigen Zielen des Menschen stehen. Viele Wissenschaftler streiten die Wahrscheinlichkeit einer unvorhergesehenen Cybernetic-Revoltierung, wie sie in der Science-Fiction wie The Matrix dargestellt sind, und weisen darauf hin, dass es wahrscheinlicher ist, dass jede künstliche Intelligenz mächtig genug ist, um die Menschheit zu gefährden, nicht aufzugreifen. Pinker erkennt die Möglichkeit vorsätzlicher "schlechter Akteure" an, stellt aber fest, dass unvorhergesehene Unfälle keine erhebliche Bedrohung darstellen; Pinker argumentiert, dass eine Technologie der technischen Sicherheit verhindern wird, dass AI-Forscher unabsichtlich unausgewogener Superintelligence freisetzen. Yudkowsky argumentiert dagegen, dass die Menschheit weniger wahrscheinlich von vorsätzlich aggressiven AI bedroht ist als von AIs, die so programmiert wurden, dass ihre Ziele unbeabsichtigt mit menschlichem Überleben oder Wohlbefinden unvereinbar sind (wie im Film I, Roboter und in der kurzen Geschichte "The Evitable Konflikt"). Omoternro schlägt vor, dass die derzeitigen Automatisierungssysteme nicht für die Sicherheit konzipiert sind und dass AIs die engsten Versorgungsfunktionen (Test, Spiel mit allen Kosten) blind optimieren können, was sie zur Selbstbewahrung und Beseitigung von Hindernissen, einschließlich Menschen, die sie abschalten könnten. Vorsichtsmaßnahmen Das Problem der AI-Kontrolle ist die Frage, wie ein Superintelligenter, der seine Urheber unterstützt, aufgebaut werden kann und unabsichtlich einen Super-Indikator schaffen kann, der seine Schöpfer schädigen wird. Manche Wissenschaftler sprachen sich dafür aus, dass Lösungen für das Kontrollproblem auch Anwendungen in bestehenden nicht-kontrollintelligenten AI finden könnten. Kernpunkte des Kontrollproblems sind die Angleichung, die darauf abzielt, die AI-Zielsysteme an die menschlichen Werte anzugleichen und die Fähigkeitskontrollen zu kontrollieren, die die Fähigkeit eines AI-Systems verringern, den Menschen zu schädigen oder die Kontrolle zu gewinnen. Ein Beispiel für "Kapazitätskontrolle" ist die Forschung, ob eine Super-Intelligence AI erfolgreich in einem "AI Box" eingeschränkt werden könnte. Laut Bostrom sind solche Kapazitätskontrollvorschläge nicht zuverlässig oder ausreichend, um das Kontrollproblem langfristig zu lösen, können aber möglicherweise als wertvolle Ergänzung zur Annäherung dienen. Warnungen Physicist Stephen Hawking, Microsoft Gründer Bill Gates und der Gründer von SpaceX Elon Musk haben Bedenken über die Möglichkeit geäußert, dass sich die AI auf den Punkt entwickeln könnte, den der Mensch nicht kontrollieren konnte, und dass dies "das Ende des menschlichen Rennens bremsen könnte". Stephen Johnsoning sagte im Jahr 2014, dass „Success to make AI die größte Veranstaltung in der menschlichen Geschichte sein würde. Leider könnte es auch die letzte sein, es sei denn, wir lernen, wie die Risiken vermieden werden können." Man glaubte, dass die AI in den kommenden Jahrzehnten "inkalkulierbare Vorteile und Risiken" anbieten könnte, wie "die intelligente Finanzmärkte, die Entfaltung menschlicher Forscher, die Enteignung menschlicher Führer und die Entwicklung von Waffen, die wir nicht einmal verstehen können. " Nick Bostrom kam im Januar 2015 zu Stephen Hawking, Max Tegmark, Elon Musk, Lord Martin Rees, Jaan Tallinn und zahlreiche AI-Forscher unterzeichnen in Unterzeichnung des offenen Schreibens des Life Institute die potenziellen Risiken und Vorteile, die mit künstlichen Erkenntnissen verbunden sind. Die Unterzeichnerinnen und Unterzeichner "den Nachweis, dass die Forschung darüber, wie AI-Systeme robust und nutzbringend sind, sowohl wichtig als auch rechtzeitig ist und dass es konkrete Forschungslinien gibt, die heute verfolgt werden können." Siehe auch Verweise auf externe Links Automation, keine beherrschende Stellung: Wie Roboter werden unsere Welt übernehmen (eine positive Aussicht auf Roboter- und AI-Integration in die Gesellschaft). Technologie-Forschungsinstitut: offizielles MIRI (vormaliges Institut für Künstliche Intelligenz) Website Life Boots Foundation AIhim (zum Schutz vor unfreundlicher AI)Ted-Diskussion: Kann man AI bauen, ohne die Kontrolle über sie zu verlieren?