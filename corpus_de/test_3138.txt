Peer-to-peer (P2P) Computing oder Networking ist eine verteilte Anwendungsarchitektur, die Aufgaben oder Workloads zwischen Peers verteilt. Peers sind ebenso privilegierte, ausrüstende Teilnehmer an der Anwendung. Sie sollen ein Peer-to-Peer-Netzwerk von Knoten bilden. Peers stellt einen Teil ihrer Ressourcen, wie z.B. die Verarbeitung von Strom, Festplattenspeicher oder Netzwerkbandbreite, direkt anderen Netzwerkteilnehmern zur Verfügung, ohne dass eine zentrale Koordination durch Server oder stabile Hosts erforderlich ist. Peers sind sowohl Lieferanten als auch Verbraucher von Ressourcen, im Gegensatz zu dem traditionellen Client-Server-Modell, in dem der Verbrauch und die Bereitstellung von Ressourcen geteilt wird. Während P2P-Systeme bisher in vielen Anwendungsbereichen eingesetzt wurden, wurde die Architektur durch das 1999 veröffentlichte Dateifreigabesystem Napster populär. Das Konzept hat in vielen Bereichen der menschlichen Interaktion neue Strukturen und Philosophien inspiriert. In solchen sozialen Kontexten bezieht sich Peer-to-Peer als Meme auf die egalitäre soziale Vernetzung, die in der gesamten Gesellschaft entstanden ist, die durch Internet-Technologien im Allgemeinen ermöglicht wird. Historische Entwicklung Während P2P-Systeme bisher in vielen Anwendungsbereichen eingesetzt wurden, wurde das Konzept von Dateifreigabesystemen wie der Musikaustausch-Anwendung Napster (ursprünglich 1999 veröffentlicht) populär gemacht. Die Peer-to-Peer-Bewegung ermöglichte Millionen von Internet-Nutzern, "direkt zu verbinden, Gruppen zu bilden und zusammenzuarbeiten, um nutzergeschaffene Suchmaschinen, virtuelle Supercomputer und Dateisysteme zu werden". Das Grundkonzept der Peer-to-Peer-Computing wurde in früheren Softwaresystemen und Networking-Diskussionen vorgestellt, die auf Prinzipien zurückgreifen, die in der ersten Anfrage für Kommentare, RFC 1 genannt wurden. Tim Berners-Lees Vision für das World Wide Web war in der Nähe eines P2P-Netzwerks, indem es angenommen, dass jeder Benutzer des Internets ein aktiver Editor und Beitrag, Erstellung und Verknüpfung von Inhalten zu einem vernetzten Web von Links. Das frühe Internet war offener als heute, wo zwei mit dem Internet verbundene Maschinen Pakete ohne Firewalls und andere Sicherheitsmaßnahmen aneinander senden konnten. Dies steht im Gegensatz zur strahlenartigen Struktur des Internets, wie es sich im Laufe der Jahre entwickelt hat. Als Vorläufer im Internet war ARPANET ein erfolgreiches Client-Server-Netzwerk, in dem "jeder teilnehmende Knoten Inhalte anfordern und bedienen konnte. " ARPANET war jedoch nicht selbstorganisiert, und es fehlte an der Fähigkeit, "eine Möglichkeit für Kontext- oder Content-basierte Routing über einfache adressbasierte Routing zu "vorzustellen. " Daher wurde Usenet, ein verteiltes Messaging-System, das oft als frühe Peer-to-Peer-Architektur bezeichnet wird, gegründet. Es wurde 1979 als System entwickelt, das ein dezentrales Kontrollmodell durchsetzt. Das Grundmodell ist ein Client-Server-Modell aus der Benutzer- oder Client-Perspektive, das einen selbstorganisierenden Ansatz für Newsgroup-Server bietet. Nachrichtenserver kommunizieren jedoch untereinander als Peers, um Usenet-Nachrichtenartikel über die gesamte Gruppe von Netzwerkservern zu propagieren. Das gleiche gilt für SMTP-E-Mails in dem Sinne, dass das Kern-E-Mail-Relaying-Netzwerk von Postübertragungsmitteln einen Peer-to-Peer-Charakter hat, während die Peripherie von E-Mail-Clients und deren direkte Verbindungen streng eine Client-Server-Beziehung ist. Im Mai 1999, mit Millionen mehr Menschen im Internet, Shawn Fanning führte die Musik und Datei-Sharing-Anwendung namens Napster. Napster war der Anfang von Peer-to-Peer-Netzwerken, wie wir sie heute kennen, wo "die Teilnehmer ein virtuelles Netzwerk aufbauen, völlig unabhängig vom physischen Netzwerk, ohne irgendwelche Verwaltungsbehörden oder Einschränkungen zu gehorchen". Architektur Ein Peer-to-Peer-Netzwerk ist um den Begriff der gleichen Peer-Knoten entworfen, die gleichzeitig als Clients und Server zu den anderen Knoten im Netzwerk funktionieren. Dieses Modell der Netzwerk-Anordnung unterscheidet sich vom Client-Server-Modell, wo die Kommunikation in der Regel zu und von einem zentralen Server ist. Ein typisches Beispiel für eine Dateiübertragung, die das Client-Server-Modell verwendet, ist der File Transfer Protocol (FTP) Service, in dem die Client- und Server-Programme deutlich sind: Die Clients starten die Übertragung und die Server erfüllen diese Anfragen. Routing und Ressourcen-Erkennung Peer-to-Peer-Netzwerke implementieren in der Regel eine Form von virtuellem Overlay-Netzwerk auf der physischen Netzwerk-Topologie, wo die Knoten in der Overlay eine Teilmenge der Knoten im physischen Netzwerk bilden. Die Daten werden noch direkt über das zugrunde liegende TCP/IP-Netzwerk ausgetauscht, aber an der Applikationsschicht können Peers direkt über die logischen Overlay-Links miteinander kommunizieren (jeweils einem Pfad durch das darunterliegende physikalische Netzwerk entspricht). Overlays werden für die Indexierung und Peer Discovery verwendet und machen das P2P-System unabhängig von der physikalischen Netzwerktopologie. Basierend darauf, wie die Knoten im Overlay-Netzwerk miteinander verknüpft sind und wie Ressourcen indiziert und lokalisiert werden, können wir Netzwerke als unstrukturiert oder strukturiert (oder als Hybrid zwischen den beiden) klassifizieren. Unstrukturierte Netze Unstrukturierte Peer-to-Peer-Netzwerke verleihen dem Overlay-Netzwerk keine besondere Struktur, sondern werden durch Knoten gebildet, die zufällig Verbindungen zueinander bilden.(Gnutella, Gossip und Kazaa sind Beispiele für unstrukturierte P2P-Protokolle). Denn es gibt keine weltweit auf sie auferlegte Struktur, unstrukturierte Netzwerke sind einfach zu bauen und ermöglichen lokalisierte Optimierungen in verschiedene Bereiche des Overlay. Auch, weil die Rolle aller Peers im Netzwerk gleich ist, sind unstrukturierte Netzwerke angesichts hoher Chur-Raten sehr robust", d.h., wenn viele Peers häufig ins Netz kommen und verlassen. Aus diesem Mangel an Struktur ergeben sich jedoch auch die primären Einschränkungen unstrukturierter Netze. Insbesondere, wenn ein Peer ein gewünschtes Datenstück im Netzwerk finden will, muss die Suchanfrage über das Netzwerk geflutet werden, um möglichst viele Peers zu finden, die die Daten teilen. Fluten verursacht eine sehr hohe Anzahl von Signalisierungs-Verkehr im Netzwerk, verwendet mehr CPU/Speicher (durch die Anforderung jedes Peer, alle Suchanfragen zu bearbeiten), und stellt nicht sicher, dass Suchanfragen immer behoben werden. Da zwischen einem Peer und dem von ihm verwalteten Inhalt keine Korrelation besteht, besteht keine Garantie dafür, dass Überschwemmungen einen Peer finden, der die gewünschten Daten aufweist. Beliebte Inhalte sind wahrscheinlich bei mehreren Peers verfügbar und jedes Peer, das nach ihm sucht, ist wahrscheinlich das gleiche zu finden. Aber wenn ein Peer auf der Suche nach seltenen Daten, die von nur einigen anderen Peers geteilt werden, ist es sehr unwahrscheinlich, dass die Suche erfolgreich sein wird. Strukturierte Netze In strukturierten Peer-to-Peer-Netzwerken wird das Overlay in eine bestimmte Topologie organisiert, und das Protokoll stellt sicher, dass jeder Knoten das Netzwerk effizient für eine Datei/Ressource suchen kann, auch wenn die Ressource extrem selten ist. Die häufigste Art von strukturierten P2P-Netzwerken implementieren eine verteilte Hash-Tabelle (DHT), in der eine Variante der konsequenten Hashing verwendet wird, um das Eigentum jeder Datei einem bestimmten Peer zuzuordnen. Dies ermöglicht es Peers, mittels einer Hash-Tabelle nach Ressourcen im Netzwerk zu suchen: d.h. (Schlüssel, Wert)-Paare werden im DHT gespeichert und jeder teilnehmende Knoten kann den mit einem bestimmten Schlüssel verbundenen Wert effizient abrufen. Um den Verkehr jedoch effizient durch das Netzwerk zu führen, müssen Knoten in einem strukturierten Overlay Listen von Nachbarn aufrecht erhalten, die bestimmte Kriterien erfüllen. Dies macht sie weniger robust in Netzwerken mit einer hohen Rate von churn (d.h. mit großen Anzahl von Knoten häufig verbinden und verlassen das Netzwerk). Neuere Auswertungen von P2P-Ressourcen-Entdeckungslösungen unter realen Workloads haben mehrere Probleme in DHT-basierten Lösungen wie hohe Kosten für Werbung/Entdeckung von Ressourcen und statische und dynamische Belastungsungleichgewichte aufgezeigt. Wichtige verteilte Netzwerke, die DHTs verwenden, sind Tixati, eine Alternative zu BitTorrents verteiltem Tracker, dem Kad-Netzwerk, dem Storm Botnet, YaCy und dem Coral Content Distribution Network. Einige prominente Forschungsprojekte umfassen das Chord-Projekt, Kademlia, PAST-Speicherprogramm, P-Grid, ein selbstorganisiertes und aufstrebendes Overlay-Netzwerk und CoopNet-Inhalte-Verteilungssystem. DHT-basierte Netzwerke wurden auch weit verbreitet, um eine effiziente Ressourcen-Entdeckung für Netz Computing-Systeme zu erreichen, da es bei der Ressourcenverwaltung und der Planung von Anwendungen hilft. Hybrid-Modelle Hybrid-Modelle sind eine Kombination von Peer-to-Peer- und Client-Server-Modellen. Ein gemeinsames Hybrid-Modell ist, einen zentralen Server zu haben, der hilft, sich gegenseitig zu finden. Spotify war ein Beispiel eines Hybridmodells [bis 2014]. Es gibt eine Vielzahl von Hybrid-Modellen, die alle Trade-offs zwischen der zentralisierten Funktionalität eines strukturierten Server/Client-Netzwerks und der Knotengleichheit machen, die von den reinen Peer-to-Peer unstrukturierten Netzwerken bereitgestellt wird. Derzeit haben Hybridmodelle eine bessere Leistung als reine unstrukturierte Netzwerke oder reine strukturierte Netzwerke, da bestimmte Funktionen wie Suchen eine zentrale Funktionalität erfordern, aber von der dezentralen Aggregation von Knoten profitieren, die von unstrukturierten Netzwerken bereitgestellt werden. CoopNet Content Distribution System CoopNet (Cooperative Networking) war ein vorgeschlagenes System für Off-loading, das Peers diente, die kürzlich Inhalte heruntergeladen haben, vorgeschlagen von Informatikern Venkata N. Padmanabhan und Kunwadee Sripanidkulchai, die an der Microsoft Research und Carnegie Mellon University arbeiten. Wenn ein Server eine Belastungssteigerung erfährt, leitet er ankommende Peers an andere Peers um, die vereinbart haben, den Inhalt zu spiegeln und somit die Balance vom Server aus zu laden. Alle Informationen werden auf dem Server gespeichert. Dieses System nutzt die Tatsache, dass der Engpass in der ausgehenden Bandbreite am wahrscheinlichsten ist als die CPU, also sein serverzentriertes Design. Es ordnet seinen Nachbarn Peers anderen Peers zu, die "in IP schließen" sind, in einem Versuch, Lokalität zu verwenden. Wenn mehrere Peers mit der gleichen Datei gefunden werden, bezeichnet es, dass der Knoten die schnellste seiner Nachbarn wählen. Streaming-Medien werden übertragen, indem Clients den vorherigen Stream abspeichern und dann stückweise an neue Knoten übertragen. Sicherheit und Vertrauen Peer-to-Peer-Systeme stellen einzigartige Herausforderungen aus der Sicht der Computersicherheit. Wie jede andere Form von Software, P2P-Anwendungen können Schwachstellen enthalten. Was dies für P2P-Software besonders gefährlich macht, ist jedoch, dass Peer-to-Peer-Anwendungen als Server und Clients fungieren, was bedeutet, dass sie für Remote Exploits empfindlicher sein können. Angriffe Da jeder Knoten eine Rolle beim Routing-Verkehr durch das Netzwerk spielt, können bösartige Benutzer eine Vielzahl von "Routing-Angriffen" oder Denial von Service-Angriffen durchführen. Beispiele für häufige Routing-Angriffe sind "unkorrektes Lookup Routing", bei dem schädliche Knoten bewusst Anfragen falsch weiterleiten oder falsche Ergebnisse zurückgeben, "richtige Routing-Updates", bei denen schädliche Knoten die Routing-Tabellen benachbarter Knoten durch Senden von ihnen falsche Informationen, und "unkorrekt Routing-Netzwerk-Partition" wo, wenn neue Knoten werden sie Boottrap über einen schädlichen, die neuen Knoten, die den neuen Knoten, die platziert den neuen Knoten, einem Beschädigte Daten und Malware Die Prävalenz von Malware variiert zwischen verschiedenen Peer-to-Peer-Protokollen. Studien analysieren die Verbreitung von Malware auf P2P-Netzwerken gefunden, zum Beispiel, dass 63% der beantworteten Download-Anfragen auf dem Gnutella-Netzwerk enthalten einige Form von Malware, während nur 3% der Inhalte auf OpenFT enthalten Malware. In beiden Fällen entfielen die drei häufigsten Malware-Typen auf die große Mehrheit der Fälle (99% in Gnutella, und 65% in OpenFT). Eine weitere Studie zur Analyse des Verkehrs auf dem Kasa-Netzwerk ergab, dass 15% der 500.000 aufgenommenen Dateiprobe von einem oder mehreren der 365 verschiedenen Computerviren infiziert wurden, auf die getestet wurden. Korrupte Daten können auch auf P2P-Netzwerken verteilt werden, indem Dateien geändert werden, die bereits im Netzwerk geteilt werden. Zum Beispiel gelang es der RIAA im FastTrack-Netzwerk, gefälschte Stücke in Downloads und heruntergeladene Dateien einzuführen (meist MP3-Dateien). Dateien, die mit dem RIAA Virus infiziert wurden, waren nach und nach unbrauchbar und enthielten bösartigen Code. Die RIAA ist auch bekannt, gefälschte Musik und Filme in P2P-Netzwerke hochgeladen zu haben, um illegale Dateifreigabe zu vernichten. Daher haben die P2P-Netzwerke von heute eine enorme Erhöhung ihrer Sicherheits- und Dateiverifikationsmechanismen gesehen. Moderne Hashing, Verschlüsselung und verschiedene Verschlüsselungsverfahren haben die meisten Netzwerke beständig gegen fast jede Art von Angriff gemacht, auch wenn große Teile des jeweiligen Netzwerks durch gefälschte oder nicht-funktionale Hosts ersetzt wurden. Belastbare und skalierbare Computernetze Die dezentrale Natur der P2P-Netzwerke erhöht die Robustheit, da sie den einzigen Fehlerpunkt entfernt, der in einem Client-Server-basierten System inhärent sein kann. Da Knoten ankommen und die Nachfrage auf dem System steigt, erhöht sich auch die Gesamtkapazität des Systems und die Wahrscheinlichkeit des Ausfalls sinkt. Wenn ein Peer im Netzwerk nicht richtig funktioniert, wird das gesamte Netzwerk nicht beeinträchtigt oder beschädigt. In einer typischen Client-Server-Architektur teilen die Kunden hingegen nur ihre Anforderungen an das System, nicht aber ihre Ressourcen. In diesem Fall, da mehr Clients dem System beitreten, stehen für jeden Client weniger Ressourcen zur Verfügung, und wenn der zentrale Server ausfällt, wird das gesamte Netzwerk abgeschaltet. Verteilte Lagerung und Suche Es gibt sowohl Vorteile als auch Nachteile in P2P-Netzwerken im Zusammenhang mit dem Thema Datensicherung, Wiederherstellung und Verfügbarkeit. In einem zentralisierten Netzwerk sind die Systemadministratoren die einzigen Kräfte, die die Verfügbarkeit von Dateien steuern, die geteilt werden. Entscheiden die Administratoren, eine Datei nicht mehr zu verteilen, müssen sie sie einfach von ihren Servern entfernen, und sie wird den Benutzern nicht mehr zur Verfügung stehen. Zusammen mit dem Verlassen der Nutzer machtlos bei der Entscheidung, was in der gesamten Gemeinschaft verteilt wird, macht dies das gesamte System an Bedrohungen und Forderungen der Regierung und anderer großer Kräfte anfällig. Zum Beispiel wurde YouTube von der RIAA-, MPAA- und Unterhaltungsindustrie unter Druck gesetzt, um urheberrechtlich geschützte Inhalte auszufiltern. Obwohl Server-Client-Netzwerke in der Lage sind, die Verfügbarkeit von Inhalten zu überwachen und zu verwalten, können sie mehr Stabilität in der Verfügbarkeit der Inhalte haben, die sie für den Host wählen. Ein Client sollte keine Probleme haben, auf obskure Inhalte zuzugreifen, die auf einem stabilen zentralen Netzwerk geteilt werden. P2P-Netzwerke sind jedoch unzuverlässiger bei der Freigabe von unpopulären Dateien, weil die Freigabe von Dateien in einem P2P-Netzwerk erfordert, dass mindestens ein Knoten im Netzwerk die angeforderten Daten aufweist, und dieser Knoten muss in der Lage sein, mit dem die Daten anfordernden Knoten zu verbinden. Diese Anforderung ist gelegentlich schwer zu erfüllen, weil die Nutzer Daten jederzeit löschen oder stoppen können. In diesem Sinne ist die Nutzergemeinschaft in einem P2P-Netzwerk völlig verantwortlich für die Entscheidung, welche Inhalte zur Verfügung stehen. Unpopuläre Dateien werden schließlich verschwinden und nicht verfügbar werden, da mehr Menschen aufhören, sie zu teilen. Beliebte Dateien werden jedoch sehr und leicht verteilt. Beliebte Dateien auf einem P2P-Netzwerk haben tatsächlich mehr Stabilität und Verfügbarkeit als Dateien auf zentralen Netzwerken. In einem zentralisierten Netzwerk genügt ein einfacher Verlust der Verbindung zwischen Server und Clients, um einen Ausfall zu verursachen, aber in P2P-Netzwerken müssen die Verbindungen zwischen jedem Knoten verloren gehen, um einen Datenaustausch zu verursachen. In einem zentralisierten System sind die Administratoren für alle Datenrettung und Backups verantwortlich, während in P2P-Systemen jeder Knoten ein eigenes Backup-System benötigt. Aufgrund des Mangels an zentraler Autorität in P2P-Netzwerken sind Kräfte wie die Aufnahmeindustrie, RIAA, MPAA und die Regierung nicht in der Lage, den Inhalt auf P2P-Systemen zu löschen oder zu stoppen. Anwendungen Inhaltslieferung In P2P-Netzwerken bieten und nutzen Kunden Ressourcen. Dies bedeutet, dass im Gegensatz zu Client-Server-Systemen die Content-serving-Kapazität von Peer-to-Peer-Netzwerken tatsächlich zunehmen kann, da mehr Benutzer beginnen, auf den Inhalt zuzugreifen (insbesondere mit Protokollen wie Bittorrent, die Benutzer benötigen, um zu teilen, eine Performance-Messstudie zu verweisen). Diese Eigenschaft ist einer der Hauptvorteile der Verwendung von P2P-Netzwerken, weil sie die Setup- und Laufkosten für den ursprünglichen Content-Vertrieb sehr klein macht. Filesharing-Netzwerke Viele Datei-Peer-to-Peer-Datei-Sharing-Netzwerke, wie Gnutella, G2, und das eDonkey-Netzwerk populärisierten Peer-to-Peer-Technologien. Peer-to-Peer Content Delivery Networks. Peer-to-Peer-Inhalte-Dienste, z.B. Caches für verbesserte Leistung wie Correli Caches Software Publikation und Distribution (Linux Distribution, mehrere Spiele); über File-Sharing-Netzwerke. Urheberrechtsverletzungen Die Peer-to-Peer-Netzwerkierung beinhaltet die Datenübertragung von einem Benutzer zum anderen ohne Verwendung eines Zwischenservers. Unternehmen, die P2P-Anwendungen entwickeln, sind in zahlreichen Rechtsfällen, vor allem in den Vereinigten Staaten, über Konflikte mit dem Urheberrecht beteiligt. Zwei Hauptfälle sind Grokster vs RIAA und MGM Studios, Inc. v. Grokster, Ltd.. Im letzten Fall hat das Gericht einstimmig festgestellt, dass die Beklagten Peer-to-Peer-Datei-Sharing-Unternehmen Grokster und Streamcast wegen der Verletzung des Urheberrechts verklagt werden könnten. Multimedia Die P2PTV- und PDTP-Protokolle. Einige proprietäre Multimedia-Anwendungen nutzen ein Peer-to-Peer-Netzwerk zusammen mit Streaming-Servern, um Audio und Video an ihre Kunden zu streamen. Peercasting für Multicasting-Streams. Pennsylvania State University, MIT und Simon Fraser University führen ein Projekt namens LionShare, das für die Erleichterung des Aktenaustauschs zwischen Bildungseinrichtungen weltweit konzipiert ist. Osiris ist ein Programm, das es seinen Nutzern ermöglicht, anonyme und autonome Web-Portale zu erstellen, die über P2P-Netzwerk verteilt werden. Das Theta Network ist eine Kryptowährung-Token-Plattform, die Peer-to-Peer Streaming und CDN-Cache ermöglicht. Andere P2P-Anwendungen Bitcoin und weitere wie Ether, Nxt und Peercoin sind peer-to-peer-basierte digitale Kryptowährungen. Dalesa, ein Peer-to-Peer-Web-Cache für LANs (basierend auf IP-Multicasting). Dat, eine verteilte Versionsplattform. Filecoin ist ein Open Source, Public, Kryptocurrency und digitales Zahlungssystem, das als Blockchain-basierte kooperative digitale Speicherung und Datenabrufmethode dienen soll. I2P, ein Overlay-Netzwerk verwendet, um das Internet anonym zu durchsuchen. Im Gegensatz zu dem verwandten I2P ist das Tor-Netzwerk nicht selbst peer-to-peer; es kann jedoch ermöglichen, Peer-to-Peer-Anwendungen über Onion-Dienste darauf zu bauen. Das InterPlanetary File System (IPFS) ist ein Protokoll und ein Netzwerk, das darauf abzielt, eine Content-addressierbare, Peer-to-Peer-Methode zum Speichern und Teilen von Hypermedia Distribution Protokoll zu erstellen. Knoten im IPFS-Netzwerk bilden ein verteiltes Dateisystem. Jami, ein Peer-to-Peer-Chat und SIP-App. JXTA, ein Peer-to-Peer-Protokoll für die Java-Plattform. Netsukuku, ein drahtloses Community-Netzwerk, das unabhängig vom Internet ist. Open Garden, Verbindungsfreigabe-Anwendung, die den Internetzugang mit anderen Geräten mit Wi-Fi oder Bluetooth teilt. Resilio Sync, eine Verzeichnis-Syncing-App. Forschung wie das Chord-Projekt, das PAST-Speicherprogramm, das P-Grid und das CoopNet Content Distribution System. Syncthing, eine Verzeichnis-Syncing-App.Tradepal und M-Commerce-Anwendungen, die Echtzeit-Marktplätze antreiben. Das US-Verteidigungsministerium forscht im Rahmen seiner modernen Netzwerkkriegsstrategie an P2P-Netzwerken. Im Mai 2003 bezeugte Anthony Tether, damals Direktor von DARPA, dass das Militär der Vereinigten Staaten P2P-Netzwerke nutzt. WebTorrent ist ein P2P-Streaming-Client in JavaScript für den Einsatz in Web-Browsern, sowie in der WebTorrent Desktop-Stand alleine Version, die WebTorrent und BitTorrent Serverless-Netzwerke überbrückt. Microsoft in Windows 10 verwendet eine proprietäre Peer-to-Peer-Technologie namens "Delivery Optimization", um Betriebssystem-Updates mit Endbenutzer-PCs entweder auf dem lokalen Netzwerk oder anderen PCs bereitstellen. Laut Microsofts Channel 9 führte es zu einer Reduzierung der Internet-Bandbreitennutzung um 30%-50. Artisofts LANtastic wurde als Peer-to-Peer-Betriebssystem gebaut.Maschinen können sowohl Server als auch Workstations gleichzeitig sein. Soziale Auswirkungen Die Zusammenarbeit zwischen einer Teilnehmergemeinschaft ist entscheidend für den anhaltenden Erfolg von P2P-Systemen, die auf lässige menschliche Nutzer abzielen; diese erreichen ihr volles Potenzial nur, wenn viele Knoten Ressourcen beitragen. Aber in der aktuellen Praxis enthalten P2P-Netzwerke oft eine große Anzahl von Benutzern, die Ressourcen nutzen, die von anderen Knoten geteilt werden, aber die nichts selbst teilen (oft als "Freilade-Problem" bezeichnet). Das Freiladen kann einen tiefgreifenden Einfluss auf das Netzwerk haben und in einigen Fällen kann die Gemeinschaft zu einem Zusammenbruch führen. In diesen Netzen haben die Nutzer natürliche Nachteile, weil die Zusammenarbeit ihre eigenen Ressourcen verbraucht und ihre eigene Leistung abbauen kann. Das Erlernen der sozialen Eigenschaften von P2P-Netzwerken ist aufgrund der großen Umsatzpopulationen, der Asymmetrie von Interesse und der Null-Kosten-Identität eine Herausforderung. Es wurden verschiedene Anreizmechanismen eingeführt, um Knoten zu ermutigen oder sogar zu zwingen, Ressourcen zu leisten. Einige Forscher haben die Vorteile der Selbstorganisation und Einführung von Anreizen für den Ressourcenaustausch und die Zusammenarbeit von virtuellen Gemeinschaften erforscht und argumentiert, dass der soziale Aspekt, der aus den heutigen P2P-Systemen fehlt, sowohl als Ziel als auch als Mittel für selbstorganisierte virtuelle Gemeinschaften betrachtet werden sollte. Die laufenden Forschungsbemühungen zur Gestaltung effektiver Anreizmechanismen in P2P-Systemen, basierend auf Prinzipien der Spieltheorie, beginnen, eine psychologischere und Informationsverarbeitungsrichtung zu übernehmen. Datenschutz und AnonymitätEinige Peer-to-Peer-Netzwerke (z.B. Freenet) legen großen Wert auf Privatsphäre und Anonymität – das heißt, sicherstellen, dass der Inhalt der Kommunikation vor Eavesdroppern verborgen ist und dass die Identitäten/Orts der Teilnehmer verdeckt werden. Mit der Public Key Kryptographie können Verschlüsselung, Datenvalidierung, Autorisierung und Authentifizierung für Daten/Nachrichten bereitgestellt werden. Zwiebel Routing und andere Mischnetzwerkprotokolle (z.B. Tarzan) können zur Anonymität verwendet werden. Täter von Live-Streaming sexuellen Missbrauch und andere Cyberkriminalität haben Peer-to-Peer-Plattformen verwendet, um Aktivitäten mit Anonymität durchzuführen. Politische Implikationen Geistiges Eigentumsrecht und illegaler Austausch Obwohl Peer-to-Peer-Netzwerke für legitime Zwecke verwendet werden können, haben Rechteinhaber Peer-to-Peer über die Beteiligung an der Weitergabe von urheberrechtlich geschütztem Material gezielt. Die Peer-to-Peer-Netzwerkierung beinhaltet die Datenübertragung von einem Benutzer zum anderen ohne Verwendung eines Zwischenservers. Unternehmen, die P2P-Anwendungen entwickeln, sind an zahlreichen Rechtsfällen beteiligt, vor allem in den Vereinigten Staaten, vor allem in Fragen des Urheberrechts. Zwei Hauptfälle sind Grokster vs RIAA und MGM Studios, Inc. v. Grokster, Ltd. In beiden Fällen wurde die Datei-Sharing-Technologie als legal angesehen, solange die Entwickler keine Möglichkeit hatten, die Weitergabe des urheberrechtlich geschützten Materials zu verhindern. Zur strafrechtlichen Haftung für die Urheberrechtsverletzung auf Peer-to-Peer-Systemen muss die Regierung nachweisen, dass der Beklagte ein Urheberrecht zum Zwecke des persönlichen finanziellen Gewinns oder kommerziellen Vorteils freiwillig verletzt hat. Mit Ausnahme der fairen Nutzung kann ein eingeschränkter Gebrauch von urheberrechtlich geschütztem Material heruntergeladen werden, ohne die Rechteinhaber zu berechtigen. Diese Dokumente sind in der Regel Nachrichtenberichterstattung oder unter den Zeilen der Forschung und wissenschaftlichen Arbeit. Kontroversen haben sich im Hinblick auf die unerlaubte Nutzung von Peer-to-Peer-Netzwerken hinsichtlich der öffentlichen Sicherheit und der nationalen Sicherheit entwickelt. Wenn eine Datei über ein Peer-to-Peer-Netzwerk heruntergeladen wird, ist es unmöglich zu wissen, wer die Datei erstellt hat oder welche Benutzer zu einem bestimmten Zeitpunkt mit dem Netzwerk verbunden sind. Die Vertrauenswürdigkeit der Quellen ist eine potenzielle Sicherheitsbedrohung, die mit Peer-to-Peer-Systemen zu sehen ist. Eine von der Europäischen Union bestellte Studie ergab, dass illegales Herunterladen zu einer Erhöhung der gesamten Videospielverkäufe führen kann, weil neuere Spiele für zusätzliche Funktionen oder Levels berechnen. Das Papier kam zu dem Schluss, dass Piraterie negative finanzielle Auswirkungen auf Filme, Musik und Literatur hatte. Die Studie stützte sich auf selbst gemeldete Daten über Spielkäufe und Nutzung illegaler Download-Seiten. Schmerzen wurden genommen, um Effekte von falschen und falschen Reaktionen zu entfernen. Netzwerkneutralität Peer-to-Peer-Anwendungen stellen eines der Kernprobleme in der Netzwerkneutralitätskontroverse dar. Internet-Dienstleister (ISPs) sind bekannt, den P2P-Datei-Sharing-Verkehr aufgrund seiner hohen Bandbreitennutzung zu drosseln. Im Vergleich zu Web-Browsing, E-Mail oder vielen anderen Nutzungen des Internets, bei denen Daten nur in kurzen Abständen und relativ geringen Mengen übertragen werden, besteht die P2P-Datei-Sharing häufig aus einer relativ hohen Bandbreitennutzung durch laufende Dateiübertragungen und swarm/network-Koordinationspakete. Im Oktober 2007 begann Comcast, einer der größten Breitband-Internetanbieter in den USA, P2P-Anwendungen wie BitTorrent zu blockieren. Ihre Rationalität war, dass P2P hauptsächlich verwendet wird, um illegale Inhalte zu teilen, und ihre Infrastruktur ist nicht für kontinuierlichen, hochbandbreiten Verkehr konzipiert. Kritiker weisen darauf hin, dass P2P-Netzwerke legitime rechtliche Nutzungen haben, und dass dies eine andere Art ist, dass große Anbieter versuchen, die Nutzung und den Inhalt im Internet zu kontrollieren, und Menschen auf eine Client-Server-basierte Anwendungsarchitektur zu lenken. Das Client-Server-Modell bietet finanzielle Barrieren für kleine Verlage und Einzelpersonen und kann weniger effizient für den Austausch großer Dateien sein. Als Reaktion auf diese Bandbreitendrosselung begannen mehrere P2P-Anwendungen, die Protokollobfuskation, wie die BitTorrent Protokollverschlüsselung, implementieren. Die Techniken zum Erreichen der "Protokollobfuskation" beinhalten das Entfernen von ansonsten leicht identifizierbaren Eigenschaften von Protokollen, wie deterministische Byte-Sequenzen und Paketgrößen, indem die Daten wie zufällig aussehen. Die Lösung des ISP auf die hohe Bandbreite ist P2P-Caching, wo ein ISP den Teil von Dateien speichert, die am meisten von P2P-Clients aufgerufen werden, um den Zugriff auf das Internet zu speichern. Aktuelle Forscher haben Computersimulationen verwendet, um das komplexe Verhalten von Individuen im Netzwerk zu verstehen und zu bewerten. "Netzwerkforschung setzt häufig auf Simulation, um neue Ideen zu testen und auszuwerten. Eine wichtige Voraussetzung für diesen Prozess ist, dass die Ergebnisse reproduzierbar sein müssen, damit andere Forscher bestehende Arbeiten replizieren, validieren und erweitern können". Wenn die Forschung nicht reproduziert werden kann, wird die Möglichkeit zur weiteren Forschung behindert. " Auch wenn neue Simulatoren weiterhin freigegeben werden, neigt die Forschungsgemeinschaft zu nur einer Handvoll Open-Source-Simulatoren. Die Nachfrage nach Features in Simulatoren, wie nach unseren Kriterien und Umfragen gezeigt, ist hoch. Daher sollte die Community zusammenarbeiten, um diese Funktionen in Open-Source-Software zu erhalten. Dies würde den Bedarf an benutzerdefinierten Simulatoren verringern und damit die Wiederholbarkeit und Reputierbarkeit von Experimenten erhöhen. "Neben allen oben genannten Tatsachen wurde auf ns-2 Open Source Netzwerk-Simulator gearbeitet. Eine Forschungsfrage im Zusammenhang mit der freien Fahrererkennung und Strafe wurde hier mit ns-2 Simulator untersucht. Siehe auch Referenzen = Externe Links ==