Die Faktorenanalyse ist eine statistische Methode, mit der die Variabilität der beobachteten, korrelierten Variablen in Bezug auf eine möglicherweise geringere Anzahl nicht beobachteter Variablen, die Faktoren genannt werden, beschrieben wird. Beispielsweise ist es möglich, dass Variationen in sechs beobachteten Variablen vor allem die Variationen in zwei nicht beobachteten (unterliegenden) Variablen widerspiegeln. Faktorenanalyse sucht nach solchen gemeinsamen Variationen in Reaktion auf nicht beobachtete latente Variablen. Die beobachteten Variablen werden als lineare Kombinationen der potenziellen Faktoren modelliert, plus Fehlerbegriffe. Einfach ausgedrückt, quantifiziert die Faktorbelastung einer Variable das Ausmaß, in dem die Variable mit einem bestimmten Faktor verknüpft ist. Eine gemeinsame Begründung hinter Faktoranalytischen Methoden ist, dass die über die Interdependenzen zwischen beobachteten Variablen gewonnenen Informationen später verwendet werden können, um den Satz von Variablen in einem Datensatz zu reduzieren. Die Faktorenanalyse wird häufig in Biologie, Psychometrie, Persönlichkeitstheorien, Marketing, Produktmanagement, Betriebsforschung und Finanzen verwendet. Es kann helfen, mit Datensätzen zu umgehen, wo es große Anzahl von beobachteten Variablen gibt, die gedacht sind, eine kleinere Anzahl von zugrunde liegenden / latenten Variablen zu reflektieren. Es ist eine der am häufigsten verwendeten Interdependenztechniken und wird verwendet, wenn die relevanten Variablen eine systematische Interdependenz zeigen und das Ziel ist es, die latenten Faktoren herauszufinden, die eine Gemeinsamkeit schaffen. Statistisches Modell Definition Das Modell versucht, eine Reihe von p-Beobachtungen in jedem von n Individuen mit einer Reihe von k gemeinsamen Faktoren (F) zu erklären, in denen es weniger Faktoren pro Einheit gibt als Beobachtungen pro Einheit (k<p). Jede Person hat k ihrer eigenen gemeinsamen Faktoren, und diese sind mit den Beobachtungen über Faktorladematrix (L ε R p × k {\displaystyle L\in \mathbb {R} {^p\times k} ), für eine einzelne Beobachtung, gemäß x i, m -μ i = l i, 1 f 1 , m + ⋯ + lstyle i, k f k, m + ε i m}-\mu i}=l_{i,1}f_{1,m}+\dots L_{i,k}f_{k,m}+\epsilon {_i,m} wobei ε i, m {\displaystyle \epsilon {_i,m} der unbeobachtete stochastische Fehlerterm mit mittlerer Null- und endlicher Varianz ist, und μ i {\displaystyle \mu {_i} die Beobachtung für die ith-Beobachtung bedeutet. In der Matrix-Notation X - M = L + ε {\displaystyle X-\mathr {M} =LF+\epsilon }, wobei Beobachtungsmatrix X ε R p × n {\displaystyle X\in \mathbb {R} {p\times n}, Faktormatrix F ε R k × n {\displaystyle F\in \mathbb {R} .Also werden wir die folgenden Annahmen auf F {\displaystyle F} verhängen : F und ε {\displaystyle \epsilon } sind unabhängig. E (F ) = 0 {\displaystyle \mathrm {E} (F) = 0}; wobei E = Erwartungen C o v (F) = I {\displaystyle \mathrm {Cov} (F)=I} wobei Cov die Kovarianzmatrix ist, um sicherzustellen, dass die Faktoren unkorreliert sind, und ich ist die Identitätsmatrix. Angenommen C o v ( X - M ) = Σ {\displaystyle \mathrm {Cov} (X-\mathrm {M} =\)Sigma } .Then Σ = C o v (X - M ) = C o v (L F + ε ), {\displaystyle\Sigma =\mathrm {Cov} (X-\mathrm {M} =\) C o v ( ε ) {\displaystyle \Psi =\mathrm {Cov} \(epsilon )}, Σ = L L T + Ψ . {\displaystyle \Sigma =LL^{T}+\Psi .\,} Beachten Sie, dass für jede orthogonale Matrix Q, wenn wir L' = L Q {\displaystyle L^{\prime = LQ und F' = Q T F {\displaystyle F^{\prime =Q^{T}F , die Kriterien für das Sein von Faktoren und Faktorbelastungen noch bestehen. Somit ist eine Reihe von Faktoren und Faktorbelastungen einzigartig nur bis zu einer orthogonalen Transformation. Beispiel Angenommen, ein Psychologe hat die Hypothese, dass es zwei Arten von Intelligenz, "verbale Intelligenz" und "mathematische Intelligenz", von denen keiner direkt beobachtet wird. Nachweis für die Hypothese wird in den Prüfungspunkten aus je 10 verschiedenen akademischen Bereichen von 1000 Studenten gesucht. Wenn jeder Schüler zufällig aus einer großen Bevölkerung ausgewählt wird, sind die 10 Punkte jedes Schülers zufällige Variablen. Die Hypothese des Psychologen kann sagen, dass für jeden der 10 akademischen Felder, die Punktzahl gemittelt über die Gruppe aller Studenten, die einige gemeinsame Werte für verbale und mathematische Intelligenz zu teilen ist einige konstante Zeiten ihr Niveau der verbalen Intelligenz plus andere konstante Zeiten ihr Niveau der mathematischen Intelligenz, d.h. es ist eine lineare Kombination dieser beiden Faktoren". Die Zahlen für ein bestimmtes Thema, mit denen die beiden Arten von Intelligenz multipliziert werden, um die erwartete Punktzahl zu erhalten, werden durch die Hypothese, die für alle Intelligenz Level Paare gleich ist, und werden für dieses Thema als "Faktorladung" bezeichnet. Beispielsweise kann die Hypothese halten, dass die prognostizierte durchschnittliche studentische Eignung im Bereich der Astronomie ist {10 × die verbale Intelligenz des Schülers} + {6 × die mathematische Intelligenz des Schülers}. Die Zahlen 10 und 6 sind die mit der Astronomie verbundenen Faktorbelastungen. Andere akademische Fächer können unterschiedliche Faktorbelastungen haben. Zwei Studenten, die annahmen, identische Grade von verbalen und mathematischen Intelligenz zu haben, können unterschiedliche gemessene Neigungen in der Astronomie haben, weil einzelne Breiten von durchschnittlichen Breitengraden (vorhergesagt) und wegen des Messfehlers selbst. Solche Unterschiede bilden das, was gemeinsam als Fehler bezeichnet wird — ein statistischer Begriff, der bedeutet, dass der Betrag, um den ein Individuum, gemessen, von dem abweicht, was durch seine oder seine Intelligenz prognostiziert wird (siehe Fehler und Reste in der Statistik). Die beobachtbaren Daten, die in die Faktoranalyse gehen, wären 10 Punkte jeder der 1000 Studenten, insgesamt 10.000 Zahlen. Der Faktor Belastungen und Ebenen der beiden Arten von Intelligenz jedes Schülers müssen aus den Daten abgeleitet werden. Mathematisches Modell des gleichen Beispiels Im folgenden werden Matrizen durch Indexvariablen angezeigt. "Subjekt-Indizes werden mit Buchstaben a {\displaystyle a}, b {\displaystyle b} und c {\displaystyle c} angezeigt, wobei Werte von 1 {\displaystyle 1} bis p {\displaystyle p} laufen, die 10 {\displaystyle 10} im obigen Beispiel sind. Faktorindizes werden mit den Buchstaben p {\displaystyle p}, q {\displaystyle q} und r {\displaystyle r} angezeigt, wobei Werte von 1 {\displaystyle 1} bis k {\displaystyle k} laufen, die gleich 2 {\displaystyle 2} im obigen Beispiel sind. Indizes der Instanz oder der Stichprobe werden mit den Buchstaben i {\displaystyle i}, j {\displaystyle j} und k {\displaystyle k} angegeben, mit Werten, die von 1 {\displaystyle 1} zu N {\displaystyle N} laufen. Ziel der Faktoranalyse ist es, die Korrelationen zwischen den Variablen x a {\displaystyle x_{a} zu charakterisieren, von denen die x a i {\displaystyle x_{ai} eine bestimmte Instanz oder Beobachtungsreihe sind. Damit die Variablen auf gleichem Fuß stehen, werden sie in Standard-Scores z {\displaystyle z} normiert: z a i = x a i - μ a σ a {\displaystyle z} z_{ai}={\frac x_{ai}-\mu_a}{\sigma {_a}, wobei das Probenmittel ist: μ a = 1 N Σ i x a i {\displaystyle \mu _a}={\tfrac 1}{N}\sum i}x_{ai und die Probenvarianz ist gegeben durch: σ a 2 = 1 N - 1 Σ i ( x a i - μ a ) 2 {\displaystyle \sigma * 1{N-1}\sum i}(x_{ai}-\mu_a}^^^^^^^^^ Das Faktoranalysemodell für diese bestimmte Stichprobe ist dann: z 1 , i = l 1 , 1 F 1 , i + l 1 , 2 F 2 , i + ε 1 , i ⋮ ⋮ ⋮ ⋮ ⋮ , z 10 , i = l 10 , 1 F 1 , i + l 10 , 2 F 2 & ε 10 , i {\displaystyle begin{}z_{1, 1.1}F_{1,i}&+&ull 1.2}F_{2,i}&+&\varepsilon {_1,i}\vdots \&vdots \&vdots \&vdots \&vdots z_{10,i}&=&\ell 10,1 10,2}F_{2,i}&+&\varepsilon _10,i}\end{matrix oder mehr succinctly: z a i = Σ p l a p f p i + ε a i {\displaystyle Z_{ai) {_p}\ell ap}F_{pi}+\varepsilon {_ai} wobei F 1 i {\displaystyle F_{1i} die i {\displaystyle i} die "verbale Intelligenz" des Schülers ist, F 2 i {\displaystyle F_{2i} ist die i {\displaystyle i} die "mathematische Intelligenz des Schülers" ε {\displaystyle Z=LF+\varepsilon Beachten Sie, dass durch die Verdoppelung der Skala, auf der "verbale Intelligenz" - die erste Komponente in jeder Spalte von F {\displaystyle F} - gemessen wird, und gleichzeitig die Halbierung der Faktorbelastungen für verbale Intelligenz macht keinen Unterschied zum Modell. So geht keine Allgemeinheit verloren, indem man annimmt, dass die Standardabweichung der Faktoren für verbale Intelligenz 1 ist {\displaystyle 1 .Likewise für mathematische Intelligenz. Außerdem geht aus ähnlichen Gründen keine Allgemeinheit verloren, indem man annimmt, dass die beiden Faktoren nicht miteinander korrelieren. Mit anderen Worten: Σ i F p i F q i = δ p q {\displaystyle \sum i}F_{pi}F_{qi}=\delta δ p q {\displaystyle \delta {_pq} ist die Kronecker delta ( 0 {\displaystyle 0} wenn p ≠ q {\displaystyle p\neq q} und 1 {\displaystyle 1} wenn p = q{\displaystyle p=q}.) Die Theerroren werden als unabhängig von den Faktoren angenommen: EPMATHMARKEREP EPMATHMARKEREP Beachten Sie, dass, da jede Rotation einer Lösung auch eine Lösung ist, macht dies die Interpretation der Faktoren schwierig. Siehe unten Nachteile. In diesem speziellen Beispiel, wenn wir vorher nicht wissen, dass die beiden Arten von Intelligenz unkorreliert sind, können wir die beiden Faktoren nicht als die beiden verschiedenen Arten von Intelligenz interpretieren. Selbst wenn sie unkorreliert sind, können wir nicht sagen, welcher Faktor der verbalen Intelligenz entspricht und der mathematischen Intelligenz ohne äußeres Argument entspricht. Die Werte der Belastungen L {\displaystyle L}, die Mittelwerte μ {\displaystyle \mu } und die Varianzen der Fehler ε {\displaystyle \varepsilon } sind bei den beobachteten Daten X {\displaystyle X} und F {\displaystyle F} (die Annahme über die Pegel der Faktoren wird für ein gegebenes F {\display} festgelegt). Das "fundamentale Theorem" kann aus den obigen Bedingungen abgeleitet werden: Σ i z a i z b i = Σ j l a j l b j + Σ i ε i ε b i ä i ä i ä i ä i ä i ä i ä i ä i ä i ä ä i ä i ä ä i ä i ä i ä i ä i ä i ä i ä ä i ä ä i e i ä i ä ä i ä ä ä n ä ä ä ä ä ä ä ä ä ä n ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ä ***________________________________________________________________ - Ja. **** Ich kriege die Nase voll. {\bi} Der Begriff auf der linken Seite ist die (a, b ) {\displaystyle (a,b}) -term der Korrelationsmatrix (a p × p {\displaystyle p\times p} Matrix, die als Produkt der p × N {\displaystyle p\times abgeleitet wird N} Matrix von standardisierten Beobachtungen mit seiner Transpose) der beobachteten Daten, und seine p {\displaystyle p} Diagonalelemente werden 1 {\displaystyle 1} s.Der zweite Begriff auf der rechten Seite wird eine Diagonalmatrix mit Begriffen kleiner als Einheit sein. Der erste Begriff rechts ist die "reduzierte Korrelationsmatrix" und wird gleich der Korrelationsmatrix sein mit Ausnahme ihrer Diagonalwerte, die kleiner als Einheit sein werden. Diese Diagonalelemente der reduzierten Korrelationsmatrix werden als Kommunalitäten bezeichnet (die den Bruchteil der Varianz in der beobachteten Größe darstellen, die von den Faktoren berücksichtigt wird): h a 2 = 1 - ψ a = Σ j l a j l a j l a j {\displaystyle h_{a}{2}=1\psi {\cH00FF} {\cH00FF} {\cH00FF} {\cH00FF}} {\cH00FF}} {\cH00FF}}} {\cH00FF}}} {\cH00FF}}} {\cH00FF}}}\cH00FF}}}\cH00FF}}}}}}}\c\cH00}{\cH00}{\cH00}}\cH00}}}}}{\cH}}}}}}}}\cH}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\cH}}}}\cH}}}}}}}}}}}}}}}}\cH}}}}}}}}}}}}}}}}}}}}}}}}}\cH}}}}}}}}}}}}}}}}}}}}}}}} Die Stichprobendaten z a i {\displaystyle z_{ai} werden natürlich nicht genau der oben gegebenen Grundgleichung aufgrund von Stichprobenfehlern, Unzulänglichkeit des Modells usw. gehorchen. Ziel jeder Analyse des obigen Modells ist es, die Faktoren F p i {\displaystyle F_{pi} zu finden und l a p {\displaystyle \ell {_ap} zu laden, die in gewisser Weise einen "best fit" zu den Daten geben. In der Faktoranalyse wird die beste Passform als Minimum des mittleren quadratischen Fehlers in den außerdiagonalen Resten der Korrelationsmatrix definiert: ε 2 = Σ a ≠ b [ Σ i z a i z b i - Σ j l a j l b j ] 2 {\displaystyle \varepsilon {\cH00FF} Ich bin nicht in der Lage. - Ja. Dies entspricht der Minimierung der außerdiagonalen Komponenten der Fehlerkovarianz, die in den Modellgleichungen erwartete Werte von Null haben. Dies ist mit der Hauptkomponentenanalyse zu kontrastieren, die versucht, den mittleren quadratischen Fehler aller Reste zu minimieren. Vor dem Erscheinen von Hochgeschwindigkeitsrechnern war es mit großem Aufwand darum bemüht, dem Problem nähere Lösungen zu finden, insbesondere bei der Schätzung der Kommunitäten auf andere Weise, was dann das Problem durch eine bekannte reduzierte Korrelationsmatrix erheblich vereinfacht. Dies wurde dann verwendet, um die Faktoren und die Belastungen zu schätzen. Mit dem Aufkommen von Hochgeschwindigkeitsrechnern kann das Minimierungsproblem iterativ mit ausreichender Geschwindigkeit gelöst werden, und die Kommunalitäten werden dabei berechnet, anstatt vorher benötigt zu werden. Der MinRes-Algorithmus eignet sich besonders für dieses Problem, ist aber kaum das einzige iterative Mittel, eine Lösung zu finden. Sollen die Lösungsfaktoren korreliert werden (wie z.B. bei der Obliminrotation), so verwendet das entsprechende mathematische Modell eher Skewkoordinaten als orthogonale Koordinaten. Geometrische Interpretation Die Parameter und Variablen der Faktoranalyse können geometrische Interpretationen gegeben werden. Die Daten (z a i {\displaystyle z_{ai}), die Faktoren (F p i {\displaystyle F_{pi}) und die Fehler (ε a i {\displaystyle \varepsilon {_ai} ) können als Vektoren in einem N\displaystyle N} -dimensionalen Euclidean-Raum (sample space,) dargestellt als z ath\displaystyle Da die Daten standardisiert sind, sind die Datenvektoren von der Einheitslänge ( | | z a | | = 1 {\displaystyle |\mathbf {z} {_a}||=1 .)Die Faktorvektoren definieren in diesem Raum einen k {\displaystyle k} -dimensionalen linearen Subraum (d.h. ein Hyperplane), auf dem die Datenvektoren orthogonal projiziert werden. Dies folgt aus der Modellgleichung z a = Σ j l a j F j + ε a {\displaystyle \mathbf {z} {_a}=\sum {_j}\ell {_aj}\mathbf {F}_j}+{\boldsymbol {\varepsilon {_a und die Unabhängigkeit der Faktoren und der Fehler: F j ⋅ ε a = 0 {\displaystyle \mathbf {F} {_j}\cdot {\boldsymbol {\varepsilon {_a}=0. Im obigen Beispiel ist das Hyperplane nur eine durch die beiden Faktorvektoren definierte 2-dimensionale Ebene. Die Projektion der Datenvektoren auf das Hyperplane wird durch z ^ a = Σ j l a j F j {\displaystyle {\hat {\mathbf {z} gegeben. {_a}=\sum {_j}\ell {_aj}\mathbf {F} {_j} und die Fehler sind Vektoren von diesem projizierten Punkt zum Datenpunkt und stehen senkrecht zum Hyperplan. Ziel der Faktoranalyse ist es, ein Hyperplane zu finden, das "best fit" für die Daten in gewissem Sinne ist, also spielt es keine Rolle, wie die Faktorvektoren, die dieses Hyperplan definieren, gewählt werden, solange sie unabhängig sind und im Hyperplan liegen. Wir können sie sowohl orthogonal als auch normal angeben (F j ⋅ F q = δ p q {\displaystyle \mathbf {F} {_j}\cdot \mathbf {F} {_q}=\delta {_pq} ) ohne Verallgemeinerung. Nachdem eine geeignete Menge von Faktoren gefunden werden, können sie auch willkürlich innerhalb des Hyperplans gedreht werden, so dass jede Drehung der Faktorvektoren das gleiche Hyperplane definieren und auch eine Lösung sein wird. Infolgedessen ist im obigen Beispiel, in dem das passende Hyperplan zweidimensional ist, wenn wir vorher nicht wissen, dass die beiden Arten von Intelligenz unkorreliert sind, dann können wir die beiden Faktoren nicht als die beiden unterschiedlichen Arten von Intelligenz interpretieren. Selbst wenn sie unkorreliert sind, können wir nicht sagen, welcher Faktor der verbalen Intelligenz entspricht und der mathematischen Intelligenz entspricht, oder ob die Faktoren lineare Kombinationen von beiden sind, ohne ein äußeres Argument. Die Datenvektoren z a {\displaystyle \mathbf {z} {_a} haben eine Einheitslänge. Die Einträge der Korrelationsmatrix für die Daten werden durch r a b = z a ∙ z b {\displaystyle r_{ab}=\mathbf {z}\cdot \mathbf {z} {_b} angegeben. Die Korrelationsmatrix kann geometrisch als Cosinus des Winkels zwischen den beiden Datenvektoren z a {\displaystyle \mathbf} {z} {c} interpretiert werden. Die "reduzierte Korrelationsmatrix" ist definiert als r ^ a b = z ^ a ⋅ z ^ b {\displaystyle {\hat r}={\hat {\hat {\mathbf {z} - Was? .Ziel der Faktoranalyse ist es, das passende Hyperplan so zu wählen, dass die reduzierte Korrelationsmatrix die Korrelationsmatrix möglichst nahezu wiedergibt, abgesehen von den Diagonalelementen der Korrelationsmatrix, die als Einheitswert bekannt sind. Mit anderen Worten ist es das Ziel, die Kreuzkorrelationen in den Daten möglichst genau wiederzugeben. Für den passenden Hyperplan wird nämlich der mittlere quadratische Fehler in den außerdiagonalen Komponenten ε 2 = Σ a ≠ b ( r a b - r ^ a b ) 2 {\displaystyle \varepsilon {^2}=\sum {_a\neq b}\left(r_{}-{\hat r}_{{ab}\right)^^{2 mit dem eingestellten Faktor a minimiert. Es ist ersichtlich, daß r a b - r ^ a b = ε a Θ ε b {\displaystyle r_{ab}-{\hat *_{ab}={\boldsymbol - Ja. Der Begriff rechts ist nur die Kovarianz der Fehler. Im Modell wird die Fehlerkovarianz als Diagonalmatrix angegeben, so dass das obige Minimierungsproblem tatsächlich eine "best fit" zum Modell ergibt: Es wird eine Stichprobenschätzung der Fehlerkovarianz ergeben, die ihre außerdiagonalen Komponenten im mittleren quadratischen Sinn minimiert hat. Da die z ^ a {\displaystyle {\hat z}_{a orthogonale Projektionen der Datenvektoren sind, wird ihre Länge kleiner oder gleich der Länge des projizierten Datenvektors sein, der Einheit ist. Das Quadrat dieser Längen sind nur die diagonalen Elemente der reduzierten Korrelationsmatrix. Diese diagonalen Elemente der reduzierten Korrelationsmatrix sind als Kommunitäten bekannt: h a 2 = | ^ z ^ a | | 2 = Σ j l a j 2 {\displaystyle * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * _a}{2}=\sum _j}{\ell _aj}{2} Große Werte der Kommunalitäten geben an, dass das passende Hyperplan die Korrelationsmatrix ziemlich genau wiedergibt. Die Mittelwerte der Faktoren müssen auch auf Null beschränkt sein, aus dem folgt, dass die Mittelwerte der Fehler auch Null sein werden. Praktische Umsetzung Arten der Faktoranalyse Exploratory Factor Analysis (EFA) wird verwendet, um komplexe Zusammenhänge zwischen Elementen und Gruppenelementen zu identifizieren, die Teil von einheitlichen Konzepten sind. Der Forscher macht keine a priori Annahmen über Beziehungen unter Faktoren. Confirmatory Factor Analysis (CFA) ist ein komplexerer Ansatz, der die Hypothese untersucht, dass die Elemente mit spezifischen Faktoren verbunden sind. CFA nutzt die Strukturgleichungsmodellierung, um ein Messmodell zu testen, wobei die Belastung der Faktoren die Auswertung von Zusammenhängen zwischen beobachteten Variablen und unbeobachteten Variablen ermöglicht. Strukturgleichungsmodellierungsansätze können Messfehler aufnehmen und sind weniger restriktiv als Mindest-Quadrat-Schätzung. Hypothesenmodelle werden gegen aktuelle Daten getestet, und die Analyse würde Belastungen von beobachteten Variablen auf den latenten Variablen (Faktoren) sowie die Korrelation zwischen den latenten Variablen demonstrieren. Art der Faktorextraktion Hauptkomponentenanalyse (PCA) ist eine weit verbreitete Methode zur Faktorextraktion, die die erste Phase der EFA ist. Faktorgewichte werden berechnet, um die größtmögliche Varianz zu extrahieren, wobei sukzessive Factoring bis keine weitere aussagekräftige Varianz übrig bleibt. Das Faktormodell muss dann zur Analyse gedreht werden. Die kanonische Faktoranalyse, auch Raos kanonische Factoring genannt, ist eine andere Methode der Berechnung des gleichen Modells wie PCA, das die Hauptachsenmethode verwendet. Canonical Factor Analysis sucht Faktoren, die die höchste kanonische Korrelation mit den beobachteten Variablen haben.Die kanonische Faktoranalyse wird durch eine willkürliche Reskalierung der Daten nicht beeinflusst. Eine gemeinsame Faktoranalyse, die auch als Hauptfaktoranalyse (PFA) oder Hauptachsenfaktorierung (PAF) bezeichnet wird, sucht die wenigen Faktoren, die für die gemeinsame Varianz (Korrelation) einer Reihe von Variablen verantwortlich sind. Die Bildfaktorierung basiert auf der Korrelationsmatrix von vorhergesagten Variablen und nicht auf tatsächlichen Variablen, wobei jede Variable von den anderen mit Mehrfachregression vorhergesagt wird. Alpha Factoring basiert auf der Maximierung der Zuverlässigkeit von Faktoren, vorausgesetzt Variablen werden zufällig aus einem Universum von Variablen abgetastet. Alle anderen Verfahren nehmen an, dass Proben entnommen werden und Variablen fixiert werden. Faktorregressionsmodell ist ein kombinatorisches Modell von Faktormodell und Regressionsmodell; oder alternativ kann es als Hybridfaktormodell betrachtet werden, dessen Faktoren teilweise bekannt sind. Terminologie Faktorladungen: Die Kommune ist das Quadrat der standardisierten äußeren Belastung eines Gegenstandes. Analog zu Pearson's r-squared, ist die quadratische Faktorbelastung der Prozent der Varianz in dieser Indikatorgröße, die durch den Faktor erklärt. Um den Prozentsatz der Varianz in allen um jeden Faktor berechneten Variablen zu erhalten, addieren Sie die Summe der quadratischen Faktorbelastungen für diesen Faktor (Spalte) und teilen Sie die Anzahl der Variablen. (Anmerken Sie, die Anzahl der Variablen entspricht der Summe ihrer Varianzen, da die Varianz einer normierten Variablen 1 ist). Dies ist die gleiche wie die Aufteilung des Eigenwerts des Faktors durch die Anzahl der Variablen. Interpretfaktorbelastungen: Durch eine Faustregel bei der Bestätigungsfaktoranalyse sollten die Belastungen .7 oder höher sein, um zu bestätigen, dass die a priori identifizierten unabhängigen Variablen durch einen bestimmten Faktor repräsentiert werden, wenn die .7-Ebene etwa der Hälfte der durch den Faktor erläuterten Varianz des Indikators entspricht. Der .7-Standard ist jedoch ein hoher Standard und reale Daten können dieses Kriterium wohl nicht erfüllen, weshalb einige Forscher, insbesondere für explorative Zwecke, ein niedrigeres Niveau wie .4 für den zentralen Faktor und .25 für andere Faktoren verwenden. In jedem Fall müssen Faktorbelastungen im Licht der Theorie interpretiert werden, nicht durch willkürliche Cutoff-Spiegel. Bei schräger Rotation kann sowohl eine Mustermatrix als auch eine Strukturmatrix untersucht werden. Die Strukturmatrix ist einfach die Faktorbeladungsmatrix wie bei orthogonaler Rotation, die die Varianz in einer gemessenen Größe darstellt, die durch einen Faktor sowohl auf einer einzigartigen als auch gemeinsamen Beitragsbasis erläutert wird. Die Mustermatrix enthält dagegen Koeffizienten, die nur einzigartige Beiträge darstellen. Je mehr Faktoren, desto niedriger die Musterkoeffizienten in der Regel, da es häufiger Beiträge zur Varianz erklärt werden. Bei schräger Rotation betrachtet der Forscher sowohl die Struktur- als auch die Musterkoeffizienten, wenn er ein Etikett auf einen Faktor zurückzuführen ist. Die Prinzipien der Schrägdrehung können sowohl von der Kreuzentropie als auch von ihrer Doppelentropie abgeleitet werden. Gemeinde: Die Summe der quadratischen Faktorbelastungen für alle Faktoren für eine bestimmte Variable (Reihe) ist die Varianz in dieser Variable, die von allen Faktoren berücksichtigt wird. Die Kommunalität misst den Prozentsatz der Varianz in einer bestimmten Varianz, die von allen Faktoren gemeinsam erklärt wird und kann als die Zuverlässigkeit des Indikators im Zusammenhang mit den Faktoren interpretiert werden. Gefährliche Lösungen: Übersteigt die Kommunalität 1,0, so gibt es eine anstrengende Lösung, die zu klein eine Probe oder die Wahl widerspiegelt, zu viele oder zu wenige Faktoren zu extrahieren. Einzigartige Variable: Die Variabilität einer Variablen minus ihrer Kommunalität. Eigenwerte/Charakteristikwurzeln:Eigenwerte messen die Variationsmenge der Gesamtprobe um jeden Faktor. Das Verhältnis von Eigenwerten ist das Verhältnis von erläuternder Bedeutung der Faktoren zu den Variablen. Hat ein Faktor einen niedrigen Eigenwert, so trägt er wenig zur Erläuterung von Varianzen in den Variablen bei und kann als weniger wichtig ignoriert werden als die Faktoren mit höheren Eigenwerten. Extraktionssummen von quadratischen Belastungen: Erste Eigenwerte und Eigenwerte nach der Extraktion (gelistet von SPSS als "Extraktionssums of Squared Loadings") sind für die PCA-Extraktion gleich, für andere Extraktionsverfahren werden aber Eigenwerte nach der Extraktion niedriger sein als ihre ursprünglichen Gegenstücke. SPSS druckt auch "Rotation Sums of Squared Loadings" und auch für PCA werden diese Eigenwerte von Anfangs- und Extraktionseigenwerten abweichen, obwohl ihre Summe gleich sein wird. Faktor-Scores (auch als Komponenten-Scores in PCA genannt:) sind die Scores von jedem Fall (row) auf jedem Faktor (Spalte). Um den Faktorwert für einen gegebenen Fall für einen gegebenen Faktor zu berechnen, nimmt man auf jeder Variablen den normierten Wert des Falles, multipliziert durch die entsprechenden Belastungen der Variablen für den gegebenen Faktor und summiert diese Produkte. Computing-Faktor-Scores können nach Faktor-Outliers suchen. Auch können Faktorscores als Variablen in der nachfolgenden Modellierung verwendet werden.( Erklärt von PCA nicht aus der Perspektive der Faktoranalyse.) Kriterien für die Bestimmung der Anzahl von Faktoren Forscher wollen solche subjektiven oder willkürlichen Kriterien für die Faktorrückhaltung als "es hat Sinn für mich". Zur Lösung dieses Problems wurden eine Reihe objektiver Methoden entwickelt, die es den Anwendern ermöglichen, eine geeignete Lösungspalette zu ermitteln. Methoden können nicht zustimmen. So kann die parallele Analyse 5 Faktoren andeuten, während Velicers MAP 6 nahelegt, so dass der Forscher sowohl 5 als auch 6-Faktor-Lösungen anfordern und jeweils in Bezug auf ihre Beziehung zu externen Daten und Theorie diskutieren kann. Moderne Kriterien Horns parallele Analyse (PA:) Eine Monte-Carlo-basierte Simulationsmethode, die die beobachteten Eigenwerte mit denen aus unkorrelierten Normalvariablen vergleicht. Ein Faktor oder eine Komponente wird beibehalten, wenn der zugehörige Eigenwert größer ist als die 95. Perzentile der Verteilung von aus den Zufallsdaten abgeleiteten Eigenwerten. PA gehört zu den häufiger empfohlenen Regeln für die Bestimmung der Anzahl der Komponenten zu halten, aber viele Programme schließen diese Option nicht ein (eine bemerkenswerte Ausnahme ist R). Formann gab jedoch sowohl theoretische als auch empirische Beweise vor, dass seine Anwendung in vielen Fällen nicht angemessen sein könnte, da ihre Leistung durch Stichprobengröße, Punktdiskriminierung und Art des Korrelationskoeffizienten erheblich beeinflusst wird. Velicers (1976) MAP-Test, wie von Courtney (2013) beschrieben, "eine komplette Hauptkomponentenanalyse, gefolgt von der Prüfung einer Reihe von Matrizen von Teilkorrelationen" (S. 397 (obwohl beachten, dass dieses Zitat nicht in Velicer (1976) auftritt und die zitierte Seitennummer außerhalb der Seiten der Zitation liegt). Die quadratische Korrelation für Schritt "0" (siehe Abbildung 4) ist die durchschnittliche quadratische off-diagonale Korrelation für die unbeteiligte Korrelationsmatrix. Auf Schritt 1 werden die erste Hauptkomponente und deren zugehörige Elemente herausgetrennt. Anschließend wird für Schritt 1 die durchschnittliche quadrierte off-diagonale Korrelation für die nachfolgende Korrelationsmatrix berechnet. Auf Schritt 2 werden die ersten beiden Hauptkomponenten herauspart und die resultierende durchschnittliche quadratische Offdiagonalkorrelation wieder berechnet. Die Berechnungen werden für k minus einen Schritt durchgeführt (k repräsentiert die Gesamtzahl der Variablen in der Matrix). Anschließend werden alle durchschnittlichen quadratischen Korrelationen für jeden Schritt aufgereiht und die Schrittzahl in den Analysen, die zu der niedrigsten durchschnittlichen quadratischen Teilkorrelation führte, bestimmt die Anzahl der Komponenten oder Faktoren, die zu halten sind. Durch dieses Verfahren werden Komponenten erhalten, solange die Varianz in der Korrelationsmatrix eine systematische Varianz im Gegensatz zu Rest- oder Fehlervarianz darstellt. Die MAP-Technik hat sich zwar methodisch auf die Hauptkomponentenanalyse angewiesen, aber bei der Bestimmung der Anzahl der Faktoren, die in mehreren Simulationsstudien beibehalten werden sollen, durchaus bewährt. Dieses Verfahren wird über die Benutzeroberfläche von SPSS sowie das Psych-Paket für die R-Programmiersprache zur Verfügung gestellt. Ältere Methoden Kaiserkriterium: Die Kaiser-Regel besteht darin, alle Komponenten mit Eigenwerten unter 1,0 zu fallen – das ist der Eigenwert, der der von einem durchschnittlichen Einzelteil erfassten Information entspricht. Das Kaiser-Kriterium ist der Standard in SPSS und der meisten statistischen Software, wird aber nicht empfohlen, wenn es als einziges Cut-off-Kriterium zur Schätzung der Anzahl der Faktoren verwendet wird, da es zu überaus attraktiven Faktoren neigt. Eine Variation dieser Methode wurde erstellt, bei der ein Forscher Vertrauensintervalle für jeden Eigenwert berechnet und nur Faktoren festhält, die das gesamte Vertrauensintervall größer als 1,0 haben. Scree Grundstück: Der Cattell scree Test legt die Komponenten als X-Achse und die entsprechenden Eigenwerte als Y-Achse auf. Wenn man sich nach rechts bewegt, zu späteren Komponenten, fallen die Eigenwerte. Wenn der Tropfen aufhört und die Kurve einen Ellbogen zu einem weniger steilen Rückgang macht, sagt Cattell's scree test, alle weiteren Komponenten nach dem, der am Ellbogen beginnt zu fallen. Diese Regel wird manchmal kritisiert, dass sie für forschungsgesteuerte Vergasung geeignet ist. Das heißt, da die Ellbogenaufnahme subjektiv sein kann, weil die Kurve mehrere Ellbogen hat oder eine glatte Kurve ist, kann der Forscher versucht werden, die Abschaltung auf die Anzahl der von ihrer Forschungsagenda gewünschten Faktoren einzustellen. Varianz erläuterte Kriterien: Einige Forscher verwenden einfach die Regel, genug Faktoren zu halten, um 90% (manchmal 80%) der Variation zu berücksichtigen. Wenn das Ziel des Forschers die Parsimony betont (die Varianz mit möglichst wenigen Faktoren zu erklären), könnte das Kriterium so niedrig sein, wie 50 % Bayesian Methode A Bayesian Ansatz auf Basis des indischen Buffet-Prozesses liefert eine Wahrscheinlichkeitsverteilung über die plausible Anzahl latenter Faktoren. Rotationsverfahren Die unverdrehte Ausgabe maximiert die durch die ersten und nachfolgenden Faktoren bedingte Varianz und zwingt die orthogonal zu seinden Faktoren. Diese Datenkompression kommt zu den Kosten, dass die meisten Elemente Belastung der frühen Faktoren, und in der Regel, dass viele Elemente Belastung wesentlich auf mehr als einen Faktor. Die Rotation dient dazu, die Ausgabe verständlicher zu machen, indem man so genannte "Einfache Struktur" sucht: Ein Muster von Belastungen, bei denen jedes Produkt nur einen der Faktoren stark belastet, und viel schwächer auf die anderen Faktoren. Die Rotation kann orthogonal oder schräg sein (unter den zu korrelierenden Faktoren). Die Varimax-Drehung ist eine orthogonale Drehung der Faktorachsen, um die Varianz der quadratischen Belastungen eines Faktors (Spalte) auf alle Variablen (Spalte) in einer Faktormatrix zu maximieren, was die Differenzierung der ursprünglichen Variablen durch extrahierten Faktor bewirkt. Jeder Faktor wird entweder große oder kleine Belastungen einer bestimmten Größe haben. Eine Varimax-Lösung liefert Ergebnisse, die es so einfach wie möglich machen, jede Variable mit einem einzigen Faktor zu identifizieren. Dies ist die häufigste Rotationsoption. Die Orthogonalität (d.h. Unabhängigkeit) von Faktoren ist jedoch oft eine unrealistische Annahme. Oblique Rotationen sind inklusive orthogonaler Rotation, weshalb Schrägdrehungen ein bevorzugtes Verfahren sind. In der psychometrischen Forschung ist die Aufnahme von Faktoren, die miteinander korreliert sind, besonders anwendbar, da Einstellungen, Meinungen und intellektuelle Fähigkeiten in der Regel miteinander korreliert werden und da es in vielen Situationen unrealistisch wäre, anderweitig anzunehmen. Quartimax Rotation ist eine orthogonale Alternative, die die Anzahl der Faktoren minimiert, die benötigt werden, um jede Variable zu erklären. Diese Art der Rotation erzeugt oft einen allgemeinen Faktor, auf dem die meisten Variablen in einem hohen oder mittleren Grad geladen werden. Eine solche Faktorstruktur ist für den Forschungszweck in der Regel nicht hilfreich. Equimax Rotation ist ein Kompromiss zwischen varimax und quartimax Kriterien. Direkte Obliminrotation ist die Standardmethode, wenn man eine nicht-orthogonale (oblique) Lösung wünscht – also eine, in der die Faktoren korreliert werden dürfen. Dies führt zu höheren Eigenwerten, aber zu einer verminderten Interpretation der Faktoren. Siehe unten. Promax Rotation ist eine alternative nicht-orthogonale (oblique) Rotationsmethode, die rechnerisch schneller als die direkte Oblimin-Methode ist und daher manchmal für sehr große Datensätze verwendet wird. Höhere Auftragsfaktoranalyse Die Faktoranalyse der höheren Ordnung ist eine statistische Methode, bestehend aus der Wiederholung von Schritten Faktoranalyse – Schrägdrehung – Faktoranalyse von gedrehten Faktoren. Sein Verdienst ist es, dem Forscher zu ermöglichen, die hierarchische Struktur der untersuchten Phänomene zu sehen. Um die Ergebnisse zu interpretieren, geht man entweder durch Nachmultiplizieren der Primärfaktormustermatrix durch die höherwertigen Faktormustermatrizen (Gorsuch, 1983) und eventuelle Anwendung einer Varimax-Rotation auf das Ergebnis (Thompson, 1990) oder durch Verwendung einer Schmid-Leiman-Lösung (SLS, Schmid & Leiman, 1957, auch Schmid-Leiman-Transformation genannt) fort, die die die zweite Variation von den Primärfaktoren auf die Hauptfaktoren zurückzuführen. In der Psychometrie Geschichte Charles Spearman war der erste Psychologe, um über gemeinsame Faktoranalyse zu diskutieren und tat dies in seinem 1904 Papier. Es lieferte einige Details zu seinen Methoden und beschäftigte sich mit Ein-Faktor-Modellen. Er entdeckte, dass die Noten der Schulkinder auf einer Vielzahl von scheinbar unabhängigen Fächern positiv korreliert waren, was ihn dazu führte, dass eine einzige allgemeine geistige Fähigkeit postuliert, oder g, unterliegt und formt menschliche kognitive Leistung. Die erste Entwicklung der gemeinsamen Faktoranalyse mit mehreren Faktoren wurde von Louis Thurstone in zwei Papieren in den frühen 1930er Jahren gegeben, in seinem 1935 Buch, The Vector of Mind. Thurstone führte mehrere wichtige Faktoranalysekonzepte ein, darunter Kommunalität, Einzigartigkeit und Rotation. Er plädierte für "einfache Struktur" und entwickelte Methoden der Rotation, die als Weg zur Erreichung dieser Struktur verwendet werden könnten. In der Q-Methodik unterscheidet Stephenson, ein Student von Spearman, zwischen R-Faktor-Analyse, orientiert an der Untersuchung der interindividuellen Unterschiede, und Q-Faktor-Analyse, die auf subjektive intraindividuelle Unterschiede ausgerichtet ist. Raymond Cattell war ein starker Befürworter von Faktoranalyse und Psychometrie und nutzte Thurstones Mehrfaktortheorie, um Intelligenz zu erklären. Cattell entwickelte auch den Scoree-Test und Ähnlichkeitskoeffizienten. Anwendungen in der Psychologie Faktoranalyse wird verwendet, um Faktoren zu identifizieren, die eine Vielzahl von Ergebnissen an verschiedenen Tests erklären. Zum Beispiel, Intelligenz Forschung fand, dass Menschen, die eine hohe Punktzahl auf einem Test der verbalen Fähigkeit erhalten sind auch gut auf anderen Tests, die verbale Fähigkeiten erfordern. Die Forscher erklärten dies durch die Verwendung von Faktoranalyse, um einen Faktor zu isolieren, oft als verbale Intelligenz bezeichnet, die den Grad darstellt, in dem jemand Probleme mit verbalen Fähigkeiten lösen kann. Die Faktorenanalyse in der Psychologie ist am häufigsten mit der Intelligenzforschung verbunden. Es wurde jedoch auch verwendet, um Faktoren in einer breiten Palette von Bereichen wie Persönlichkeit, Einstellungen, Überzeugungen usw. zu finden. Sie ist mit Psychometrie verbunden, da sie die Gültigkeit eines Instruments beurteilen kann, indem sie feststellen kann, ob das Instrument tatsächlich die postulierten Faktoren misst. Die Faktoranalyse ist eine häufig verwendete Technik in der interkulturellen Forschung. Es dient der Gewinnung kultureller Dimensionen. Die bekanntesten Kulturdimensionsmodelle sind die von Geert Hofstede, Ronald Inglehart, Christian Welzel, Shalom Schwartz und Michael Minkov erarbeiteten. Vorteile Reduzierung der Anzahl der Variablen durch Kombination von zwei oder mehr Variablen in einen einzigen Faktor. So könnte beispielsweise die Leistung beim Laufen, Ballwerfen, Schlägern, Springen und Gewichtheben zu einem einzigen Faktor wie allgemeine Leichtathletik kombiniert werden. In der Regel werden in einem Element von Menschen Matrix Faktoren durch Gruppierung verwandten Elementen ausgewählt. In der Q-Faktor-Analyse-Technik wird die Matrix transponiert und Faktoren werden durch Gruppierung verwandter erzeugt. Zum Beispiel könnten sich Liberale, Bibliothekare, Konservative und Sozialisten in separate Gruppen einfügen. Identifizierung von Gruppen von miteinander verbundenen Variablen, um zu sehen, wie sie miteinander verbunden sind. Zum Beispiel, Carroll verwendet Faktor-Analyse, um seine Drei Stratum Theorie zu bauen. Er fand heraus, dass ein Faktor "breite visuelle Wahrnehmung" betrifft, wie gut ein Individuum an visuellen Aufgaben ist. Er fand auch einen "großen auditiven Wahrnehmungsfaktor" in Bezug auf auditive Aufgabenfähigkeit. Darüber hinaus fand er einen globalen Faktor, genannt g oder allgemeine Intelligenz, der sowohl "breite visuelle Wahrnehmung" als auch "breite auditive Wahrnehmung" betrifft. Dies bedeutet, dass jemand mit einem hohen g wahrscheinlich sowohl eine hohe "visuelle Wahrnehmung" Fähigkeit und eine hohe "auditory Wahrnehmung" Fähigkeit haben, und dass g erklärt daher einen guten Teil, warum jemand in beiden dieser Domänen gut oder schlecht ist. Nachteile ."Jede Orientierung ist ebenso akzeptabel mathematisch. Aber unterschiedliche Faktortheorien erwiesen sich in Bezug auf die Orientierungen von Faktorachsen für eine bestimmte Lösung, wie in Bezug auf alles andere, so dass Modellbeschlagung nicht als nützlich bei der Unterscheidung zwischen Theorien zu sein."(Sternberg, 1977). Das bedeutet, dass alle Rotationen unterschiedliche zugrunde liegende Prozesse darstellen, aber alle Rotationen sind gleichermaßen gültige Ergebnisse der Standardfaktoranalyseoptimierung. Daher ist es nicht möglich, die richtige Rotation allein mit der Faktoranalyse zu wählen. Faktoranalyse kann nur so gut sein, wie die Daten erlauben. In der Psychologie, wo Forscher oft auf weniger gültige und zuverlässige Maßnahmen wie Selbstberichte verlassen müssen, kann dies problematisch sein. Die Interpretation der Faktoranalyse basiert auf der Verwendung einer heuristischen, eine Lösung, die "bequem ist, auch wenn nicht absolut wahr". Mehr als eine Interpretation kann aus den gleichen Daten gemacht werden, die auf dieselbe Weise faktorisiert werden, und die Faktoranalyse kann die Kausalität nicht erkennen. Exploratory Factor Analysis (EFA) gegen Hauptkomponentenanalyse (PCA)Factor Analyse ist mit der Hauptkomponentenanalyse (PCA), aber die beiden sind nicht identisch. Im Bereich der Unterschiede zwischen den beiden Techniken gab es erhebliche Kontroversen. PCA kann als eine grundlegende Version der exploratory Factor Analysis (EFA) betrachtet werden, die in den frühen Tagen vor dem Aufkommen von High-Speed-Computern entwickelt wurde. Sowohl die PCA- als auch die Faktoranalyse sollen die Dimensionalität einer Reihe von Daten reduzieren, aber die diesbezüglichen Ansätze sind für die beiden Techniken unterschiedlich. Die Analyse der Faktoren ist klar mit dem Ziel, bestimmte nicht zu beobachtende Faktoren aus den beobachteten Variablen zu identifizieren, während PCA dieses Ziel nicht direkt anspricht; im besten Fall bietet PCA eine Annäherung an die benötigten Faktoren. Aus der Sicht der explorativen Analyse sind die Eigenwerte der PCA aufgeblasene Bauteilbelastungen, d.h. mit Fehlervarianz verunreinigt. Während EFA und PCA in einigen Bereichen der Statistik als synonyme Techniken behandelt werden, wurde dies kritisiert. Die Faktoranalyse "dealsiert unter der Annahme einer zugrunde liegenden Kausalstruktur: [it] geht davon aus, dass die Kovariation in den beobachteten Variablen auf das Vorhandensein einer oder mehrerer latenter Variablen (Faktoren) zurückzuführen ist, die einen ursächlichen Einfluss auf diese beobachteten Variablen ausüben". Dagegen nimmt PCA weder an, noch hängt von einer solchen zugrunde liegenden Kausalbeziehung ab. Die Forscher haben argumentiert, dass die Unterscheidungen zwischen den beiden Techniken bedeuten kann, dass es objektive Vorteile für die Bevorzugung eines über dem anderen basierend auf dem analytischen Ziel. Wenn das Faktormodell falsch formuliert wird oder die Annahmen nicht erfüllt sind, dann ergibt die Faktoranalyse fehlerhafte Ergebnisse. Die Faktoranalyse wurde erfolgreich angewandt, wenn ein ausreichendes Verständnis des Systems gute Anfangsmodellformulierungen erlaubt. PCA verwendet eine mathematische Transformation zu den ursprünglichen Daten ohne Annahmen über die Form der Kovarianzmatrix. Ziel der PCA ist es, lineare Kombinationen der ursprünglichen Variablen zu bestimmen und einige auszuwählen, die verwendet werden können, um den Datensatz zu bündeln, ohne viel Informationen zu verlieren. Argumente, die PCA und EFA Fabrigar et al.(1999) kontrastieren, adressieren eine Reihe von Gründen, die verwendet werden, um zu vermuten, dass PCA nicht der Faktoranalyse entspricht: Es wird manchmal vorgeschlagen, dass PCA rechnerisch schneller ist und weniger Ressourcen benötigt als Faktoranalyse. Fabrigar et al. Schlussfolgerung, dass leicht verfügbare Computerressourcen dieses praktische Anliegen irrelevant gemacht haben.PCA und Faktoranalyse können ähnliche Ergebnisse liefern. Dieser Punkt wird auch von Fabrigar et al. angesprochen; in bestimmten Fällen, in denen die Waren niedrig sind (z.B. 0,4), ergeben die beiden Techniken unterschiedliche Ergebnisse. In der Tat, Fabrigar et al.argue, dass in Fällen, in denen die Daten den Annahmen des gemeinsamen Faktormodells entsprechen, die Ergebnisse der PCA ungenaue Ergebnisse sind. Es gibt bestimmte Fälle, in denen die Faktoranalyse zu "Heywood-Fällen" führt. Diese umfassen Situationen, in denen 100 % oder mehr der Varianz in einer Messgröße vom Modell berücksichtigt werden. Fabrigar et al. deuten darauf hin, dass diese Fälle tatsächlich informativ für den Forscher sind, was ein falsch spezifiziertes Modell oder eine Verletzung des gemeinsamen Faktormodells anzeigt. Der Mangel an Heywood-Fällen im PCA-Ansatz kann bedeuten, dass solche Probleme unbemerkt bleiben. Forscher erhalten zusätzliche Informationen von einem PCA-Ansatz, wie z.B. einer individuellen Bewertung einer bestimmten Komponente; diese Informationen werden nicht aus der Faktoranalyse gewonnen. Wie Fabrigar et al. contend, das typische Ziel der Faktoranalyse – d.h. die Faktoren, die die Struktur der Korrelationen zwischen gemessenen Variablen berücksichtigen – erfordert keine Kenntnis von Faktorscores und somit wird dieser Vorteil vernachlässigt. Es ist auch möglich, Faktorpunkte aus einer Faktoranalyse zu berechnen. Die Varianz-Versus-Kovarianzfaktor-Analyse berücksichtigt den zufälligen Fehler, der der Messung inhärent ist, während die PCA dies nicht tut. Dieser Punkt wird von Brown (2009) erläutert, der anzeigte, dass in Bezug auf die an den Berechnungen beteiligten Korrelationsmatrizen: "In PCA werden 1,00 in die Diagonale gesetzt, was bedeutet, dass alle Varianz in der Matrix berücksichtigt werden soll (einschließlich Varianz, die für jede Varianz, die bei Varianzen und Fehlervarianzen üblich ist). Das würde also definitionsgemäß alle Varianzen in den Variablen einschließen. Im Gegensatz dazu werden in der EFA die Waren in die Diagonale gesetzt, was bedeutet, dass nur die Varianz, die mit anderen Variablen geteilt wird, berücksichtigt werden soll (ohne Varianz, die für jede Varianz und Fehlervarianz einzigartig ist). Das würde also definitionsgemäß nur Varianzen beinhalten, die unter den Variablen üblich sind." Aus diesem Grund empfiehlt Brown (2009) die Verwendung von Faktoranalysen, wenn theoretische Ideen über Zusammenhänge zwischen Variablen existieren, während PCA verwendet werden sollte, wenn das Ziel des Forschers ist, Muster in ihren Daten zu erforschen. Unterschiede in Verfahren und Ergebnissen Die Unterschiede zwischen PCA und Faktoranalyse (FA) werden durch Suhr (2009) weiter veranschaulicht: PCA führt zu Hauptkomponenten, die eine maximale Varianz für beobachtete Varianzen ausmachen; FA führt zu einer gemeinsamen Varianz in den Daten. PCA setzt auf die Diagonalen der Korrelationsmatrix ein; FA passt die Diagonalen der Korrelationsmatrix mit den einzigartigen Faktoren an. PCA minimiert die Summe des quadratischen senkrechten Abstands zur Bauteilachse; FA schätzt Faktoren, die Auswirkungen auf beobachtete Variablen beeinflussen. Die Komponentenpunkte in PCA stellen eine lineare Kombination der von Eigenvektoren gewichteten beobachteten Variablen dar; die beobachteten Variablen in FA sind lineare Kombinationen der zugrunde liegenden und einzigartigen Faktoren. In der PCA sind die gelieferten Komponenten uninterpretisch, d.h. sie stellen keine zugrunde liegenden „Konstrukte“ dar; in der FA können die zugrunde liegenden Konstrukte bei einer genauen Modellspezifikation markiert und leicht interpretiert werden. Im Marketing Die grundlegenden Schritte sind: Identifizierung der verfremdeten Attribute Verbraucher verwenden, um Produkte in dieser Kategorie zu bewerten. Verwenden Sie quantitative Marketing-Forschungsmethoden (wie Umfragen), um Daten von einer Stichprobe potenzieller Kunden über ihre Bewertungen aller Produktattribute zu sammeln. Geben Sie die Daten in ein statistisches Programm ein und führen Sie das Faktoranalyseverfahren aus. Der Computer liefert eine Reihe von zugrunde liegenden Attributen (oder Faktoren). Verwenden Sie diese Faktoren, um Wahrnehmungskarten und andere Produktpositioniereinrichtungen zu konstruieren. Informationssammlung Die Datenerhebung wird in der Regel von Marketing-Forschern durchgeführt. Umfragefragen stellen den Befragten auf die Rate einer Produktprobe oder Beschreibungen von Produktkonzepten auf einer Reihe von Attributen. Überall werden fünf bis zwanzig Attribute gewählt. Sie könnten Dinge wie: einfache Bedienung, Gewicht, Genauigkeit, Haltbarkeit, Farbstärke, Preis oder Größe. Die gewählten Attribute variieren je nach untersuchtem Produkt. Die gleiche Frage wird über alle Produkte in der Studie gestellt. Die Daten für mehrere Produkte werden kodiert und in ein statistisches Programm wie R, SPSS, SAS, Stata, STATISTICA, JMP und SYSTAT eingegeben. Analyse Die Analyse wird die zugrunde liegenden Faktoren isolieren, die die Daten anhand einer Matrix von Verbänden erklären. Die Faktoranalyse ist eine Interdependenztechnik. Der komplette Satz von voneinander abhängigen Beziehungen wird untersucht. Es gibt keine Angabe von abhängigen Variablen, unabhängigen Variablen oder Kausalität. Die Faktoranalyse geht davon aus, dass alle Bewertungsdaten auf verschiedene Attribute auf wenige wichtige Dimensionen reduziert werden können. Diese Reduktion ist möglich, da einige Attribute miteinander verwandt sein können. Die Bewertung eines Attributs ist teilweise das Ergebnis des Einflusses anderer Attribute. Der statistische Algorithmus dekonstruiert die Bewertung (genannte Rohnote) in seine verschiedenen Komponenten und rekonstruiert die Teilergebnisse in zugrunde liegende Faktorpunkte. Der Korrelationsgrad zwischen der anfänglichen Rohnote und der Endfaktornote wird als Faktorbelastung bezeichnet. Vorteile Sowohl objektive als auch subjektive Attribute können verwendet werden, sofern die subjektiven Attribute in Scores umgewandelt werden können. Die Analyse der Faktoren kann latente Dimensionen oder Konstrukte identifizieren, die nicht direkt analysiert werden können. Es ist einfach und kostengünstig. Nachteile Nützlichkeit hängt von der Fähigkeit der Forscher ab, eine ausreichende Menge an Produktattributen zu sammeln. Werden wichtige Attribute ausgeschlossen oder vernachlässigt, wird der Wert des Verfahrens reduziert. Wenn Sätze von beobachteten Variablen einander sehr ähnlich sind und sich von anderen Elementen unterscheiden, wird ihnen die Faktoranalyse einen einzigen Faktor zuordnen. Dies kann beunruhigende Faktoren, die interessantere Beziehungen darstellen. Namensfaktoren können theoretische Kenntnisse erfordern, weil scheinbar unähnliche Attribute aus unbekannten Gründen stark korrelieren können. In der physikalischen und biologischen Wissenschaft wurde auch die Faktoranalyse in physikalischen Wissenschaften wie Geochemie, Hydrochemie, Astrophysik und Kosmologie sowie biologischen Wissenschaften wie Ökologie, Molekularbiologie, Neurowissenschaften und Biochemie weit verbreitet. Im Grundwasserqualitätsmanagement ist es wichtig, die räumliche Verteilung verschiedener chemischer Parameter auf verschiedene mögliche Quellen zu beziehen, die unterschiedliche chemische Signaturen aufweisen. Beispielsweise wird eine Sulfidmine mit hohen Säuregehalten, gelösten Sulfaten und Übergangsmetallen verbunden. Diese Signaturen können als Faktoren durch R-Mode-Faktor-Analyse identifiziert werden, und der Ort der möglichen Quellen kann durch Konturierung der Faktorpunkte vorgeschlagen werden. In der Geochemie können verschiedene Faktoren unterschiedlichen Mineralverbänden und damit der Mineralisierung entsprechen. Bei der Mikroarray-Analyse können Faktoranalysen verwendet werden, um hochdichte Oligonukleotid-DNA-Mikroarrays-Daten auf Sondenebene für Affymetrix GeneChips zusammenzufassen. Dabei entspricht die latente Größe der RNA-Konzentration in einer Probe. Implementierung Faktor-Analyse wurde in mehreren statistischen Analyseprogrammen seit den 1980er Jahren durchgeführt: BMDP JMP (statistische Software)Mplus (statistische Software)]Python: Modul Scikit-learn R (mit der Basisfunktions-Factanal- oder Fa-Funktion in Paket-Psych). Im GPArotation R-Paket sind Rotationen implementiert. SAS (mit PROC FACTOR oder PROC CALIS)SPSS Stata Siehe auch Referenzen weiterlesen Child, Dennis (2006,) The Essentials of Factor Analysis (3rd ed,.) Continuum International, ISBN 978-0-8264-8000-2.Fabrigar, L.R; Wegener, D.T; MacCallum, R.C; Strahan, E.J (September 1999). " Bewertung der Verwendung von exploratorischen Faktoranalysen in der psychologischen Forschung". Psychologische Methoden. 4 (3): 272–299.doi:10.1037/1082-989X.4.3.272.B.T Gray (1997)Higher-Order Factor Analysis (Conference paper)Jennrich, Robert I. "Rotation to Simple Loadings using Component Loss Function: The Oblique Case", Psychometrika, Vol.71, No. 1, pp.173–191, März 2006. Katz, Jeffrey Owen und Rohlf, F. James. Primärproduktfunktionsplan: Eine schräge Drehung auf einfache Struktur. Multivariate Verhaltensforschung, April 1975, Vol.10, S.219–232.Katz, Jeffrey Owen und Rohlf, F. James.Functionplane: Ein neuer Ansatz zur einfachen Strukturdrehung. Psychometrika, März 1974, Band 39, Nr. 1, S.37–51.Katz, Jeffrey Owen und Rohlf, F. James. Funktions-Punkt-Cluster-Analyse. Systematic Zoology, September 1973, Vol.22, No. 3, pp.295–301.Mulaik, S. A. (2010,) Grundlagen der Faktoranalyse, Chapman & Hall. Preacher, K.J; MacCallum, R.C (2003). " Reparatur von Tom Swift's Electric Factor Analysis Machine" (PDF).Verstehen von Statistiken.2 (1): 13–43. doi:10.1207/S15328031US0201_02.hdl:1808/1492.J.Schmid und J. M. Leiman (1957). Die Entwicklung von hierarchischen Faktorlösungen. Psychometrika, 22(1,) 53–61.Thompson, B. (2004), Exploratory and Confirmatory Factor Analysis: Konzepte und Anwendungen verstehen, Washington DC: American Psychological Association, ISBN 978-1591470939. Hans-Georg Wolff, Katja Preising (2005) Mit der schmid-leiman-Lösung werden Produkt- und Auftragsfaktorstruktur ausgenutzt: Syntax-Codes für SPSS- und SASBehavior-Forschungsmethoden, Instrumente & Computer, 37 (1,) 48-58 Externe Links Ein Leitfaden für die Analyse von Faktoranalyse Exploratory Factor. Ein Book Manuskript von Tucker, L. & MacCallum R. (1993). Retrieved June 8, 2006, von: [1] Garson, G. David, "Factor Analysis", aus Statnotes: Themen in multivariate Analyse. Abgeholt am 13. April 2009 von StatNotes: Themen in Multivariate Analyse, von G. David Garson an der North Carolina State University, Public Administration Program Factor Analysis at 100 — Konferenzmaterial FARMS — Factor Analysis for Robust Microarray Summarization, a R Paket