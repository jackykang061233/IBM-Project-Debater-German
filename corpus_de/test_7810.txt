Mojibake (文 angegebene Bedeutungけ; IPA: [modibaibake]) ist der Text, der das Ergebnis ist, dass Text mit einer unbeabsichtigten Zeichencodierung decodiert wird. Das Ergebnis ist ein systematischer Ersatz von Symbolen mit völlig unabhängigen, oft aus einem anderen Schreibsystem. Diese Anzeige kann das generische Ersatzzeichen ( ) an Orten enthalten, an denen die binäre Darstellung als ungültig angesehen wird. Ein Ersatz kann auch mehrere aufeinanderfolgende Symbole umfassen, wie in einer Kodierung betrachtet, wenn der gleiche binäre Code ein Symbol in der anderen Kodierung darstellt. Dies ist entweder durch unterschiedliche konstante Längencodierung (wie in asiatischen 16-Bit-Codierungen gegenüber europäischen 8-Bit-Codierungen), oder die Verwendung von variablen Längen-Codierungen (insbesondere UTF-8 und UTF-16). Verfehltes Rendern von Glyphen durch fehlende Schriften oder fehlende Glyphen in einer Schrift ist ein anderes Problem, das nicht mit Mojibake verwechselt werden soll. Symptome dieser gescheiterten Rendering umfassen Blöcke mit dem in hexadezimal angezeigten Codepunkt oder mit dem generischen Ersatzzeichen. Wichtig ist, dass diese Ersetzungen gültig sind und das Ergebnis einer korrekten Fehlerbehandlung durch die Software sind. Etymology Mojibake bedeutet "Charaktertransformation" auf Japanisch. Das Wort besteht aus 文 angegeben (moji, IPA: [mod͡ʑi,]) Zeichen und Þけ (bake, IPA: [bäke vermehrt,] ausgesprochen bah-keh,) transform. Ursachen Um den kodierten Originaltext korrekt wiederzugeben, muss die Korrespondenz zwischen den kodierten Daten und dem Begriff seiner Kodierung erhalten bleiben. Da mojibake der Fall der Nichteinhaltung zwischen diesen ist, kann es durch die Manipulation der Daten selbst oder einfach eine Neumarkierung erreicht werden. Mojibake wird oft mit Textdaten gesehen, die mit einer falschen Codierung verschlagwortet wurden; es darf nicht einmal verschlagwortet werden, sondern zwischen Computern mit unterschiedlichen Standardcodierungen bewegt werden. Eine große Quelle von Problemen sind Kommunikationsprotokolle, die auf Einstellungen auf jedem Computer vertrauen, anstatt Metadaten zusammen mit den Daten zu senden oder zu speichern. Die unterschiedlichen Standardeinstellungen zwischen Computern sind zum Teil auf unterschiedliche Einsatzbereiche von Unicode unter Betriebssystemfamilien zurückzuführen, zum Teil auf die Spezialisierungen der Legacy-Encodings für unterschiedliche Schreibsysteme menschlicher Sprachen. Während Linux-Distributionen im Jahr 2004 meist auf UTF-8 geschaltet werden, verwendet Microsoft Windows im Allgemeinen UTF-16 und verwendet manchmal 8-Bit-Code-Seiten für Textdateien in verschiedenen Sprachen. Für einige Schreibsysteme, beispielsweise Japanisch, wurden in der Vergangenheit mehrere Kodierungen eingesetzt, wodurch die Nutzer relativ oft Mojibake sehen. Als japanisches Beispiel könnte das als EUC-JP gespeicherte Wort mojibake 文 angegeben werden, das als ハ „Lebensmittel“ oder „Shift JIS-2004“ falsch angezeigt werden kann. Derselbe Text, der als UTF-8 gespeichert wird, wird als 蟄怜喧縺 蟄怜喧縺 angezeigt, wenn er als Shift JIS interpretiert wird. Dies ist weiter verschärft, wenn andere Lokale beteiligt sind: derselbe UTF-8 Text erscheint wie æ–‡å—åÃ„ in Software, die annimmt, dass Text in den Windows-1252 oder ISO-8859-1 Kodierungen, in der Regel markiert westlich, oder (zum Beispiel) als 鏂囧瓧鍖栥亼 wenn als in einem GBK (Festland China) Locale interpretiert. Unterspezifikation Wenn die Kodierung nicht angegeben ist, liegt es an der Software, sie auf andere Weise zu entscheiden. Je nach Softwaretyp ist die typische Lösung entweder Konfigurations- oder Zeichenerkennungs-Huristik. Beide sind anfällig für Fehlprädiktion in nicht-so-uncommon Szenarien. Die Kodierung von Textdateien wird von der lokalen Einstellung beeinflusst, die von der Sprache des Benutzers, der Marke des Betriebssystems und möglicherweise anderen Bedingungen abhängt. Daher ist die angenommene Codierung für Dateien, die von einem Computer mit einer anderen Einstellung kommen, oder sogar von einer anders lokalisierten Software innerhalb desselben Systems systematisch falsch. Für Unicode ist eine Lösung eine Byte-Bestellmarke zu verwenden, aber für Quellcode und anderen maschinenlesbaren Text, viele Parser tolerieren dies nicht. Ein anderer speichert die Codierung als Metadaten im Dateisystem. Dateisysteme, die erweiterte Dateiattribute unterstützen, können dies als Benutzer speichern. Ein Zeichen. Dies erfordert auch Unterstützung in Software, die von ihr profitieren will, aber keine andere Software stört. Während einige Kodierungen leicht zu erkennen sind, insbesondere UTF-8, gibt es viele, die schwer zu unterscheiden sind (siehe Zeichenerkennung). Ein Webbrowser kann eine in EUC-JP kodierte Seite und eine andere in Shift-JIS nicht unterscheiden können, wenn das Kodierungsprogramm nicht explizit mit HTTP-Headern zugewiesen wird, die zusammen mit den Dokumenten gesendet werden, oder mithilfe der Meta-Tags des HTML-Dokuments, die verwendet werden, um fehlende HTTP-Header zu ersetzen, wenn der Server nicht konfiguriert werden kann, um die richtigen HTTP-Header zu senden; siehe Zeichencodierungen in HTML. Mis-Spezifikation Mojibake tritt auch auf, wenn die Kodierung falsch angegeben ist. Dies geschieht oft zwischen Codierungen, die ähnlich sind. Zum Beispiel war der E-Mail-Client von Eudora für Windows bekannt, E-Mails zu senden, die als ISO-8859-1 gekennzeichnet sind, die in der Realität waren Windows-1252. Die Mac OS-Version von Eudora zeigte dieses Verhalten nicht. Windows-1252 enthält extra bedruckbare Zeichen im C1-Bereich (die am häufigsten gesehen sind gebogene Anführungszeichen und zusätzliche Armaturen), die nicht ordnungsgemäß in der Software entsprechend der ISO-Norm angezeigt wurden; diese besonders betroffen Software läuft unter anderen Betriebssystemen wie Unix. Menschliche Ignoranz Von den noch verwendeten Codierungen sind viele teilweise miteinander kompatibel, mit ASCII als überwiegender gemeinsamer Teilsatz. Dies setzt die Bühne für die menschliche Unwissenheit: Kompatibilität kann eine deceptive Eigenschaft sein, da die gemeinsame Teilmenge von Zeichen durch eine Mischung von zwei Codierungen nicht beeinflusst wird (siehe Probleme in verschiedenen Schreibsystemen). Die Leute denken, sie verwenden ASCII und neigen dazu, zu etikettieren, was übereinstimmt von ASCII sie tatsächlich als ASCII verwenden". Vielleicht zur Vereinfachung, aber auch in der akademischen Literatur, kann das Wort ASCII als Beispiel von etwas verwendet werden, das nicht mit Unicode kompatibel ist, wo offensichtlich ASCII Windows-1252 und Unicode ist UTF-8. Beachten Sie, dass UTF-8 rückwärtskompatibel mit ASCII ist. Überspezifikation Wenn es Protokollschichten gibt, die versuchen, die Kodierung auf der Grundlage verschiedener Informationen festzulegen, können die wenigsten bestimmten Informationen dem Empfänger irreführend sein. Betrachten Sie beispielsweise einen Webserver, der über HTTP eine statische HTML-Datei bedient. Der Zeichensatz kann dem Client in einer beliebigen Anzahl von 3 Arten mitgeteilt werden: im HTTP-Header. Diese Informationen können auf der Server-Konfiguration (z.B. beim Servieren einer Datei-Off-Disk) basieren oder von der auf dem Server laufenden Anwendung (für dynamische Webseiten) gesteuert werden. in der Datei als HTML-Meta-Tag (http-equiv oder charset) oder das Kodierungsattribut einer XML-Deklaration. Dies ist die Kodierung, dass der Autor die jeweilige Datei speichern wollte. in der Datei, als Byte-Bestellmarke. Dies ist die Kodierung, dass der Herausgeber des Autors es tatsächlich gespeichert. Es sei denn, es ist eine zufällige Kodierung (durch Öffnen in einer Kodierung und Rettung in einer anderen) passiert, wird dies richtig sein. Es ist jedoch nur in Unicode-Kodierungen wie UTF-8 oder UTF-16 erhältlich. Mangel an Hardware oder Software-Unterstützung Viele ältere Hardware ist typischerweise dazu ausgelegt, nur einen Zeichensatz zu unterstützen und der Zeichensatz kann typischerweise nicht geändert werden. Die in der Display-Firmware enthaltene Zeichentabelle wird lokalisiert, um Zeichen für das Land zu haben, in das das Gerät verkauft werden soll, und in der Regel unterscheidet sich die Tabelle von Land zu Land. Als solche werden diese Systeme mojibake möglicherweise anzeigen, wenn Text auf einem System aus einem anderen Land erzeugt. Ebenso unterstützen viele frühe Betriebssysteme nicht mehrere Kodierungsformate und werden damit enden, Mojibake anzuzeigen, wenn gemacht, um nicht-Standard-Text anzuzeigen - vor allem Versionen von Microsoft Windows und Palm OS, werden auf einem pro-country-Basis lokalisiert und unterstützt nur Kodierungsstandards, die für das Land relevant sind, die lokalisierte Version verkauft wird, und wird mojibake anzeigen, wenn eine Datei, die einen Text in einem anderen Kodierungsformat enthält. Anwendungen, die UTF-8 als Standardcodierung verwenden, können aufgrund ihrer weit verbreiteten Verwendung und Rückwärtskompatibilität mit US-ASCII eine größere Interoperabilität erreichen. UTF-8 hat auch die Möglichkeit, direkt durch einen einfachen Algorithmus erkannt zu werden, so dass gut geschriebene Software in der Lage sein sollte, UTF-8 mit anderen Codierungen zu vermischen. Die Schwierigkeit, eine Instanz von Mojibake zu lösen, variiert je nach Anwendung, in der es auftritt und deren Ursachen. Zwei der häufigsten Anwendungen, bei denen Mojibake auftreten kann, sind Webbrowser und Wortprozessoren. Moderne Browser und Wortprozessoren unterstützen oft eine Vielzahl von Zeichencodierungen. Browser erlauben es einem Benutzer oft, die Kodierung der Rendering-Engine auf der Fliege zu ändern, während Wortprozessoren dem Benutzer erlauben, die entsprechende Kodierung beim Öffnen einer Datei auszuwählen. Es kann einige Test und Fehler für Benutzer, um die richtige Codierung zu finden. Das Problem wird komplizierter, wenn es in einer Anwendung auftritt, die normalerweise keine breite Palette von Zeichencodierung, wie in einem nicht-Unicode Computerspiel unterstützt. In diesem Fall muss der Benutzer die Kodierungseinstellungen des Betriebssystems ändern, um die des Spiels anzupassen. Eine Änderung der systemweiten Kodiereinstellungen kann jedoch auch Mojibake in bereits vorhandenen Anwendungen verursachen. In Windows XP oder später hat ein Benutzer auch die Möglichkeit, Microsoft AppLocale zu verwenden, eine Anwendung, die den Wechsel von Per-Application Locale-Einstellungen ermöglicht. Auch so ist bei früheren Betriebssystemen, wie Windows 98, eine Änderung der Betriebssystemcodierungseinstellungen nicht möglich; um dieses Problem bei früheren Betriebssystemen zu lösen, müsste ein Benutzer eine Drittanbieter-Schriftierungsapplikation verwenden. Probleme in verschiedenen Schreibsystemen Englisch Mojibake in englischen Texten treten in der Regel in der Pünctuation auf, wie em dashes (-,) en dashes (–,) und lockly zitates (“,„,“), aber selten im Zeichentext, da die meisten Kodierungen mit ASCII über die Kodierung des englischen Alphabets übereinstimmen. Zum Beispiel erscheint das Pfund-Zeichen £ als Â£, wenn es vom Absender als UTF-8 kodiert, aber vom Empfänger als CP1252 oder ISO 8859-1 interpretiert wurde. Wenn es mit CP1252 betrieben wird, kann dies zu Ã‚Â Â£, Ãâ€šÃ‚Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Einige Computer haben in älteren Zeiten herstellerspezifische Kodierungen, die auch für den englischen Text Fehlanpassungen verursachten. Commodore-Marke 8-Bit-Computer verwendet PETSCII-Codierung, besonders bemerkenswert für die Invertierung des oberen und unteren Gehäuse im Vergleich zu Standard ASCII. PETSCII-Drucker arbeiteten gut an anderen Computern der Zeit, aber kippte den Fall aller Buchstaben. IBM Mainframes verwenden die EBCDIC-Codierung, die überhaupt nicht mit ASCII übereinstimmt. Andere westeuropäische Sprachen Die Alphabete der norddeutschen Sprachen, Katalanisch, Finnisch, Deutsch, Französisch, Portugiesisch und Spanisch sind alle Erweiterungen des lateinischen Alphabets. ê, ê, ê, und deren Oberkörper, gegebenenfalls. Dies sind Sprachen, für die der ISO-8859-1 Zeichensatz (auch bekannt als Latein 1 oder Western) im Einsatz ist. ISO-8859-1 wurde jedoch durch zwei konkurrierende Standards, die rückwärtskompatibel Windows-1252 und die leicht veränderte ISO-8859-15 überstrichen. Beide ergänzen das Euro-Zeichen € und das Französische œ, aber ansonsten schafft jede Verwirrung dieser drei Zeichensätze nicht mojibake in diesen Sprachen. Darüber hinaus ist es immer sicher, ISO-8859-1 als Windows-1252 zu interpretieren, und ziemlich sicher, es als ISO-8859-15 zu interpretieren, insbesondere im Hinblick auf das Euro-Zeichen, das das selten verwendete Währungszeichen ersetzt (¤). Mit dem Aufkommen von UTF-8 ist mojibake jedoch in bestimmten Szenarien häufiger geworden, z.B. durch den Austausch von Textdateien zwischen UNIX und Windows-Computern, aufgrund der Unverträglichkeit von UTF-8 mit Latin-1 und Windows-1252. Aber UTF-8 hat die Fähigkeit, direkt durch einen einfachen Algorithmus erkannt zu werden, so dass gut geschriebene Software in der Lage sein sollte, UTF-8 mit anderen Kodierungen zu vermischen, so dass dies am häufigsten war, wenn viele Software hatte nicht unterstützt UTF-8. Die meisten dieser Sprachen wurden durch MS-DOS Standard CP437 und andere Standardcodierungen der Maschine unterstützt, außer ASCII, so dass Probleme beim Kauf einer Betriebssystemversion weniger häufig waren. Windows und MS-DOS sind jedoch nicht kompatibel. Auf Schwedisch, Norwegisch, Dänisch und Deutsch werden die Vokale selten wiederholt, und es ist in der Regel offensichtlich, wenn ein Charakter beschädigt wird, z.B. der zweite Buchstabe in kÃ¤rk (kärlek, love). Auf diese Weise, obwohl der Leser zwischen å, ä und ö er erraten muss, bleiben fast alle Texte lesbar. Der finnische Text hingegen enthält die Wiederholung von Vokalen in Wörtern wie hääyö ("Hochzeitsnacht"), die manchmal Text sehr schwer zu lesen machen können (z.B. hääyö erscheint als hÃ¤ bloßstellend ". Icelandic und Faroese haben zehn bzw. acht möglicherweise mitbegründete Charaktere, die es so schwieriger machen können, verdorbene Charaktere zu erraten; isländische Wörter wie þjóðlöð ("ausstehende Gastfreundschaft") werden fast völlig unverständlich, wenn es als Ã3⁄4jÃ3Ã°lÃ°" gemacht wird. In deutscher Sprache ist Buchstabensalat ("Briefsalat") ein allgemeiner Begriff für dieses Phänomen und in spanischer, deformación (literal deformación). Einige Benutzer transliterate ihre Schrift bei der Verwendung eines Computers, entweder durch das Ausgeben der problematischen Diakritik, oder durch die Verwendung von Digraph-Ersatz (å → aa, ä/æ → ae, ö/ø → oe, ü → ue usw.) So könnte ein Autor ueber statt über schreiben, die Standardpraxis in Deutsch ist, wenn Umlauts nicht verfügbar sind. Letztere Praxis scheint im deutschen Sprachbereich besser toleriert zu sein als in den nordischen Ländern. Zum Beispiel in Norwegisch, Digraphen sind mit archaischen Dänen verbunden, und kann jokingly verwendet werden. Aber Digraphen sind nützlich in der Kommunikation mit anderen Teilen der Welt. Als Beispiel hatte der norwegische Fußballspieler Ole Gunnar Solskjær seinen Namen SOLSKJAER auf seinem Rücken, als er für Manchester United spielte. Ein Artefakt von UTF-8 misinterpretiert wie ISO-8859-1, "Ring meg nÃ¥" ("Ring meg nå,") wurde im Juni 2014 in einem SMS Betrug in Norwegen gesehen. Auch mittel- und osteuropäische Nutzer zentraler und osteuropäischer Sprachen können betroffen sein. Da die meisten Computer während der Mitte bis Ende der 1980er Jahre nicht mit einem Netzwerk verbunden waren, gab es für jede Sprache mit diakritischen Zeichen (siehe ISO/IEC 8859 und KOI-8) unterschiedliche Zeichencodierungen, die oft auch durch Betriebssystem variieren. Ungarisch ist eine weitere befallene Sprache, die die 26 grundlegenden englischen Zeichen verwendet, plus die akzentuierten Formen á, é, í, ó, ú, ö, ü (alle in der Latin-1 Zeichensatz,) plus die beiden Zeichen ő und ű, die nicht in Latin-1 sind. Diese beiden Zeichen können in Latin-2, Windows-1250 und Unicode korrekt kodiert werden. Bevor Unicode in E-Mail-Clients üblich wurde, hatten E-Mails mit ungarischem Text oft die Buchstaben ő und ű beschädigt, manchmal bis zum Punkt der Unerkennbarkeit. Es ist üblich, auf eine E-Mail zu antworten, die unlesbar gemacht wird (siehe Beispiele unten) durch Charakter-Mangling (als betűszemét, Bedeutung "Brief Müll") mit der Phrase "Árvíztűrő tükörfúrógép," ein nicht-sense Phrase (literal "Flood-resistente Spiegelbohrmaschine"), die alle verwendeten Zeichen enthält. Beispiele Polnisch Vor der Erstellung von ISO 8859-2 im Jahr 1987 nutzten Anwender verschiedener Rechenplattformen ihre eigenen Charakterkodierungen wie AmigaPL auf Amiga, Atari Club auf Atari ST und Masovia, IBM CP852, Mazovia und Windows CP1250 auf IBM PCs. Polnische Unternehmen, die frühe DOS-Computer verkaufen, haben ihre eigenen, gegenseitig unvereinbaren Möglichkeiten geschaffen, um polnische Zeichen zu kodieren und einfach die EPROMs der Videokarten (typischerweise CGA, EGA oder Hercules) neu zu programmieren, um Hardware-Code-Seiten mit den benötigten Glyphen für Polnisch zu liefern – und zwar unabhängig davon, wo andere Computerverkäufer sie platziert hatten. Die Situation begann zu verbessern, wenn ISO 8859-2 nach dem Druck von akademischen und Benutzergruppen als "Internetstandard" mit begrenzter Unterstützung der Software der dominanten Anbieter (heute weitgehend durch Unicode ersetzt). Mit den zahlreichen Problemen, die durch die Vielfalt der Kodierungen verursacht werden, neigen auch heute einige Benutzer dazu, sich auf polnische diakritische Zeichen als krzaczki [(kshach-kih,] lit."little Sträucher. " Russische und andere kyrillische Alphabete Mojibake kann kolloquial krakozyabry (кракозябрыбры [krönk betitelt“zjæbrς) auf Russisch genannt werden, was durch mehrere Systeme zur Kodierung von Cyrillic kompliziert war und bleibt. Die Sowjetunion und die frühe Russische Föderation entwickelten KOI-Encodings (Kod Obmena Informatsiey, Код Обмена Информацией, was zu "Code for Information Exchange" übersetzt. Dies begann mit kyrillisch-nur 7-bit KOI7, basierend auf ASCII, aber mit lateinischen und einigen anderen Zeichen ersetzt durch kyrillische Buchstaben. Dann kam die 8-Bit-KOI8-Codierung, die eine ASCII-Erweiterung ist, die kodiert kyrillische Buchstaben nur mit hochbit-Set Oktetten entsprechend 7-bit-Codes von KOI7. Aus diesem Grund bleibt der KOI8-Text, auch Russisch, nach dem Abstreifen des achten Bits, der im Alter von 8BITMIME-unaware-E-Mail-Systemen als wesentlicher Vorteil angesehen wurde, teilweise lesbar. Z.B. Wörter "Икола руссского языка" shkola russkogo yazyka, kodiert in KOI8 und dann durch den hohen Bit Stripping-Prozess, end up rendered as ["KOLA RUSSKOGO qZYKA". Schließlich gewann KOI8 verschiedene Aromen für Russisch und Bulgarisch (KOI8-R,) Ukrainisch (KOI8-U,) Weißrussisch (KOI8-RU) und sogar Tajik (KOI8-T). Inzwischen, im Westen, Code Seite 866 unterstützt ukrainisch und Belarussisch sowie Russisch/Bulgarisch in MS-DOS. Für Microsoft Windows, Code Page 1251 zusätzlich Unterstützung für serbische und andere slawische Varianten von Cyrillic. Vor kurzem enthält die Unicode-Kodierung Codepunkte für praktisch alle Zeichen aller Weltsprachen, einschließlich aller kyrillischen Zeichen. Vor Unicode war es notwendig, die Textcodierung mit einer Schrift mit dem gleichen Codierungssystem anzupassen. Nicht zu tun, produzierte unlesbare Gibberish, deren spezifisches Aussehen je nach der exakten Kombination von Textcodierung und Schriftcodierung variierte. Zum Beispiel, versuchen, nicht-Unicode Cyrillic Text mit einer Schrift, die auf das lateinische Alphabet beschränkt ist, oder mit der Standard- (westlichen) Kodierung, führt in der Regel zu Text, der fast vollständig aus Vokalen mit diakritischen Marken besteht. (KOI8 Библиотека (biblioteka, bibliothek) wird âÉÉÉÉÄÅËÁ" Die Verwendung der Windows-Codepage 1251 zur Ansicht von Text in KOI8 oder umgekehrt führt zu garbled Text, der überwiegend aus Großbuchstaben besteht (KOI8 und Codepage 1251 teilen die gleiche ASCII-Region, aber KOI8 hat Großbuchstaben in der Region, in der die Codepage 1251 Kleinbuchstaben hat und umgekehrt). In der Regel, Cyrillic Gibberish ist symptomatisch der Verwendung der falschen Cyrillic Schrift. In den frühen Jahren des russischen Sektors des World Wide Web waren sowohl KOI8 als auch die Codepage 1251 gemeinsam. Ab 2017 kann man noch HTML-Seiten in der Codepage 1251 und, selten, KOI8 Encodings, sowie Unicode treffen. ( Schätzungsweise 1,7% aller Webseiten weltweit – alle Sprachen sind enthalten – sind in der Codepage 1251 codiert.) Obwohl der HTML-Standard die Möglichkeit beinhaltet, die Kodierung für eine bestimmte Webseite in seiner Quelle anzugeben, wird dies manchmal vernachlässigt, indem der Benutzer die Kodierung im Browser manuell schaltet. In Bulgarisch wird Mojibake oft als majmunica (маймуница) bezeichnet, was "monkey's [alphabet]" bedeutet. In Serbien heißt es đubre (уубре,) Müll". Im Gegensatz zur ehemaligen UdSSR, South Slavs nie etwas wie KOI8 verwendet, und Code Page 1251 war die dominante Cyrillic Kodierung dort vor Unicode. Daher erlebten diese Sprachen weniger Kodierung Unverträglichkeit Probleme als Russisch. In den 1980er Jahren nutzten die bulgarischen Computer ihre eigene MIK-Codierung, die oberflächlich ähnlich ist (wenngleich unvereinbar mit) CP866.Yugoslawische Sprachen Kroatisch, Bosnisch, Serbisch (die Dialekte der jugoslawischen Serbo-Kroatisch-Sprache) und Slowenisch fügen dem lateinischen Grundbuch die Buchstaben š, đ, č, ć, ž und ihre Kapitalgegenden Š, Đ, Č, Ć, Ž (nur č/Č, š/Š und ž/Ž in Slowen). Alle diese Buchstaben sind in Latein-2 und Windows-1250 definiert, während nur einige (š, Š, ž, Ž, Đ) im üblichen OS-Default Windows-1252 existieren und aufgrund einiger anderer Sprachen dort sind. Obwohl Mojibake mit einem dieser Zeichen auftreten kann, sind die Buchstaben, die nicht in Windows-1252 enthalten sind viel anfälliger für Fehler. So wird auch heute oft "šđčćž ŠđČĆŽ" als "šðèæž ŠÐÈÆŽ" angezeigt, obwohl ð, è, æ, È, Æ nie in slawischen Sprachen verwendet werden. Wenn man sich auf grundlegende ASCII (zum Beispiel die meisten Benutzernamen) beschränkt, sind häufige Ersetzungen: š→s, đ→dj, č→c, ć→c, ž→z (Hauptformen analog, mit Đ→Dj oder Đ →DJ je nach Wortfall). Alle diese Ersetzungen stellen Mehrdeutigkeiten ein, so dass die Rekonstruktion des Originals aus einer solchen Form in der Regel manuell durchgeführt wird, wenn erforderlich. Die Windows-1252 Kodierung ist wichtig, weil die englischen Versionen des Windows-Betriebssystems am weitesten verbreitet sind, nicht lokalisiert. Die Gründe hierfür sind ein relativ kleiner und fragmentierter Markt, die Erhöhung des Preises für hochwertige Lokalisierung, ein hohes Maß an Software-Piraterie (der wiederum durch einen hohen Preis von Software im Vergleich zu Einkommen verursacht), die Lokalisierungsbemühungen entmutigt, und Menschen bevorzugen englische Versionen von Windows und andere Software. Die Fahrt, kroatisch von serbisch, bosnisch von kroatisch und serbisch zu unterscheiden, und jetzt sogar Montenegrin von den anderen drei schafft viele Probleme. Es gibt viele verschiedene Lokalisierungen, die verschiedene Standards und von unterschiedlicher Qualität verwenden. Es gibt keine gemeinsamen Übersetzungen für die große Menge an Computerterminologie mit Ursprung in Englisch. Am Ende verwenden die Leute angenommene englische Wörter (kompjuter für Computer, kompajlirati für compile, etc.), und wenn sie nicht an die übersetzten Begriffe gewöhnt sind, kann nicht verstehen, was einige Option in einem Menü auf der Grundlage der übersetzten Phrase tun soll. Daher, Menschen, die Englisch verstehen, sowie diejenigen, die an englische Terminologie gewöhnt sind (die am meisten sind, weil englische Terminologie wird auch in Schulen wegen dieser Probleme unterrichtet) regelmäßig wählen Sie die originalen englischen Versionen von nicht-spezialistischen Software. Wenn das kyrillische Skript (für Mazedonisch und teilweise Serbisch) verwendet wird, ist das Problem ähnlich wie andere kyrillische Skripte. Neue Versionen von englischen Windows erlauben die Änderung der Code-Seite (ältere Versionen erfordern spezielle englische Versionen mit diesem Support), aber diese Einstellung kann und oft falsch eingestellt werden. Zum Beispiel Windows 98 und Windows Ich kann auf die meisten nicht-rechts-linken Single-byte-Code-Seiten einschließlich 1250, aber nur zu installieren Zeit eingestellt werden. Kaukasische Sprachen Die Schreibsysteme bestimmter Sprachen der Kaukasus-Region, einschließlich der Schriften Georgisch und Armenisch, können Mojibake produzieren. Dieses Problem ist besonders akut bei ArmSCII oder ARMSCII, einer Reihe von veralteten Zeichencodierungen für das armenische Alphabet, die durch Unicode-Standards ersetzt wurden. ArmSCII ist wegen mangelnder Unterstützung in der Computerindustrie nicht weit verbreitet. Zum Beispiel, Microsoft Windows unterstützt es nicht. Asiatische Kodierungen Eine andere Art von Mojibake tritt auf, wenn Text irrtümlich in einer Multibyte-Codierung, wie einer der Kodierungen für ostasiatische Sprachen, parsiert wird. Mit dieser Art von mojibake werden mehr als ein (typischerweise zwei) Zeichen auf einmal beschädigt, z.B. k舐lek (kärlek) auf Schwedisch, wo är als  of" bezeichnet wird. Im Vergleich zum obigen Mojibake ist dies schwerer zu lesen, da Briefe, die nicht mit dem problematischen å, ä oder ö verwandt sind, fehlen und besonders problematisch für kurze Wörter beginnend mit å, ä oder ö wie än (was 舅") ist. Da zwei Buchstaben kombiniert werden, scheint die Mojibake auch zufälliger (über 50 Varianten im Vergleich zu den normalen drei, nicht die selteneren Kapitale zählen). In einigen seltenen Fällen kann eine ganze Textstring, die ein Muster bestimmter Wortlängen umfasst, wie der Satz "Bush versteckt die Fakten", falsch interpretiert werden. Japan Auf Japanisch ist das Phänomen, wie erwähnt, Mojibake genannt (文 angegebene Bezeichnung). Es ist ein besonderes Problem in Japan aufgrund der zahlreichen unterschiedlichen Kodierungen, die für den japanischen Text existieren. Neben Unicode-Encodings wie UTF-8 und UTF-16 gibt es andere Standard-Encodings wie Shift-JIS (Windows-Maschinen) und EUC-JP (UNIX-Systeme). Mojibake, sowie von japanischen Benutzern begegnet, wird auch oft von nicht-japanischen beim Versuch, Software für den japanischen Markt geschrieben zu führen. China In Chinesisch heißt das gleiche Phänomen Luàn mđ (Pinyin, Simplified Chinese 码lösch, Traditional Chinese 亂碼, was 'chaotic code' bedeutet) und kann auftreten, wenn computergestützter Text in einem chinesischen Zeichen kodiert, aber mit der falschen Kodierung angezeigt wird. Wenn dies geschieht, kann das Problem oft behoben werden, indem die Zeichencodierung ohne Datenverlust geschaltet wird. Die Situation ist kompliziert wegen der Existenz mehrerer chinesischer Zeichencodiersysteme im Einsatz, wobei die häufigsten sind: Unicode, Big5 und Guobiao (mit mehreren rückwärtskompatiblen Versionen) und die Möglichkeit, chinesische Zeichen mit japanischer Codierung zu kodieren. Es ist einfach, die ursprüngliche Kodierung zu identifizieren, wenn Luanma in Guobiao Kodierungen auftritt: Ein zusätzliches Problem wird verursacht, wenn Kodierungen fehlende Zeichen sind, die häufig mit seltenen oder antiquierten Zeichen, die noch in persönlichen oder Platznamen verwendet werden. Beispiele hierfür sind die taiwanesischen Politiker Wang Chien-shien (chinesisch: ・建; pinyin: Wáng Jiànxuān)'s 煊, Yu Shyi-kun (vereinfachtes Chinesisch: 游; traditionelles Chinesisch: ̄游; pinyin: Yóu Xíkūn)'s s and Sänger David Tao (chinesisch: ‡喆; pinyin: Táo Zhé)'s s vermisst in Big5, ex-PRC Premier Zhu Rongji (chinesisch: ‡基; pinyin: Zhū Róngjī)'s 镕 vermisst in GB2312, Copyright symbol © vermisst in GBK. Zeitungen haben dieses Problem auf verschiedene Weise behandelt, einschließlich der Verwendung von Software, um zwei bestehende, ähnliche Zeichen zu kombinieren; mit einem Bild der Persönlichkeit; oder einfach ein Homophon für den seltenen Charakter in der Hoffnung, dass der Leser in der Lage, die richtige Inferenz zu machen. Indischer Text Ein ähnlicher Effekt kann in Brahmic- oder Indic-Skripten Südasiens auftreten, die in solchen Indo-Aryan- oder Indic-Sprachen wie Hindustani (Hindi-Urdu,) Bengali, Punjabi, Marathi und anderen verwendet werden, auch wenn der verwendete Zeichensatz durch die Anwendung richtig erkannt wird. Dies liegt daran, dass in vielen Indischen Skripten die Regeln, mit denen einzelne Buchstabensymbole kombinieren, um Symbole für Silben zu erstellen, von einem Computer nicht richtig verstanden werden, der die entsprechende Software fehlt, auch wenn die Glyphen für die einzelnen Buchstabenformen zur Verfügung stehen. Ein Beispiel dafür ist das alte Wikipedia-Logo, das versucht, das Zeichen analog zu wi (das erste Silbe von Wikipedia) auf jedem von vielen Puzzle-Stücken zu zeigen. Das Puzzle-Stück soll das Devanagari-Zeichen für wi tragen statt verwendet, um das wa-Zeichen gefolgt von einem unpaired i modifier vowel, leicht erkennbar als mojibake erzeugt von einem Computer nicht konfiguriert, um Indic Text anzuzeigen. Das seit Mai 2010 neu gestaltete Logo hat diese Fehler behoben. Die Idee von Plain Text erfordert, dass das Betriebssystem eine Schriftart zur Anzeige von Unicode-Codes zur Verfügung stellt. Diese Schriftart unterscheidet sich von OS zu OS für Singhala und macht orthographisch falsche Glyphen für einige Buchstaben (Syllables) über alle Betriebssysteme. So ist z.B. der Reph, die kurze Form für r eine Diakritik, die normalerweise über einen einfachen Brief geht. Es ist jedoch falsch, auf einige Buchstaben wie ya oder la in bestimmten Kontexten zu gehen. Für Sanskritische Wörter oder Namen, die durch moderne Sprachen geerbt werden, wie ♥erweiterung fort, IAST: kārya, oder ∈ ∈ ψ ψ आ, es ist geeignet, es auf diese Buchstaben zu setzen. Für ähnliche Geräusche in modernen Sprachen, die sich aus ihren spezifischen Regeln ergeben, wird dagegen nicht auf den Kopf gestellt, wie z.B. das Wort ≠ ≠ Erschütterungen, IAST: karaṇāryā, eine Stammform des gängigen Wortes ≠ Erschütterungen und Schlieren, IAST: karaṇārā/rī, in der Marathi-Sprache. Aber es passiert in den meisten Betriebssystemen. Dies scheint ein Fehler der internen Programmierung der Schriften zu sein. In Mac OS und iOS, die muurdhaja l (dark l) und u Kombination und seine lange Form beide ergeben falsche Formen. Einige Indic und Indic-derived Skripte, vor allem Lao, wurden nicht offiziell von Windows XP bis zur Veröffentlichung von Vista unterstützt. Allerdings haben verschiedene Seiten kostenlose Fonts gemacht. Burmese Aufgrund der westlichen Sanktionen und der späten Ankunft der burmesischen Sprachunterstützung in Computern war ein Großteil der frühen burmesischen Lokalisierung ohne internationale Zusammenarbeit zu Hause. Das vorherrschende Mittel der Burmese-Unterstützung ist über die Zawgyi-Schrift, eine Schriftart, die als Unicode-Schrift erstellt wurde, aber eigentlich nur teilweise Unicode-konform war. In der Zawgyi-Schrift wurden einige Codepunkte für Burmese-Skript gemäß Unicode implementiert, andere waren jedoch nicht. Das Unicode Consortium bezieht sich auf dies als Ad-hoc-Font-Encodings. Mit dem Aufkommen von Mobiltelefonen ersetzten mobile Anbieter wie Samsung und Huawei einfach die Unicode-konformen Systemfonts mit Zawgyi-Versionen. Durch diese Ad-hoc-Kodierungen würde die Kommunikation zwischen den Benutzern von Zawgyi und Unicode als garbled Text. Um diese Frage zu lösen, würden Content-Produzenten sowohl in Zawgyi als auch in Unicode Beiträge machen. Myanmar Regierung hat den 1. Oktober 2019 als U-Day bezeichnet, um offiziell auf Unicode umzuschalten. Der vollständige Übergang wird auf zwei Jahre geschätzt. Afrikanische Sprachen In bestimmten Schriftsystemen Afrikas ist uncodierter Text unlesbar. Texte, die Mojibake produzieren können, gehören jene aus dem Horn von Afrika wie das Ge'ez-Skript in Äthiopien und Eritrea, verwendet für Amharic, Tigre, und andere Sprachen, und die somalische Sprache, die das Osmanya-Alphabet verwendet. In Südafrika wird das Mwangwego-Alphabet verwendet, um Sprachen von Malawi zu schreiben und das Mandombe-Alphabet wurde für die Demokratische Republik Kongo erstellt, aber diese werden nicht allgemein unterstützt. Verschiedene andere in Westafrika heimische Schreibsysteme stellen ähnliche Probleme, wie das N'Ko Alphabet, verwendet für Manding-Sprachen in Guinea, und das Vai-Soldaten, verwendet in Liberia. Arabisch Eine andere Sprache ist Arabisch (siehe unten). Der Text wird unlesbar, wenn die Kodierungen nicht übereinstimmen. Beispiele Die Beispiele in diesem Artikel haben nicht UTF-8 als Browser-Einstellung, weil UTF-8 leicht erkennbar ist, also wenn ein Browser UTF-8 unterstützt, sollte er ihn automatisch erkennen und nicht versuchen, etwas anderes als UTF-8 zu interpretieren. Siehe auch Code Punkt Ersatzzeichen Substitute Charakter Newline – Die Konventionen zur Darstellung der Zeilenumbruch unterscheiden sich zwischen Windows und Unix-Systemen. Obwohl die meisten Software sowohl Konventionen (die trivial ist) Software unterstützt, die den Unterschied bewahren oder anzeigen müssen (z.B. Versionssteuerungssysteme und Datenvergleichstools) kann wesentlich schwieriger zu bedienen, wenn nicht an einer Konvention festhalten. Byte Bestellmarke – Die am meisten in-Band Weise, die Kodierung zusammen mit den Daten zu speichern – präpendieren Sie sie. Dies ist für den Menschen mit konformer Software unsichtbar, wird aber durch Design als "Garbage-Zeichen" für inkonforme Software (einschließlich vieler Dolmetscher) wahrgenommen werden. HTML-Entitäten – Eine Kodierung von Sonderzeichen in HTML, meist optional, aber für bestimmte Zeichen erforderlich, um Interpretation als Markup zu entgehen. Während die Nichtanwendung dieser Transformation eine Verwundbarkeit ist (siehe Cross-Site-Skripting), die sie zu oft anwendet, führt zu einem Gewandeln dieser Zeichen. Zum Beispiel wird das Angebotszeichen " &quot, &amp;quot, &amp;quot und so weiter. Bush versteckte die Fakten Referenzen Externe Links Die Wörterbuchdefinition von Mojibake in Wiktionary Media im Zusammenhang mit Mojibake bei Wikimedia CommonsThrowing oder Drop Menschen aus großen Höhen wurde seit der Antike als eine Form der Hinrichtung verwendet. Menschen, die so ausgeführt werden, sterben an Verletzungen, die durch das Auftreffen des Bodens mit hoher Geschwindigkeit verursacht werden. Im alten Delphi wurden die sakrilegierten von der Spitze der Hyampeia, der hohen Klippe der Phaedriaden im Osten des katalanischen Frühlings geeilt. Im vorrömischen Sardinien wurden ältere Menschen, die sich nicht unterstützen konnten, rituell getötet. Sie wurden mit einer neurotoxischen Pflanze, die als "sardonic Kräuter" bekannt ist (die einige Wissenschaftler denken, ist hemmlock Wassertropfenkraut) berauscht und dann von einem hohen Felsen fallen oder zu Tode geschlagen. Während der Römischen Republik wurde der Tarpeian Rock, eine steile Klippe am Südgipfel des Kapitolinischen Hügels, für öffentliche Hinrichtungen verwendet. Mörder und Verräter, wenn sie von den Quaestores parricidii verurteilt wurden, wurden von der Klippe zu ihrem Tod geflüchtet. Suetonius zeichnet die Gerüchte der Grausamkeit von Tiberius während des späteren Teils seiner Herrschaft auf, während er in Capri lebte, Tiberius würde Menschen ausführen, indem sie sie von einer Klippe ins Meer geworfen, während er beobachtete. Diese Leute wurden gefoltert, bevor sie hingerichtet wurden und wenn sie den Sturz überlebten, würden Männer, die unten in Booten warteten, ihre Knochen mit Rudern und Boothuken brechen. Im vorkolonialen Südafrika hatten mehrere Stämme, darunter die Xhosa und die Zulu, Execution Hills genannt, aus denen Missstände zu ihrem Tode geeilt wurden. Diese Gesellschaften hatten keine Form der Inhaftierung, so dass Strafe war Körper, Kapital oder Ausweisung. Es wird behauptet, dass während des namibischen Unabhängigkeitskrieges zahlreiche SWAPO-Rebellen von südafrikanischen Hubschraubern über das Meer abgeschafft wurden. Während des Spanischen Bürgerkriegs nutzten sowohl die rechtsextremen nationalistischen als auch die linken republikanischen Konfliktseiten diese Hinrichtungsmethode auf ihre Gefangenen. Während Argentiniens Dirty Der Krieg der späten 1970er Jahre, die heimlich entführt wurden oft aus Flugzeugen geworfen, in den sogenannten Todesflügen. Iran könnte diese Form der Hinrichtung für das Verbrechen der Sodomie verwendet haben. Nach Amnesty International im Jahr 2008 wurden zwei Männer verurteilt, zwei Studenten zu erschlagen und zu Tode verurteilt. Sie sollten von einer Klippe oder von einer großen Höhe weggeworfen werden. Andere Männer, die an diesem Vorfall beteiligt waren, wurden zu Wimpern verurteilt, vermutlich weil sie sich nicht an penetrativem Sex mit den Opfern beteiligt. Im Jahr 2015, Mitglieder des Islamischen Staates Irak und die Levant hingerichtete Männer, die beschuldigt wurden, Homosexuell zu sein, indem sie aus Türmen. Siehe auch Defenestration Death Flights == Referenzen ==