In diesem Artikel werden Rechenwerkzeuge behandelt, die in künstlicher Intelligenz verwendet werden. Suche und Optimierung Viele Probleme in der KI lassen sich theoretisch durch intelligente Suche durch viele mögliche Lösungen lösen: Die Begründung kann reduziert werden, um eine Suche durchzuführen. Beispielsweise kann der logische Beweis als Suche nach einem Weg angesehen werden, der von den Räumlichkeiten zu Schlussfolgerungen führt, wobei jeder Schritt die Anwendung einer Inferenzregel ist. Planungsalgorithmen suchen durch Bäume von Zielen und Subgoals, versuchen, einen Pfad zu einem Zielziel zu finden, ein Prozess namens Mittel-Ends-Analyse. Robotikalgorithmen zum Bewegen von Gliedmaßen und zum Greifen von Objekten verwenden lokale Suchen im Konfigurationsraum. Viele Lernalgorithmen verwenden Suchalgorithmen basierend auf Optimierung. Einfache erschöpfende Suchanfragen sind für die meisten realen Probleme selten ausreichend: Der Suchraum (die Anzahl der gesuchten Orte) wächst schnell zu astronomischen Zahlen. Das Ergebnis ist eine Suche, die zu langsam ist oder nie vollendet. Die Lösung ist für viele Probleme, heuristics oder "Regeln des Daumens" zu verwenden, die die Wahlen zugunsten derjenigen priorisieren, die eher ein Ziel erreichen und dies in einer kürzeren Anzahl von Schritten zu tun. In einigen Suchmethoden kann heuristics auch dazu dienen, ganz zu eliminieren einige Entscheidungen unwahrscheinlich zu einem Ziel führen (genannt "Pruning the search tree"). Heuristik liefert das Programm mit einer "besten Vermutung" für den Weg, auf dem die Lösung liegt. Heuristik begrenzt die Suche nach Lösungen in eine kleinere Stichprobengröße. Eine ganz andere Art der Suche kam in den 1990er Jahren, basierend auf der mathematischen Theorie der Optimierung. Für viele Probleme ist es möglich, die Suche mit irgendeiner Form einer Vermutung zu beginnen und dann die Vermutung inkremental zu verfeinern, bis keine weiteren Verfeinerung möglich ist. Diese Algorithmen können als blindes Bergsteigen visualisiert werden: Wir beginnen die Suche an einem zufälligen Punkt auf der Landschaft, und dann, durch Sprünge oder Stufen, bewegen wir unsere Vermutung bergauf, bis wir die Spitze erreichen. Andere Optimierungsalgorithmen werden annealing, Strahlsuche und zufällige Optimierung simuliert. Evolutionäre Berechnung verwendet eine Form der Optimierungssuche. Zum Beispiel können sie mit einer Population von Organismen (die Vermutungen) beginnen und sie dann zu Mutieren und zu rekombinieren, wählen nur den Fittest, um jede Generation zu überleben (die Vermutungen zu definieren). Klassische evolutionäre Algorithmen umfassen genetische Algorithmen, Genexpressions-Programmierung und genetische Programmierung.* Alternativ können verteilte Suchvorgänge über swarm Intelligence Algorithmen koordinieren. Zwei beliebte Schwarmalgorithmen, die in der Suche verwendet werden, sind Partikelschwarm-Optimierung (inspiriert durch Vogelflockung) und Ameisenkolonie-Optimierung (inspiriert durch Ameisenwege). Logik Logic wird für Wissensdarstellung und Problemlösung verwendet, aber es kann auch auf andere Probleme angewendet werden. Beispielsweise verwendet der Satplan-Algorithmus Logik zur Planung und induktiven Logik-Programmierung ist ein Verfahren zum Lernen. In der KI-Forschung werden verschiedene Formen von Logik verwendet. Propositionslogik beinhaltet Wahrheitsfunktionen wie oder und nicht". Die First-Ordner-Logik fügt Quantifier und Prädikate hinzu und kann Fakten über Objekte, ihre Eigenschaften und ihre Beziehungen zueinander zum Ausdruck bringen. Fuzzy set Theorie ordnet einen "Grad der Wahrheit" (zwischen 0 und 1) zu vage Aussagen wie "Alice ist alt" (oder reich, oder groß, oder hungrig), die zu sprachlich ungenau sind, um vollständig wahr oder falsch zu sein. Fuzzy-Logik wird erfolgreich in Steuerungssystemen verwendet, um Experten zu ermöglichen, vage Regeln wie "Wenn Sie in der Nähe der Zielstation sind und sich schnell bewegen, erhöhen den Bremsdruck des Zugs" beizutragen; diese vagen Regeln können dann numerisch im System verfeinert werden. Fuzzy-Logik lässt sich nicht gut in Wissensbasen skalieren; viele KI-Forscher hinterfragen die Gültigkeit von Kettenfuzzy-Logic Inferenzen. Default-Logiken, nicht-monotonische Logiken und Umschreibungen sind Formen der Logik, die dazu bestimmt sind, mit der Standard-Grenzierung und dem Qualifikationsproblem zu helfen. Mehrere Erweiterungen der Logik wurden entwickelt, um bestimmte Wissensdomänen zu behandeln, wie: Beschreibung Logiken; Situationsrechnung, Ereignisrechnung und fließende Berechnung (für die Darstellung von Ereignissen und Zeit); Kausalrechnung; Glaubensrechnung (Belief-Revision); und Modal-Logik. Logiken zu Modell widersprüchlichen oder inkonsistenten Aussagen, die sich in multiagent Systemen ergeben, wurden ebenfalls entwickelt, wie z.B. parakonsistente Logiken. Probabilistische Methoden zur unsicheren Begründung Viele Probleme in der KI (in der Begründung, Planung, Lernen, Wahrnehmung und Robotik) erfordern den Agenten mit unvollständigen oder unsicheren Informationen zu arbeiten. KI-Forscher haben eine Reihe von leistungsstarken Werkzeugen entwickelt, um diese Probleme mit Methoden aus der Wahrscheinlichkeitstheorie und der Wirtschaft zu lösen. Bayesische Netzwerke sind ein sehr allgemeines Werkzeug, das für verschiedene Probleme verwendet werden kann: Argumentation (unter Verwendung des Bayesischen Inferenzalgorithmus), Lernen (unter Verwendung des Erwartungsmaximierungsalgorithmus), Planung (unter Verwendung von Entscheidungsnetzwerken) und Wahrnehmung (unter Verwendung dynamischer Bayesischer Netzwerke). Probabilistische Algorithmen können auch zum Filtern, Prädiktion, Glätten und Finden von Erklärungen für Datenströme verwendet werden, um Wahrnehmungssysteme zur Analyse von Prozessen zu unterstützen, die im Laufe der Zeit auftreten (z.B. versteckte Markov-Modelle oder Kalman-Filter). Im Vergleich zur symbolischen Logik ist die formale Bayesische Inferenz rechnerisch teuer. Die meisten Beobachtungen müssen bedingt unabhängig voneinander sein. Komplikierte Graphen mit Diamanten oder anderen Schleifen (undirektierte Zyklen) können eine anspruchsvolle Methode wie Markov Kette Monte Carlo erfordern, die ein Ensemble von zufälligen Walkern im gesamten Bayesischen Netzwerk verbreitet und versucht, eine Bewertung der bedingten Wahrscheinlichkeiten zu konvergieren. Bayesische Netzwerke werden auf Xbox Live verwendet, um Spieler zu bewerten und zu treffen; Gewinne und Verluste sind Beweis dafür, wie gut ein Spieler ist. AdSense verwendet ein Bayesisches Netzwerk mit über 300 Millionen Kanten, um zu erfahren, welche Anzeigen zu dienen. Ein Schlüsselbegriff aus der Wirtschaftswissenschaft ist Nutzen: ein Maß dafür, wie wertvoll etwas für einen intelligenten Agenten ist. Präzise mathematische Werkzeuge wurden entwickelt, die analysieren, wie ein Agent Entscheidungen treffen und planen kann, mit Entscheidungstheorie, Entscheidungsanalyse und Informationswerttheorie. Diese Tools umfassen Modelle wie Markov Entscheidungsprozesse, dynamische Entscheidungsnetzwerke, Spieltheorie und Mechanismus-Design. Klassifikatoren und statistische Lernmethoden Die einfachsten KI-Anwendungen können in zwei Arten unterteilt werden: Klassifikatoren ("wenn glänzend dann Diamant") und Controller ("wenn glänzend dann Pick up"). Die Controller klassifizieren aber auch Bedingungen, bevor sie Handlungen unterziehen, und daher bildet die Klassifikation einen zentralen Teil vieler KI-Systeme. Klassifikatoren sind Funktionen, die Musteranpassung verwenden, um eine engste Übereinstimmung zu bestimmen. Sie können nach Beispielen abgestimmt werden, was sie für den Einsatz in KI sehr attraktiv macht. Diese Beispiele sind als Beobachtungen oder Muster bekannt. Bei beaufsichtigtem Lernen gehört jedes Muster zu einer bestimmten vordefinierten Klasse. Eine Klasse ist eine Entscheidung, die getroffen werden muss. Alle Beobachtungen in Verbindung mit ihren Klassenetiketten sind als Datensatz bekannt. Wenn eine neue Beobachtung empfangen wird, wird diese Beobachtung anhand früherer Erfahrungen klassifiziert. Ein Klassifikator kann auf verschiedene Weise ausgebildet werden; es gibt viele statistische und maschinelle Lernansätze. Der Entscheidungsbaum ist vielleicht der am weitesten verbreitete maschinelle Lernalgorithmus. Andere weit verbreitete Klassifikatoren sind das neuronale Netzwerk, k-nearest Nachbaralgorithmus, Kernel-Methoden wie die Unterstützung Vektor Maschine (SVM,) Gaussian Mischung Modell, und der äußerst beliebte naive Bayes Klassifikator. Die Klassifikatorleistung hängt stark von den Merkmalen der zu klassifizierenden Daten ab, wie z.B. der Datensatzgröße, der Verteilung von Proben über Klassen, der Dimensionalität und dem Geräuschpegel. Modellbasierte Klassifikatoren funktionieren gut, wenn das angenommene Modell eine äußerst gute Passform für die tatsächlichen Daten ist. Ansonsten, wenn kein passendes Modell zur Verfügung steht, und wenn die Genauigkeit (anstatt Geschwindigkeit oder Skalierbarkeit) das alleinige Anliegen ist, ist konventionelle Weisheit, dass diskriminierende Klassifikatoren (insbesondere SVM) tendenziell genauer sein als modellbasierte Klassifikatoren wie "nive Bayes" auf den meisten praktischen Datensätzen. Künstliche neuronale Netzwerke Neural-Netzwerke wurden von der Architektur von Neuronen im menschlichen Gehirn inspiriert. Ein einfaches Neuron N akzeptiert den Eingang von anderen Neuronen, von denen jede, wenn aktiviert (oder gefeuert), eine gewichtete Stimme für oder gegen ob Neuron N selbst aktivieren sollte. Lernen erfordert einen Algorithmus, um diese Gewichte basierend auf den Trainingsdaten einzustellen; ein einfacher Algorithmus (gedämpft "Feuer zusammen, Draht zusammen") ist, um das Gewicht zwischen zwei angeschlossenen Neuronen zu erhöhen, wenn die Aktivierung des einen die erfolgreiche Aktivierung des anderen auslöst. Das neuronale Netz bildet Konzepte, die unter einem Subnetz von geteilten Neuronen verteilt werden, die dazu neigen, gemeinsam zu feuern; ein Konzept bedeutet Bein könnte mit einem Subnetz verbunden sein, das den Klang für Fuß einschließt". Neuronen haben ein kontinuierliches Spektrum an Aktivierungen; außerdem können Neuronen nichtlineare Eingaben verarbeiten, sondern gerade Stimmen wägen. Moderne neuronale Netzwerke können sowohl kontinuierliche Funktionen als auch überraschend digitale logische Operationen erlernen. Frühere Erfolge von Neural-Netzwerken waren die Vorhersage des Aktienmarktes und (1995) ein meist selbstfahrendes Auto. In den 2010er Jahren trugen Fortschritte in neuronalen Netzwerken mit tiefem Lernschub KI in weit verbreitetes öffentliches Bewusstsein bei und trugen zu einer enormen Umschichtung in den Ausgaben für Unternehmens-KI bei; zum Beispiel war KI-bezogene M&A im Jahr 2017 mehr als 25 Mal so groß wie im Jahr 2015. Die Studie über nicht-lernende künstliche neuronale Netze begann im Jahrzehnt, bevor der Bereich AI-Forschung gegründet wurde, in der Arbeit von Walter Pitts und Warren McCullouch. Frank Rosenblatt erfand den Perceptron, ein Lernnetzwerk mit einer einzigen Schicht, ähnlich dem alten Konzept der linearen Regression. Zu den frühen Pionieren gehören auch Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello und andere. Die Hauptkategorien von Netzwerken sind azyklische oder zukunftsweisende neuronale Netzwerke (wo das Signal nur in einer Richtung passiert) und wiederkehrende neuronale Netzwerke (die Rückkopplung und Kurzzeitspeicher von vorherigen Eingangsereignissen ermöglichen). Zu den beliebtesten Feedforward-Netzwerken gehören Perceptrons, Mehrschicht-Perceptrons und radiale Basis-Netzwerke. Neurale Netzwerke können auf das Problem der intelligenten Steuerung (für Robotik) oder des Lernens angewendet werden, unter Verwendung von Techniken wie Hebbian Learning ("Feuer zusammen, Draht zusammen") GMDH oder wettbewerbsfähiges Lernen. Heute werden neuronale Netzwerke oft durch den Backpropagationsalgorithmus trainiert, der seit 1970 als umgekehrte Art der automatischen Differenzierung von Seppo Linnainmaa veröffentlicht wurde und von Paul Werbos in neuronale Netzwerke eingeführt wurde. Hierarchischer zeitlicher Speicher ist ein Ansatz, der einige der strukturellen und algorithmischen Eigenschaften des Neokortex modelliert. Zusammenfassend lässt sich sagen, dass die meisten neuronalen Netzwerke auf einer handgeschaffenen neuronalen Topologie eine Art Gradientenabstieg verwenden. Einige Forschergruppen wie Uber argumentieren jedoch, dass eine einfache Neuroevolution zur Mutation neuer neuronaler Netzwerktopologien und Gewichte mit anspruchsvollen Gradientenabstiegsansätzen wettbewerbsfähig sein kann. Ein Vorteil der Neuroevolution ist, dass es weniger anfällig sein kann, in "toten Enden" gefangen zu werden. Deep Feedforward neuronal networks Deep Learning ist die Verwendung künstlicher neuronaler Netzwerke, die mehrere Neuronenschichten zwischen den Eingängen und Ausgängen des Netzwerks aufweisen. Deep Learning hat die Leistung von Programmen in vielen wichtigen Teilbereichen der künstlichen Intelligenz drastisch verbessert, einschließlich Computer Vision, Spracherkennung, natürliche Sprachverarbeitung und andere. Nach einer Übersicht wurde der Ausdruck "Deep Learning" von Rina Dechter im Jahr 1986 in die Machine Learning Community eingeführt und nach Igor Aizenberg und Kollegen in künstliche neuronale Netzwerke im Jahr 2000 Traktion gewonnen. Die ersten funktionellen Deep Learning-Netzwerke wurden 1965 von Alexey Grigorevich Ivakhnenko und V. G. Lapa veröffentlicht. Diese Netzwerke werden zu einer Zeit eine Schicht ausgebildet. Ivakhnenko's 1971 Papier beschreibt das Erlernen eines tiefen Feedforward-Multilayer-Perceptron mit acht Schichten, bereits viel tiefer als viele spätere Netzwerke. Im Jahr 2006 führte eine Veröffentlichung von Geoffrey Hinton und Ruslan Salakhutdinov eine weitere Möglichkeit ein, viele Schichten nach vorn gerichteten neuronalen Netzwerken (FNNs) eine Schicht zu einer Zeit vorzubilden, wobei jede Schicht wiederum als unübertroffene eingeschränkte Boltzmann-Maschine behandelt wird und dann eine überwachte Backpropagation zur Feinabstimmung verwendet wird. Ähnlich wie bei flachen künstlichen neuronalen Netzwerken können tiefe neuronale Netzwerke komplexe nichtlineare Beziehungen modellieren. Deep Learning verwendet oft konvolutionale neuronale Netzwerke (CNNs), deren Ursprung sich auf das von Kunihiko Fukushima 1980 eingeführte Neocognitron zurückverfolgen lässt. Im Jahr 1989 wendeten Yann LeCun und Kollegen eine solche Architektur an. In den frühen 2000er Jahren verarbeiteten CNNs in einer industriellen Anmeldung bereits schätzungsweise 10% bis 20% aller in den USA geschriebenen Kontrollen. Seit 2011 haben schnelle Implementierungen von CNNs auf GPUs viele visuelle Mustererkennung Wettbewerbe gewonnen. CNNs mit 12 Faltungsschichten wurden mit Verstärkungslernen von Deepminds "AlphaGo Lee", dem Programm, das 2016 einen Top-Go-Meister schlug, verwendet. Tief rezidive neuronale Netze Früher wurde auch das tiefe Lernen auf das Sequenzlernen mit wiederkehrenden neuronalen Netzwerken (RNs) angewendet, die theoretisch Turing komplett sind und willkürliche Programme ausführen können, um beliebige Sequenzen von Eingaben zu verarbeiten. Die Tiefe eines RNN ist unbegrenzt und hängt von der Länge seiner Eingabesequenz ab; so ist ein RNN ein Beispiel für tiefes Lernen. RNNs können durch Gradientenabstieg trainiert werden, aber leiden unter dem verschwindenden Gradientenproblem. 1992 wurde gezeigt, dass ununterbrochenes Vortraining eines Stapels von wiederkehrenden neuronalen Netzwerken das anschließende beaufsichtigte Lernen tiefer sequentieller Probleme beschleunigen kann. Zahlreiche Forscher nutzen nun Varianten eines Deep Learning-Rezidivs NN, das 1997 von Hochreiter & Schmidhuber veröffentlichte langfristige Kurzzeitgedächtnis (LSTM). LSTM wird oft von Connectionist Temporal Classification (CTC) trainiert. Bei Google, Microsoft und Baidu hat dieser Ansatz die Spracherkennung revolutioniert. Zum Beispiel, im Jahr 2015, Googles Spracherkennung erlebt einen dramatischen Leistungssprung von 49 % durch CTC-trained LSTM, die jetzt über Google Voice zu Milliarden von Smartphone-Nutzern zur Verfügung steht. Google verwendete auch LSTM zur Verbesserung der maschinellen Übersetzung, der Sprachmodellierung und der Mehrsprachigen Sprachverarbeitung. LSTM kombiniert mit CNNs verbesserte auch die automatische Bildbeschriftung und eine Vielzahl anderer Anwendungen. Die Bewertung des Fortschritts KI, wie Strom oder Dampfmaschine, ist eine allgemeine Anwendungstechnologie. Es gibt keinen Konsens darüber, wie man charakterisieren kann, welche Aufgaben KI zu übertreffen neigt. Während Projekte wie Alpha Zero hat es geschafft, ihr eigenes Wissen von Grund auf zu generieren, viele andere maschinelle Lernprojekte erfordern große Trainingsdatensätze. Forscher Andrew Ng hat vorgeschlagen, als "hohe unvollkommene Faustregel", dass "fast alles, was ein typischer Mensch mit weniger als einer Sekunde des Gedankens tun kann, können wir wahrscheinlich jetzt oder in naher Zukunft mit AI automatisieren." Moravecs Paradox schlägt vor, dass KI Menschen bei vielen Aufgaben hinterlässt, die das menschliche Gehirn speziell entwickelt hat, um gut zu funktionieren. Spiele bieten einen gut publizierten Benchmark zur Bewertung der Fortschrittsraten. AlphaGo um 2016 brachte die Ära der klassischen Board-Game-Benchmarks zu einem Ende. Spiele von unvollkommenem Wissen bieten neue Herausforderungen für KI in der Spieltheorie. E-Sports wie StarCraft bieten weiterhin zusätzliche öffentliche Benchmarks. Viele Wettbewerbe und Preise, wie die Imagenet Challenge, fördern die Forschung in der künstlichen Intelligenz. Zu den häufigsten Wettbewerbsbereichen gehören allgemeine Maschinenintelligenz, Konversationsverhalten, Data-mining, Roboterautos und Roboterfußball sowie konventionelle Spiele. Das "ähnliche Spiel" (eine Interpretation des 1950er Turing-Tests, der beurteilt, ob ein Computer einen Menschen imitieren kann) wird heute als zu ausnutzefähig angesehen, um ein sinnvolles Benchmark zu sein. Eine Ableitung des Turing-Tests ist der vollständig automatisierte Public Turing-Test, um Computer und Humans Apart (CAPTCHA) zu erzählen. Wie der Name impliziert, hilft dies zu bestimmen, dass ein Benutzer eine tatsächliche Person ist und nicht ein Computer, der als Mensch posiert. Im Gegensatz zum Standard Turing-Test, CAPTCHA wird von einer Maschine verabreicht und richtet sich an einen Menschen, im Gegensatz zu von einem Menschen verabreicht und auf eine Maschine gerichtet. Ein Computer fordert einen Benutzer auf, einen einfachen Test abzuschließen und erzeugt dann eine Note für diesen Test. Computer sind nicht in der Lage, das Problem zu lösen, so gelten korrekte Lösungen als das Ergebnis einer Person, die den Test nimmt. Eine gemeinsame Art von CAPTCHA ist der Test, der die Eingabe von verzerrten Buchstaben, Zahlen oder Symbolen erfordert, die in einem Bild von einem Computer unentschlüsselbar erscheinen. Vorgeschlagene "universale Intelligenz"-Tests zielen darauf ab, zu vergleichen, wie gut Maschinen, Menschen und sogar nicht-menschliche Tiere auf möglichst generischen Problemgruppen ausführen. Im Extremfall kann die Test-Suite jedes mögliche Problem enthalten, gewichtet von Kolmogorov Komplexität; leider, diese Problem-Sets neigen dazu, von verarmten Muster-Matching-Übungen dominiert werden, wo eine abgestimmte KI kann leicht über die menschlichen Leistungsniveaus. Verbesserungen der Hardware Seit den 2010er Jahren haben Fortschritte in beiden maschinellen Lernalgorithmen und Computerhardware zu effizienteren Methoden für die Ausbildung von tiefen neuronalen Netzwerken geführt, die viele Schichten von nichtlinearen versteckten Einheiten und eine sehr große Ausgangsschicht enthalten. Bis 2019 hatten grafische Verarbeitungseinheiten (GPUs), oft mit KI-spezifischen Erweiterungen, CPUs als dominante Methode der Ausbildung großräumiger kommerzieller Cloud-KI vertrieben. OpenAI schätzte die Hardware-Rechnung, die in den größten Deep Learning-Projekten von AlexNet (2012) bis AlphaZero (2017) verwendet wird, und fand eine 300.000-fache Erhöhung der benötigten Rechensumme mit einer Laufzeit von 3,4 Monaten. Quellen Russell, Stuart J.; Norvig, Peter (2003). Künstliche Intelligenz: Ein moderner Ansatz (2. ed.). Upper Saddle River, New Jersey: Prentice Hall.ISBN 978-0-13-790395-5.Poole, David; Mackworth, Alan; Goebel, Randy (1998). Berechnung Intelligenz: Logical Approach.New York: Oxford University Press.ISBN 978-0-19-510270-3.Luger, George; Stubblefield, William (2004). Künstliche Intelligenz: Strukturen und Strategien für komplexe Problemlösung (5. ed.). Benjamin/Cummings.ISBN 978-0-8053-4780-7.Nilsson, Nils (1998). Künstliche Intelligenz: Eine neue Synthese. Morgan Kaufmann.ISBN 978-1-55860-467-4.