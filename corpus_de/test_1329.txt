t-distributed stochastische Nachbareinbettung (t-SNE) ist ein statistisches Verfahren zur Visualisierung hochdimensionaler Daten, indem jedem Datenpunkt ein Ort in einer zwei- oder dreidimensionalen Karte gegeben wird. Es basiert auf Stochastic Neighbor Embedding ursprünglich von Sam Roweis und Geoffrey Hinton entwickelt, wo Laurens van der Maaten die t-distributed Variante vorgeschlagen. Es handelt sich um eine nichtlineare Dimensionierungsreduktionstechnik, die für die Einbettung von hochdimensionalen Daten zur Visualisierung in einem flachen Raum von zwei oder drei Dimensionen geeignet ist. Konkret wird jedes hochdimensionale Objekt durch einen zwei- oder dreidimensionalen Punkt so modelliert, dass ähnliche Objekte von nahe gelegenen Punkten modelliert werden und unähnliche Objekte von entfernten Punkten mit hoher Wahrscheinlichkeit modelliert werden. Der t-SNE-Algorithmus umfasst zwei Hauptstufen. Zunächst konstruiert t-SNE eine Wahrscheinlichkeitsverteilung über Paare hochdimensionaler Objekte derart, dass ähnliche Objekte eine höhere Wahrscheinlichkeit zugewiesen werden, während dissimilare Punkte eine geringere Wahrscheinlichkeit zugeordnet werden. Zweitens definiert t-SNE eine ähnliche Wahrscheinlichkeitsverteilung über die Punkte in der Low-dimensionalen Karte, und es minimiert die Kullback-Leibler Divergenz (KL Divergenz) zwischen den beiden Verteilungen in Bezug auf die Orte der Punkte in der Karte. Während der ursprüngliche Algorithmus den euklidischen Abstand zwischen Objekten als Basis seiner Ähnlichkeitsmetrie verwendet, kann dies gegebenenfalls geändert werden. t-SNE wurde für die Visualisierung in einer Vielzahl von Anwendungen verwendet, darunter Genomik, Computersicherheitsforschung, natürliche Sprachverarbeitung, Musikanalyse, Krebsforschung, Bioinformatik, geologische Domäneninterpretation und biomedizinische Signalverarbeitung. Während t-SNE-Plots oft Cluster anzeigen, können die visuellen Cluster stark durch die gewählte Parametrierung beeinflusst werden und daher ein gutes Verständnis der Parameter für t-SNE notwendig ist. Solche Cluster können gezeigt werden, dass sie sogar in nicht abgeschlossenen Daten erscheinen und somit falsche Ergebnisse sein können. Interaktive Explorationen können daher notwendig sein, um Parameter auszuwählen und Ergebnisse zu validieren. Es hat sich gezeigt, dass t-SNE oft in der Lage ist, gut getrennte Cluster zurückzugewinnen, und mit speziellen Parameterwahlen nähert sich eine einfache Form der spektralen Clusterung. Details Bei einem Satz von N {\displaystyle N} hochdimensionale Objekte x 1 , ... , x N {\displaystyle \mathbf {x} {_1},\dots ,\mathbf {x} {_N}, t-SNE berechnet zunächst Wahrscheinlichkeiten p i j {\displaystyle p_{ij}, die proportional zur Ähnlichkeit von Objekten x i {\displaystyle \mathbf {x}_i} und x jth\display} Für i ≠ j {\displaystyle i\neq j} definieren Sie p j ∣ i = exp ≠ ( − ‡ x i − x j ‡ 2 / 2 σ i 2 σ i 2 ) ≠ k ≠ i exp ≠ ( − ‡ x i − x k ¶\ 2 / 2 σ i 2 σ σ i 2 ) {\displaystyle p_{j\mid i}={\\\\\\\\c {\c {\c {\c {\exp( Vert \mathbf {x) - Ja. (...) Vert \mathbf {x) - Ja. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i}=0. Beachten Sie, dass Σ j p j ∣ i = 1 {\displaystyle \sum j}p_{j\mid i}=1 für alle i {\displaystyle i} . Wie Van der Maaten und Hinton erklärten: "Die Ähnlichkeit des Datenpunktes x j {\displaystyle x_{j} zu datapoint x i {\displaystyle x_{i} ist die bedingte Wahrscheinlichkeit, p j  i i {\displaystyle p_{j|, dass x ifin\displaystyle x_{i} x j {\displaystyle 2 N {\displaystyle p_{ij}={\frac P_{j\mid ) + p_ j}{2N und merke, dass p i j = p j i p_{ij}=p_{ji , p i i = 0 {\displaystyle p_{ii}=0 und Σ i, j p i j = 1 {\displaystyle \sum i,j}p_{ij}=1. Die Bandbreite der Gaussischen Kerne σ i {\displaystyle \sigma {_i} wird so eingestellt, dass die Perplexität der bedingten Verteilung eine vorgegebene Perplexität mit dem Bisection-Verfahren entspricht. Dadurch wird die Bandbreite an die Dichte der Daten angepasst: In dichteren Teilen des Datenraums werden kleinere Werte von σ i {\displaystyle \sigma {_i} verwendet. Da der Gaussian-Kernel die Euclidean-Entfernung verwendet, wird x i - x j Vert }, es wird durch den Fluch der Dimensionalität beeinflusst, und in hochdimensionalen Daten, wenn Entfernungen die Fähigkeit zu diskriminieren verlieren, wird die p i j {\displaystyle p_{ij} zu ähnlich (aymptotisch würden sie zu einer Konstanten konvergieren). Es wurde vorgeschlagen, die Abstände mit einer Leistungstransformation, basierend auf der intrinsischen Dimension jedes Punktes, einzustellen, um diese zu lindern. t-SNE zielt darauf ab, eine d {\displaystyle d} -dimensionale Karte y 1 , ... , y N {\displaystyle \mathbf {y} {_1},\dots ,\mathbf {y} {_N} (mit y i\ ε R d {\displaystyle \mathbf {y} {y} Zu diesem Zweck misst es Ähnlichkeiten q i j {\displaystyle q_{ij} zwischen zwei Punkten in der Karte y i {\displaystyle \mathbf {y} {_i} und y j {\displaystyle \mathbf {y} {y} {_j} unter Verwendung eines sehr ähnlichen Ansatzes. Konkret definiert für i ≠ j {\displaystyle i\neq j} q i j {\displaystyle q_{ij} als q i j = ( 1 + zusammengestellt y i - y j hat 2 ) - 1 ≠ k ≠ k ( 1 + ) y k. - y l zusammengestellt 2 ) - 1 {\displaystyle q_{ij}={\frac ({1+\lVert \mathbf {y} {_i}-\mathbf {y} {_j}\rVert 2}{-1}{\sum ) Vert \mathbf {y} {_k}-\mathbf {y} {_l}\rVert 2})^{-1 und set q i = 0 {\displaystyle q_{ii}=0. Hierin wird eine hochqualifizierte Studenten-T-Verteilung (mit einem Grad der Freiheit, das ist die gleiche wie eine Cauchy-Verteilung) verwendet, um Ähnlichkeiten zwischen Low-dimensionalen Punkten zu messen, um unähnliche Objekte in der Karte weit auseinander zu modellieren. Die Orte der Punkte y i {\displaystyle \mathbf {y} {_i} in der Karte werden durch Minimierung der (nichtsymmetrischen) Kullback-Leibler Divergenz der Verteilung P {\displaystyle P} aus der Verteilung Q {\displaystyle Q} bestimmt, d.h.: K L ( P ≠ Q) = Σ i ≠ j p i j log ,,.... ) Die Minimierung der Kullback-Leibler-Divergenz gegenüber den Punkten y i {\displaystyle \mathbf {y} {_i} wird mit Gradientenabstieg durchgeführt. Das Ergebnis dieser Optimierung ist eine Karte, die die Ähnlichkeiten zwischen den hochdimensionalen Eingängen reflektiert. Software Das R-Paket Rtsne implementiert t-SNE in R. ELKI enthält tSNE, auch mit Barnes-Hut Approximation Scikit-learn, ein beliebtes Werkzeug für maschinelles Lernen in python implementiert t-SNE mit genauen Lösungen und die Barnes-Hut Approximation. Tensorboard, das Visualisierungs-Kit mit TensorFlow, implementiert auch t-SNE (online-Version) Referenzen Externe Links Visualisierung von Daten mit t-SNE, Google Tech Talk about t-SNE Implementierungen von t-SNE in verschiedenen Sprachen, Eine Linksammlung von Laurens van der Maaten