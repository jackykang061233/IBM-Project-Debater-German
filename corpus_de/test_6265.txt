Das Entscheiden von Baumlern oder Induktion von Entscheidungsbäumen ist eine der prädiktiven Modellierungsansätze, die in Statistik, Datengewinnung und maschinelles Lernen verwendet werden. Es verwendet einen Entscheidungsbaum (als prognostizierendes Modell) von Beobachtungen über einen Gegenstand (in den Zweigen vertreten) zu Schlussfolgerungen über den Zielwert des Gegenstands (in den Blättern dargestellt). Tree-Modelle, bei denen die Zielvariable einen diskreten Wertsatz annehmen kann, werden Klassifikationsbäume genannt; in diesen Baumstrukturen repräsentieren Blätter Klassenlabels und Äste Konjunktionen von Merkmalen, die zu diesen Klassenlabels führen. Entscheidungsbäume, bei denen die Zielgröße fortlaufende Werte (typischerweise reale Zahlen) annehmen kann, werden Regressionsbäume genannt. Entscheidungsbäume gehören zu den beliebtesten maschinellen Lernalgorithmen angesichts ihrer Verständlichkeit und Einfachheit. In der Entscheidungsanalyse kann ein Entscheidungsbaum verwendet werden, um Entscheidungen und Entscheidungsfindungen visuell und explizit darzustellen. Im Data Mining beschreibt ein Entscheidungsbaum Daten (aber der resultierende Klassifikationsbaum kann ein Input zur Entscheidungsfindung sein). Diese Seite behandelt Entscheidungsbäume im Datenbergbau. Allgemeines Entscheidungs-Baumlernen ist eine Methode, die häufig in der Datengewinnung verwendet wird. Ziel ist es, ein Modell zu schaffen, das den Wert einer Zielvariablen basierend auf mehreren Eingangsvariablen vorhersagt. Ein Entscheidungsbaum ist eine einfache Darstellung zur Klassifizierung von Beispielen. Für diesen Abschnitt nehmen Sie an, dass alle Eingabe-Features endlich diskrete Domänen haben, und es gibt ein einziges Ziel-Feature namens der Klassifizierung". Jedes Element der Domain der Klassifizierung wird als Klasse bezeichnet. Ein Entscheidungsbaum oder ein Klassifikationsbaum ist ein Baum, in dem jeder interne (nicht-blatt) Knoten mit einem Eingabemerkmal gekennzeichnet ist. Die aus einem mit einem Eingabemerkmal markierten Knoten kommenden Lichtbögen werden mit jedem der möglichen Werte des Zielmerkmals markiert oder der Lichtbogen führt zu einem untergeordneten Entscheidungsknoten auf einem anderen Eingabemerkmal. Jedes Blatt des Baumes ist mit einer Klasse oder einer Wahrscheinlichkeitsverteilung über die Klassen gekennzeichnet, wobei der Datensatz durch den Baum entweder in eine bestimmte Klasse oder in eine bestimmte Wahrscheinlichkeitsverteilung (die, wenn der Entscheidungsbaum gut aufgebaut ist, zu bestimmten Untergruppen von Klassen gesät wird) klassifiziert wurde. Ein Baum wird gebaut, indem der Quellsatz, der den Wurzelknoten des Baumes bildet, in Teilmengen aufgeteilt wird, die die Nachfolgekinder darstellen. Die Spaltung basiert auf einer Reihe von Spaltungsregeln basierend auf Klassifizierungsmerkmalen. Dieser Vorgang wird auf jeder abgeleiteten Teilmenge wiederkehrend als rekursive Partitionierung bezeichnet wiederholt. Die Rekursion wird beendet, wenn die Teilmenge an einem Knoten alle gleichen Werte der Zielgröße aufweist, oder wenn die Spaltung den Vorhersagen keinen Wert mehr addiert. Dieser Prozess der Top-down-Induktion von Entscheidungsbäumen (TDIDT) ist ein Beispiel eines gierigen Algorithmus, und es ist bei weitem die häufigste Strategie für das Lernen von Entscheidungsbäumen aus Daten. Im Data Mining können Entscheidungsbäume auch als Kombination von mathematischen und rechnerischen Techniken beschrieben werden, um die Beschreibung, Kategorisierung und Verallgemeinerung eines bestimmten Datensatzes zu unterstützen. Die Daten sind in Aufzeichnungen des Formulars: ( x , Y ) = ( x 1 , x 2 , x 3 , . . , x k , Y ) {\displaystyle {\(textbf x},Y)=(x_{1},x_{2},x_{3},...,x_{k},Y Die abhängige Variable Y {\displaystyle Y} ist die Zielvariable, die wir verstehen, klassifizieren oder verallgemeinern. Der Vektor x {\displaystyle {\textbf {x} besteht aus den Merkmalen x 1 , x 2 , x 3 {\displaystyle x_{1},x_{2},x_{3 usw., die für diese Aufgabe verwendet werden. Entscheidung Baumtypen Entscheidung Bäume, die in der Datengewinnung verwendet werden, sind von zwei Haupttypen: Klassifikation Baumanalyse ist, wenn das vorhergesagte Ergebnis die Klasse ist (diskret), zu der die Daten gehören. Regressionsbaumanalyse ist, wenn das vorhergesagte Ergebnis als eine reale Anzahl betrachtet werden kann (z.B. der Preis eines Hauses, oder die Länge des Aufenthaltes eines Patienten in einem Krankenhaus). Der Begriff Klassifikations- und Regressionsbaum (CART)-Analyse ist ein Begriff, der verwendet wird, um entweder auf die oben genannten Verfahren, zuerst von Breiman et al.in 1984 eingeführt. Bäume, die für Regressionen und Bäume verwendet werden, haben einige Ähnlichkeiten - aber auch einige Unterschiede, wie das Verfahren verwendet, um zu bestimmen, wo zu teilen. Einige Techniken, oft Ensemble-Methoden genannt, bauen mehr als einen Entscheidungsbaum: Gefrorene Bäume Incremental ein Ensemble zu bauen, indem jede neue Instanz trainiert wird, um die Trainingsinstanzen zu betonen, die zuvor falsch gestaltet wurden. Ein typisches Beispiel ist AdaBoost. Diese können für Regressions- und Klassifikationsprobleme verwendet werden. Bootstrap aggregierte (oder gebeutelte) Entscheidungsbäume, eine frühe Ensemble-Methode, baut mehrere Entscheidungsbäume durch wiederholtes Resampling Trainingsdaten mit Ersatz, und die Bäume für eine Konsensvorhersage abzustimmen. Ein zufälliger Waldklassifikator ist eine bestimmte Art von Bootstrap-Aggregation Rotation Forest – in dem jeder Entscheidungsbaum durch erste Anwendung der Hauptkomponentenanalyse (PCA) auf eine zufällige Teilmenge der Eingabefunktionen trainiert wird. Ein besonderer Fall eines Entscheidungsbaumes ist eine Entscheidungsliste, die ein einseitiger Entscheidungsbaum ist, so dass jeder interne Knoten genau 1 Blattknoten und genau 1 interner Knoten als Kind hat (außer dem untersten Knoten, dessen einziges Kind ein einziger Blattknoten ist). Während weniger ausdrucksstark, sind Entscheidungslisten aufgrund ihrer zusätzlichen Sparsamkeit wohl leichter zu verstehen als allgemeine Entscheidungsbäume, die nicht gierige Lernmethoden und monotone Zwänge auferlegen lassen. Wichtige Entscheidungsbaumalgorithmen umfassen: ID3 (Iterative Dichotomiser 3) C4.5 (Erfolg von ID3) CART (Klassifikation und Regressionsbaum)Chi-Quadrat automatische Interaktionserkennung (CHAID). führt Multi-Level-Splits bei der Berechnung von Klassifikationsbäumen durch. MARS: erweitert Entscheidungsbäume, um numerische Daten besser zu handhaben. Conditional Inference Trees. Statistikbasierter Ansatz, der nichtparametrische Tests als Splitting-Kriterien verwendet, korrigiert für mehrere Tests, um eine Überarbeitung zu vermeiden. Dieser Ansatz führt zu einer unvoreingenommenen Prädiktorauswahl und erfordert keine Unterbrechung. ID3 und CART wurden um 1970 und 1980 unabhängig voneinander erfunden (zwischen 1970 und 1980), doch folgen sie einem ähnlichen Ansatz für das Lernen eines Entscheidungsbaumes von Ausbildungstupeln. Es wurde auch vorgeschlagen, Konzepte der fuzzy set Theorie für die Definition einer speziellen Version von Entscheidungsbaum, bekannt als Fuzzy Decision Tree (FDT) zu nutzen. Bei dieser Art der Fuzzy-Klassifikation ist in der Regel ein Eingangsvektor x {\displaystyle {\textbf {x} mit mehreren Klassen verbunden, die jeweils einen anderen Konfidenzwert aufweisen. Auch verstärkte Ensembles von FDTs wurden vor kurzem untersucht, und sie haben Performances gezeigt, die mit denen anderer sehr effizienter Fuzzy-Klassifikatoren vergleichbar sind. Metrics Algorithmen für den Aufbau von Entscheidungsbäumen arbeiten in der Regel top-down, indem Sie eine Variable an jedem Schritt, die am besten teilt die Menge der Elemente. Verschiedene Algorithmen verwenden verschiedene Metriken zum Besten. Diese messen in der Regel die Homogenität der Zielgröße innerhalb der Teilmengen. Einige Beispiele sind unten aufgeführt. Diese Metriken werden auf jede Kandidatenuntermenge angewendet, und die resultierenden Werte werden (z.B. gemittelt) kombiniert, um ein Maß für die Qualität der Spaltung bereitzustellen. Gini Verunreinigung Verwendet durch den CART (Klassifikations- und Regressionsbaum) Algorithmus für Klassifikationsbäume, Gini Unreinheit (nach italienischem Mathematiker Corrado Gini benannt) ist ein Maß dafür, wie oft ein zufällig gewähltes Element aus dem Set falsch markiert würde, wenn es zufällig nach der Verteilung von Etiketten in der Untermenge markiert wurde. Die Gini-Unreinheit kann berechnet werden, indem die Wahrscheinlichkeit p i {\displaystyle p_{i} eines Elements mit dem Label i {\displaystyle i} addiert wird, wobei die Wahrscheinlichkeit k ≠ i p k = 1 - p i {\displaystyle \sum {_k\neq gewählt wird i}p_{k}=1-p_{i eines Fehlers bei der Kategorisierung dieses Artikels. Er erreicht sein Minimum (Null), wenn alle Fälle im Knoten in eine einzelne Zielkategorie fallen. Die Gini-Unreinheit ist auch eine informationstheoretische Maßnahme und entspricht Tsallis Entropy mit Verformungskoeffizienten q = 2 {\displaystyle q=2}, die in der Physik mit dem Fehlen von Informationen in außerequilibrium-, nicht-extensiven, dissipativen und Quantensystemen verbunden ist. Für die Grenze q → 1 {\displaystyle q\to 1} erholt man die üblichen Boltzmann-Gibbs oder Shannon Entropie. In diesem Sinne ist die Gini-Unreinheit nur eine Variation der üblichen Entropiemaßnahme für Entscheidungsbäume. Um Gini Unreinheit für eine Reihe von Gegenständen mit J {\displaystyle J} Klassen zu berechnen, annehmen i ε { 1 , 2 , . ., J } {\displaystyle i\in {1,2,...,J}, und lassen p i {\displaystyle p_{i} der Bruchteil von Gegenständen sein, die mit Klasse i {\displaystyle i} im Set markiert sind. I G schwerpunkt (p ) = Σ i = 1 J (p i Σ k ≠ i p k ≠) = Σ i = 1 J p i ( 1 - p i ) = Σ i = 1 J ( p i - p i 2 ) = Σ i = 1 J p i - Σ i = 1 J p i 2 = 1 - Σ i = 1 J p i 2 {I} {_G}(p)=\sum i=1{J}\left(p_{i}\sum {_k\neq i}p_{k}\right)=\sum i=1^{J}p_{i}(1-p_{i}=\sum i=1}^{J}(p_{i}-p_{i}^{2})=\sum I=1{J}p_{i} I=1{J}p_{i}{2}=1=\sum ) Information Gain verwendet von den Algorithmen ID3, C4.5 und C5.0 Baumgeneration. Der Informationsgewinn basiert auf dem Konzept der Entropie und Informationsinhalte aus der Informationstheorie. Entropy wird wie folgt definiert (T ) = I E ‡ ( p 1 , p 2 , ... , p J ) = - Σ i = 1 J p i log 2 {H} (T)=\Operatorname {I} E}\left(p_{1},p_{2},\ldots ,p_{J}\right)=-\sum i=1^{J}p_{i}\log 2}p_{i, wobei p 1 , p 2 , ... {\displaystyle p_{1},p_{2},\ldots } Anteile sind, die bis zu 1 addieren und den Prozentsatz jeder im Kinderknoten vorhandenen Klasse darstellen, der aus einer Spaltung im Baum resultiert. = - Σ i = 1 J p i log 2 — Σ i = 1 J − Pr ( i ∣ a ) log 2 ‡ Pr ( i ∣ a ) {\displaystyle =\-sum ) 2 _i=1{J}-\Pr(i\mid a)\log {_2}\Pr(i\mid a}) Mittelung über die möglichen Werte von A {\displaystyle A}, = - Σ i = 1 J p i log 2 — Σ a p (a) Σ i = 1 J - Pr (i ∣ a ) log 2 ) 2 Das heißt, der erwartete Informationsgewinn ist die gegenseitige Information, d.h. die Verringerung der Entropie von T ist im Durchschnitt die gegenseitige Information. Der Informationsgewinn wird verwendet, um zu entscheiden, welche Funktion bei jedem Schritt beim Bau des Baumes aufteilen soll. Einfachheit ist am besten, so wollen wir unseren Baum klein halten. Dazu sollten wir bei jedem Schritt die Spaltung wählen, die zu den konsistentsten Kindknoten führt. Ein häufig verwendetes Maß für Konsistenz wird als Information bezeichnet, die in Bits gemessen wird. Für jeden Knoten des Baumes stellt der Informationswert "die erwartete Menge an Informationen dar, die erforderlich wären, um anzugeben, ob eine neue Instanz Ja oder Nein klassifiziert werden sollte, da das Beispiel diesen Knoten erreicht hat". Betrachten Sie einen Beispieldatensatz mit vier Attributen: Ausblick (sunnych, trübe, regnerisch), Temperatur (heiß, mild, kühl,) Feuchtigkeit (hoch, normal,) und windig (wahr, falsch,) mit einer binären (ja oder nein) Zielgröße, Spiel und 14 Datenpunkte. Um einen Entscheidungsbaum für diese Daten zu konstruieren, müssen wir den Informationsgewinn jeder von vier Bäumen vergleichen, die jeweils auf eine der vier Merkmale aufgeteilt sind. Die Spaltung mit dem höchsten Informationsgewinn wird als erste Spaltung genommen und der Prozess wird fortgesetzt, bis alle Kinderknoten jeweils konsistente Daten haben, oder bis der Informationsgewinn 0 ist. Um den Informationsgewinn der Spaltung mittels windig zu finden, müssen wir zunächst die Informationen in den Daten vor der Spaltung berechnen. Die ursprünglichen Daten enthielten neun Jas und fünf Neins. I E ( [9 , 5 ] ) = - 9 14 log 9 14 - 5 14 log 2 ζ 5 14 = 0,94 {\displaystyle I_{E}([9,5])=-{\frac 9}{14}\log _2}{\frac 9}{14}-{\frac 5}{14}}\log _2}{\frac 5}{14}=0,94 Die Spaltung mit dem Feature windig Ergebnisse in zwei Kinder-Knoten, eine für einen windigen Wert von true und eine für einen windigen Wert von false. In diesem Datensatz gibt es sechs Datenpunkte mit einem wahren windigen Wert, von denen drei einen Spielwert (wo Spiel die Zielgröße ist) von Ja und drei mit einem Spielwert von Nein haben. Die acht verbleibenden Datenpunkte mit einem windigen Wert von false enthalten zwei Neins und sechs Jas. Die Information des windy=true-Knotens wird anhand der obigen Entropiegleichung berechnet. Da in diesem Knoten eine gleiche Anzahl von Ja's und Nein's vorhanden ist, haben wir I E ( [3 , 3 ] ) = - 3 6 log 2 ≠ 3 6 - 3 6 log 2 ≠ 3 6 = - 1 2 log 2 = 1 2 log 2 · 1 2 = 1 {\displaystyle I_{E}(3,3])=-{frac 3}{6}}} 3{6}-{\frac 3}{6}}\log_2}{\frac 3}{6}=-{\frac 1}{2}}\log_2}{\frac {\cH00FF} {\cH00FF}}{2}}}}}{2}}}}}}{2}{}}}}}{2}}}}}}}}{2}}}}}}}} Für den Knoten, wo windy=Es gab acht Datenpunkte, sechs Jas und zwei Neins. Es handelt sich also um I E ( [6 , 2 ] ) = - 6 8 log 2 6 8 - 2 8 log 2 = - 3 4 log 2 4 - 1 4 log 2 zusammen 1 4 = 0,81 2 3 ) 1{4}=0,81 Um die Informationen der Spaltung zu finden, nehmen wir den gewichteten Durchschnitt dieser beiden Zahlen, basierend auf, wie viele Beobachtungen in welchen Knoten fielen. I = I E ( windig oder nicht) = 6 14 ⋅ 1 + 8 14 ⋅ 0.81 = 0.89 {\displaystyle I_{E}([3,3],[6,2\]) = I_{E}(\text{windy oder nicht}={\frac 6}{14}\c}\ Jetzt können wir die durch die Aufteilung auf die windige Funktion erzielten Informationsgewinne berechnen. IG = I E ( [9 , 5 ] ) - I E ( [ 3 , 3 ], [ 6 , 2 ] ) = 0.94 - 0.89 = 0.05 {\displaystyle \operatorname {IG} \text{windy}})=I_{E}([9,5])-I_{E}([3,3],[6,2])=0.94-0.89=0.05 Um den Baum zu bauen, müsste der Informationsgewinn jedes möglichen ersten Splits berechnet werden. Die beste erste Split ist diejenige, die die meisten Informationen gewinnen. Dieser Vorgang wird für jeden unreinen Knoten wiederholt, bis der Baum vollständig ist. Dieses Beispiel wird aus dem in Witten et al. Variantenreduktion In CART eingeführt, wird Varianzreduktion häufig in Fällen verwendet, in denen die Zielgröße kontinuierlich ist (Regressionsbaum), d.h. dass die Verwendung von vielen anderen Metriken zuerst eine Diskretierung erfordern würde, bevor sie angewendet werden. Die Varianzreduktion eines Knotens N wird durch die Aufteilung an diesem Knoten als Gesamtreduktion der Varianz der Zielgröße Y definiert: I V (N ∈ = 1 | S | 2 Σ i ∈ S Σ j ∈ S 1 2 ( y i - y j ) 2 − () S t | 2 | S | 2 1 | S t | 2 ∈ 2 ∈ 2 ∈ S ∈ S t ∈ 2 ( y j play) 2 + ∈ S f ∈ 2 ∈ S ∈ S | 2 | S | 2 | S | 2 | 2 | 2 | 2 | S | 2 | 2 | S | 2 | 2 | 2 | 2 | S | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 ) 2 ) 2 ) 2 ) 2 ) 2 | 2 ) 2 ) 2  I_{V}(N)={\frac 1{}{\}}\sum {_i\in S}\sum {_j\in S}{\fra 1{2}(y_{i}-y_{j}{2}-\left({\fra) S_{t}|{2}{|}}{\fra}}{\fra\fra} {\cH00FF}} (i) S_{t} ) S_{f}{2}{\}}{}}}{\fra\fra} {\cH00FF}} (i) S_{f} S_{f}{\fra 1}{2}(y_{i}-y_{j})^^{2}\rechts, wo S {\displaystyle S}, S t {\displaystyle S_{t} und S f {\displaystyle S_{f} die Menge von voreingestellten Musterindizes, die Menge der Musterindizes, für die der Spalttest wahr ist, und die Menge der Musterindizes, für die der Spalttest falsch ist, sind. Jeder der oben genannten Summenbefehle ist in der Tat Varianzschätzungen, obwohl, in einer Form geschrieben, ohne direkt auf den Mittelwert zu beziehen. Die von CART im Jahr 1984 genutzte Maßnahme der Güte ist eine Funktion, die das Gleichgewicht der Fähigkeit einer Kandidatenspalte zu optimieren versucht, reine Kinder mit ihrer Fähigkeit zu schaffen, gleichgroße Kinder zu schaffen. Dieser Vorgang wird für jeden unreinen Knoten wiederholt, bis der Baum vollständig ist. Die Funktion φ (s ∣ t ) {\displaystyle \varphi (s\mid t}) , wobei s {\displaystyle s} ein Kandidat ist, der am Knoten t {\displaystyle t} liegt, wird wie folgt definiert φ (s ∣ t ) = 2 P L P R ≠ j = 1 Klassenzahl var P (j var t L var t_{R} sind die linken und rechten Kinder des Knotens t {\displaystyle t} unter Verwendung von Split s {\displaystyle s}; P L {\displaystyle P_{L} und P R {\displaystyle P_{R} sind die Anteile der Datensätze in t\displaystyle t} in t Betrachten Sie einen Beispieldatensatz mit drei Attributen: Einsparungen (niedrig, mittel, hoch,) Vermögenswerte(niedrig, mittel, hoch), Einkommen(numerischer Wert,) und ein binäres Ziel variables Kreditrisiko (gut, schlecht) und 8 Datenpunkte. Die vollständigen Daten werden in der nachfolgenden Tabelle dargestellt. Um einen Entscheidungsbaum zu starten, berechnen wir den Maximalwert von φ (s ∣ t ) {\displaystyle \varphi (s\mid t}) mit jeder Funktion, um zu finden, welche man den Wurzelknoten spaltet. Dieser Vorgang wird fortgesetzt, bis alle Kinder rein sind oder alle Werte φ (s ∣ t ) {\displaystyle \varphi (s\mid t}) unterhalb einer festgelegten Schwelle liegen. Um φ ( s ∣ t ) {\displaystyle \varphi (s\mid t}) der Feature-Ersparnisse zu finden, müssen wir die Menge jedes Wertes beachten. Die ursprünglichen Daten enthielten drei Lows, drei Mediums und zwei Highs. Aus dem Low's, hatte man ein gutes Kreditrisiko, während aus dem Medium und High, 4 hatte ein gutes Kreditrisiko. Nehmen Sie an, dass ein Kandidat geteilt s {\displaystyle s} so, dass Aufzeichnungen mit einer niedrigen Einsparung in das linke Kind und alle anderen Aufzeichnungen in das richtige Kind gesetzt werden. φ (s ∣ root ⋅) = 2 ⋅ 3 8 ∙ 5 8 ⋅ ( ⋅ ( 1 3 - 4 5 ) | + | ( 2 3 - 1 5 ) ) )) = 0.44 {\displaystyle \varphi (s\mid text{root}}=2\cdot\frac 3}{8}\cdot {\frac 5}{8}}}}}} 1}{3}-{\frac 4}{5}}\right)\right|+\left|\left({\frac 2}{3}-{\frac 1}{5}\right)\right.\right)=0,44 Um den Baum zu bauen, muss die Güte aller Kandidatenspalten für den Wurzelknoten berechnet werden. Der Kandidat mit dem Maximalwert wird den Wurzelknoten teilen, und der Prozess wird für jeden unreinen Knoten bis zum vollständigen Baum fortgesetzt. Im Vergleich zu anderen Metriken wie dem Informationsgewinn wird das Maß der Güte versuchen, einen ausgewogeneren Baum zu schaffen, der zu einer konsistenteren Entscheidungszeit führt. Es opfert jedoch einige Priorität für die Schaffung von reinen Kindern, die zu zusätzlichen Spaltungen führen können, die nicht mit anderen Metriken vorhanden sind. Vorteile Neben anderen Methoden der Datengewinnung haben Entscheidungsbäume verschiedene Vorteile: Einfach zu verstehen und zu interpretieren. Nach einer kurzen Erklärung sind die Menschen in der Lage, Entscheidungsbaummodelle zu verstehen. Bäume können auch grafisch in einer Weise angezeigt werden, die für Nicht-Experten leicht zu interpretieren ist. Kann sowohl numerische als auch kategorische Daten verarbeiten. Andere Techniken sind in der Regel auf die Analyse von Datensätzen spezialisiert, die nur eine Art von Variable haben. ( Beispielsweise können Beziehungsregeln nur mit Soll-Variablen verwendet werden, während neuronale Netzwerke nur mit numerischen Variablen oder Kategoriken verwendet werden können, die in 0-1 Werte umgewandelt werden.) Frühe Entscheidung Bäume waren nur in der Lage, kategorische Variablen zu behandeln, aber neuere Versionen, wie C4.5, haben diese Einschränkung nicht. Erfordert wenig Datenaufbereitung. Andere Techniken erfordern oft eine Datennormalisierung. Da Bäume mit qualitativen Vorhersagen umgehen können, ist es nicht notwendig, Dummy-Variablen zu erstellen. Verwenden Sie eine weiße Box oder Open-Box-Modell. Wenn eine gegebene Situation in einem Modell beobachtbar ist, wird die Erklärung für den Zustand leicht durch boolesche Logik erklärt. Im Gegensatz dazu ist bei einem schwarzen Kastenmodell die Erläuterung der Ergebnisse typischerweise schwierig zu verstehen, beispielsweise mit einem künstlichen neuronalen Netz. Möglich, ein Modell mit statistischen Tests zu validieren. Damit kann die Zuverlässigkeit des Modells berücksichtigt werden. Nichtparametrischer Ansatz, der keine Annahmen der Trainingsdaten oder Prädiktionsreste macht; z.B. keine Verteilungs-, Unabhängigkeits- oder Konstantvarianzannahmen Passt gut mit großen Datensätzen. Große Datenmengen können mit Standard-Computing-Ressourcen in angemessener Zeit analysiert werden. Verspiegelt die menschliche Entscheidung enger als andere Ansätze. Dies könnte nützlich sein, wenn man menschliche Entscheidungen / Verhaltensweisen modelliert. Robust gegen Co-Linearität, besonders ansteigend In der integrierten Feature-Auswahl. Zusätzliche irrelevante Funktion wird weniger verwendet, so dass sie auf nachfolgenden Laufwerken entfernt werden können. Die Hierarchie der Attribute in einem Entscheidungsbaum spiegelt die Bedeutung der Attribute wider. Es bedeutet, dass die Merkmale oben am informativsten sind. Entscheidungsbäume können jede Boolesche Funktion z.B. XOR annähern. Einschränkungen Bäume können sehr unrobust sein. Eine kleine Änderung der Trainingsdaten kann zu einer großen Veränderung des Baumes und damit zu den endgültigen Vorhersagen führen. Das Problem des Lernens eines optimalen Entscheidungsbaumes ist bekanntermaßen unter mehreren Aspekten der Optimität und auch für einfache Konzepte NP-vollständig. Praktische Entscheidungs-Lehralgorithmen basieren somit auf Heuristiken wie dem gierigen Algorithmus, in dem lokal optimale Entscheidungen an jedem Knoten getroffen werden. Solche Algorithmen können nicht garantieren, den global optimalen Entscheidungsbaum zurückzugeben. Um den gierigen Effekt der lokalen Optimität zu reduzieren, wurden einige Methoden wie der duale Informationsabstand (DID) Baum vorgeschlagen. Entscheidende Lerner können überkomplexe Bäume schaffen, die sich nicht gut aus den Trainingsdaten verallgemeinern. (Dies ist bekannt als Overfitting.) Um dieses Problem zu vermeiden, sind Mechanismen wie das Abschneiden notwendig (mit Ausnahme einiger Algorithmen wie dem Conditional Inference-Ansatz, der kein Abschneiden erfordert). Die durchschnittliche Tiefe des Baumes, der durch die Anzahl der Knoten oder Tests bis zur Klassifizierung definiert wird, ist unter verschiedenen Spaltungskriterien nicht minimal oder klein. Für Daten einschließlich kategorischer Variablen mit unterschiedlichen Ebenenzahlen wird der Informationsgewinn in Entscheidungsbäumen zugunsten von Attributen mit mehr Ebenen vorgespannt. Allerdings wird das Problem der voreingenommenen Prädiktorauswahl durch den Conditional Inference-Ansatz, einen zweistufigen Ansatz oder adaptive Leave-one-out-Funktionsauswahl vermieden. Implementierungen Viele Data Mining Softwarepakete bieten Implementierungen eines oder mehrerer Entscheidungsbaumalgorithmen. Beispiele sind Salford Systems CART (die den proprietären Code der ursprünglichen CART-Autoren lizenzierte), IBM SPSS Modeler, RapidMiner, SAS Enterprise Miner, Matlab, R (eine Open-Source-Software-Umgebung für statistisches Computing, die mehrere CART-Implementierungen wie rpart, party und randomForest-Pakete umfasst,) Weka (eine kostenlose und Open-Source-Daten-Mining-Suite, Microsoft SQL-Algorithmen,Erweiterungen Entscheidungsdiagramme In einem Entscheidungsbaum verlaufen alle Pfade vom Wurzelknoten zum Blattknoten über Konjunktion oder AND. In einem Entscheidungsdiagramm können Disjunktionen (ORs) verwendet werden, um zwei weitere Pfade mit minimaler Nachrichtenlänge (MML) zusammenzufügen. Die Entscheidungsdiagramme wurden weiter erweitert, um es zu ermöglichen, dass zuvor nicht gegliederte neue Attribute dynamisch erlernt und an verschiedenen Orten innerhalb des Diagramms verwendet werden. Das allgemeinere Kodierungsschema führt zu einer besseren Vorhersagegenauigkeit und log-losss probabilistic scoring. In der Regel unterziehen Entscheidungsdiagramme Modelle mit weniger Blättern als Entscheidungsbäume. Alternative Suchmethoden Evolutionäre Algorithmen wurden verwendet, um lokale optimale Entscheidungen zu vermeiden und den Entscheidungsbaumraum mit wenig a priori Vorspannung zu suchen. Es ist auch möglich, dass ein Baum mit MCMC abgetastet wird. Der Baum kann nach unten gesucht werden. Oder mehrere Bäume können parallel aufgebaut werden, um die erwartete Anzahl der Tests bis zur Klassifizierung zu reduzieren. Siehe auch Referenzen Weiter lesen James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2017)."Tree-Based Methods" (PDF). Einführung in das Statistische Lernen: mit Anwendungen in R. New York: Springer.pp.303–336.ISBN 978-1-4614-7137-0. Externe Links Evolutionary Learning of Decision Trees in C+ Eine sehr detaillierte Erklärung der Informationsgewinn als Splitterkriterium