Bei der digitalen Bildgebung ist ein Pixel-, Becken- oder Bildelement ein kleinstes adressierbares Element in einem Rasterbild oder das kleinste adressierbare Element in allen Punkten adressierbare Anzeigevorrichtung; es ist also das kleinste steuerbare Element eines auf dem Bildschirm dargestellten Bildes. Jedes Pixel ist eine Probe eines Originalbildes; mehr Proben liefern typischerweise genauere Darstellungen des Originals. Die Intensität jedes Pixels ist variabel. Bei Farbabbildungssystemen wird typischerweise eine Farbe durch drei oder vier Komponentenintensitäten wie Rot, Grün, Blau oder Cyan, Magenta, Gelb und Schwarz dargestellt. In einigen Kontexten (wie Beschreibungen von Kamerasensoren) bezieht sich Pixel auf ein einziges Skalarelement einer Mehrkomponentendarstellung (im Kamerasensor-Kontext als Photosit bezeichnet, obwohl sensel manchmal verwendet wird), während es sich in anderen Kontexten auf die Menge von Bauteilintensitäten für eine Raumposition beziehen kann. Etymologie Das Wort Pixel ist eine Kombination von pix (aus Bildern, verkürzt auf Bilder) und el (für Element;) ähnliche Formen mit el umfassen die Wörter voxel und texel. Das Wort pix erschien in Variety Magazin Schlagzeilen 1932, als Abkürzung für die Wortbilder, in Bezug auf Filme. 1938 wurde pix in Bezug auf noch Bilder von Photojournalisten verwendet. Das Wort Pixel wurde erstmals 1965 von Frederic C. Billingsley von JPL veröffentlicht, um die Bildelemente gescannter Bilder von Raumsonden zum Mond und Mars zu beschreiben. Billingsley hatte das Wort von Keith E. McFarland, in der Link Division of General Precision in Palo Alto, die wiederum sagte, er wusste nicht, wo es stammte. McFarland sagte einfach, es sei "in der Zeit" (ca. 1963). Das Konzept eines "Bildelements" stammt aus den frühesten Tagen des Fernsehens, zum Beispiel als Bildpunkt (das deutsche Wort für Pixel, buchstäblich "Bildpunkt)" im 1888 deutschen Patent von Paul Nipkow. Nach verschiedenen etymologien war die früheste Veröffentlichung des Begriffs Bildelement selbst im Wireless World Magazin 1927, obwohl es früher in verschiedenen US-Patententen verwendet worden war, die bereits 1911 eingereicht wurden. Einige Autoren erklären Pixel als Bildzelle, bereits 1972. In Grafiken und in der Bild- und Videoverarbeitung wird anstelle von Pixel oft Pel verwendet. Zum Beispiel, IBM verwendet es in ihrer technischen Referenz für den ursprünglichen PC.Pixels, abgekürzt als px, sind auch eine Einheit der Messung, die häufig in Grafik und Web-Design verwendet wird, entspricht etwa 1⁄96 Zoll (0,26 mm). Diese Messung wird verwendet, um sicherzustellen, dass ein bestimmtes Element wie die gleiche Größe angezeigt wird, egal, welche Bildschirmauflösung es anzeigt. Pixilation, mit einem zweiten i buchstabiert, ist eine unabhängige Filmtechnik, die zu den Anfängen des Kinos, in denen Live-Assistenten werden Frame von Frame und fotografiert, um Stop-Motion-Animation zu erstellen. Ein archaisches britisches Wort, das "Besessenheit durch Geister (Pixies,)" bedeutet, wurde verwendet, um den Animationsprozess seit den frühen 1950er-Jahren zu beschreiben; verschiedene Animatoren, darunter Norman McLaren und Grant Munro, werden mit der Popularisierung ausgezeichnet. Technische Daten Ein Pixel wird im Allgemeinen als kleinste Einzelkomponente eines digitalen Bildes betrachtet. Die Definition ist jedoch sehr kontextempfindlich. Beispielsweise können in einer Seite "gedruckte Pixel" oder Pixel mit elektronischen Signalen oder durch digitale Werte oder Pixel auf einer Anzeigeeinrichtung oder Pixel in einer digitalen Kamera (Photosensorelemente) dargestellt sein. Diese Liste ist nicht vollständig und, je nach Kontext, Synonyme enthalten Becken, Probe, Byte, Bit, Punkt und Spot. Pixel können als Maßeinheit verwendet werden, z.B.: 2400 Pixel pro Zoll, 640 Pixel pro Zeile oder 10 Pixel Abstand. Die Maße Punkte pro Zoll (dpi) und Pixel pro Zoll (ppi) werden manchmal austauschbar verwendet, haben aber insbesondere für Druckergeräte deutliche Bedeutungen, wobei dpi ein Maß für die Punktdichte des Druckers (z.B. Tintentropfen) ist. Beispielsweise kann ein hochwertiges Fotobild mit 600 ppi auf einem 1200 dpi Tintenstrahldrucker gedruckt werden. Noch höhere dpi-Zahlen, wie die 4800 dpi zitiert von Druckerherstellern seit 2002, bedeuten nicht viel in Bezug auf erreichbare Auflösung. Je mehr Pixel verwendet werden, um ein Bild darzustellen, desto näher kann das Ergebnis dem Original ähneln. Die Anzahl der Pixel in einem Bild wird manchmal als Auflösung bezeichnet, obwohl Auflösung eine spezifischere Definition hat. Pixelzählungen können als eine einzige Zahl ausgedrückt werden, wie in einer drei Megapixel Digitalkamera, die eine nominelle drei Millionen Pixel hat, oder als Zahlenpaar, wie in einem "640 by 480 Display", das 640 Pixel von Seite zu Seite und 480 von oben nach unten (wie in einem VGA-Display) hat und daher eine Gesamtzahl von 640 × 480 = 307,200 Pixel aufweist, oder 0,3. Die Pixel oder Farbmuster, die ein digitalisiertes Bild (z.B. eine JPEG-Datei, die auf einer Webseite verwendet wird) bilden, können oder dürfen nicht in einer Korrespondenz mit Bildschirmpixeln liegen, je nachdem, wie ein Computer ein Bild anzeigt. Bei der Berechnung wird ein aus Pixeln zusammengesetztes Bild als bitmapped image oder als Rasterbild bezeichnet. Das Wort raster stammt von Fernseh-Scanmustern und wurde weit verbreitet, um ähnliche Rasterdruck- und Speichertechniken zu beschreiben. Stichprobenmuster Zur Bequemlichkeit sind in der Regel Pixel in einem regelmäßigen zweidimensionalen Raster angeordnet. Durch diese Anordnung lassen sich viele gemeinsame Operationen realisieren, indem man den gleichen Betrieb auf jedes Pixel unabhängig gleichmäßig aufbringt. Andere Anordnungen von Pixeln sind möglich, wobei einige Abtastmuster sogar die Form (oder Kernel) jedes Pixels über das Bild verändern. Aus diesem Grund muss bei der Erfassung eines Bildes auf einem Gerät und der Anzeige auf einem anderen Gerät oder bei der Umwandlung von Bilddaten aus einem Pixelformat in ein anderes gesorgt werden. Zum Beispiel: LCD-Bildschirme verwenden typischerweise ein gestaffeltes Raster, wobei die roten, grünen und blauen Komponenten an leicht verschiedenen Stellen abgetastet werden. Subpixel-Rendering ist eine Technologie, die diese Unterschiede nutzt, um das Rendern von Text auf LCD-Bildschirmen zu verbessern. Die überwiegende Mehrheit der Farbdigitalkameras verwendet einen Bayer-Filter, was zu einem regelmäßigen Raster von Pixeln führt, bei denen die Farbe jedes Pixels von seiner Position auf dem Raster abhängt. Eine Clipmap verwendet ein hierarchisches Abtastmuster, bei dem die Größe der Unterstützung jedes Pixels von seinem Ort innerhalb der Hierarchie abhängt. Gesperrte Gitter werden verwendet, wenn die zugrunde liegende Geometrie nicht planar ist, wie Bilder der Erde aus dem Raum. Der Einsatz von ungleichförmigen Gittern ist ein aktives Forschungsgebiet, das versucht, die traditionelle Nyquist-Grenze zu umgehen. Pixel auf Computermonitoren sind in der Regel quadratisch (d.h. haben gleiche horizontale und vertikale Abtastteilung;) Pixel in anderen Systemen sind oft rechteckig (d.h. haben ungleich horizontale und vertikale Abtastteilung – länglich in Form), wie digitale Videoformate mit unterschiedlichen Aspektverhältnissen, wie die anamorphen Widescreen-Formate des Rec.601 digitalen Videostandards. Auflösung von Computermonitoren Computer können Pixel verwenden, um ein Bild anzuzeigen, oft ein abstraktes Bild, das eine GUI darstellt. Die Auflösung dieses Bildes wird als Anzeigeauflösung bezeichnet und durch die Videokarte des Computers bestimmt. LCD-Monitore verwenden auch Pixel, um ein Bild anzuzeigen, und haben eine native Auflösung. Jedes Pixel besteht aus Triaden, wobei die Anzahl dieser Triaden die native Auflösung bestimmt. Auf einigen CRT-Monitoren kann die Strahl-Sweep-Rate festgelegt werden, was zu einer festen nativen Auflösung führt. Die meisten CRT-Monitoren haben keine feste Strahl-Sweep-Rate, was bedeutet, dass sie überhaupt keine native Auflösung haben - stattdessen haben sie eine Reihe von Auflösungen, die gleichermaßen gut unterstützt werden. Um die spitzesten Bilder auf einem LCD zu erzeugen, muss der Benutzer sicherstellen, dass die Bildschirmauflösung des Computers mit der nativen Auflösung des Monitors übereinstimmt. Auflösung von Teleskopen Die in der Astronomie verwendete Pixelskala ist der Winkelabstand zwischen zwei Objekten am Himmel, die auf dem Detektor (CCD oder Infrarot-Chip) ein Pixel auseinanderfallen. Die in Radien gemessene Skala s ist das Verhältnis von Pixelabstand p und Brennweite f der vorhergehenden Optik, s = p/f.(Die Brennweite ist das Produkt des Brennverhältnisses durch den Durchmesser der zugehörigen Linse oder Spiegel.) Da p üblicherweise in Einheiten von Arcsekunden pro Pixel exprimiert wird, weil 1 Radianer 180/π*3600≈206,265 Arcsekunden beträgt, und weil oft Durchmesser in Millimetern und Pixelgrößen in Mikrometern angegeben werden, die einen weiteren Faktor von 1000 ergeben, wird die Formel oft als s=206p/f angegeben. Bits pro Pixel Die Anzahl der verschiedenen Farben, die durch ein Pixel dargestellt werden können, hängt von der Anzahl der Bits pro Pixel (bpp) ab. Ein 1 bpp-Bild verwendet 1 Bit für jedes Pixel, so dass jedes Pixel entweder auf oder ab sein kann. Jedes zusätzliche Bit verdoppelt die Anzahl der Farben zur Verfügung, so kann ein 2 bpp Bild 4 Farben haben, und ein 3 bpp Bild kann 8 Farben haben: 1 bpp, 21 = 2 Farben (monochrom) 2 bpp, 22 = 4 Farben 3 bpp, 23 = 8 Farben 4 bpp, 24 = 16 Farben 8 bpp, 28 = 256 Farben 16 bpp, 216 = 65,536 Farben (Highcolor ) 24 bpp, 2 Für Farbtiefen von 15 oder mehr Bits pro Pixel ist die Tiefe normalerweise die Summe der jedem der roten, grünen und blauen Komponenten zugeordneten Bits. Highcolor, in der Regel 16 bpp, hat normalerweise fünf Bits für Rot und Blau, und sechs Bits für Grün, da das menschliche Auge empfindlicher auf Fehler in Grün als in den anderen beiden Primärfarben ist. Für Anwendungen mit Transparenz können die 16 Bit in je fünf Bit Rot, Grün und Blau unterteilt werden, wobei ein Bit für Transparenz übrig bleibt. Eine 24-Bit-Tiefe ermöglicht 8 Bit pro Bauteil. Auf einigen Systemen steht eine 32-Bit-Tiefe zur Verfügung: das bedeutet, dass jedes 24-Bit-Pixel eine zusätzliche 8 Bit hat, um seine Opazität zu beschreiben (um mit einem anderen Bild zu kombinieren). Subpixel Viele Anzeige- und Bildaufnahmesysteme sind nicht in der Lage, die verschiedenen Farbkanäle an derselben Stelle anzuzeigen oder zu erfassen. Daher wird das Pixelraster in einfarbige Bereiche unterteilt, die zur angezeigten oder abgetasteten Farbe bei Betrachtung mit Abstand beitragen. In einigen Displays, wie LCD-, LED- und Plasmadisplays, sind diese einfarbigen Bereiche getrennt adressierbare Elemente, die als Subpixel, meist RGB-Farben, bekannt sind. Beispielsweise teilen LCDs typischerweise jedes Pixel vertikal in drei Subpixel. Wenn das quadratische Pixel in drei Subpixel unterteilt ist, ist jedes Subpixel notwendigerweise rechteckig. In der Display-Industrie-Terminologie werden Subpixel oft als Pixel bezeichnet, da sie die grundlegenden adressierbaren Elemente im Hinblick auf Hardware sind, und somit Pixelschaltungen anstatt Subpixelschaltungen verwendet werden. Die meisten digitalen Kamerabildsensoren verwenden einfarbige Sensorbereiche, beispielsweise mit dem Bayer-Filtermuster, und in der Kameraindustrie sind diese als Pixel wie in der Displayindustrie, nicht als Subpixel bekannt. Für Systeme mit Subpixeln können zwei verschiedene Ansätze ergriffen werden: Die Subpixel können ignoriert werden, wobei Vollfarbenpixel als kleinstes adressierbares Bildelement behandelt werden; oder die Subpixel können in Rendering-Berechnungen enthalten sein, was mehr Analyse- und Verarbeitungszeit erfordert, aber in einigen Fällen scheinbar überlegene Bilder erzeugen kann. Dieser letztgenannte Ansatz, der als Subpixel-Rendering bezeichnet wird, nutzt die Kenntnis der Pixelgeometrie, um die drei farbigen Subpixel getrennt zu manipulieren, wodurch eine Erhöhung der scheinbaren Auflösung von Farbdisplays entsteht. Während CRT-Anzeigen rot-grün-blau-maskierte Leuchtstoffbereiche verwenden, diktiert von einem Netzgitter namens der Schattenmaske, würde es erfordern, dass ein schwieriger Kalibrierschritt mit dem angezeigten Pixelraster ausgerichtet wird, und so dass CRTs derzeit nicht Subpixel-Rendering verwenden. Das Konzept von Subpixeln bezieht sich auf Proben. Megapixel Ein Megapixel (MP) ist eine Million Pixel; der Begriff wird nicht nur für die Anzahl der Pixel in einem Bild verwendet, sondern auch um die Anzahl der Bildsensorelemente von Digitalkameras oder die Anzahl der Anzeigeelemente von Digitalanzeigen auszudrücken. Beispielsweise verwendet eine Kamera, die ein 2048 × 1536 Pixel-Bild (3,145,728 fertige Bildpixel) macht, typischerweise ein paar zusätzliche Zeilen und Spalten von Sensorelementen und wird gemeinhin als "3.2 Megapixel" oder "3,4 Megapixel" bezeichnet, je nachdem, ob die gemeldete Zahl die effektive oder die Gesamtpixelzahl ist. Pixel wird verwendet, um die Auflösung eines Fotos zu definieren. Die Photoauflösung wird durch Multiplikation der Breite und Höhe eines Sensors im Pixel berechnet. Digitalkameras verwenden fotoempfindliche Elektronik, entweder ladegekoppeltes Gerät (CCD) oder komplementäre Metall-Oxid-Halbleiter (CMOS)-Bildsensoren, bestehend aus einer Vielzahl von einzelnen Sensorelementen, die jeweils einen gemessenen Intensitätspegel aufzeichnen. Bei den meisten Digitalkameras wird die Sensoranordnung mit einem gemusterten Farbfiltermosaik mit roten, grünen und blauen Bereichen in der Bayer-Filteranordnung abgedeckt, so dass jedes Sensorelement die Intensität einer einzigen Primärfarbe des Lichts erfassen kann. Die Kamera interpoliert die Farbinformationen benachbarter Sensorelemente, durch einen Prozess, der demosaiking genannt wird, um das endgültige Bild zu erstellen. Diese Sensorelemente werden oft als Pixel bezeichnet, obwohl sie nur 1 Kanal (nur rot oder grün oder blau) des letzten Farbbildes aufnehmen. So müssen zwei der drei Farbkanäle für jeden Sensor interpoliert werden und eine sogenannte N-Megapixel-Kamera, die ein N-Megapixel-Bild erzeugt, liefert nur ein Drittel der Information, dass ein Bild gleicher Größe von einem Scanner stammen könnte. So können bestimmte Farbkontraste in Abhängigkeit von der Zuordnung der Primärfarben (grün hat doppelt so viele Elemente wie rot oder blau in der Bayer-Anordnung). DxO Labs erfand den Perceptual MegaPixel (P-MPix), um die Schärfe zu messen, die eine Kamera produziert, wenn sie auf ein bestimmtes Objektiv gepaart wird – im Gegensatz zum MP steht ein Hersteller für ein Kameraprodukt, das nur auf dem Sensor der Kamera basiert. Die neue P-MPix behauptet, ein genauerer und relevanterer Wert für Fotografen zu sein, bei der Wägung der Kameraschärfe zu berücksichtigen. Ab Mitte 2013 hat das auf einem Nikon D800 montierte Sigma 35 mm f/1.4 DG HSM Objektiv den höchsten gemessenen P-MPix. Mit einem Wert von 23 MP löscht er jedoch noch mehr als ein Drittel des 36,3 MP-Sensors von D800 ab. Im August 2019, Xiaomi veröffentlicht Redmi Note 8 Pro als weltweit erstes Smartphone mit 64 MP-Kamera. Am 12. Dezember 2019 Samsung veröffentlicht Samsung A71 mit einer 64 MP-Kamera. Ende 2019 gab Xiaomi das erste Kameratelefon mit 108MP 1/1.33-Zoll über den Sensor bekannt. Der Sensor ist größer als die meisten Brückenkamera mit 1/2,3-Zoll über den Sensor. Ein neues Verfahren zum Hinzufügen von Megapixeln wurde in einer Micro Four Thirds System Kamera eingeführt, die nur einen 16 MP Sensor verwendet, aber durch zwei Belichtungen ein 64 MP RAW (40 MP JPEG) Bild erzeugen kann, indem der Sensor um ein halbes Pixel zwischen sich verschoben wird. Mit Hilfe eines Stativs, um innerhalb einer Instanz Level-Multishots zu nehmen, werden die mehreren 16 MP-Bilder dann in ein einheitliches 64 MP-Bild erzeugt. Siehe auch Referenzen Externe LinksA Pixel ist kein kleiner Platz: Microsoft Memo von Computergrafik-Pionier Alvy Ray Smith. Video von Lyons Vortrag über Pixelgeschichte am Computer History Museum Square und nicht-quare Pixel: Technische Informationen zu Pixel-Aspektverhältnissen moderner Videostandards (480i, 576i, 1080i, 720p) plus Software-Implikationen. 120 Megapixel ist jetzt hier: Viele Informationen über MegaPixel und Gigapixel. Wie ein TV in Slow Motion funktioniert - Die langsamen Mo Jungs – YouTube-Video von The Slow Mo Guys