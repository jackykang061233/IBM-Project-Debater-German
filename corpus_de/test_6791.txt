Word2vec ist eine im Jahr 2013 veröffentlichte Technik für die natürliche Sprachverarbeitung. Der Word2vec-Algorithmus verwendet ein neuronales Netzwerkmodell, um Wortverbände aus einem großen Korpus Text zu lernen. Ein solches Modell kann nach dem Training synonyme Wörter erkennen oder zusätzliche Wörter für einen Teilsatz vorschlagen. Wie der Name impliziert, stellt Wort2vec jedes einzelne Wort mit einer bestimmten Nummernliste dar, die als Vektor bezeichnet wird. Die Vektoren werden sorgfältig so gewählt, dass eine einfache mathematische Funktion (die Kosinus- Ähnlichkeit zwischen den Vektoren) die Höhe der semantischen Ähnlichkeit zwischen den durch diese Vektoren repräsentierten Wörtern angibt. Approach Word2vec ist eine Gruppe von verwandten Modellen, die zur Herstellung von Worteinbettungen verwendet werden. Diese Modelle sind flache, zweischichtige neuronale Netzwerke, die ausgebildet sind, um sprachliche Kontexte von Wörtern zu rekonstruieren. Word2vec nimmt als Eingang einen großen Korpus Text und erzeugt einen Vektorraum, typischerweise von mehreren hundert Dimensionen, wobei jedem einzelnen Wort in den Korpus ein entsprechender Vektor im Raum zugeordnet ist. Wortvektoren werden in dem Vektorraum so positioniert, dass Wörter, die gemeinsame Kontexte in den Korpus teilen, im Raum nahe zueinander liegen. History Word2vec wurde 2013 von einem Forscherteam unter der Leitung von Tomas Mikolov bei Google über zwei Papiere erstellt, patentiert und veröffentlicht. Andere Forscher halfen, den Algorithmus zu analysieren und zu erklären. Mit dem Word2vec-Algorithmus erstellte Einbettungsvektoren haben einige Vorteile gegenüber früheren Algorithmen wie latente semantische Analyse. CBOW und Scheck Gramm Word2vec kann entweder von zwei Modellarchitekturen verwenden, um eine verteilte Darstellung von Wörtern zu erzeugen: durchgehende Beutel-of-Worte (CBOW) oder kontinuierliches Sprung-Gramm. In der kontinuierlichen Gepäck-Worte-Architektur prognostiziert das Modell das aktuelle Wort aus einem Fenster umliegender Kontextwörter. Die Reihenfolge der Kontextworte beeinflusst die Vorhersage nicht (Schlagwortannahme). In der kontinuierlichen Skip-Gramm-Architektur verwendet das Modell das aktuelle Wort, um das umgebende Fenster von Kontextwörtern vorherzusagen. Die Skip-Gramm-Architektur wiegt naheliegende Kontextwörter stärker als entferntere Kontextwörter. Laut der Anmerkung der Autoren, CBOW ist schneller, während skip-gram macht einen besseren Job für seltene Wörter. Parametrisierung Ergebnisse der Wort2vec-Training können empfindlich auf Parametrisierung sein. Im Folgenden sind einige wichtige Parameter im Wort2vec-Training. Trainingsalgorithmus Ein Word2vec-Modell kann mit hierarchischem Softmax und/oder negativer Abtastung trainiert werden. Um die bedingte log-likelihood ein Modell zu maximieren versucht, die hierarchische softmax Methode verwendet einen Huffman-Baum, um die Berechnung zu reduzieren. Das negative Abtastverfahren hingegen nähert sich dem Maximierungsproblem, indem die log-ähnliche Wahrscheinlichkeit von abgetasteten negativen Instanzen minimiert wird. Laut den Autoren funktioniert hierarchischer Softmax für seltene Wörter besser, während negative Abtastung für häufige Wörter besser und besser mit niedrigen Dimensionsvektoren funktioniert. Da die Trainingsepochs zunehmen, sind hierarchische Softmax-Stopps nützlich. Sub-sampling Hochfrequenz-Wörter geben oft wenig Informationen. Wörter mit einer Frequenz über einer bestimmten Schwelle können subsampliert werden, um das Training zu beschleunigen. Dimensionalität Die Qualität der Worteinbettung erhöht sich mit höherer Dimensionalität. Aber nach Erreichen eines gewissen Punktes verringert sich der marginale Gewinn. Typischerweise wird die Dimensionalität der Vektoren zwischen 100 und 1000 eingestellt. Kontextfenster Die Größe des Kontextfensters bestimmt, wie viele Wörter vor und nach einem bestimmten Wort als Kontextwörter des gegebenen Wortes enthalten sind. Laut der Anmerkung der Autoren beträgt der empfohlene Wert 10 für Scheck-Gramm und 5 für CBOW. Erweiterungen Es wurde eine Erweiterung von Wort2vec zum Konstruieren von Einbettungen aus ganzen Dokumenten (anstatt der einzelnen Wörter) vorgeschlagen. Diese Erweiterung heißt Paragraph2vec oder doc2vec und wurde in den C-, Python- und Java/Scala-Werkzeugen (siehe unten) implementiert, wobei die Java- und Python-Versionen auch die Inferenz von Dokumenteneinbettungen auf neue, ungesehene Dokumente unterstützen. Wortvektoren für Bioinformatik:BioVectors Von Asgari und Mofrad wurde eine Erweiterung von Wortvektoren für n-Gramme in biologischen Sequenzen (z.B. DNA, RNA und Proteine) für bioinformatische Anwendungen vorgeschlagen. Als Biovektoren (BioVec) bezeichnet man biologische Sequenzen im allgemeinen mit Protein-Vektoren (ProtVec) für Proteine (Amino-Säure-Sequenzen) und Gen-Vektoren (GeneVec) für Gensequenzen, kann diese Darstellung in Anwendungen des maschinellen Lernens in Proteomik und Genomik weit verbreitet werden. Die Ergebnisse deuten darauf hin, dass Biovektoren biologische Sequenzen hinsichtlich biochemischer und biophysikalischer Interpretationen der zugrunde liegenden Muster charakterisieren können. Eine ähnliche Variante, dna2vec, hat gezeigt, dass es Korrelation zwischen Needleman-Wunsch Ähnlichkeits-Score und cosine Ähnlichkeit von dna2vec Wortvektoren gibt. Word-Vektoren für die Radiologie: Intelligent Word Embedding (IWE) Eine Erweiterung von Wortvektoren zur Erstellung einer dichten Vektordarstellung unstrukturierter Radiologieberichte wurde von Banerjee et al. Eine der größten Herausforderungen mit Word2Vec ist, wie man unbekannte oder außer-vocabulary (OOV) Wörter und morphologisch ähnliche Wörter behandelt. Dies kann insbesondere ein Problem in Bereichen wie Medizin sein, wo Synonyme und verwandte Wörter in Abhängigkeit von der bevorzugten Art des Radiologen verwendet werden können, und Wörter können selten in einem großen Korpus verwendet werden. Wenn das Wort2vec-Modell zuvor kein bestimmtes Wort gefunden hat, wird es gezwungen, einen Zufallsvektor zu verwenden, der im allgemeinen weit von seiner idealen Darstellung entfernt ist. IWE vereint Word2vec mit einer semantischen Wörterbuch-Mapping-Technik, um die großen Herausforderungen der Informationsextraktion aus klinischen Texten, die Mehrdeutigkeit von freiem Text narrativem Stil, lexische Variationen, Verwendung von ungrammatischen und telegraphischen Phasen, willkürliche Ordnung von Wörtern, und häufige Erscheinung von Abkürzungen und Akronymen. Von besonderem Interesse hat das IWE-Modell (geübt auf dem einen institutionellen Datensatz) erfolgreich in einen anderen institutionellen Datensatz übersetzt, der eine gute Verallgemeinerbarkeit des Ansatzes in allen Institutionen demonstriert. Analyse Die Gründe für eine erfolgreiche Worteinbettung des Lernens im Wort2vec-Rahmen sind schlecht verstanden. Goldberg und Levy weisen darauf hin, dass die Wort2vec-Zielfunktion dazu führt, dass Wörter, die in ähnlichen Zusammenhängen auftreten, ähnliche Einbettungen (gemessen durch Kosinus-ähnlichkeit) aufweisen und beachten, dass dies im Einklang mit J. R. Firths Verteilungshypothese steht. Sie weisen jedoch darauf hin, dass diese Erklärung "sehr handbewaffnet" ist und argumentieren, dass eine formalere Erklärung vorzuziehen wäre. Levy et al.(2015) zeigen, dass ein Großteil der überlegenen Leistung von word2vec oder ähnlichen Einbettungen in nachgelagerten Aufgaben nicht das Ergebnis der Modelle an sich, sondern der Wahl spezifischer Hyperparameter ist. Die Übertragung dieser Hyperparameter auf traditionellere Ansätze liefert ähnliche Leistungen in nachgelagerten Aufgaben. Arora et al.(2016) erläutern word2vec und verwandte Algorithmen als Inferenz für ein einfaches generatives Modell für Text, das eine zufällige Walk-Generation Prozess basierend auf loglinearem Thema Modell beinhaltet. Sie verwenden dies, um einige Eigenschaften von Worteinbettungen zu erklären, einschließlich ihrer Verwendung, um Analogien zu lösen. Erhaltung semantischer und syntaktischer Beziehungen Der Worteinbettungsansatz ist in der Lage, mehrere unterschiedliche Ähnlichkeitsgrade zwischen Wörtern zu erfassen. Mikolov et al.(2013) stellte fest, dass semantische und syntaktische Muster mit Vektor-Arithmetic wiedergegeben werden können. Muster wie "Man is to Woman as Brother is to Sister" können durch algebraische Operationen auf den Vektordarstellungen dieser Wörter erzeugt werden, so dass die Vektordarstellung von Brother - Man + Woman ein Ergebnis erzeugt, das der Vektordarstellung von Schwester im Modell am nächsten ist. Solche Beziehungen können für eine Reihe von semantischen Beziehungen (wie Country-Capital) sowie syntaktische Beziehungen (z.B. aktuelles tense-past tense) erzeugt werden. Die Beurteilung der Qualität eines Modells Mikolov et al.(2013) entwickelt einen Ansatz zur Beurteilung der Qualität eines Word2vec-Modells, das sich auf die oben diskutierten semantischen und syntaktischen Muster bezieht. Sie entwickelten eine Reihe von 8.869 semantischen Beziehungen und 10.675 syntaktischen Beziehungen, die sie als Benchmark verwenden, um die Genauigkeit eines Modells zu testen. Bei der Beurteilung der Qualität eines Vektormodells kann ein Benutzer auf diesen Genauigkeitstest, der in Word2vec implementiert ist, ziehen oder einen eigenen Testsatz entwickeln, der für die das Modell bildende Körperschaft aussagekräftig ist. Dieser Ansatz bietet einen anspruchsvolleren Test als einfach zu argumentieren, dass die Wörter, die dem gegebenen Testwort am meisten ähnlich sind, intuitiv plausibel sind. Parameter und Modellqualität Die Verwendung verschiedener Modellparameter und verschiedener Korpusgrößen kann die Qualität eines Word2vec-Modells stark beeinflussen. Die Genauigkeit kann auf verschiedene Weise verbessert werden, einschließlich der Wahl der Modellarchitektur (CBOW oder Skip-Gram), der Erhöhung des Trainingsdatensatzes, der Erhöhung der Anzahl der Vektordimensionen und der Erhöhung der Fenstergröße der Wörter, die der Algorithmus betrachtet. Jede dieser Verbesserungen kommt mit den Kosten einer erhöhten Rechenkomplexität und damit einer erhöhten Modellgenerationszeit. Bei Modellen mit großer Korporation und einer hohen Anzahl von Dimensionen liefert das Skip-Gramm-Modell die höchste Gesamtgenauigkeit und produziert konsequent die höchste Genauigkeit auf semantischen Beziehungen sowie in den meisten Fällen die höchste syntaktische Genauigkeit. Die CBOW ist jedoch weniger rechnerisch teuer und liefert ähnliche Genauigkeitsergebnisse. Insgesamt erhöht sich die Genauigkeit mit der Anzahl der verwendeten Wörter und der Anzahl der Abmessungen. Mikolov et al.report, dass die Verdoppelung der Anzahl der Trainingsdaten zu einer Erhöhung der Rechenkomplexität führt, die der Verdoppelung der Anzahl der Vektordimensionen entspricht. Altszyler und Co-Autor (2017) studierten Word2vec-Leistung in zwei semantischen Tests für unterschiedliche Korpusgröße. Sie fanden heraus, dass Word2vec eine steile Lernkurve hat, die eine andere Wort-Einbettungstechnik (LSA) übertrifft, wenn es mit mittlerer bis großer Korpus-Größe (mehr als 10 Millionen Wörter) ausgebildet wird. Mit einem kleinen Trainingskorpus zeigte LSA jedoch eine bessere Leistung. Zusätzlich zeigen sie, dass die beste Parametereinstellung von der Aufgabe und dem Trainingskorpus abhängt. Dennoch scheint für Skip-Gramm-Modelle, die in mittlerer Größe ausgebildet sind, mit 50 Dimensionen eine Fenstergröße von 15 und 10 Negativproben eine gute Parametereinstellung zu sein. Siehe auch Referenzen Externe Links Wikipedia2Vec[2] (Einführung) Implementierungen C C C # Python (Spark)Python (TensorFlow)Python (Gensim)Java/Scala R