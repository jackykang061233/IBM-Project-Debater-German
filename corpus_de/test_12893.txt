In der Informationstheorie und -statistik wird Negentropie als Maß für die Entfernung zur Normalität verwendet. Das Konzept und die Phrase "negative Entropie" wurde von Erwin Schrödinger in seinem populärwissenschaftlichen Buch "What is Life" von 1944 vorgestellt. Später verkürzte Léon Brillouin die Phrase auf Negentropie. 1974 schlug Albert Szent-Györgyi vor, den Begriff Negentropie durch Syntropie zu ersetzen. Dieser Begriff kann in den 1940er Jahren mit dem italienischen Mathematiker Luigi Fantappiè entstanden sein, der versuchte, eine einheitliche Theorie der Biologie und Physik zu konstruieren. Buckminster Fuller versuchte, diese Nutzung zu populärisieren, aber negentropy bleibt häufig. In einer Notiz zu Was ist Leben? Schrödinger erläuterte seine Verwendung dieses Satzes. ... wenn ich allein für sie [Physiker] verpflegung hätte, hätte ich die Diskussion stattdessen freie Energie abgeben sollen. Es ist die vertrautere Vorstellung in diesem Zusammenhang. Aber dieser sehr technische Begriff schien sprachlich zu nah an der Energie, um den durchschnittlichen Leser lebendig zu machen, um den Kontrast zwischen den beiden Dingen. Informationstheorie In der Informationstheorie und -statistik wird Negentropie als Maß für die Entfernung zur Normalität verwendet. Aus allen Verteilungen mit einem bestimmten Mittelwert und Varianz ist die normale oder Gaussische Verteilung diejenige mit der höchsten Entropie. Die Negentropie misst den Unterschied in der Entropie zwischen einer gegebenen Verteilung und der Gaussschen Verteilung mit dem gleichen Mittel und der Varianz. Somit ist die Negentropie immer nichtnegativ, ist invariant durch eine lineare invertierbare Koordinatenänderung und verschwindet, wenn und nur, wenn das Signal Gaussisch ist. Die Negentropie ist definiert als J (p x ) = S (φ x ) - S (p x ) {\displaystyle J(p_{x})=S(\varphi x})-S(p_{x,\ wobei S (φ x )\displaystyle S(\varphi {_x} die differentielle Entropie der Gaussdichte ist Sie ist mit der Netzentropie verbunden, die in der unabhängigen Komponentenanalyse verwendet wird. Die Negentropie einer Verteilung ist gleich der Kullback-Leibler-Divergenz zwischen p x {\displaystyle p_{x} und einer Gaussschen Verteilung mit gleichem Mittelwert und Varianz wie p x {\displaystyle p_{x} (siehe Differential entropy § Maximization in the normal distribution for aproof). Insbesondere ist es immer nicht negativ. Korrelation zwischen statistischer Negentropie und Gibbs' freier Energie Es gibt eine physikalische Menge, die eng mit freier Energie verbunden ist (freie Enthalpie), mit einer Einheit von Entropie und isomorph zu Negentropie, die in der Statistik und Informationstheorie bekannt ist. Im Jahre 1873 erstellte Willard Gibbs ein Diagramm, das das Konzept der freien Energie entsprechend der freien Enthalpie illustriert. Auf dem Diagramm kann man die Menge sehen, die als Kapazität für Entropie bezeichnet wird. Diese Menge ist die Menge an Entropie, die erhöht werden kann, ohne eine innere Energie zu ändern oder ihr Volumen zu erhöhen. Mit anderen Worten, es ist ein Unterschied zwischen maximal möglich, unter angenommenen Bedingungen, Entropie und seiner tatsächlichen Entropie. Es entspricht genau der Definition der in der Statistik und Informationstheorie angenommenen Negentropie. Eine ähnliche physikalische Menge wurde 1869 von Massieu für den isothermen Prozess eingeführt (beide Mengen unterscheiden sich nur mit einem Figurenzeichen) und dann Planck für den isotherm-isobarischen Prozess.In jüngster Zeit hat sich das thermodynamische Potential von Massieu-Planck gezeigt, das auch als freie Entropie bekannt ist, in der sogenannten entropen Formulierung statistischer Mechanik, die unter anderem in der Molekularbiologie und thermodynamischen Nicht-Equilibrium-Prozessen angewendet wird. J = S max - S = - Φ = - k ln ‡ Z {\displaystyle J=S_{\max }S=-\Phi -=k\ln Z},\ wo: S {\displaystyle S} is entropy J {\displaystyle J} is negentropy (Gibbs "power for entropy)" 0 {\displaystyle \Phi } ist das Massenpotential Z {\displaystyle Z} ist die Partitionsfunktion k {\displaystyle k} die Boltzmann Konstante Insbesondere ist mathematisch die Negentropie (die negative Entropiefunktion, in der Physik als freie Entropie interpretiert) das konvexe Konjugat von LogSumExp (in der Physik als freie Energie interpretiert). Brillouins negentropisches Prinzip der Information Im Jahr 1953 leitete Léon Brillouin eine allgemeine Gleichung ab, die besagt, dass die Änderung eines Informationsbitwerts mindestens k T ln ‡ 2 {\displaystyle kT\ln 2} Energie benötigt. Dies ist die gleiche Energie wie die Arbeit Leó Szilárd Motor produziert im idealistischen Fall. In seinem Buch erforschte er dieses Problem weiter, das zu dem Schluss führte, dass jede Ursache dieser Bitwertänderung (Messung, Entscheidung über eine ja/nein Frage, Löschung, Anzeige usw.) die gleiche Menge an Energie erfordert. Siehe auch Exergy Free entropy Entropy in Thermodynamik und Informationstheorie == Anmerkungen ==