Die Wissensextraktion ist die Erstellung von Kenntnissen aus strukturierten (relativen Datenbanken, XML) und unstrukturierten (Text-, Dokumente-, Bilder-) Quellen. Das daraus gewonnene Wissen muss in einem maschinenlesbaren und maschineninterpreierbaren Format vorliegen und muss Wissen in einer Weise darstellen, die die Inferenzierung erleichtert. Obwohl es methodisch ähnlich ist wie die Informationsextraktion (NLP) und ETL (Datenlager), sind die Hauptkriterien darin zu sehen, dass das Extraktionsergebnis über die Erstellung strukturierter Informationen oder die Transformation in ein relationales Schema hinausgeht. Es erfordert entweder die Wiederverwendung bestehender formaler Kenntnisse (Reuse Identifier oder Ontologien) oder die Erstellung eines Schemas basierend auf den Quelldaten. Die RDB2RDF W3C Gruppe standardisiert derzeit eine Sprache zur Extraktion von Ressourcenbeschreibungsrahmen (RDF) aus relationalen Datenbanken. Ein weiteres beliebtes Beispiel für die Wissensextraktion ist die Transformation von Wikipedia in strukturierte Daten und auch die Kartierung auf vorhandenes Wissen (siehe DBpedia und Freebase). Überblick Nach der Standardisierung von Wissensrepräsentationssprachen wie RDF und OWL wurde im Bereich viel Forschung durchgeführt, insbesondere in Bezug auf die Transformation von relationalen Datenbanken in RDF, Identitätsauflösung, Wissensentdeckung und Ontologielern. Das allgemeine Verfahren verwendet traditionelle Methoden aus der Informationsextraktion und -extraktion, Transformation und Last (ETL), die die Daten aus den Quellen in strukturierte Formate transformieren. Die folgenden Kriterien können verwendet werden, um Ansätze in diesem Thema zu kategorisieren (einige davon sind nur für die Extraktion aus relationalen Datenbanken verantwortlich:) Beispiele Entity-Linking DBpedia Spotlight, OpenCalais, Dandelion dataTXT, die Zemanta API, Extractiv und PoolParty Extractor analysieren freien Text über die Namenserkennung und diambiguiert dann Kandidaten über Namensauflösung und verknüpft die gefundenen Entitäten mit dem DBpedia Wissensrepository (Dandelion dataTXT demo oder DBpedia Spotlight Webdemo oder PoolParty Extractor Demo). Präsident Obama rief am Mittwoch auf den Kongress auf, eine Steuerpause für Studenten zu verlängern, die im letzten Jahr der wirtschaftlichen Stimulus-Paket enthalten sind, und argumentierte, dass die Politik großzügiger Unterstützung bietet. Da Präsident Obama mit einer DBpedia LinkedData-Ressource verbunden ist, können weitere Informationen automatisch abgerufen werden und ein Semant kann beispielsweise darauf hinweisen, dass das erwähnte Unternehmen der Typ Person (mit FOAF (Software) und der Typ Presidents der Vereinigten Staaten (mit YAGO) ist. Beispiele: Methoden, die nur Entitäten oder Links zu Wikipedia-Artikeln und anderen Zielen erkennen, die keine weitere Abrufung strukturierter Daten und formales Wissen bieten. Beziehungsdatenbanken zu RDF Triplify, D2R Server, Ultrawrap und Virtuoso RDF Ansichten sind Tools, die relationale Datenbanken in RDF transformieren. Während dieses Prozesses ermöglichen sie die Wiederverwendung bestehender Vokabeln und Ontologien während des Umwandlungsprozesses. Bei der Transformation einer typischen relationalen Tabelle namens Benutzer muss eine Spalte (z.B. Name) oder eine Aggregation von Spalten (z.B.first_name und last_name) die URI der erstellten Einheit bereitstellen. Normalerweise wird der Primärschlüssel verwendet. Jede andere Spalte kann als Beziehung zu dieser Einheit extrahiert werden. Dann werden Eigenschaften mit formal definierter Semantik zur Interpretation der Informationen verwendet (und wiederverwendet). Zum Beispiel eine Spalte in einer Benutzertabelle namens verheiratet Um als symmetrische Relation definiert werden zu können und eine Spalten-Homepage kann von dem FOAF Vocabulary namens foaf:homepage in eine Eigenschaft umgewandelt werden, um sie als inverse Funktionseigenschaft zu qualifizieren. Dann kann jeder Eintrag der Benutzertabelle eine Instanz der Klasse foaf:Person (Ontologie Bevölkerung) gemacht werden. Zusätzlich könnte das Domänenwissen (in Form einer Ontologie) aus dem status_id erstellt werden, entweder durch manuell erstellte Regeln (wenn status_id 2 ist, gehört der Eintrag zum Klassenlehrer) oder durch (semi)-automatisierte Methoden (Ontologielernen). Hier ist eine Beispieltransformation: Extraktion von strukturierten Quellen zu RDF 1:1 Mapping von RDB Tabellen/Ansichten zu RDF Entities/Attributes/Values Beim Aufbau einer RDB-Darstellung einer Problemdomäne ist der Ausgangspunkt häufig ein Entity-Relationship-Diagramm (ERD). Typischerweise wird jede Einheit als Datenbanktabelle dargestellt, jedes Attribut der Einheit wird eine Spalte in dieser Tabelle, und Beziehungen zwischen Einheiten werden durch Fremdschlüssel angezeigt. Jede Tabelle definiert typischerweise eine bestimmte Klasse von Einheit, jede Spalte eines ihrer Attribute. Jede Zeile in der Tabelle beschreibt eine Instanzinstanz, die durch einen Primärschlüssel eindeutig gekennzeichnet ist. Die Tabellenzeilen beschreiben gemeinsam einen Entity-Set.In einer gleichwertigen RDF-Darstellung des gleichen Entitätssatzes: Jede Spalte in der Tabelle ist ein Attribut (d.h. Prädikat) Jeder Spaltenwert ist ein Attributwert (d.h. Objekt) Jeder Zeilenschlüssel stellt eine Entity-ID (d.h. Subjekt) Jede Zeile stellt eine Entity-Instanz dar Jede Zeile (Zentity-Instanz) wird in RDF durch eine Sammlung von Dreifachen mit einem gemeinsamen Subjekt (Zentity-ID) repräsentiert. Um eine äquivalente Ansicht auf Basis von RDF-Semantik zu machen, wäre der grundlegende Mapping-Algorithmus wie folgt: Erstellen Sie eine RDFS-Klasse für jede Tabelle konvertieren alle Primärschlüssel und Fremdschlüssel in IRIs eine Prädikat IRI jeder Spalte zuordnen einen rdf:Typ Prädikat für jede Zeile, verknüpfen Sie es zu einer RDFS-Klasse IRI entsprechend der Tabelle für jede Spalte, die weder Teil einer primären oder fremden Schlüssel enthält. Eine frühe Erwähnung dieses grundlegenden oder direkten Mappings findet sich in Tim Berners-Lees Vergleich des ER-Modells zum RDF-Modell. Komplexe Zuordnungen von relationalen Datenbanken zu RDF Die oben erwähnte 1:1-Mapping legt die Vermächtnisdaten als RDF auf einfache Weise frei, zusätzliche Verfeinerung kann verwendet werden, um die Nützlichkeit der RDF-Ausgabe bzw. der angegebenen Use Cases zu verbessern. Normalerweise werden Informationen bei der Transformation eines Entity-Relationship-Diagramms (ERD) zu relationalen Tabellen verloren (Details finden sich in Objekt-Relational Impedanz-Mixelmatch) und müssen revers entwickelt werden. Aus konzeptioneller Sicht können Ansätze zur Extraktion aus zwei Richtungen kommen. Die erste Richtung versucht, ein OWL-Schema aus dem angegebenen Datenbankschema zu extrahieren oder zu lernen. Frühe Ansätze nutzten eine feste Menge von manuell erstellten Mapping-Regeln, um die 1:1 Mapping zu verfeinern. Mehr aufwändige Methoden setzen Heuristiken oder Lernalgorithmen ein, um schematisierte Informationen zu induzieren (Methoden überschneiden sich mit Ontologielern). Während einige Ansätze versuchen, die Informationen aus der Struktur, die dem SQL-Schema inhärent ist, zu extrahieren (z.B. Fremdschlüssel zu analysieren), andere analysieren den Inhalt und die Werte in den Tabellen, um konzeptionelle Hierarchien zu schaffen (z.B. Spalten mit wenigen Werten sind Kandidaten für das Werden von Kategorien). Die zweite Richtung versucht, das Schema und seine Inhalte auf eine vorbestehende Domänen-Onlogie abzubilden (siehe auch: Onlogie-Ausrichtung). Oft existiert jedoch eine geeignete Domain-Onlogie nicht und muss zuerst erstellt werden. XML Da XML als Baum strukturiert ist, können alle Daten leicht in RDF dargestellt werden, die als Graph strukturiert ist. XML2RDF ist ein Beispiel für einen Ansatz, der RDF leere Knoten verwendet und XML-Elemente und Attribute auf RDF-Eigenschaften transformiert. Das Thema ist jedoch komplexer, wie bei relationalen Datenbanken. In einer relationalen Tabelle ist der Hauptschlüssel ein idealer Kandidat für das Thema der extrahierten Tripel. Ein XML-Element kann jedoch - je nach Kontext - als Subjekt, Prädikat oder Objekt eines Dreifachen transformiert werden. XSLT kann eine Standard-Transformationssprache verwendet werden, um XML manuell in RDF zu konvertieren. Erhebung von Methoden / Tools Extraktion aus natürlichen Sprachquellen Der größte Teil der in Geschäftsdokumenten enthaltenen Informationen (ca. 80%) wird in natürlicher Sprache kodiert und daher unstrukturiert. Da unstrukturierte Daten eher eine Herausforderung für die Wissensextraktion sind, sind anspruchsvollere Methoden erforderlich, die im allgemeinen zu schlechteren Ergebnissen im Vergleich zu strukturierten Daten neigen. Das Potenzial für einen massiven Erwerb von extrahiertem Wissen sollte jedoch die erhöhte Komplexität und geringere Qualität der Extraktion kompensieren. Im Folgenden werden natürliche Sprachquellen als Informationsquellen verstanden, wobei die Daten unstrukturiert als Klartext angegeben werden. Wird der angegebene Text zusätzlich in ein Markup-Dokument (z.B. HTML-Dokument) eingebettet, entfernen die genannten Systeme normalerweise die Markup-Elemente automatisch. Linguistische Anmerkung / natürliche Sprachverarbeitung (NLP)Als Vorverarbeitungsschritt zur Wissensextraktion kann es notwendig sein, sprachliche Anmerkungen durch ein oder mehrere NLP-Tools durchzuführen. Individuelle Module in einem NLP-Workflow bauen normalerweise auf werkzeugspezifischen Formaten für Eingabe und Ausgabe, aber im Kontext der Wissensextraktion wurden strukturierte Formate zur Darstellung sprachlicher Annotationen angewendet. Typische NLP-Aufgaben, die für die Wissensextraktion relevant sind, umfassen: Teil-of-speech (POS) Tagging Lemmatization (LEMMA) oder Stemming (STEM) Wortsinn-Diambiguation (WSD, bezogen auf semantische Annotation unten) benannte Entitätserkennung (NER, siehe auch IE unten) syntaktische Parsing, oft Übernahme syntaktischer Abhängigkeiten (DEP)seichten syntaktischen Parsing (CHUNK:) wenn die Leistung ein Problem ist, ergibt das Zerkleinern eine schnelle Extraktion von nominalen und anderen Phrasen Anaphor-Auflösung (siehe Coreference Resolution in IE unten, aber hier als Aufgabe betrachtet, Verbindungen zwischen textuellen Erwähnungen anstatt zwischen der Erwähnung einer Einheit und einer abstrakten Darstellung der Einheit) semantische Rollenmarkierung (SRL, bezogen auf die Relationship-Extraktion; nicht mit semantisch dargestellt Für Wissensextraktions-Workflows wurden RDF-Ansichten auf solche Daten gemäß den folgenden Gemeinschaftsnormen erstellt: NLP Interchange Format (NIF, für viele häufige Arten von Anmerkungen) Web-Annotation (WA, oft für Entity-Linking verwendet) CoNLL-RDF (für Anmerkungen, die ursprünglich in TSV-Formaten vertreten sind) Weitere, plattformspezifische Formate sind LAPPS Interchange Format (LIF, im LAPPS Grid)NLP Annotation Format (NAF, im NewsReader Workflow Management System verwendet) Traditionelle Informationsextraktion (IE) Traditionelle Informationsextraktion ist eine Technologie der natürlichen Sprachverarbeitung, die Informationen aus typisch natürlichen Sprachtexten und Strukturen extrahiert, die diese in geeigneter Weise. Die Art der zu identifizierenden Informationen muss in einem Modell vor Beginn des Prozesses angegeben werden, weshalb der gesamte Prozess der traditionellen Informationsextraktion Domain abhängig ist. Die IE wird in den folgenden fünf Subtasks aufgeteilt. Namenserkennung (NER)Coreference Resolution (CO) Template-Elemente-Konstruktion (TE)Template relation construction (TR) Template-Szenario-Produktion (ST)Die Aufgabe der benannten Entity-Erkennung ist es, alle benannten in einem Text enthaltenen Entitäten zu erkennen und zu kategorisieren (Zuweisung einer benannten Einheit zu einer vordefinierten Kategorie). Dies funktioniert durch Anwendung grammatikalischer Methoden oder statistischer Modelle. In der Coreference-Resolution werden Äquivalente identifiziert, die von NER innerhalb eines Textes anerkannt wurden. Es gibt zwei relevante Arten von Gleichwertigkeit Beziehung. Der erste bezieht sich auf die Beziehung zwischen zwei verschiedenen vertretenen Einheiten (z.B. IBM Europe und IBM) und dem zweiten auf die Beziehung zwischen einer Einheit und ihren anaphorischen Referenzen (z.B. IBM und IBM). Beide Arten können durch die Kernreferenzlösung erkannt werden. Das IE-System identifiziert während der Template-Elemente-Konstruktion deskriptive Eigenschaften von Einrichtungen, die von NER und CO erkannt werden. Diese Eigenschaften entsprechen gewöhnlichen Qualitäten wie rot oder groß. Die Template-Beziehungs-Konstruktion identifiziert die Beziehungen, die zwischen den Template-Elementen bestehen. Diese Beziehungen können von mehreren Arten sein, wie z.B. Arbeiten-für oder Standort-in, mit der Einschränkung, dass sowohl Domäne als auch Bereich Entitäten entsprechen. Im Template-Szenario werden Produktionsereignisse, die im Text beschrieben werden, in Bezug auf die von NER und CO und den von TR identifizierten Unternehmen identifiziert und strukturiert. Ontologiebasierte Informationsextraktion (OBIE) Ontologiebasierte Informationsextraktion ist ein Unterfeld der Informationsextraktion, mit dem mindestens eine Ontologie verwendet wird, um den Prozess der Informationsextraktion aus natürlichem Text zu führen. Das OBIE-System verwendet Methoden der traditionellen Informationsextraktion, um Konzepte, Instanzen und Beziehungen der verwendeten Ontologien im Text zu identifizieren, die nach dem Prozess zu einer Ontologie strukturiert werden. Die Input-Onlogien stellen somit das Modell der zu extrahierenden Information dar. Ontologie-Lernen (OL) Ontologie-Lernen ist die automatische oder halbautomatische Erstellung von Ontologien, einschließlich der Auszüge der Bedingungen der entsprechenden Domain aus dem natürlichen Sprachtext. Da Gebäude-Onlogien manuell extrem arbeitsintensiv und zeitaufwendig sind, gibt es große Motivation, den Prozess zu automatisieren. Semantische Annotation (SA) Während der semantischen Annotation wird der natürliche Text mit Metadaten (oft in RDFa vertreten) erweitert, die die Semantik der enthaltenen Begriffe maschinenverstanden machen sollten. Bei diesem in der Regel halbautomatischen Prozess wird Wissen im Sinne extrahiert, dass eine Verbindung zwischen lexischen Begriffen und beispielsweise Konzepten aus Onlogien hergestellt wird. So wird Wissen gewonnen, die Bedeutung eines Begriffs im bearbeiteten Kontext beabsichtigt war und daher die Bedeutung des Textes in maschinenlesbaren Daten mit der Fähigkeit, Inferenzen zu ziehen, geerdet wird.Die semantische Annotation wird typischerweise in die folgenden zwei Subtasks aufgeteilt. Terminologie Extraktion Entity Linking Auf der Terminologie-Extraktionsebene werden lexische Begriffe aus dem Text extrahiert. Dazu bestimmt ein Tokenizer zunächst die Wortgrenzen und löst Abkürzungen. Anschließend werden Begriffe aus dem Text, die einem Konzept entsprechen, mit Hilfe eines Domänen-spezifischen Lexikons extrahiert, um diese bei der Entity-Linking zu verknüpfen. In Entität wird eine Verbindung zwischen den extrahierten lexischen Begriffen aus dem Quelltext und den Konzepten aus einer Ontologie oder Wissensbasis wie DBpedia hergestellt. Dazu werden Kandidaten-Konzepte entsprechend den mehreren Bedeutungen eines Begriffs mit Hilfe eines Lexikons nachgewiesen. Schließlich wird der Kontext der Begriffe analysiert, um die am besten geeignete Disambiguation zu bestimmen und den Begriff dem richtigen Konzept zuzuordnen. Beachten Sie, dass "semantische Annotation" im Kontext der Wissensextraktion nicht mit semantischem Parsing verwechselt werden soll, wie es in der natürlichen Sprachverarbeitung verstanden wird (auch als "semantische Annotation:)" Das semantische Parsing zielt auf eine vollständige, maschinenlesbare Darstellung der natürlichen Sprache ab, während die semantische Annotation im Sinne der Wissensextraktion nur einen ganz elementaren Aspekt davon annimmt. Werkzeuge Die folgenden Kriterien können verwendet werden, um Werkzeuge zu kategorisieren, die Kenntnisse aus dem natürlichen Text der Sprache extrahieren. Die folgende Tabelle charakterisiert einige Tools zur Wissensextraktion aus natürlichen Sprachquellen. Knowledge Discovery Knowledge Discovery beschreibt den Prozess der automatischen Suche großer Datenmengen nach Mustern, die als Wissen über die Daten angesehen werden können. Es wird oft als Ableitung von Wissen aus den Eingabedaten beschrieben. Wissensentdeckung aus der Data Mining-Domain entwickelt und ist eng mit ihr sowohl in Bezug auf Methodik und Terminologie verbunden. Der bekannteste Zweig des Data Mining ist die Wissensentdeckung, auch bekannt als Wissensentdeckung in Datenbanken (KDD). Genauso wie viele andere Formen der Wissensentdeckung erzeugt sie Abstraktionen der Eingabedaten. Das durch den Prozess gewonnene Wissen kann zu zusätzlichen Daten werden, die zur weiteren Nutzung und Entdeckung verwendet werden können. Oft sind die Ergebnisse aus der Wissensentdeckung nicht handlungsfähig, handlungsfähige Wissensentdeckung, auch bekannt als Domain-getriebene Datenabbau, zielt darauf ab, handlungsfähige Kenntnisse und Erkenntnisse zu entdecken und zu liefern. Eine weitere vielversprechende Anwendung der Wissensentdeckung liegt im Bereich der Softwaremodernisierung, der Schwachstelle und der Compliance, die das Verständnis bestehender Software-Artefakte beinhaltet. Dieser Prozess ist mit einem Konzept der Reverse Engineering verbunden. Üblicherweise wird das aus der bestehenden Software gewonnene Wissen in Form von Modellen präsentiert, auf die bei Bedarf spezifische Abfragen vorgenommen werden können. Eine Einheitsbeziehung ist ein häufiges Format, um Wissen aus der bestehenden Software zu repräsentieren. Die Object Management Group (OMG) hat die Spezifikation Knowledge Discovery Metamodel (KDM) entwickelt, die eine Ontologie für die Software-Assets und ihre Beziehungen zum Zweck der Wissensentdeckung im bestehenden Code definiert. Wissensentdeckungen bestehender Softwaresysteme, auch als Software-Mining bekannt, sind eng mit dem Data-Mining verbunden, da bestehende Software-Artefakte enormen Wert für Risikomanagement und Geschäftswert enthalten, der Schlüssel für die Auswertung und Entwicklung von Softwaresystemen ist. Anstatt einzelne Datensätze zu produzieren, konzentriert sich der Softwarebergbau auf Metadaten, wie Prozessflüsse (z.B. Datenflüsse, Steuerflüsse, & Call Maps), Architektur, Datenbankschemas und Geschäftsregeln/Zeiten/Prozess. Eingabedaten Datenbanken Beziehungsdaten Datenbank Dokumentenlager Datenlager Software Quellcode Konfigurationsdateien Bauen von Skripten Text Konzept Bergbau Graphen Molecule Bergbau Sequenzen Datenstrom Bergbau Lernen aus zeitverändernden Datenströmen unter Konzept Drift Web Output Formate Datenmodell Metadaten Metamodels Ontologie Wissensdarstellung Wissenskennzeichnung Wissenskennzeichen Wissen Entdeckung Metamodel (KDM) Geschäftsprozessmodellierung Notation (BPMN) Ressourcenbeschreibung Framework (RDF)Software Metriken Siehe auch Clusteranalyse Datenarchäologie == Referenzen ==