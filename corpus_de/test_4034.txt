Bei der Computerspeicherung ist die Fragmentierung ein Phänomen, bei dem Speicherplatz, Hauptspeicher oder Sekundärspeicher ineffizient verwendet wird, die Kapazität oder Leistung reduziert und oft beides. Die genauen Folgen der Fragmentierung hängen vom spezifischen System der Speicherzuordnung im Gebrauch und der jeweiligen Form der Fragmentierung ab. In vielen Fällen führt die Fragmentierung dazu, dass der Speicherplatz verschwendet wird, und in diesem Fall bezieht sich der Begriff auch auf den verschwendeten Raum selbst. Arten der Fragmentierung Es gibt drei verschiedene, aber verwandte Formen der Fragmentierung: externe Fragmentierung, interne Fragmentierung und Datenfragmentierung, die in Isolation oder Konjunktion vorliegen können. Fragmentierung wird oft im Gegenzug für Verbesserungen in der Geschwindigkeit oder Einfachheit akzeptiert. Analoge Phänomene treten für andere Ressourcen wie Prozessoren auf; siehe unten. Grundprinzip Wenn ein Computerprogramm Speicherblöcke aus dem Computersystem fordert, werden die Blöcke in Blöcken zugeordnet. Wenn das Computerprogramm mit einem Stück fertig ist, kann es es zurück zum System freigeben, so dass es später wieder einem anderen oder demselben Programm zugeordnet werden kann. Die Größe und die Menge der Zeit, die ein Stück von einem Programm gehalten wird, variiert. Während seiner Lebensdauer kann ein Computerprogramm viele Speicherblöcke anfordern und befreien. Wenn ein Programm gestartet wird, sind die freien Speicherbereiche lang und zusammenhängend. Im Laufe der Zeit und unter Verwendung werden die langen zusammenhängenden Bereiche in kleinere und kleinere zusammenhängende Bereiche zersplittert. Schließlich kann es für das Programm unmöglich werden, große zusammenhängende Blöcke des Speichers zu erhalten. Types Interne Fragmentierungs-Speicherpaging erzeugt interne Fragmentierung, weil ein gesamter Seitenrahmen zugeordnet wird, ob viel Speicher benötigt wird. Aufgrund der Regeln für die Speicherzuweisung wird manchmal mehr Computerspeicher zugeordnet als benötigt. Beispielsweise kann Speicher nur für Programme in Stücken (in der Regel ein Vielfaches von 4 Bytes) bereitgestellt werden, und als Ergebnis, wenn ein Programm vielleicht 29 Bytes anfordert, wird es tatsächlich ein Stück von 32 Bytes bekommen. Wenn das passiert, verschwendet der überschüssige Speicher. In diesem Szenario ist der unbrauchbare Speicher in einem zugeordneten Bereich enthalten. Diese Anordnung, bezeichnet als feste Partitionen, leidet unter ineffizientem Speichereinsatz - jeder Prozess, egal wie klein, nimmt eine ganze Partition ein. Dieser Abfall wird als interne Fragmentierung bezeichnet. Im Gegensatz zu anderen Arten der Fragmentierung ist die interne Fragmentierung schwierig zu wiederholen; in der Regel der beste Weg, sie zu entfernen ist mit einem Designwechsel. Beispielsweise in der dynamischen Speicherzuordnung schneiden Speicherpools die interne Fragmentierung drastisch, indem der Raum über Kopf über eine größere Anzahl von Objekten verteilt wird. Externe Fragmentierung Externe Fragmentierung entsteht, wenn der freie Speicher in kleine Blöcke getrennt ist und durch zugeordneten Speicher durchsetzt wird. Es ist eine Schwäche bestimmter Speicherzuordnungsalgorithmen, wenn sie den Speicher, der von Programmen verwendet wird, nicht effizient bestellen. Das Ergebnis ist, dass, obwohl freier Speicher zur Verfügung steht, es effektiv unbrauchbar ist, weil es in Stücke unterteilt ist, die zu klein sind, um den Anforderungen der Anwendung gerecht zu werden. Der Begriff extern bezieht sich auf die Tatsache, dass der unbrauchbare Speicher außerhalb der zugeordneten Bereiche liegt. Betrachten Sie beispielsweise eine Situation, in der ein Programm drei kontinuierliche Blöcke des Speichers zuordnet und dann den mittleren Block freigibt. Der Speicher Allocator kann diesen freien Speicherblock für zukünftige Zuordnungen verwenden. Dieser Block kann jedoch nicht verwendet werden, wenn der zuzuordnende Speicher größer ist als dieser freie Block. Externe Fragmentierung tritt auch in Dateisystemen auf, da viele Dateien unterschiedlicher Größe erstellt werden, die Größe ändern und gelöscht werden. Der Effekt ist noch schlimmer, wenn eine Datei, die in viele kleine Stücke aufgeteilt wird gelöscht wird, weil diese ähnlich kleine Bereiche von Freiräumen verlässt. Datenfragmentierung Datenfragmentierung erfolgt, wenn eine Datensammlung im Speicher in viele Stücke zerlegt wird, die nicht in der Nähe sind. Es ist typischerweise das Ergebnis des Versuches, ein großes Objekt in eine Lagerung einzufügen, die bereits eine externe Fragmentierung erlitten hat. Beispielsweise werden Dateien in einem Dateisystem in der Regel in Einheiten verwaltet, die Blocks oder Cluster genannt werden. Wenn ein Dateisystem erstellt wird, gibt es freien Speicherplatz, um Dateiblöcke zusammen zusammenhängend zu speichern. Dies ermöglicht schnelle sequentielle Datei liest und schreibt. Da jedoch Dateien hinzugefügt, entfernt und in der Größe geändert werden, wird der Freiraum extern fragmentiert, so dass nur kleine Löcher, in denen neue Daten platziert werden. Wenn eine neue Datei geschrieben wird, oder wenn eine bestehende Datei erweitert wird, legt das Betriebssystem die neuen Daten in neue nicht-kontinuierliche Datenblöcke, um in die verfügbaren Löcher passen. Die neuen Datenblöcke werden zwangsläufig verstreut, verlangsamen Zugriff aufgrund von Suchzeit und Drehlatz des Schreib-/Lesekopfes und verursachen zusätzliche Overhead, um zusätzliche Standorte zu verwalten. Dies wird Dateisystem Fragmentation genannt. Beim Schreiben einer neuen Datei bekannter Größe, wenn es leere Löcher gibt, die größer sind als diese Datei, kann das Betriebssystem Datenfragmentierung vermeiden, indem die Datei in eine dieser Löcher gesetzt wird. Es gibt eine Vielzahl von Algorithmen für die Auswahl, welche dieser potenziellen Löcher, um die Datei zu setzen; jeder von ihnen ist eine heuristische ungefähre Lösung für das bin-Packung Problem. Der "best fit" Algorithmus wählt das kleinste Loch, das groß genug ist. Der "wurst fit" Algorithmus wählt das größte Loch. Der "First-fit-Algorithmus" wählt das erste Loch, das groß genug ist. Der "next fit" Algorithmus hält den Überblick darüber, wo jede Datei geschrieben wurde. Der "next fit" Algorithmus ist schneller als "first fit", was wiederum schneller ist als "best fit", was die gleiche Geschwindigkeit wie "worst fit". Ebenso wie die Verdichtung eine externe Fragmentierung ausschließen kann, kann die Datenfragmentierung durch eine Neuanordnung der Datenspeicherung eliminiert werden, so dass verwandte Stücke eng miteinander verbunden sind. Zum Beispiel ist der primäre Job eines Defragmentierungs-Tools, Blöcke auf Festplatte umzuordnen, so dass die Blöcke jeder Datei zusammenhängend sind. Die meisten defragmentierenden Dienstprogramme versuchen auch, die Freiraumfragmentierung zu reduzieren oder zu beseitigen. Einige sich bewegende Müllsammler, Dienstprogramme, die automatische Speicherverwaltung durchführen, werden auch verwandte Objekte in der Nähe (dies wird Kompaktierung genannt) bewegen, um die Cache-Leistung zu verbessern. Es gibt vier Arten von Systemen, die nie Daten Fragmentierung erleben – sie speichern immer jede Datei zusammenhängend. Alle vier Arten haben erhebliche Nachteile im Vergleich zu Systemen, die mindestens eine temporäre Datenfragmentierung erlauben: Schreiben Sie einfach jede Datei zusammenhängend. Wenn es nicht bereits genug zusammenhängenden Freiraum, um die Datei zu halten, das System nicht sofort die Datei zu speichern - auch wenn es viele kleine Bits freier Speicher aus gelöschten Dateien, die bis zu mehr als genug, um die Datei zu speichern. Wenn es nicht schon genug zusammenhängenden Freiraum gibt, um die Datei zu halten, verwenden Sie einen Kopiersammler, um viele kleine Bits Freiraum in eine zusammenhängende freie Region zu konvertieren groß genug, um die Datei zu halten. Dies dauert viel mehr Zeit, als die Datei in Fragmente zu zerbrechen und diese Fragmente in den verfügbaren Freiraum zu bringen. Schreiben Sie die Datei in jeden freien Block, durch Fest-Größe-Blöcke Speicher. Wenn ein Programmierer eine feste Blockgröße zu klein wählt, kann das System sofort einige Dateien speichern – Dateien größer als die Blockgröße – auch wenn es viele freie Blöcke gibt, die mehr als genug hinzufügen, um die Datei zu speichern. Wenn ein Programmierer eine Blockgröße zu groß wählt, wird viel Platz auf der internen Fragmentierung verschwendet. Einige Systeme vermeiden die dynamische Zuordnung ganz, vorspeichern (kontinuierliche) Raum für alle möglichen Dateien, die sie benötigen - zum Beispiel MultiFinder vor-allokiert einen RAM-Koch für jede Anwendung, wie es gestartet wurde, nach wie viel RAM, dass die Programmierer der Anwendung es brauchte. Übersicht Im Vergleich zu externen Fragmentierung, Overhead und interne Fragmentierung machen wenig Verlust in Bezug auf verschwendete Speicher und reduzierte Leistung. Es ist definiert als: Externe Gedächtnis Fragmentation = 1 − Größter Block des freien Speichers Total Freier Speicher \{displaystyle \text{External Memory Fragmentation}}}=1-}{\frac \text{Largest Block des freien Speichers}{\text{Gesamt Free Memory} Fragmentation von 0% bedeutet, dass der gesamte freie Speicher in einem einzigen großen Block ist; Fragmentierung ist 90% (zum Beispiel), wenn 100 MB freier Speicher vorhanden ist, aber der größte freie Speicherblock für die Speicherung ist nur 10 MB. Externe Fragmentierung neigt dazu, in Dateisystemen weniger problematisch zu sein als in primären Speicher (RAM) Speichersystemen, weil Programme in der Regel erfordern, dass ihre RAM-Speicheranforderungen mit zusammenhängenden Blöcken erfüllt werden, aber Dateisysteme sind typischerweise dazu ausgelegt, jede Sammlung von verfügbaren Blöcken (Fragmente) verwenden zu können, um eine Datei zu montieren, die logischerweise zusammenhängend erscheint. Wenn also eine hoch fragmentierte Datei oder viele kleine Dateien aus einem vollen Volumen gelöscht werden und dann eine neue Datei mit der Größe gleich dem neu freigegebenen Raum erstellt wird, wird die neue Datei einfach wieder dieselben Fragmente verwenden, die durch die Löschung freigegeben wurden. Wenn das, was gelöscht wurde, eine Datei war, wird die neue Datei genauso fragmentiert sein, wie diese alte Datei war, aber in jedem Fall wird es keine Barriere geben, um alle (hoch fragmentierten) freien Raum zu verwenden, um die neue Datei zu erstellen. Im RAM hingegen können die verwendeten Speichersysteme oft einen großen Block nicht zusammenbauen, um eine Anfrage von kleinen nicht kontinuierlichen freien Blöcken zu erfüllen, so dass die Anfrage nicht erfüllt werden kann und das Programm nicht fortfahren kann, zu tun, was es für den Speicher benötigt (es sei denn, es kann die Anfrage als eine Reihe kleinerer separater Anfragen wiedergeben). Probleme Speicherausfall Das schwerste Problem, das durch Fragmentierung verursacht wird, führt zu einem Ausfall eines Prozesses oder Systems durch vorzeitige Ressourcenerschöpfung: Wenn ein zusammenhängender Block gespeichert werden muss und nicht gespeichert werden kann, tritt ein Ausfall auf. Fragmentierung bewirkt, dass dies auch dann geschieht, wenn es genug der Ressource gibt, aber nicht eine zusammenhängende Menge. Ist beispielsweise ein Computer 4 GiB Speicher und 2 GiB frei, der Speicher jedoch in einer alternierenden Sequenz von 1 MiB verwendet, 1 MiB frei, so kann eine Anforderung für 1 zusammenhängendes GiB Speicher nicht erfüllt werden, obwohl 2 GiB insgesamt frei sind. Um dies zu vermeiden, kann der Allokator anstelle eines Ausfalls eine Defragmentation (oder Speicherverdichtungszyklus) oder eine andere Ressourcenreklamation, wie ein großer Müllsammelzyklus, in der Hoffnung auslösen, dass er dann in der Lage sein wird, die Anfrage zu befriedigen. Dadurch kann der Prozess fortfahren, kann aber die Leistung stark beeinflussen. Performance Degradation Fragmentation verursacht Leistungsdegradation aus mehreren Gründen. Im Wesentlichen erhöht die Fragmentierung die Arbeit, die erforderlich ist, um eine Ressource zuzuordnen und zuzugreifen. Beispielsweise auf einer Festplatte oder einem Bandlaufwerk sind sequentielle Datenlesungen sehr schnell, aber auf der Suche nach einer anderen Adresse ist langsam, so dass das Lesen oder Schreiben einer fragmentierten Datei zahlreiche Suchanfragen erfordert und ist somit viel langsamer, zusätzlich zu einer größeren Verschleiß auf dem Gerät. Wenn eine Ressource nicht fragmentiert ist, können Zuordnungsanfragen einfach dadurch erfüllt werden, dass ein einziger Block von Anfang an der Freifläche zurückgegeben wird. Es ist jedoch fragmentiert, die Anforderung erfordert entweder die Suche nach einem großen, ausreichend freien Block, der eine lange Zeit dauern kann, oder die Anforderung durch mehrere kleinere Blöcke (wenn dies möglich ist), die dazu führen, dass diese Zuordnung fragmentiert wird, und zusätzliche Overhead erfordern, um die mehreren Stücke zu verwalten. Ein subtileres Problem ist, dass die Fragmentierung einen Cache vorzeitig ausschöpfen kann, wodurch das Drashen, aufgrund von Caches, die Blöcke halten, nicht einzelne Daten verursacht. Nehmen Sie zum Beispiel an, ein Programm hat einen Arbeitssatz von 256 KiB, und läuft auf einem Computer mit einem 256 KiB-Cache (Test L2 Instruktion + Daten-Cache), so dass der gesamte Arbeitssatz in Cache passt und somit schnell, zumindest in Bezug auf Cache-Hits. Nehmen Sie weiter an, dass es 64 Translation-Lookaside-Puffer (TLB)-Einträge hat, jeweils für eine 4 KiB-Seite: Jeder Speicherzugriff erfordert eine virtuelle-physische Übersetzung, die schnell ist, wenn die Seite im Cache (hier TLB) ist. Ist das Arbeitsset unfragmentiert, dann passt es auf genau 64 Seiten (der Seitenarbeitssatz wird 64 Seiten sein) und alle Speicher-Lookups können von Cache bedient werden. Wird der Arbeitssatz jedoch fragmentiert, so passt er nicht in 64 Seiten, und die Ausführung verlangsamt sich aufgrund des Drausierens: Seiten werden wiederholt hinzugefügt und im Betrieb von der TLB entfernt. So muss die Cache-Größe bei der Systemauslegung einen Spielraum zur Fragmentierung aufweisen. Die Gedächtnisfragmentierung ist eines der schwersten Probleme, denen Systemmanager begegnen. Im Laufe der Zeit führt es zu einem Abbau der Systemleistung. Schließlich kann die Speicherfragmentierung zu einem vollständigen Verlust des (anwendungsfähigen) freien Speichers führen. Memory Fragmentation ist ein Kernel Programmierebene Problem. Beim Echtzeit-Computing von Anwendungen können Fragmentierungsstufen bis zu 99% erreichen und zu Systemunfällen oder anderen Instabilitäten führen. Diese Art von Systemunfall kann schwer zu vermeiden sein, da es unmöglich ist, den kritischen Anstieg der Speicherfragmentierung zu antizipieren. Während es jedoch nicht möglich sein kann, dass ein System bei übermäßiger Speicherfragmentierung alle Programme weiterläuft, sollte ein gut gestaltetes System aus der kritischen Fragmentierungsbedingung durch Bewegung in einigen von dem System selbst verwendeten Speicherblöcken erholen können, um eine Konsolidierung des freien Speichers in weniger, größere Blöcke zu ermöglichen, oder im schlimmsten Fall durch Beendigung einiger Programme, um ihren Speicher zu befreien und dann die resultierende Summe aus freiem Speicher zu defragen. Dies wird zumindest einen echten Crash im Sinne des Systemausfalls vermeiden und es dem System ermöglichen, einige Programme weiterzulaufen, Programmdaten zu speichern usw. Es ist auch wichtig zu beachten, dass Fragmentierung ein Phänomen des System-Software-Designs ist; verschiedene Software wird anfällig für Fragmentierung auf verschiedene Grade sein, und es ist möglich, ein System zu entwerfen, das nie gezwungen wird, Abschaltung oder Abtötung von Prozessen durch Gedächtnisfragmentierung. Analoge Erscheinungen Während Fragmentierung am besten als Problem bei der Speicherzuordnung bekannt ist, treten für andere Ressourcen, insbesondere Prozessoren, analoge Phänomene auf. Zum Beispiel in einem System, das eine zeitliche Aufteilung zur präemptiven Multitasking verwendet, das jedoch nicht überprüft, ob ein Prozess blockiert ist, ein Verfahren, das für einen Teil seiner Zeitscheibe ausführt, dann aber blockiert und nicht für den Rest seiner Zeitscheibe Abfallzeit aufgrund der resultierenden internen Fragmentierung von Zeitscheiben ablaufen kann. Grundsätzlich verursacht die zeitliche Aufteilung selbst eine externe Fragmentierung von Prozessen, da sie in fragmentierten Zeitscheiben geführt werden, anstatt in einem einzigen ungebrochenen Lauf. Die daraus resultierenden Kosten der Prozessumschaltung und der erhöhte Cachedruck aus mehreren Prozessen mit denselben Caches können zu einer verminderten Leistung führen. Bei gleichzeitigen Systemen, insbesondere verteilten Systemen, wenn eine Gruppe von Prozessen interagieren muss, um fortzufahren, wenn die Prozesse zu getrennten Zeiten oder auf separaten Maschinen (gefragt über Zeit oder Maschinen) vorgesehen sind, kann die Zeit, die für einander wartet oder in der Kommunikation miteinander verbracht wird, die Leistung stark abbauen. Stattdessen erfordern performante Systeme die Koscheduling der Gruppe. Einige Flash-Dateisysteme haben mehrere verschiedene Arten von internen Fragmentierung mit "Dead-Raum" und "Dark-Raum". Siehe auch Defragmentation Dateisystem Fragmentation Memory Management (Betriebssysteme)Block (Datenspeicher)Datencluster Referenzen Quellen http://www.edn.com/design/systems-design/43346/Handling-memory-fragmentation http://www.sqlservercentral.com/articles/performance+tuning/performancemonitoringbyinternalfragmentationmeasur/2014/C Footprint and Performance Optimization, R. Alexander; G. Bensley, Sams Publisher, First edition, Page no:128, ISBN no:9780672319044 Ibid, Seite no:129