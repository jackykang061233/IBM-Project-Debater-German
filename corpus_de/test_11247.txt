Ein Computer ist eine Maschine, die programmiert werden kann, um Sequenzen von arithmetic oder logischen Operationen automatisch durchzuführen. moderne Computer können generische Arten von Operationen durchführen, die als Programme bekannt sind. Diese Programme ermöglichen Computern eine breite Palette von Aufgaben. Ein Computersystem ist ein vollständiger Computer, der die Hardware, das Betriebssystem (hauptsächliche Software) und die für den vollen Betrieb erforderlichen Peripheriegeräte umfasst. Dieser Begriff kann auch auf eine Gruppe von Computern verweisen, die miteinander verbunden sind und gemeinsam funktionieren, wie ein Computer-Netzwerk oder Computercluster. Eine breite Palette von Industrie- und Verbraucherprodukten verwenden Computer als Kontrollsysteme. Einfache Spezialgeräte wie Mikrowellenherden und Fernkontrollen sind enthalten, wie z.B. Fabrikgeräte wie industrielle Roboter und computergestütztes Design, sowie allgemeine Geräte wie persönliche Computer und mobile Geräte wie Smartphones. Computer Macht das Internet, das Hunderte von Millionen von Computern und Nutzern verbindet. Frühcomputer sollten nur für Berechnungen verwendet werden. einfache manuelle Instrumente wie die Entacus haben Menschen seit alten Zeiten bei der Berechnung unterstützt. Früh in der industriellen Revolution wurden einige mechanische Geräte gebaut, um langwierige Aufgaben zu automatisieren, wie z.B. Leitfäden für Schlingen. Mehr ausgefeilte elektrische Maschinen haben Anfang des 20. Jahrhunderts spezielle analoge Berechnungen vorgenommen. Die ersten digitalen elektronischen Berechnungsmaschinen wurden während des Zweiten Weltkriegs entwickelt. Die ersten Halbleitertransistoren in den späten 1940er Jahren folgten dem Silizium-basierten MOSFET (MOS-Transistor) und dem monolithischen integrierten Schaltkreis (IC) Chiptechnologien in den späten 1950er Jahren, der zum Mikroprozessor und der Mikrocomputerrevolution in den 1970er Jahren führte. Geschwindigkeit, Leistung und Vielseitigkeit von Computern sind seit damals dramatisch angestiegen, wobei der Transistor rasch zunimmt (wie von dem US-Gesetz von Moore vorhergesagt), was zu der Digitalen Revolution in den späten 20 bis frühen 21st Jahrhunderten führt. Konventiv besteht ein moderner Computer aus mindestens einem Verarbeitungselement, in der Regel eine zentrale Verarbeitungseinheit (CPU) in Form eines Mikroprozessors sowie einer Art Computerspeicher, in der Regel Halbleiterspeicherchips. Das Verarbeitungselement führt arithmetische und logische Operationen durch, und eine Sequenzierungs- und Kontrolleinheit kann die Reihenfolge der Vorgänge entsprechend gespeicherten Informationen ändern. Periphere Geräte umfassen Inputgeräte (Schlüsselplatten, Mäuse, Freudesmaß usw.), Outputgeräte (Überwachungsbildschirme, Drucker usw.) und Input-/Output-Geräte, die beide Funktionen erfüllen (z.B. das 2000-era-App-Screen). Periphere Geräte ermöglichen es, Informationen von einer externen Quelle zu erhalten, und sie ermöglichen es, das Ergebnis von Operationen zu retten und zu finden. Etymologie Laut dem Oxford-englischsprachigen Wörterbuch war der erste bekannte Einsatz von Computern in einem 1613 Buch namens The Yong Mans Gleanings durch den englischen Schriftsteller Richard Braithwait: "Ich haue [sic] lesen den wahren Computer der Times und die beste Arithmeticianianianianisch, die [sic] atmetten, und er senkte Thym-Tage in eine kurze Zahl. Diese Nutzung des Begriffs, der einem menschlichen Computer, einer Person, die Berechnungen oder Berechnungen durchgeführt hat, zugrunde liegt. Das Wort blieb bis Mitte des 20. Jahrhunderts mit derselben Bedeutung. In letzterem Teil dieses Zeitraums wurden Frauen oft als Computer eingestellt, weil sie weniger bezahlt werden könnten als ihre männlichen Kollegen. Bis 1964 waren die meisten menschlichen Computer Frauen. Das Online-Etymology-Dokument gibt den ersten betesteten Einsatz von Computern in den 1640er Jahren, d. h. „eine, die berechnet“; dies ist ein „Erzeugnisse von Rechen (v.)“. Laut dem Online-Etymology-Urteil ist die Verwendung des Begriffs ""Schätzmaschine" (je nach Art) von 1972. Laut dem Online-Etymology-Text ist die „moderne Nutzung“ des Begriffs „programmierbarer digitaler Computer“ von „1945 unter diesem Namen; [in a] theoretischer [sense] von 1937 als Turing-Maschine. Geschichte Pre-20th Jahrhundert Devices wurden für Tausende von Jahren zur Berechnung der Hilfe verwendet, vor allem mit einem einzigen Brief mit Fingern. Das frühe Zählgerät war wahrscheinlich eine Form von Talg. Spätere Rekordhaltungshilfen im gesamten Fertile-Halbmond waren kalkuli (Skalbereiche, Kones usw.). welche Teile von Gegenständen, wahrscheinlich Vieh oder Körnern, in ungedeckten Tonbehältern versiegelt sind. Ein Beispiel ist die Verwendung von Zählstangen. Die Abacus wurde zunächst für arithmtische Aufgaben verwendet. Die römische Abacus wurde bereits ab 2400 BC von Geräten entwickelt. Inzwischen wurden viele andere Formen der Rückführungstafeln oder Tabellen entwickelt. In einem mittelalterlichen europäischen Zählhaus würde ein geprüftes Tuch auf einen Tisch gelegt, und die Marker, die nach bestimmten Regeln umgesiedelt werden, als eine Beihilfe zur Berechnung der Geldbeträge. Der Antikythera-Mechanismus wird nach dem Preis von Derek J. de Solla als frühester mechanischer analoger Computer betrachtet. Es wurde entworfen, um die lettischen Positionen zu berechnen. Sie wurde in 1901 im Antikythera-Wrack auf der griechischen Insel Antikythera, zwischen Kythera und Kreta entdeckt und wurde von 100 BC entfernt. Geräte einer Komplexität, die mit dem des Antikythera-Mechanismus vergleichbar sind, würden bis zu tausend Jahre später nicht wieder auftreten. Viele mechanische Hilfsmittel zur Berechnung und Messung wurden für den Einsatz von Astronomen und Navigationsgeräten entwickelt. Die Planisphäre war ein Sterndiagramm, das von Abū Rayhān al-Bīrūnī im frühen 11. Jahrhundert entwickelt wurde. The astrolabe wurde in der Hellenistischen Welt in der 1. oder 2. Jahrhunderte BC entwickelt und wird oft an Hipparchus vergeben. Eine Kombination aus Planisphäre und dioptra war der Astrolabe ein analoger Computer, der in der Lage ist, verschiedene Arten von Problemen in spherical astronomy zu untersuchen. Ein Astrolabe mit einem mechanischen Kalendercomputer und Getrieberädern wurde von Abi Bakr von Isfahan, Persia 1235 entwickelt. Abū Rayhān al-Bīrūnī hat den ersten mechanischen lunisolar Kalender astrolabe, einer frühen, festen Wissensverarbeitungsmaschine mit einem Getriebe- und Getrieberaden, c. 1000 AD. Der Sektor, ein Berechnungsinstrument, das zur Lösung von Problemen im Verhältnis, Trigonmetrie, Multiplikation und Spaltung sowie für verschiedene Funktionen wie Quadrate und Plattenwurzeln verwendet wird, wurde Ende des 16. Jahrhunderts entwickelt und stellte Anwendung in Waffenerei, Erhebung und Navigation fest. Der Planimeter war ein manuelles Instrument zur Berechnung des Gebiets einer geschlossenen Zahl, indem er ihn mit einem mechanischen Link aufspürt. In Kürze nach der Veröffentlichung des Logarithm-Konzepts wurde die Klärschregel von dem englischen Philosophen William Oughtredredredrot entworfen. Es handelt sich um einen handverarbeitenden analogen Computer, um Multi-Dissionen und Spaltungen durchzuführen. Wie die Trendentwicklung vorangebracht wurde, boten zusätzliche Größen Gegenseitigkeit, Quadrate und Quadratwurzeln, Platten und Plattenwurzeln sowie überfällige Funktionen wie Logarithms und exonentials, Kreislauf- und Hyperbolic Trigonmetrie und andere Funktionen. Lenkungsregeln mit speziellen Größen werden immer noch für eine schnelle Leistung von Routineberechnungen verwendet, wie z.B. die für Zeit- und Fernberechnungen auf leichten Flugzeugen verwendete E6B-Zwischenschieberegel. Pierre Jaquet-Droz, ein Schweizer Monitorerin, errichtete in den 1770er Jahren ein mechanisches Puppen (automaton), das einen quill-Präsen abschreiben könnte. Indem die Zahl und Ordnung ihrer internen Räder unterschiedliche Buchstaben umschalten, und damit unterschiedliche Botschaften könnten produziert werden. Konkret könnte es mechanisch programmiert werden, um Anweisungen zu lesen. Neben zwei weiteren komplexen Maschinen ist die Puppen im Musée d'Art et d'Histoire von Neuchâtel, der Schweiz und ist immer noch tätig. 1831-1835, mathematician und ingenieur Giovanni Plana, die eine beglaubigte Kalendermaschine entworfen haben, die, obwohl ein System von Soreys und Zylindern und darüber hinaus, den ständigen Kalender für jedes Jahr von AD 0 (d. h. 1 BC) bis AD 4000 vorhersagen könnte, die Laufjahre und die unterschiedliche Tageslänge halten. Die vom schottischen Wissenschaftler Sir William Thomson im Jahre 1872 entwickelte Gezeitenvorhersage war von großer Bedeutung für die Navigation in flachen Gewässern. Es nutzte ein System von Soreys und Drähten, um die vorhergesagten Gezeitenwerte für einen bestimmten Zeitraum automatisch zu berechnen. Der Differentialanalyser, ein mechanischer analoger Computer zur Lösung von Unterschieden bei der Integration, benutzte Rad- und unterschiedslose Mechanismen zur Integration. Sir William Thomson hatte 1876 bereits den möglichen Bau solcher Rechner erörtert, aber er wurde durch das begrenzte Output-Leistungs-Verhältnis der ball- und platten-Integratoren verstopft. In einem differenzierten Analysegerät drückte die Produktion eines Integratoren den Input des nächsten Integratorens oder eine Graphikierung aus. Der Drehmomentverstärker war der Vorschuss, der diesen Maschinen erlaubte. Vannevar Bush und andere entwickelten mechanische Differentialprüfer. Charles B außerge, ein englischer mechanischer Ingenieur und Poly Mathematik, stammten aus dem Konzept eines programmierbaren Computers. er betrachte den "Besitzstand des Computers", so konzeptiert er den ersten mechanischen Computer im frühen 19. Jahrhundert. Nach der Arbeit an seinem bahnbrechenden Unterschiedmotor, der für die Navigationsberechnung bestimmt ist, stellte er fest, dass ein viel allgemeineres Design, ein Analytical Engine, möglich war. Die Eingabe von Programmen und Daten war an die Maschine über gestrichene Karten zu liefern, eine Methode, die zum Zeitpunkt der direkten mechanischen Ansteckungen wie das schwinde. Für die Produktion hätte die Maschine einen Drucker, eine Kurve und eine Schale. Die Maschine wäre auch in der Lage, Zahlen auf Karten zu reißen, um später zu lesen. Der Motor hat eine arithmetische Logikeinheit, den Kontrollfluss in Form von bedingten Zweigniederlassungen und Teilnehmeranschlüssen und integriertem Gedächtnis aufgenommen, wodurch es das erste Design für einen allgemein verwendeten Computer, der in modernen Zeiten als Turing-vollende beschrieben werden könnte. Die Maschine war etwa ein Jahrhundert vor ihrer Zeit. Alle Teile für seine Maschine mussten Hand gemacht werden – das war ein großes Problem für ein Gerät mit Tausenden von Teilen. Letztendlich wurde das Projekt mit der Entscheidung der britischen Regierung, die Finanzierung einzustellen, gelöst. Babbage kann vor allem auf politische und finanzielle Schwierigkeiten zurückgeführt werden, und sein Wunsch, einen immer anspruchsvolleren Computer zu entwickeln und schneller als jeder andere zu verfolgen. Jedoch hat sein Sohn Henry Babbage eine vereinfachte Version der Analysemaschine (die Mühle) in 1888 fertiggestellt. Er gab eine erfolgreiche Demonstration seiner Verwendung in Computertabellen im Jahr 1952. Analoge Computer In der ersten Hälfte des 20. Jahrhunderts wurden viele wissenschaftliche Rechenbedürfnisse durch immer anspruchsvollere analoge Computer gedeckt, die ein direktes mechanisches oder elektrisches Modell des Problems als Grundlage für die Berechnung nutzten. Diese waren jedoch nicht programmierbar und fehlten im Allgemeinen der Vielfalt und Genauigkeit moderner digitaler Computer. Der erste moderne analoge Computer war eine von Sir William Thomson (lateer to Lord Kelvin) im Jahre 1872 entwickelte Gezeitenvorhersagemaschine. Der Differentialanalyser, ein mechanischer analoger Computer zur Lösung von Unterschieden bei der Integration durch Rad- und wahlfreie Mechanismen, wurde 1876 von James Thomson, dem elderischen Bruder des berühmten Sir William Thomson, entworfen. Mit dem von H. L. Pain und Vannevar Bush gegründeten differenzierten Analysegerät erreichte die Kunst des mechanischen analogen Rechentechniks ihren Zenith. Dies basiert auf den mechanischen Integratoren von James Thomson und den von H. W. Nieman entwickelten Drehmomentverstärkern. Ein Dutzend dieser Geräte wurden gebaut, bevor ihre Obsoleszenz offensichtlich wurde. Durch die 1950er Jahre war der Erfolg digitaler elektronischer Computer das Ende für die meisten analogen Rechenmaschinen, aber analoge Computer waren in den 1950er Jahren in einigen Spezialanwendungen wie Bildung (Abfallregel) und Flugzeug (Kontrollsysteme) in Einsatz. Digital Computer Elektromechanisch bis 1938, die amerikanische Marine hatte einen elektromechanischen analogen Computer klein entwickelt, um einen U-Boot zu verwenden. Dies war der Torpedo Data Computer, der Trigonmetrie benutzte, um das Problem der Förmung eines Torpedo an einem Umzugsziel zu lösen. Während des Zweiten Weltkriegs wurden ähnliche Geräte auch in anderen Ländern entwickelt. Frühe digitale Computer waren elektromechanisch; Elektroschalter trieben mechanische Relais, um die Berechnung durchzuführen. Diese Geräte hatten eine geringe Betriebsgeschwindigkeit und wurden schließlich durch viel schnellere Elektrocomputer, die ursprünglich mit Vakuumröhren eingesetzt wurden, überdacht. Die von der deutschen Ingenieur Konrad Zuse im Jahr 1939 geschaffene Z2, war eines der frühesten Beispiele für einen elektromechanischen Relaiscomputer. Im Jahr 1941 folgte Zuse seine frühere Maschine mit dem Z3, dem weltweit ersten elektromechanischen programmierbaren, vollautomatisierten digitalen Computer. Die Z3 wurde mit 2000 Relais gebaut, eine 22 Bit-Sprachlänge, die in einer Uhrhäufigkeit von etwa 5–10 Hz betrieben wurde. Programmcode wurde auf gestrichenenen Film geliefert, während Daten in 64 Wörtern des Gedächtnisses oder aus der Tastatur gespeichert werden könnten. In gewisser Hinsicht ähnelte es ganz ähnlichen modernen Maschinen, und viele Vorschüsse wie schwimmende Nummern. Mehr als das in dem früheren Design von Charles Babbage verwendete harte Decimal-System bedeutete, dass die Maschinen von Zuse dank der damals verfügbaren Technologien einfacher und zuverlässiger werden. Das Z3 war nicht selbst ein universeller Computer, könnte aber auf Turing ausgedehnt werden. Vakuumrohre und digitale elektronische Schaltkreise Rein elektronische Schaltelemente ersetzten in Kürze ihre mechanische und elektromechanische gleichwertige, gleichzeitig aber die digitale Berechnung analog. In den 30er Jahren begann der Ingenieur Tommy Flowers, in der Post Office Research Station in London, die mögliche Verwendung von Elektronik für den Telefonaustausch zu erkunden. Versuchsgeräte, die er im Jahr 1934 gebaut hat, wurden fünf Jahre später in Betrieb genommen, wobei ein Teil des Telefonaustauschnetzes in ein elektronisches Datenverarbeitungssystem umgewandelt wurde, das Tausende von Vakuumröhren nutzt. John Vincent Atanasoff und Clifford E.uter der Universität von Illinois entwickelten und getesteten im Jahr 1942 den Atanasoff-Berry Computer (ABC). Dieses Design war auch alle-elektronisch und verwendet rund 300 Vakuumröhren, mit Kondensatoren, die in einem mechanisch rotierenden Trommel für den Speicher fix sind. Während des Zweiten Weltkriegs erreichten die britischen Code-Dealer im Bletchley Park eine Reihe von Erfolgen bei der Zersetzung der verschlüsselten deutschen militärischen Kommunikation. Die deutsche Verschlüsselungsmaschine, der Farbstoff, wurde zuerst mit Hilfe der elektromechanischen Bomben angegriffen, die häufig von Frauen betrieben wurden. Um die anspruchsvollere deutsche Lorenz SZ 40/42 Maschine, die für die Hochebenenkommunikation verwendet wird, zu blockieren, beauftragte Max Newman und seine Kollegen Blumen, den Coverlustus zu bauen. Er verbrachte elf Monate ab Anfang Februar 1964 den ersten Coverlustus. Nach einem funktionalen Test im Dezember 1944 wurde Coverlustus am Bletchley Park versendet, wo er am 18. Januar 1944 geliefert wurde und seine erste Botschaft am 5. Februar angriff. Coverlustus war der erste elektronische programmierbare Computer der Welt. Es nutzte eine große Zahl von Ventilen (vacuumrohre). Es hatte einen papiertechnischen Input und war in der Lage, eine Vielzahl von koroleanischen Maßnahmen auf ihren Daten durchzuführen, aber es war nicht Turing-voll. Neun Mk II Colossi wurden gebaut (Die Mk I wurde in ein Mk II umgewandelt, das insgesamt zehn Maschinen produzierte. Co Lossus Mark I enthielt 1.500 diermionic Ventile (tuben), aber Mark II mit 2400 Ventilen war fünfmal schneller und einfacher als Mark I, was den Entschlüsselungsprozess erheblich beschleunigte. ENIAC (Electronic Numerical Integrator und Computer) war der erste elektronische programmierbare Computer, der in den USA gebaut wurde, obwohl die ENIAC dem Coverlustus ähnlich war, war es viel schneller, flexibler und war Turing-voll. Wie der Coverlustus wurde ein Programm auf der ENIAC von den Staaten seiner Pfandkabel und Schalter festgelegt, was weit von dem gespeicherten Programm elektronische Maschinen, die später aufgetreten sind, entfernt war. Nachdem ein Programm geschrieben wurde, musste es mechanisch in die Maschine mit manueller Neufestsetzung von Steckern und Schaltern gesetzt werden. Die Programmierer der ENIAC waren sechs Frauen, die häufig als "ENIAC-Küte" bekannt sind. Es kombinierte die hohe Geschwindigkeit der Elektronik mit der Möglichkeit, für viele komplexe Probleme programmiert zu werden. Man könnte 5000 mal ein zweites, tausendmal schneller als jede andere Maschine hinzufügen oder unterlegen. Es hatte auch Module zur Vervielfachung, Spaltung und Quadratwurzel. Hochgeschwindigkeitsspeicher war auf 20 Wörter beschränkt (rund 80 bytes). Leitung von John Mauchly und J. Presper Scholz an der University of Pennsylvania haben die Entwicklung und den Bau von ENIAC Ende 1945 abgeschlossen. Die Maschine war groß, mit 30 Tonnen, mit 200 Kilowatt elektrischer Kraft und enthielt über 18.000 Vakuumröhren, 1.500 Relais und hunderttausenden Widerstandoren, Kondensatoren und Induktoren. Moderne Computerkonzept für moderne Computer Der Grundsatz des modernen Computers wurde von Alan Turing in seinem halben Papier von 1936, On Computable Numbers, vorgeschlagen. Turing schlug ein einfaches Gerät vor, das er "Universal Computing" bezeichnete und nun als universelle Turing-Maschine bekannt ist. Er verwies darauf, dass eine solche Maschine in der Lage ist, alles zu comp, was durch die Ausführung von Anweisungen (Programm), die auf dem bürokratischen Aufwand gespeichert sind, komprochen werden kann. Das grundlegende Konzept der Gestaltung von Turing ist das gespeicherte Programm, bei dem alle Anweisungen für die Rechentechnik in Erinnerung bleiben. Von Neumann räumte ein, dass das zentrale Konzept des modernen Computers auf dieses Papier zurückzuführen ist. Turingmaschinen sind an diesem Tag ein zentrales Thema der Studie in der Theorie der Berechnung. Mit Ausnahme der von ihren Finkenspeichern auferlegten Beschränkungen werden moderne Computer als Turing- unvollständig bezeichnet, d. h. sie verfügen über einen Algorithmus, der einer universellen Turing-Maschine entspricht. Stored Programme Frühcomputer hatten feste Programme. Eine Änderung ihrer Funktion erforderte die Umstrukturierung und Umstrukturierung der Maschine. Mit dem Vorschlag des gespeicherten Software-Programms ändert sich dies. Ein gespeicherter Software-Computer enthält eine Anleitung und kann im Gedächtnis eine Reihe von Anweisungen (ein Programm) speichern, die die Berechnung beschreiben. Alan Turing hat in seinem Papier von 1936 die theoretische Basis für den gespeicherten Computer festgelegt. 1945 trat Turing dem National Physical Laboratory bei und begann mit der Entwicklung eines elektronischen gespeicherten Digital-Programms. Seine 1945-Bericht „Vorschlag Electronic Computer“ war die erste Spezifikation für ein solches Gerät. John von Steiner an der University of Pennsylvania hat auch seinen ersten Entwurf eines Berichts über die EDVAC 1945 umgeleitet. The Manchester Baby war der erste Speichercomputer der Welt. Sie wurde an der Universität Manchester in England von Frederic C. Williams, Tom Kilburn und Geoff Tootill gebaut und lief am 21. Juni 1948 sein erstes Programm. Es wurde als Testbedin für die Williamstube konzipiert, das erste zufällige digitale Speichergerät. Obwohl der Computer von den Normen seiner Zeit als "klein und ehrgeizig" angesehen wurde, war es die erste Arbeitsmaschine, alle für einen modernen elektronischen Computer wesentlichen Elemente zu enthalten. Sobald das Baby die Durchführbarkeit seines Entwurfs bewiesen hatte, wurde ein Projekt an der Universität initiiert, um es zu einem mehr nutzbaren Computer zu entwickeln, der Manchester Mark 1. Grace Hopper war die erste Person, um einen Pool für Programmiersprache zu entwickeln. Die Marke 1 wurde wiederum schnell zum Prototyp für den Ferranti Mark 1, der weltweit erste kommerziell verfügbare allgemeine Computer. Gebaut von Ferranti wurde es im Februar 1951 an die Universität Manchester geliefert. Mindestens sieben dieser späteren Maschinen wurden zwischen 1953 und 1957 geliefert, eines davon an Shell Labors in Amsterdam. Im Oktober 1947 beschlossen die Direktoren des britischen Cateringunternehmens J. Lyons & Company eine aktive Rolle bei der Förderung der kommerziellen Entwicklung von Computern.LEO I Computer wurde im April 1951 in Betrieb genommen und war die erste reguläre Büro-Computerstelle der Welt. Kernwaffen Das Konzept eines intereffektiven Transistor wurde von Julius Edgar LIenfeld im Jahr 1916 vorgeschlagen. John Bardeen und Walter Brattain, die unter William Schockley in Bell Labs arbeiteten, gründeten den ersten Mitarbeitertransistor, den punkt-contact transistor, 1947, der anschließend von der bipolaren Kreuzung von Schockley im Jahr 1948 folgte. Ab 1955 ersetzten Transistors die Vakuumröhren in Computermodellen, was zur „zweiten Generation“ von Computern führt. Im Vergleich zu Vakuumröhren haben die Transisten viele Vorteile: sie sind kleiner und erfordern weniger Strom als Vakuumrohre, so dass weniger Wärme entsteht. Bahntransistors waren viel zuverlässiger als Vakuumröhren und hatten länger, unbestimmtes, Serviceleben. Turboisierte Computer könnten Zehntausende von binären Logikkreisen in einem relativ kompakten Raum enthalten. Kurzstreckentransistors waren jedoch relativ große Geräte, die auf einer Massenproduktionsbasis schwer herzustellen waren, die auf eine Reihe spezialisierter Anwendungen beschränkt waren. In der Universität Manchester hat ein Team unter Leitung von Tom Kilburn eine Maschine entwickelt und aufgebaut, die die neu entwickelten Transistors anstelle von Ventilen nutzt. Ihr erster transistorisierter Computer und der erste in der Welt war bis 1953 in Betrieb, und eine zweite Version wurde im April 1955 fertiggestellt. Jedoch nutzte die Maschine die Ventile, um ihre 125 kHz-Wellenforms und in der Schaltstelle zu erzeugen, um auf ihrem Magneten-Stauchen zu lesen und zu schreiben, so dass es nicht der erste vollständig transistorisierte Computer war. Diese Unterscheidung geht auf die Harwell CADET von 1955, die von der Elektroniksparte der Atomforschung in Harwell gebaut wurde. Der Metall-Oxide-silicon-Feld-Wirkungstransistor (MOSFET), auch als MOS-Transistor bekannt, wurde 1959 von Mohamed M. Atalla und Dawon Kahng in Bell Labs erfunden. Es war der erste wirklich kompakte Transistor, der für eine breite Palette von Anwendungen Miniaturiert und produziert werden könnte. Mit seiner hohen Skalierbarkeit und viel niedrigerem Stromverbrauch und einer höheren Dichte als bipolare Kreuzungstransistors ermöglichte das MOSFET den Aufbau hochdichter integrierter Schaltkreise. Neben der Datenverarbeitung ermöglichte sie auch die praktische Verwendung von MOS-Transisten als Speicherelemente, die zur Entwicklung des MOS-Halbleiterspeichers führen, das frühere Magnetkernspeicher in Computern ersetzte. Die MOSFET führte zur Mikrocomputerrevolution und wurde die treibende Kraft hinter der Computerrevolution. MOSFET ist der am häufigsten verwendete Transistor in Computern und ist der grundlegende Baustein der digitalen Elektronik. Integrierte Schaltkreise Der nächste große Fortschritt in der Rechenleistung kam mit dem Konvent des integrierten Schaltkreises (IC). Die Idee des integrierten Schaltkreises wurde zunächst von einem Radarwissenschaftler entwickelt, der für die Royal Radar-Einrichtung des Verteidigungsministeriums, Geoffrey W.A Dummer, arbeitet. Dummer stellte am 7. Mai 1952 die erste öffentliche Beschreibung eines integrierten Schaltkreises im Symposium über Fortschritte in der Qualität der elektronischen Komponenten in Washington vor. Jack Kilby wurde in Texas Instruments und Robert Noyce bei Fairchild Semiconductor entwickelt. Kilby hat seine ursprünglichen Ideen zum integrierten Schaltkreis im Juli 1958 aufgenommen und das erste integrierte Beispiel am 12. September 1958 erfolgreich demonstriert. Kilby hat in seiner Patentanwendung vom 6. Februar 1959 sein neues Gerät als "ein Halbleiter-Material" bezeichnet, bei dem alle Komponenten des elektronischen Schaltkreises vollständig integriert sind. Kilbys Erfindung war jedoch ein hybrider integrierter Schaltkreis (hybrid IC), anstatt ein plastischer integrierter Schaltkreis (IC). Kilby's IC hatte externe Drahtverbindungen, die es schwer machte, Massenproduktion zu verursachen. Noyce kam auch mit seiner eigenen Idee eines integrierten Schaltkreises ein halbes Jahr später als Kilby. Noyce's Erfindung war der erste echte monolithische IC-Chip. Sein Chip löste viele praktische Probleme, die Kilby nicht hatte. Gefertigt an Fairchild Semiconductor wurde es aus Silizium hergestellt, während Kilbys Chip aus Germanium hergestellt wurde. Noyce's monolithic IC wurde mit dem von seinem Kollegen Jean Hoerni Anfang 1959 entwickelten Planar-Prozess fertig gestellt. wiederum stützte sich der Planar-Prozess auf die Arbeit von Mohamed M. Atalla auf die Halbleiteroberflächentransferierung durch Siliziumdioxid in den späten 1950er Jahren. Moderner monolithischer ICs sind überwiegend MOS (Metall-Oxide-semiconduktor) integrierte Schaltkreise, die aus MOSFETs (MOS-Transistors) gebaut werden. Die früheste experimentelle MOS IC, die im Jahr 1962 von Fred Heiman und Steven Hofstein auf RCA gebaut wurde, war ein 16-transistor-Chip. General Microelectronics hat im Jahr 1964 den ersten kommerziellen MOS IC eingeführt, der von Robert Norman entwickelt wurde. Nach der Entwicklung des selbstgerichteten Tors (silicon-Gate) MOS Transistor von Robert Kerwin, Donald Klein und John Sarace in Bell Labs im Jahr 1967 wurden die ersten Silgate MOS IC mit selbstgerichteten Toren von Federico Faggin bei Fairchild Semiconductor 1968 entwickelt. Die MOSFET ist inzwischen die wichtigste Komponente der modernen ICs. Die Entwicklung des integrierten MOS führte zu der Erfindung des Mikroprozessors und erholte eine Explosion im kommerziellen und persönlichen Einsatz von Computern. Obwohl das Gerät genau das erste Mikroprozessor ist, was teilweise auf die genaue Definition des Begriffs Mikroprozessor zurückzuführen ist, ist es weitgehend unbestreitbar, dass der erste Einzelchip-Mikroprozessor der Intel 4004 war, der von Federico Faggin mit seiner Silizium-Gate-Technologie mit Ted Hoff, Masatoshi Shima und Stanley Mazor auf Intel entworfen und realisiert wurde. Anfang der siebziger Jahre ermöglichte die MOS IC-Technologie die Integration von mehr als 10 000 Transisten auf einem einzigen Chip. System auf einem Chip (SoCs) sind komplette Computer auf einem Mikrochip (oder Chip) die Größe einer Münze. Sie können oder haben möglicherweise kein integriertes RAM und Flashspeicher. Liegt das RAM nicht in der Regel direkt oben (bekannt als Paket auf Paket) oder unterhalb (gegenseitiger Seite des Schaltkreises) des SoC, und das Flashspeicher ist in der Regel in der Nähe des SoC platziert, so dass alles getan wird, um die Datenübermittlungsgeschwindigkeiten zu verbessern, da die Datensignale nicht über lange Entfernungen verfügen müssen. Seit der ENIAC im Jahr 1945 haben Computer enorme Fortschritte erzielt, wobei moderne SoCs (Such wie die Laser 865) die Größe einer Medaille darstellen, während auch hunderttausende mal mächtiger als ENIAC sind, die Milliarden von Transistoren integrieren und nur wenige Kilowatt von Macht verbrauchen. Mobile Computer Die ersten mobilen Computer waren schwer und liefen aus der Macht. 50lb IBM 5100 war ein frühes Beispiel. Spätere tragbare Geräte wie das Osborne 1 und Compaq waren deutlich leichter, aber es ist noch notwendig, in den Griff zu bekommen. Die ersten Notebooks, wie das Grid Compass, haben dieses Erfordernis durch die Einbeziehung von Batterien gestrichen – und mit der fortwährenden Miniaturisierung von Rechenressourcen und Fortschritten im tragbaren Batterieleben stiegen tragbare Computer in den 2000er Jahren an Popularität. Mit den gleichen Entwicklungen konnten die Hersteller die Rechenressourcen in zelluläre Mobiltelefone Anfang 2000 integrieren. Smartphones und Tablets laufen auf einer Vielzahl von Betriebssystemen und wurden zuletzt zum marktbeherrschenden Rechengerät. Diese werden durch System auf einem Chip (SoCs) angetrieben, der Computer auf einem Mikrochip der Größe einer Münze. Art Computer können in einer Reihe unterschiedlicher Arten eingestuft werden, darunter: Computer Digital Computer Hybrid Computer Harvard Architektur Komplexe Anleitung Set Computer-Reduktionsform Set Computer mit Größe, Form-Faktor und Zweck Supercomputer Mainframe Computer Minicomputer (Termin nicht mehr verwendet)ServerRabe Server Personalcomputer Laptops Personalcomputer Audiocomputer (Termin nicht mehr verwendet) Home Computer-Chip-Chip-Computer (nichtlineare Computer, Videobearbeitung PCs und ähnliche) Spiele Computer All-in-one PCs (kleine Form-PCs, Mini-PCs) Home Theater PC Tastatur Computer Leichter Kunden-Internet-Gerät Laptop Desktop Ersatz Computer-Browser Pocketd Laptop 2-in-1 PC Ultrabook Chromebook Subnotebook Netbook Mobile Computer: Tablet-Computer Smartphone Ultra-mobile PC Pocket PC Palmtop PCWearable Computer Smartglasses Singleboard Computer Plug computer-on-Modul System on Modul System in einem Paketsystem-on-Chip (auch als Antragsverarbeiter oder AP bekannt, wenn es keine Schaltkreise wie Radiokreise gibt) Mikrokontrolle Hardware Hardware deckt alle Teile eines Computers ab, die materielle materielle Gegenstände sind. Schaltkreise, Computerchips, Grafikkarten, Schallkarten, Gedächtnis (RAM), Mutterboard, Bildschirme, Stromversorgung, Kabel, Tastaturen, Drucker und Mäuse-Einführungsgeräte sind alle Hardware. Geschichte der Rechner-Hardware Andere Hardware-Themen Ein allgemein verwendeter Computer verfügt über vier Hauptkomponenten: die arithmetic Logik Unit (ALU), die Kontrolleinheit, den Speicher und die Input- und Outputgeräte (zusammengefasst I/O). Diese Teile sind mit Bussen verbunden, die häufig aus Kabelgruppen bestehen. Innerhalb jeder dieser Teile sind Tausende bis Billionen kleiner elektrischer Schaltkreise, die durch einen elektronischen Wechsel abgeschaltet werden können. Jeder Schaltkreis stellt ein wenig (mittlere Digitalisierung) der Informationen dar, so dass der Kreis auf ihm eine 1 darstellt und wenn er eine 0 (in positiver Logik) repräsentiert. Die Schaltkreise sind in Logik Tore angeordnet, damit ein oder mehrere der Schaltkreise den Zustand eines oder mehrerer anderer Schaltkreise kontrollieren können. Messgeräte Wenn unverarbeitete Daten an den Computer mit Hilfe von Inputgeräten übermittelt werden, werden die Daten verarbeitet und an Outputgeräte übermittelt. Die Eingabegeräte können handbetrieben oder automatisiert werden. Die Verarbeitungshandlung ist hauptsächlich durch die CPU geregelt. Manche Beispiele für Inputgeräte sind: Computer Tastatur Digital Kamera Digital Video Grafik Tablet Bildscanner Freude Mikrofon Maus Overlay Tastatur Real-Time-Goverball-Licht-Produktionsgeräte Die Mittel, über die Computer Outputs verfügen, sind als Outputgeräte bekannt. Manche Beispiele für Outputgeräte sind: Computer-Monitorer-PC-Szenario Projector Sound-Karte-Kontrolleinheit Die Kontrolleinheit (häufig als Kontrollsystem oder Zentralkontrolle) verwaltet die verschiedenen Komponenten des Computers; sie liest und interpretiert (decodes) die Programmanleitung und verwandelt sie in Kontrollsignale, die andere Teile des Computers aktivieren. Kontrollsysteme in fortgeschrittenen Computern können die Ausführung einiger Anweisungen zur Verbesserung der Leistung ändern. Eine Schlüsselkomponente, die allen CPUs gemeinsam ist, ist das Programm Gegen, eine spezielle Speicherzelle (ein Register), die den Standort in Erinnerung hält, der nächste Anleitung ist zu lesen. Die Funktion des Kontrollsystems ist wie folgt – dies ist eine vereinfachte Beschreibung, und einige dieser Schritte können je nach Art der CPU gleichzeitig oder in einer anderen Reihenfolge durchgeführt werden: Lesen Sie den Code für die nächste Anleitung aus der Zelle, die im Programmanzeiger angegeben ist. Decodeieren Sie den numerischen Code für die Anleitung in eine Reihe von Befehlen oder Signalen für jedes der anderen Systeme. Kontrolle des Programms an die nächste Anleitung. Lesen Sie alle Daten, die die Anleitung benötigt, von Zellen in Erinnerung (oder vielleicht von einem Eingangsgerät). Die Lage dieser erforderlichen Daten ist in der Regel innerhalb des Unterrichtscodes gespeichert. Übermittlung der erforderlichen Daten an einen ALU- oder Register. Wenn die Anleitung ein ALU- oder Spezialgerät benötigt, um den gewünschten Betrieb zu vollenden. Schreiben Sie das Ergebnis der ALU zurück zu einem Speicherstandort oder zu einem Register oder vielleicht einem Outputgerät. Sprung nach Schritt (1) Da das Programm (vorläufig) nur eine andere Reihe von Speicherzellen ist, kann es durch Berechnungen im ALU geändert werden. Mehr als 100 zu dem Programmteil würde die nächste Anleitung, die von 100 Standorten, die das Programm weiter abziehen, lesen. Anweisungen, die das Programm ändern, sind häufig als Sprungbretter bekannt und ermöglichen den Anschluss (die durch den Computer wiederholt werden) und die häufig vorgeschriebene Ausführung (sowohl Beispiele für Kontrollfluss). Die Reihenfolge der Vorgänge, die die Kontrolleinheit durch den Prozess einer Anleitung führt, ist selbst wie ein kurzes Computerprogramm, und in einigen komplexeren CPU-Designs gibt es noch einen kleineren Computer namens Mikrosequencer, der ein Mikrocode-Programm läuft, das alle diese Ereignisse verursacht. Zentrale Verarbeitungseinheit (CPU)The Control Unit, ALU und Register sind gemeinsam als zentrale Verarbeitungseinheit (CPU) bekannt. frühen CPUs bestehen aus vielen verschiedenen Komponenten. Seit den siebziger Jahren wurden CPUs in der Regel auf einem einzigen integrierten MOS-Chip namens Mikroprozessor aufgebaut. Arithmetic Logik Unit (ALU)The ALU ist in der Lage, zwei Arten von Operationen durchzuführen: arithmetic und Logik. Eine Reihe von arithmetic Operationen, die eine bestimmte ALU-Unterstützung unterstützen, können auf zusätzliche und subtraction beschränkt sein oder könnten Multiplikation, Spaltung, Trigonmetriefunktionen wie Sinus, Kosin usw. und Quadratwurzel umfassen. Manche können nur auf allen Nummern (integers) arbeiten, während andere schwimmende Punkte verwenden, um echte Zahlen zu vertreten, wenn auch mit beschränkter Präzision. Jeder Computer, der in der Lage ist, nur die einfachen Operationen durchzuführen, kann jedoch programmiert werden, um die komplexeren Operationen in einfache Schritte zu brechen, die er durchführen kann. Jeder Computer kann daher programmiert werden, um eine arithmtische Operation durchzuführen wenn der ALU den Betrieb nicht unmittelbar unterstützt, wird er mehr Zeit nehmen. Eine ALU kann auch Zahlen vergleichen und booleanische Wahrheitswerte (trugen oder fälschlich), je nachdem, ob eine gleichwertig ist, mehr als oder weniger als die andere („is 64 größer als 65“). Logistische Operationen umfassen die Logik: AND, OR, XOR und NICHT. Diese können für die Erstellung komplizierter Auflagen und die Verarbeitung der booleanischen Logik nützlich sein. Superscalar Computer können mehrere ALUs enthalten, so dass sie gleichzeitig mehrere Anweisungen verarbeiten können. Prozessoren und Computer mit SIMD- und MIMD-Funktionen enthalten häufig ALUs, die Arithmetik auf Vektoren und matrices ausüben können. Gedächtnis Ein Computerspeicher kann als Liste der Zellen angesehen werden, in denen die Nummern angegeben oder gelesen werden können. Jede Zelle hat eine Nummer und kann eine einzige Nummer speichern. Der Computer kann beauftragt werden, "die Nummer 123 in die Zelle mit Nummer 1357" einzurechnen oder "die Zahl, die in der Zelle 1357 liegt, auf die Zahl zu erhöhen, die in der Zelle 2468 liegt und die Antwort in die Zelle 1595 stellen". Die in Erinnerung gespeicherten Informationen können praktisch alles darstellen. Buchstaben, Nummern, selbst Computeranweisungen können in Erinnerung gebracht werden, die mit gleicher Einfachheit zu sehen ist. Da sich die CPU nicht zwischen verschiedenen Arten von Informationen unterscheidet, ist es die Verantwortung der Software, die Bedeutung des Gedächtnisses als nichts, aber eine Reihe von Nummern zu verleihen. In fast allen modernen Computern wird jede Speicherzelle eingerichtet, um binäre Nummern in Gruppen von acht Bits (genannte A-Tonne) zu speichern. Jede von ihnen ist in der Lage, 256 verschiedene Nummern (28 = 256); entweder von 0 bis 255 oder -128 bis +127. Um größere Zahlen zu speichern, können mehrere aufeinanderfolgende Amphen verwendet werden (typische, zwei, vier oder acht). Wenn negative Zahlen erforderlich sind, werden sie in der Regel in zwei Komplementation gelagert. Andere Regelungen sind möglich, werden jedoch in der Regel nicht außerhalb von Spezialanwendungen oder historischen Kontexten gesehen. Ein Computer kann jede Art von Informationen im Gedächtnis speichern, wenn er numerisch vertreten werden kann. moderne Computer haben Milliarden oder sogar Billionen von Gedächtnistieren. Die CPU enthält ein spezielles Paket von Speicherzellen, die Register genannt werden, die viel schneller lesen und geschrieben werden können als die Hauptspeicherfläche. In der Regel gibt es zwischen zwei und hundert Registern je nach Art der CPU. Register werden für die am häufigsten benötigten Daten verwendet, um zu vermeiden, dass jederzeit Daten zum Hauptspeicher gelangen. Da die Daten ständig auf den Weg gebracht werden, erhöht der Zugriff auf Hauptspeicher (die oft langsam im Vergleich zu den ALU- und Kontrolleinheiten ist) die Geschwindigkeit des Computers. Computer Hauptspeicher ist in zwei Hauptsorten: Zufallsspeicher oder RAM-Abruf-nur Gedächtnis oder ROMRAM können jederzeit gelesen und geschrieben werden, aber ROM wird mit Daten und Software vorgezogen, die nie geändert werden, so dass die CPU nur von ihm lesen kann. ROM wird in der Regel verwendet, um die anfänglichen Start-up-Anweisungen des Computers zu speichern. Insgesamt wird der Inhalt von RAM gelöscht, wenn die Macht auf den Computer abläuft, aber ROM behält seine Daten unbefristet. In einem PC enthält die ROM ein spezielles Programm namens BIO, das das Betriebssystem des Computers von der Festplattenantriebe in RAM beladen, wenn der Computer aufgelöst oder neu angesiedelt ist. In eingebetteten Computern, die häufig keine Festplatten besitzen, können alle erforderlichen Software in ROM gespeichert werden. Software, die in ROM gespeichert ist, wird häufig als Software bezeichnet. Flash-Speicher verwischt die Unterscheidung zwischen ROM und RAM, da sie ihre Daten bei der Abschaltung bewahrt, aber auch rewritierbar ist. Es ist in der Regel viel langsamer als herkömmliche ROM und RAM, so dass der Einsatz auf Anwendungen beschränkt ist, bei denen hohe Geschwindigkeit unnötig ist. In anspruchsvolleren Computern kann es sich um einen oder mehrere RAM-Chole-Speicher geben, die langsamer sind als Register, aber schneller als der Hauptspeicher. In der Regel werden Computer mit dieser Art von Cache entwickelt, um häufig benötigte Daten automatisch in den Cache zu bringen, oft ohne dass eine Intervention auf dem Programmteil erforderlich ist. Input/Output (I/O)I/O ist das Mittel, mit dem ein Computeraustausch mit der Außenwelt stattfindet. Geräte, die Eingaben oder Output in den Computer liefern, werden als Peripheriegeräte bezeichnet. Auf einem typischen persönlichen Computer umfassen Peripheriegeräte wie die Tastatur und Maus sowie Outputgeräte wie die Anzeige und den Drucker. Festplattenantriebe, Festplattenlaufwerke und optische Scheiben dienen sowohl als Input- als auch Outputgeräte. Computernetzwerke sind eine weitere Form von I/O. I/O-Geräte sind oft komplexe Computer in ihrem eigenen Recht, mit eigenem CPU und Gedächtnis. Eine Grafikverarbeitungseinheit könnte 50 oder mehr kleine Computer enthalten, die die für die Anzeige der 3D-Bilder erforderlichen Berechnungen durchführen. moderne Desktop-Computer enthalten viele kleinere Computer, die die wichtigste CPU bei der Ausführung von I/O unterstützen. Ein flacher Bildschirm 2016-era enthält seinen eigenen Computerkreis. Mehr Aufgaben Obwohl ein Computer als ein riesiges in seinem Hauptspeicher gespeichertes Programm angesehen werden kann, ist es in einigen Systemen notwendig, das Aussehen mehrerer Programme gleichzeitig zu verleihen. Dies wird durch Multitasking erreicht, d. h. wenn der Computer schnell zwischen dem Betrieb jedes einzelnen Programms wechseln wird. Eines der Mittel, mit denen dies geschieht, ist ein spezielles Signal, das als Unterbrechung bezeichnet wird, das den Computer in regelmäßigen Abständen dazu veranlassen kann, Anweisungen, wo es war und etwas anderes tun, zu erfüllen. Indem man bedenkt, wo es vor der Unterbrechung vollstreckt wurde, kann der Computer später auf diese Aufgabe zurückkommen. Wenn mehrere Programme gleichzeitig laufen. dann könnten mehrere hundert Unterbrechungen pro Sekunde verursachen, was zu einem Programmwechsel führt. Da moderne Computer in der Regel die Anweisungen mehrerer Größenordnungen schneller ausführen als die menschliche Wahrnehmung, kann es erscheinen, dass viele Programme gleichzeitig laufen, auch wenn nur ein jemals in irgendeiner Form durchgeführt wird. Diese Methode der Multitasking wird manchmal als zeitliche Aufteilung bezeichnet, da jedes Programm ein Stück Zeit zur Verfügung stellt. Vor der Zeit der preiswerten Computer war die wichtigste Verwendung für Multitasking, damit viele Menschen denselben Computer teilen können. Multitasking würde einen Computer verursachen, der sich zwischen mehreren Programmen langsam umsetzt, in direktem Verhältnis zur Anzahl der laufenden Programme, aber die meisten Programme verbringen viel Zeit für langsame Input-/Output-Geräte, um ihre Aufgaben zu erfüllen.Wenn ein Programm auf die Maus warten oder einen Schlüssel auf die Tastatur drücken soll, dann wird es bis zum Ereignis, auf dem es wartet, nicht einen "Zeitraum" geben. Mehr Zeit für andere Programme, damit viele Programme gleichzeitig ohne unannehmbaren Geschwindigkeitsverlust durchgeführt werden können. Mehr Manche Computer sollen ihre Arbeit über mehrere CPUs in einer mehrverarbeitenden Konfiguration verteilen, eine Technik, die einmal in nur großen und leistungsfähigen Maschinen wie Supercomputer, Mainframe-Computer und Server beschäftigt ist. Multiprozessor und Multicore (Multiple CPUs auf einem einzigen integrierten Schaltkreis) sind inzwischen weit verbreitet und werden zunehmend in niedrigeren Märkten genutzt. Insbesondere Supercomputer verfügen oft über sehr einzigartige Architekturen, die sich erheblich von der Basisspeicher-Programmarchitektur und von allgemein verwendeten Computern unterscheiden. Sie sind oft Tausende von CPUs, maßgeschneiderte Hochgeschwindigkeitsverbindungen und Spezialcomputer-Hardware. Solche Gestaltungen sind in der Regel für nur spezialisierte Aufgaben aufgrund des großen Umfangs der Programmorganisation, die erforderlich sind, um die meisten verfügbaren Ressourcen einmal optimal zu nutzen. Supercomputer sehen in der Regel die Verwendung in großmaßstäblichen Simulations-, Grafik- und Kryptographieanwendungen sowie mit anderen sogenannten "embarrassingly parallelen" Aufgaben. Software-Software bezieht sich auf Teile des Computers, die keine materielle Form haben, wie Programme, Daten, Protokolle usw. Software ist der Teil eines Computersystems, das aus codierten Informationen oder Computeranweisungen besteht, im Gegensatz zu der physischen Hardware, aus der das System gebaut wird. Computersoftware umfasst Computerprogramme, Bibliotheken und verwandte nicht geschäftsführende Daten wie Online-Dokumente oder digitale Medien. Es wird oft in Systemsoftware und Software Computer Hardware und Software aufgeteilt, die voneinander getrennt werden müssen und auch nicht realistisch genutzt werden können. Wenn Software in Hardware gespeichert wird, die nicht leicht geändert werden kann, wie etwa mit BIO ROM in einem kompatiblen Computer von IBM, wird es manchmal als Firmware bezeichnet. Sprachen Tausende unterschiedlicher Programmierungssprachen sind für allgemeine Zwecke vorgesehen, andere für nur hoch spezialisierte Anwendungen. Programme Kennzeichnend für moderne Computer, die sie von allen anderen Maschinen unterscheiden, ist, dass sie programmiert werden können. Man kann sagen, dass einige Arten von Anweisungen (das Programm) dem Computer zugestellt werden können, und es wird sie verarbeiten. moderne Computer, die auf der Architektur von WhatsApp basieren, verfügen oft über Maschinencode in Form einer zwingenden Programmiersprache. Konkret kann ein Computerprogramm nur einige Anweisungen enthalten oder auf viele Millionen von Anweisungen ausgedehnt werden, wie z.B. die Programme für Textverarbeiter und Web-Browser. Ein typischer moderner Computer kann Milliarden von Anweisungen pro Sekunde (Giga Flos) ausführen und macht selten einen Fehler über viele Jahre hinweg. Große Computerprogramme, die aus mehreren Millionen Anweisungen bestehen, können Teams von Programmierern jahrelang schreiben und aufgrund der Komplexität der Aufgabe fast sicher Fehler enthalten. Systemarchitektur Dieser Abschnitt gilt für die meisten gemeinsamen RAM-Maschine-basierten Computer. In den meisten Fällen sind Computeranweisungen einfach: eine Nummer zu einem anderen hinzufügen, einige Daten von einem Standort zu einem anderen bewegen, eine Botschaft an einige externe Geräte usw. senden. Diese Anweisungen werden aus dem Speicher des Computers gelesen und werden in der Regel (ausgezeichnet) in der Reihenfolge ausgeführt. Jedoch gibt es in der Regel spezielle Anweisungen, um den Computer zu informieren, um auf einen anderen Platz im Programm zu kommen und die Ausführung von dort durchzuführen. Diese werden als Sprunganweisung (oder Zweigniederlassungen) bezeichnet. Außerdem können bedingte Sprunganweisungen erfolgen, damit unterschiedliche Anweisungen je nach Ergebnis einiger vorheriger Berechnung oder einiger externer Ereignisse verwendet werden können. Viele Computer unterstützen direkt Subroutine, indem sie eine Art von Sprung bieten, der sich an die Stelle erinnert, die er von und an eine andere Anleitung zur Rückkehr in die Anleitung nach diesem Sprunglehrgang aufgenommen hat. Die Ausführung des Programms könnte gerne ein Buch lesen. Während eine Person in der Regel jedes Wort und jede Linie in der Sequenz lesen wird, können sie jederzeit wieder auf einen früheren Platz im Text oder Ausführer, die nicht von Interesse sind, zurückkommen. Ebenso kann ein Computer manchmal zurückgehen und die Anweisungen in einigen Abschnitten des Programms wiederholen, bis einige interne Voraussetzungen erfüllt sind. Dies ist der Kontrollfluss innerhalb des Programms und es ist das, was es dem Computer ermöglicht, die Aufgaben wiederholt ohne menschliche Eingriff auszuführen. Vergleichlich kann eine Person, die einen Taschenrechner benutzt, eine grundlegende arithmtische Operation durchführen, wie z.B. zwei Nummern mit nur wenigen Knopfpressen. All die Nummern von 1 bis 1000 würden jedoch Tausende von Knopfpressen und viel Zeit mit einer nahen Gewissheit machen, einen Fehler zu machen. Andererseits kann ein Computer programmiert werden, um dies mit nur wenigen einfachen Anweisungen zu tun. Das folgende Beispiel ist in der MIPS-Versammlungssprache geschrieben: Nachdem der Computer zugesagt hatte, dieses Programm durchzuführen, wird der Computer die Wiederholungsaufgabe ohne weitere menschliche Intervention ausführen. Man wird fast nie einen Fehler machen und ein moderner PC kann die Aufgabe in einem Bruchteil eines zweiten Teils ausfüllen. Maschinencode In den meisten Computern werden individuelle Anweisungen als Maschinencode mit jeder Anleitung gespeichert, die eine einzigartige Nummer (Betriebscode oder Leercode) erhält. Der Befehl, zwei Nummern zusammen hinzuzufügen, wäre ein opcode; der Befehl, sie zu vervielfachen, hätte einen anderen opcode. Die einfachen Computer sind in der Lage, jede Hand unterschiedlicher Anweisungen zu erfüllen; die komplexeren Computer verfügen über mehrere hundert, um von jedem mit einem einzigartigen numerischen Code zu wählen. Da der Speicher des Computers Nummern speichern kann, kann er auch die Unterrichtscodes speichern. Dies führt zu der wichtigen Tatsache, dass alle Programme (die nur Listen dieser Anweisungen sind) als Listen von Nummern vertreten sein können und sich selbst in der gleichen Weise wie numerische Daten missbrauchen können. Das grundlegende Konzept der Speicherung von Programmen im Computerspeicher neben den Daten, die sie auf dem Markt betreiben, ist die Krippe des vonheimer oder das gespeicherte Programm, Architektur. In einigen Fällen könnte ein Computer einige oder alle seines Programms in Erinnerung speichern, das von den Daten getrennt wird, die er aufbringt. Dies ist die Harvard Architektur nach dem Harvard Mark I Computer. Modern von Neumann Computern zeigen einige Merkmale der Harvard-Architektur in ihren Designs, wie etwa in CPU-Caches. Obwohl es möglich ist, Computerprogramme so lange Listen von Nummern (Maschinensprache) zu schreiben und diese Technik mit vielen frühen Computern zu verwenden, ist es äußerst schuldhaft und potenziell fehlerversprechend, dies in der Praxis insbesondere für komplizierte Programme zu tun. Jede Grundanweisung kann vielmehr einen kurzen Namen geben, der seiner Funktion als Richtwert und leicht zu erinnern ist – ein mnemonic wie ADD, SUB, MULT oder JUMP. Diese mn Martyics sind gemeinsam als Computer-Kollegensprache bekannt. Konversion von Programmen, die in der Sammelsprache geschrieben sind, in etwas, was der Computer tatsächlich verstehen kann (Maschinensprache), erfolgt in der Regel durch ein Computerprogramm, das als Sammler bezeichnet wird. Programmiersprachen Programmierungssprachen bieten verschiedene Möglichkeiten, Programme für Computer durchzuführen. Im Gegensatz zu natürlichen Sprachen sind Programmierungssprachen so konzipiert, dass keine Ambiguity und knapp werden können. Sie sind rein geschriebene Sprachen und sind oft schwer zu lesen. Sie werden in der Regel entweder von einem Sammler oder einem Sammler vor dem Start in den Maschinencode übersetzt oder von einem Dolmetscher direkt übersetzt. Manchmal werden Programme durch eine Hybridmethode der beiden Techniken ausgeführt. Sprachen auf niedrigem Niveau und die Montagesprachen, die sie repräsentieren (kollektive Programmierungssprachen auf niedrigem Niveau) sind in der Regel einzigartig in der besonderen Architektur der zentralen EDV-Einrichtung (CPU). Beispielsweise kann eine ARM-Architektur CPU (z.B. in einem Smartphone oder einem handgehaltenen Videospiel) die Maschinensprache einer x86-Prozessor, die in einem PC sein könnte, nicht verstehen. Historisch wurde eine große Zahl anderer cpu-Architekturen geschaffen und eine umfassende Nutzung vor allem der MOS Technology 6502 und 6510 zusätzlich zum Zilog Z80. Sprachen auf hoher Ebene Obwohl viel einfacher als in der maschinellen Sprache, ist das Schreiben langer Programme in der Montagesprache oft schwierig und ist auch Fehler. Die meisten praktischen Programme werden daher in mehr abstrakten Programmsprachen auf hoher Ebene geschrieben, die in der Lage sind, die Bedürfnisse des Programmanbieters besser auszudrücken (und dadurch den Fehler der Programmanbieter zu verringern). Sprachen auf hoher Ebene werden in der Regel in die Maschinensprache (oder manchmal in die Sammelsprache und dann in die Maschinensprache) mit einem anderen Computerprogramm namens Sammler zusammengefasst. Sprachen auf hoher Ebene sind weniger mit den Arbeiten des Zielcomputers als der Sammelsprache verknüpft, und mehr mit der Sprache und Struktur des Problems, das durch das endgültige Programm gelöst werden soll. Es ist daher oft möglich, verschiedene Sammler zu verwenden, um das gleiche hochwertige Sprachprogramm in die Maschinensprache vieler unterschiedlicher Computerarten umzusetzen. Dies ist Teil der Mittel, mit denen Software wie Videospiele für verschiedene Computerarchitekturen wie Computer und verschiedene Videospielkonsolen zur Verfügung gestellt werden können. Programmdesign für kleine Programme ist relativ einfach und umfasst die Analyse des Problems, die Erfassung von Inputs, die Nutzung der Programmierungsstrukturen in Sprachen, die Entwicklung oder Verwendung von etablierten Verfahren und Algorithmen, die Bereitstellung von Daten für Outputgeräte und Lösungen für das Problem. Da Probleme größer und komplexer werden, sind Merkmale wie Subprogramme, Module, formale Dokumentation und neue Paradigmen wie objektorientierte Programmierung zu finden. Großprogramme, an denen Tausende von Code-Linien beteiligt sind, erfordern formale Softwaremethoden. Die Aufgabe, große Softwaresysteme zu entwickeln, stellt eine erhebliche intellektuelle Herausforderung dar. Software mit einer annehmbar hohen Zuverlässigkeit innerhalb eines vorhersehbaren Zeitplans und Budgets ist historisch schwierig; die akademische und professionelle Disziplin der Softwaretechnik konzentriert sich speziell auf diese Herausforderung. Bugsfehler in Computerprogrammen werden als Fehler bezeichnet. Sie können die Zweckmäßigkeit des Programms beeinträchtigen oder nur geringe Auswirkungen haben. In einigen Fällen können sie jedoch das Programm oder das gesamte System dazu führen, unverantwortlich zu Inputs wie Maus- oder Leitfäden zu werden, ganz zu scheitern oder zu töten. Kommt es zu wünschen, kann man manchmal für böswillige Fehler eingesetzt werden, indem ein unkontrollierter Nutzer einen Wettbewerbsvorteil, einen Code, der zum Vorteil eines Fehlers verwendet und eine ordnungsgemäße Ausführung eines Computers stören soll. Bugs sind in der Regel nicht das Fehler des Computers. Seit Computern, die lediglich die Anweisungen ausführen, sind die Fehler fast immer das Ergebnis von Programmfehlern oder eine Aufsicht im Programmdesign. Admiral Grace Hopper, ein US-amerikanischer Computerwissenschaftler und Entwickler des ersten Forschers, wird für die erste Verwendung der Begriffsfehler im Computer nach einem toten Moth im September 1947 in der Harvard Mark II-Technologie gutgeschrieben. Networking und Internet Computer wurden seit den 1950er Jahren zur Koordinierung der Informationen zwischen mehreren Orten genutzt. Das britische SAGE-System der USA war das erste groß angelegte Beispiel eines solchen Systems, das zu einer Reihe von Spezialsystemen wie Sabre führte. In den siebziger Jahren begannen Computeringenieure an Forschungseinrichtungen in den Vereinigten Staaten, ihre Computer zusammen mit der Telekommunikationstechnik zu verbinden. Die Bemühungen wurden von ARPA (jetzt DARPA) finanziert, und das EDV-Netz, das zur Folge hatte, wurde als ARPANET bezeichnet. Technologien, die das Arpanet ermöglichten, verbreiten und weiterentwickeln. Zeitlang erstreckt sich das Netz über akademische und militärische Einrichtungen und wurde als Internet bekannt. Das Entstehen der Vernetzung führte zu einer Neudefinition der Natur und Grenzen des Computers. Computer-Betriebssysteme und Anwendungen wurden geändert, um die Fähigkeit, die Ressourcen anderer Computer auf dem Netz zu definieren und zu nutzen, wie etwa periphere Geräte, gespeicherte Informationen und die wie die Erweiterung der Ressourcen eines einzelnen Computers. Anfangs waren diese Einrichtungen hauptsächlich für Menschen in High-Tech-Umgebungen verfügbar, aber in den 1990er Jahren wird die Verbreitung von Anwendungen wie E-Mail und World Wide Web, kombiniert mit der Entwicklung günstiger, schneller Vernetzungstechnologien wie Ethernet und ADSL, fast allgegenwärtig. Konkret wächst die Zahl der vernetzten Computer. Ein sehr großer Teil der persönlichen Computer verbindet regelmäßig mit dem Internet, um Informationen zu kommunizieren und zu erhalten. " Drahtlose Vernetzung, die häufig mobile Telefonnetze nutzt, bedeutet, dass die Vernetzung immer noch in mobilen Rechnerumgebungen zunehmend ubiquitär wird. Nichtkonventionelle Computer Kein Computer muss nicht elektronisch sein, oder auch haben einen Prozessor, noch RAM, oder sogar eine Festplatten. Obwohl die populäre Nutzung des Wortcomputers mit einem persönlichen elektronischen Computer gleichzusetzen ist, ist die moderne Definition eines Computers genau: „Ein Gerät, das insbesondere eine programmierbare [normale] elektronische Maschine berechnet, die hochgeschwindigkeits mathematische oder logische Vorgänge abbringt oder die Geschäfte, Geschäfte, Korrespondenten oder andere Prozesse zusammenbringt. " Jedes Gerät, das Informationen verarbeitet, wird als Computer anerkannt, insbesondere wenn die Verarbeitung zweckmäßig ist. Zukunft Es gibt eine aktive Forschung, um Computer aus vielen vielversprechenden neuen Technologiearten wie optische Computer, DNA-Computer, Neuralcomputer und Quantencomputer zu machen. Die meisten Computer sind universell und können alle vernachlässigbaren Funktionen berechnen und sind nur durch ihre Speicherkapazität und Betriebsgeschwindigkeit beschränkt. unterschiedliche Gestaltungen von Computern können jedoch für bestimmte Probleme sehr unterschiedlich sein; beispielsweise Quantencomputer können einige moderne Verschlüsselungsgorithmen (durch Quantenfaktoring) sehr schnell brechen. Leitfäden der Computerarchitektur Viele Arten von Computerarchitekturen: Quantencomputer vs. Chemischer Computer Italiar Verarbeiter vs. Vector Prozessor Non-Uniform Memory Access (NUMA) Computer-Register maschine vs. Stapel-Maschine Harvard-Architektur vs. von Bibliotheken Architektur Von all diesen abstrakten Maschinen ist ein Quantencomputer das beste Versprechen für die Revolutionierung des Rechners. Logistische Tore sind eine gemeinsame abstrakte, die für die meisten der oben genannten digitalen oder analogen Paradigmen gelten kann. Die Fähigkeit, Listen der genannten Programme zu speichern und auszuführen, macht Computer extrem vielseitig und unterscheidet sie von Rechnern. Die Kirche – dies ist eine mathematische Erklärung dieser Vielfalt: Jeder Computer mit einer Mindestkapazität (Wohlstand) ist grundsätzlich in der Lage, dieselben Aufgaben auszuführen, die jeder andere Computer erfüllen kann. Jede Art von Computer (netbook, Supercomputer, zellulare Automatisierung usw.). ist in der Lage, die gleichen Rechenaufgaben zu erfüllen, da genügend Zeit und Speicherkapazität vorhanden sind. Künstliche Intelligenz Ein Computer wird Probleme in genau der Weise lösen, wie es programmiert ist, ohne dass Effizienz, alternative Lösungen, mögliche Kurzstrecken oder mögliche Fehler im Code. Computerprogramme, die lernen und anpassen, sind Teil des aufstrebenden Bereichs der künstlichen Intelligenz und des Maschinenlernens. Künstliche Intelligenzprodukte fallen in der Regel in zwei große Kategorien: Systeme der Regel und Mustererkennungssysteme. Regelbasierte Systeme versuchen, die von menschlichen Experten verwendeten Regeln zu vertreten und sind für die Entwicklung teuer. Musterbasierte Systeme verwenden Daten über ein Problem, um Schlussfolgerungen zu ziehen. Beispiele für Musterbasierte Systeme sind die Spracherkennung, die Schrifterkennung, die Übersetzung und der aufstrebende Bereich der Online-Marketing. Berufe und Organisationen Da die Nutzung von Computern in der gesamten Gesellschaft verbreitet ist, gibt es eine zunehmende Zahl von Karrieren, an denen Computer beteiligt sind. Die Notwendigkeit, dass Computer gut zusammenarbeiten und Informationen austauschen können, hat die Notwendigkeit für viele Normenorganisationen, Vereine und Gesellschaften formaler und informeller Art gedient. Lesen Sie auch externe Links Medien im Zusammenhang mit Computern auf der Wikiversity des Wikiversity hat einen Quiz zu diesem Artikel Warhol & The Computer