Hardware-Beschleunigung ist die Verwendung von Computer-Hardware zur effizienteren Ausführung bestimmter Funktionen im Vergleich zu Software, die auf einer allgemeinen zentralen Verarbeitungseinheit (CPU) läuft. Jede Umwandlung von Daten, die in einer auf einer generischen CPU betriebenen Software berechnet werden können, kann auch in maßgeschneiderten Hardware oder in einigen Kombinationen von beiden berechnet werden. Um Rechenaufgaben schneller auszuführen (oder in irgendeiner anderen Weise besser zu gestalten), kann in der Regel eine Zeit und Geld in die Verbesserung der Software, die Verbesserung der Hardware oder beide investieren. Es gibt verschiedene Ansätze mit Vorteilen und Nachteilen in Bezug auf verringerte Latenzzeit, einen höheren Durchsatz und einen geringeren Energieverbrauch. Typische Vorteile, die sich auf die Software konzentrieren, können eine schnellere Entwicklung, niedrigere Kosten für die Nichterfüllung von Ingenieuren, eine höhere Übertragbarkeit und eine leichte Aktualisierung von Merkmalen oder Zerkleinerungsfehlern, zu den Kosten für die Berechnung allgemeiner Operationen umfassen. Vorteile des Schwerpunkts auf Hardware können Geschwindigkeit, niedrigerer Stromverbrauch, niedrigerer Parallelismus und Bandbreite sowie eine bessere Nutzung von Flächen und funktionellen Komponenten, die auf einem integrierten Schaltkreis verfügbar sind, zu den Kosten einer geringeren Fähigkeit gehören, nach Silizium und höheren Kosten der funktionellen Überprüfung und Zeiten des Marktes zu aktualisieren. In der Hierarchie der digitalen Rechensysteme, die von den allgemeinen Mode-Verarbeitern bis hin zu vollständig kundenspezifischer Hardware reicht, besteht ein Abspiel zwischen Flexibilität und Effizienz, wobei die Effizienz durch Bestellungen von Größenordnungen erhöht wird, wenn eine bestimmte Anwendung dieser Rangordnung höher ist. Diese Hierarchie umfasst allgemeine Zweckverarbeiter wie CPUs, spezialisiertere Prozessoren wie z.B. Drucker, Festfunktion, die auf programmierbaren Gates (FPGAs) angewendet werden, und Festfunktion, die auf anwendungsspezifischen integrierten Schaltkreisen (ASICs) angewendet wird. Hardware-Beschleunigung ist für Leistung und Praxis vorteilhaft, wenn die Funktionen festgelegt werden, so dass Aktualisierungen nicht so notwendig sind wie bei Softwarelösungen. Mit dem Erlös aus reprogrammierbaren Logikgeräten wie FPGAs hat sich die Beschränkung der Hardware-Beschleunigung auf vollständig fixe Algorithmen seit 2010 erleichtert, so dass die Hardware-Beschleunigung auf Problembereiche angewendet werden kann, die eine Änderung der Algorithmen und die Verarbeitungskontrolle erfordern. Übersicht Integrierte Schaltkreise können geschaffen werden, um willkürliche Operationen auf analogen und digitalen Signale durchzuführen. Häufig sind die Signale digital und können als binäre Nummerdaten interpretiert werden. Computer Hardware und Software sind auf Informationen in der binären Darstellung zur Leistung von Computern tätig; dies wird durch die Berechnung der booleanischen Funktionen auf den Inputs und die Produktion des Ergebnisses für die Lagerung oder Weiterverarbeitung erreicht. Leichte Gleichwertigkeit von Hardware und Software, weil alle Turing-Maschinen eine vernachlässigbare Funktion ausüben können, ist es immer möglich, maßgeschneiderte Hardware zu entwerfen, die die gleiche Funktion wie eine bestimmte Software spielt. Umgekehrt kann Software immer verwendet werden, um die Funktion eines bestimmten Hardwarestücks zu verkörpern. Individuelle Hardware kann für dieselben Funktionen, die in der Software angegeben werden können, höhere Leistung pro Kilowatt anbieten. Hardwarebeschreibung Sprachen (HDLs) wie Verilog und VHDL können dieselben semantischen Modelle wie Softwaremodellieren und das Design in eine Netlist einbringen, die für ein RPGA programmiert werden kann oder in die Logik Tore eines ASIC bestehen. Stored-Programm Computer Die überwiegende Mehrheit der Software-basierten Computer findet sich auf Maschinen, die die von Düsseldorf Architektur umsetzen, die gemeinsam als gespeicherte Software-Programme bekannt ist. Computerprogramme werden als Daten gespeichert und von den Verarbeitern ausgeführt. Solche Verarbeiter müssen die Anweisungen , sowie die Beladung von Daten, die aus dem Gedächtnis (als Teil des Unterrichtszyklus) betrieben werden, um die Anweisungen für das Softwareprogramm auszuführen. Durch die Rückführung eines gemeinsamen Caches für Code und Daten wird die "von der IDE Engneck" zu einer grundlegenden Beschränkung des Durchsatzes von Software für die Verarbeiter, die die von-E-Architektur umsetzen, geführt. Selbst in der geänderten Harvard-Architektur, wo Anweisungen und Daten in der Speicherhierarchie getrennt sind, gibt es über die Entschlüsselung von Anweisungen und mehrfach verfügbaren Ausführungseinheiten auf einem Mikroprozessor oder Mikrochip, was zu einer geringen Kreislaufnutzung führt. moderne Prozessoren, die gleichzeitig eine multithreadierende Nutzung der unzureichenden Nutzung der verfügbaren Prozessorfunktionseinheiten und des parallelen Unterrichtsniveaus zwischen verschiedenen Hardware-Spitze bieten. Hardware-Durchführungseinheiten setzen sich im Allgemeinen nicht auf die von Düsseldorf oder geänderten Harvard-Architekturen ein und müssen nicht die Unterrichts- und Decode-Schritte eines Unterrichtszyklus durchführen und diese Stufen übersteigen. Wenn die erforderlichen Berechnungen in einem Registrierungsüberschuss (RTL)-Hardware-Design angegeben werden, können die Kosten für die Zeit und den Kreisbereich, die durch die Anleitung Fänge und Decode-Phasen entstehen, zurückgefordert und auf andere Verwendungen gesetzt werden. Diese Aufwertung spart Zeit, Strom und Schaltbereich in der Berechnung. Die zurückgeforderten Ressourcen können für mehr parallele Berechnungen, andere Funktionen, Kommunikation oder Gedächtnis sowie verstärkte Input-/Output-Fähigkeiten verwendet werden. Hier geht es um die Kosten des allgemeinen Gebrauchs. Neue Hardwarearchitekturen Größere RTL-Anpassung von Hardware-Designern ermöglicht neue Architekturen wie unmemory Computing, verkehrsgeförderte Architekturen (TTA) und Netz-on-Chip (NoC), um weitere Vorteile durch die zunehmende lokale Vielfalt von Daten für den Ausführungskontext zu erzielen und dadurch die Verkürzung von Rechen- und Kommunikation zwischen Modulen und funktionalen Einheiten. Individuelle Hardware ist nur durch die auf der integrierten Schaltkreise verfügbaren Flächen und Logikblöcke eingeschränkt. Hardware ist daher viel mehr frei, einen massiven Parallelismus als Software für allgemeine Zweckverarbeiter anzubieten, was die Möglichkeit bietet, das Parallel-Abfall-Modell (PRAM) anzuwenden. Es ist üblich, Multicore- und vielecore-Verarbeitungseinheiten aus Mikroprozessor IP-Grundsätze auf einer einzigen FPGA oder ASIC zu bauen. In ähnlicher Weise können spezialisierte Funktionseinheiten parallel wie bei der digitalen Signalverarbeitung bestehen, ohne in einem Prozessor IP Kern eingebettet zu sein. Hardware-Beschleunigung wird daher häufig für wiederholte, feste Aufgaben eingesetzt, bei denen kleine abhängige Zweigniederlassungen, insbesondere bei großen Datenmengen. Dies ist, wie die CUDA-Linie von AMD umgesetzt wird. Umsetzungsparameter Da die Gerätemobilität zugenommen hat, wurden neue Parameter entwickelt, die die relative Leistung bestimmter Beschleunigungsprotokolle messen, wobei die Merkmale wie physikalische Hardwareabmessungen, Stromverbrauch und Betriebsdurchsatz berücksichtigt werden. Diese können in drei Kategorien zusammengefasst werden: Leistungsfähigkeit, Umsetzungseffizienz und Flexibilität. Geeignete Parameter betrachten den Bereich der Hardware neben dem entsprechenden Betriebsdurchsatz und der verbrauchten Energie. Beispiel Task Force beschleunigt Herstellen zweier Anlagen in ein drittes Spektrum In diesem Beispiel wird eine Reihe von zwei anderen geschaffen; das Endergebnis, das sogenannte ArraySum, wird {5, 7, 9}.A CPU mit einem einzigen Verarbeitungskern die Schritte zu einem Zeitpunkt ausführen, und wenn es an dem Zug angekommen ist, wäre es die drei Ergänzungen zu einem Zeitpunkt: 1+4 ist 5, 2+5 ist 7, und 3+6 ist 9. Es würde auch die erforderliche Beladung und Speicherung der Daten zu einer Zeit dauern. Kommt die Hardware drei (oder mehr) Prozessoren zur Verfügung, könnte sie jedoch auf alle drei Ergänzungen einmal arbeiten und schließlich die Aufgabe schneller beenden. Wenn der Computer einen Prozessor hatte, der speziell für die Aufstockung von Nummern in verschiedenen Bereichen konzipiert wurde, könnte er diese Aufgabe schneller beenden als es andere Aufgaben beenden würde. Anwendungen Beispiele für die Hardware-Beschleunigung sind Bit blit Beschleunigungsfunktion in den grafischen Verarbeitungseinheiten (GPUs), die Verwendung von Memristern zur Beschleunigung von Neuralnetzen und die regelmäßige Hardware-Beschleunigung für Spam-Kontrollen in der Server-Industrie, um eine regelmäßige Meinungsverweigerung von Service-Angriffen (ReDoS) zu verhindern. Die Hardware, die die Beschleunigung durchführt, kann Teil einer gängigen CPU oder einer separaten Einheit namens Hardwarebeschleunigung sein, obwohl sie in der Regel mit einem spezifischen Begriff wie 3D-Beschleuniger oder Kryptografinbeschleunigung bezeichnet werden. Künftig wurden die Verarbeiter sequential (Instruktionen werden von einem ausgeführt) und wurden entwickelt, um allgemeine Zweckalgorithmen zu betreiben, die von der Anleitung Fänge kontrolliert werden (z.B. zeitweilige Ergebnisse und ein Register). Hardware-Beschleuniger verbessern die Ausführung eines bestimmten Algorithmus, indem sie eine größere Konwährung ermöglichen, spezifische Datenpfade für ihre vorübergehenden Variablen aufweisen und die Kontrolle über die Unterrichtsführung im fetch-decode-Excute-Zyklus verringern. Moderne Verarbeiter sind Multicore und sind häufig parallel „einheitliche Strukturierung; mehrere Daten“ (SIMD) Einheiten. Selbst so zieht die Hardware-Beschleunigung noch Vorteile. Hardware Beschleunigung ist für jeden rechnerintensiven Algorithmus geeignet, der häufig in einer Aufgabe oder einem Programm ausgeführt wird. Je nach Granularität kann die Hardware-Beschleunigung von einer kleinen Funktionseinheit bis zu einem großen funktionalen Block (wie Bewegungsschätzung in MPEG-2) variieren. Hardware-Beschleunigungseinheiten nach der Anwendung Siehe auch Coprozessor DirectX Video-Beschleunigung (DXVA)Richtlinie Speicherzugang (DMA) High Level Synthese C bis HDL Flow to HDL Soft Microprozessor Flynnns Taxonomy of paralleler Computerarchitekturen Einzelanweisung, Multiple Daten (SIMD) Einzelanweisung, mehrfache Anleitungen (SIMT) Computer für Operationen mit Funktionen im Zusammenhang mit der Hardware-Beschleunigung an der Wikimedia Commons*aaS ist ein Akronym für eine Dienstleistung (z.B. X als Dienstleistung) und verweist auf etwas, das einem Kunden, sowohl intern als auch extern, als Dienstleistung, immer im Zusammenhang mit Cloud Computing vorgestellt wird. Als Service oder XaaS(Anything as a Service) bieten Kunden/Verbraucher Endpunkte an, mit denen normalerweise API angetrieben wird, können aber häufig über eine Web-Konsole im Webbrowser eines Nutzers kontrolliert werden. Intern verfügen diese oft komplexen Systeme in der Regel über ein hohes Maß an interner Automatisierung, die in der Regel unterschiedliche Fehlertoleranzen und Rücksilienz aufweisen, die Fähigkeit, die Kapazitäts- und Leistungsanforderungen der Arbeitsbelastungen, die ihren Nutzern/Verbrauchern zur Verfügung gestellt werden, aufzustocken oder abzugleichen, und sind in der Regel bestrebt, ihren Tag ohne die Notwendigkeit menschlicher Eingriffe zu betreiben. IaaS-Funktionen, die am häufigsten in diesem Automatisierungspaket enthalten sind, sind Rechen-, Speicher-, Netz-, Telekommunikations- und Datenspeicher-/Rechenbarkeitsfunktionen, aber die meisten IaaS-Komponenten haben einen Teil ihrer Arbeitsbelastung zu diesen Diensten. Das aktuelle Cloud-.-Ökosystem enthält mehrere Cloud-Anbieter, die jeweils mit ihrem eigenen Menü dieser Dienste für ihre Kunden, auf Abruf oder in einigen Fällen sogar mit vorskopischen Kapazitätsvereinbarungen zu konsumieren. Da (X)aaS-Dienste in der Regel auf Open-Source-Projekten mit wenig bis ohne Lizenzkosten basieren, erfordern im Idealfall nur wenig menschliche Interventionen, um ihre Aufgaben/Registrierungen auszuführen, möglicherweise auf der Nachfrage nach der vollen physischen Ressourcenkapazität einer gesamten Datencenterregion, werden beibehalten und intern unterstützt von dem Cloud-Anbieter und verwenden Ressourcen, die für die Wiederverwendung durch andere IaaS-Aufgaben und ‐Dienste unerheblich sind, stellen XaaS-Dienste große Einsparungen gegenüber herkömmlichen Infrastruktur- und Server-Equivalenzen dar, die häufig auf der Basis von Produkten und auf der Angebotsseite liegen. Beispiele Siehe auch Cloud Computing# Servicemodelle Referenzen =OKEx ist ein auf Seychellen basierender Krypto-Wechsel, der eine Plattform für den Handel verschiedener Kryptokraten bietet. Manche Kernelemente des Austauschs umfassen den Spot- und Derivathandel. Es wurde 2017 gegründet. OKEx ist im Besitz der Ok Group, die auch den Krypto-Wechsel Okcoin besitzt. Geschichte 2018 Am 11. April 2018 kündigte das Unternehmen seine Expansion nach Malta an, da das Land sich bemüht hat, einen soliden Regelungsrahmen für MV-Geschäfte und den Austausch digitaler Vermögenswerte zu schaffen. Im Mai 2018 wurde der Austausch zum weltweit größten Kryptoaustausch durch den gemeldeten Umsatz. Im Juni 2018 wurde die Plattform zu einem der größten Börsen, um einen Krypto-Wechsel-weiß-Label-Dienst zu starten und anzubieten, in dem die Antragsteller über solide Industrieerfahrungen verfügen müssen und 2,5 Millionen $ in ihren Konten haben. 2019In November 2019 wurden von der Hong Kong Securities & Futures Commission (SFC) neue Regeln für den Austausch digitaler Vermögenswerte angekündigt. Laut Reuters sagte OKEx, dass sie nicht erwarten, dass eine große Anzahl von Austauschmaßnahmen, die sich für die neuen Vorschriften entscheiden, in die neuen Vorschriften einfließen, aber die neuen Vorschriften sind für die Branche positiv. Am 19. November 2019 sagte der Krypto-Austausch, dass sie ihre indische Telegram-Gruppe ein Jahr vor der Öffnung ihrer indischen Operationen öffneten. November 2019 kündigte das Unternehmen vier große Partner für sein weltweites Versorgungsunternehmen OKB an. 2021According to Reuters, im Februar 2021 sah OKEx ihr größtes Handelsvolumen in der Geschichte, das sind 26 % vom vorherigen Monat auf 188 Milliarden Euro. In der Regel nach einer neuen Krypto-Liste auf OKEx hat die neue Münze fünf Tage später einen positiven Preissprung gezeigt. Kontroversy im März 2018 gab es einige wichtige Krypto-Vertausche, um bis zu einer Million Dollar zu berechnen, um ihre Listen aufgeführt zu haben. In einer Antwort auf Insider verweigerte ein PR-Manager für OKEx dies, dass es keine Listengebühren für alle Token gibt. Juli 2018 hat ein Händler berichtet, dass er die Zukunft von Hyperlinks mit einem Wert von 416 Millionen nicht an Marge gekauft hat, bevor er durch den Austausch gezwungen wird, seine Position bei einem großen Verlust zu liquidieren. Der Austausch injizierte 2.500 Bitcoins - rund 18 Millionen $ zu aktuellen Preisen - zu einem Versicherungsfonds, um die Auswirkungen auf Kunden minimieren zu können. Händler, die während der letzten Woche einen unrealisierten Gewinn erzielt hatten, mussten jedoch noch einen Rückschlag von 17 Prozent, sogenannten "sozialisierten clawback" zahlen. Am 16. Oktober 2020 setzte OKEx die Rücknahmen nach mutmaßlicher Festnahme vorübergehend aus. Am 19. November 2020 kündigte OKEx an, dass die Rücknahmen wieder geöffnet werden und bis November 27 wieder aufgenommen werden. Siehe auch Liste der Krypto-Vertausche Cryptowährung Liste der Bitcoin-Unternehmen Externe Links offizielle Webseite. Links