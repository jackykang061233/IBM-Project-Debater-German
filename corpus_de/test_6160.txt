Wahrscheinlichkeit ist der Zweig der Mathematik in Bezug auf numerische Beschreibungen, wie wahrscheinlich ein Ereignis auftreten, oder wie wahrscheinlich ist es, dass eine Position wahr ist. Die Wahrscheinlichkeit eines Ereignisses ist eine Zahl zwischen 0 und 1, wobei etwa 0 die Unmöglichkeit des Ereignisses angibt und 1 Gewissheit angibt. Je höher die Wahrscheinlichkeit eines Ereignisses ist, desto wahrscheinlicher wird das Ereignis auftreten. Ein einfaches Beispiel ist die Tossierung einer (unvoreingenommenen) Münze. Da die Münze fair ist, sind die beiden Ergebnisse (Kopf und Schwänze) gleichermaßen wahrscheinlich; die Wahrscheinlichkeit der Köpfe gleicht der Wahrscheinlichkeit der Schwänze; und da keine anderen Ergebnisse möglich sind, beträgt die Wahrscheinlichkeit entweder der Köpfe oder der Schwänze 1/2 (die auch als 0,5 oder 50% geschrieben werden könnte). Diese Konzepte wurden einer axiomatischen mathematischen Formalisierung in der Wahrscheinlichkeitstheorie gegeben, die weit verbreitet in Studienbereichen wie Statistiken, Mathematik, Wissenschaft, Finanzen, Glücksspiel, künstliche Intelligenz, maschinelles Lernen, Informatik, Spieltheorie und Philosophie verwendet wird, um beispielsweise Inferenzen über die erwartete Häufigkeit von Ereignissen zu ziehen. Die Wahrscheinlichkeitstheorie wird auch verwendet, um die zugrunde liegenden Mechaniken und Regelmäßigkeiten komplexer Systeme zu beschreiben. Terminologie der Wahrscheinlichkeitstheorie Experiment: Eine Operation, die einige gut definierte Ergebnisse produzieren kann, wird als Experiment bezeichnet. Beispiel: Wenn wir eine Münze werfen, wissen wir, dass entweder Kopf oder Schwanz auftaucht. So kann man sagen, dass die Operation der Verkleinerung einer Münze zwei gut definierte Ergebnisse haben soll, nämlich (a) Köpfe auftauchen; und (b) Schwänze auftauchen. Random Experiment: Wenn wir eine Düse rollen, sind wir uns der Tatsache bewusst, dass jede der Ziffern 1,2,3,4,5 oder 6 auf dem oberen Gesicht erscheinen kann, aber wir können nicht sagen, dass die genaue Zahl auftauchen wird. Ein solches Experiment, bei dem alle möglichen Ergebnisse bekannt sind und das genaue Ergebnis nicht vorhergesagt werden kann, wird als Random Experiment bezeichnet. Sample Space:Alle möglichen Ergebnisse eines Experiments als Ganzes bilden den Sample Space. Beispiel: Wenn wir einen sterben, können wir jedes Ergebnis von 1 bis 6 erhalten. Alle möglichen Zahlen, die auf der Oberseite erscheinen können, bilden den Sample Space (bezeichnet durch S). Der Probenraum einer Würfelrolle ist also S={1,2,3,4,5,6}Ausgang: Jedes mögliche Ergebnis aus dem Sample Space S für ein Random Experiment wird als Outcome bezeichnet. Beispiel: Wenn wir einen Stempel rollen, können wir 3 oder wenn wir eine Münze werfen, können wir Köpfe erhalten. Ereignis: Jede Teilmenge des Sample Space S wird als Ereignis bezeichnet (bezeichnet durch E). Wenn ein Ergebnis, das zur Teilmenge E gehört, stattfindet, wird gesagt, dass ein Ereignis stattgefunden hat. Während, wenn ein Ergebnis, das nicht zur Untermenge E gehört, ist das Ereignis nicht aufgetreten. Beispiel: Betrachten Sie den Versuch, eine Diät zu werfen. Hier drüben der Sample Space S={1,2,3,4,5,6}. Lassen Sie E das Ereignis von 'einer Zahl erscheinen weniger als 4.' So ist das Ereignis E={1,2,3}. Wenn die Nummer 1 erscheint, sagen wir, dass Ereignis E aufgetreten ist. In ähnlicher Weise, wenn die Ergebnisse 2 oder 3 sind, können wir sagen, dass Ereignis E stattgefunden hat, da diese Ergebnisse Teil E.Trial gehören: Mit einem Versuch meinen wir, ein zufälliges Experiment durchzuführen. Beispiel: (i) Tossing a fair coin, (ii) Rolling an unvoreingenommen die Interpretationen Beim Umgang mit Experimenten, die in einer rein theoretischen Einstellung zufällig und gut definiert sind (wie z.B. eine gerechte Münze,) können Wahrscheinlichkeiten numerisch durch die Anzahl der gewünschten Ergebnisse beschrieben werden, geteilt durch die Gesamtzahl aller Ergebnisse. Zum Beispiel, eine faire Münze zweimal zu werfen wird Kopf-Kopf, Kopf-Tail, Schwanz-Kopf und Schwanz-Tail-Ergebnisse. Die Wahrscheinlichkeit, ein Ergebnis von Kopf-Kopf ist 1 von 4 Ergebnissen, oder, in Zahlen ausgedrückt, 1/4, 0,25 oder 25%. Bei der praktischen Anwendung gibt es jedoch zwei große konkurrierende Kategorien von Wahrscheinlichkeitsinterpretationen, deren Anhänger unterschiedliche Ansichten über die grundsätzliche Natur der Wahrscheinlichkeit halten: Objektivisten geben Zahlen an, um einen objektiven oder physischen Zustand zu beschreiben. Die beliebteste Variante der objektiven Wahrscheinlichkeit ist die häufige Wahrscheinlichkeit, die behauptet, dass die Wahrscheinlichkeit eines zufälligen Ereignisses die relative Häufigkeit des Auftretens eines Experimentsergebnisses bezeichnet, wenn das Experiment unbestimmt wiederholt wird. Diese Interpretation betrachtet die Wahrscheinlichkeit, die relative Frequenz "im langen Verlauf" der Ergebnisse zu sein. Eine solche Modifikation ist die Wahrscheinlichkeit der Propensität, die die Wahrscheinlichkeit als die Tendenz eines Experiments interpretiert, ein bestimmtes Ergebnis zu liefern, auch wenn es nur einmal durchgeführt wird. Subjektivisten vergeben Zahlen pro subjektive Wahrscheinlichkeit, das heißt, als Grad des Glaubens. Der Grad des Glaubens wurde als "der Preis, zu dem Sie kaufen oder verkaufen eine Wette, die zahlt 1 Einheit des Dienstprogramms, wenn E, 0 wenn nicht E." Die populärste Version der subjektiven Wahrscheinlichkeit ist die Bayesische Wahrscheinlichkeit, die Expertenwissen sowie experimentelle Daten enthält, um Wahrscheinlichkeiten zu erzeugen. Das Expertenwissen wird durch einige (subjektive) vorherige Wahrscheinlichkeitsverteilung dargestellt. Diese Daten werden in eine Wahrscheinlichkeitsfunktion integriert. Das Produkt des Vor- und der Wahrscheinlichkeit, wenn normalisiert, führt zu einer posterior Wahrscheinlichkeitsverteilung, die alle bisher bekannten Informationen enthält. Durch Aumanns Vereinbarungstheorem enden Bayesische Agenten, deren frühere Überzeugungen ähnlich sind, mit ähnlichen posterioren Überzeugungen. Allerdings können ausreichend unterschiedliche Vorkommnisse zu unterschiedlichen Schlussfolgerungen führen, unabhängig davon, wie viele Informationen die Agenten teilen. Etymologie Die Wortwahrscheinlichkeit ergibt sich aus den lateinischen Probabilitas, die auch Probität bedeuten kann, ein Maß für die Autorität eines Zeugen in einem Rechtsfall in Europa und oft mit der Adel des Zeugen korreliert. In gewisser Weise unterscheidet sich dies wesentlich von der modernen Bedeutung der Wahrscheinlichkeit, die im Gegensatz zu einem Maß für das Gewicht empirischer Beweise ist, und ist von induktiver Argumentation und statistischer Inferenz angekommen. Geschichte Die wissenschaftliche Studie der Wahrscheinlichkeit ist eine moderne Entwicklung der Mathematik. Gambling zeigt, dass es ein Interesse daran gab, die Ideen der Wahrscheinlichkeit für Jahrtausende zu quantifizieren, aber genaue mathematische Beschreibungen entstanden viel später. Es gibt Gründe für die langsame Entwicklung der Mathematik der Wahrscheinlichkeit. Während die Spiele des Zufalls den Impuls für die mathematische Untersuchung der Wahrscheinlichkeit boten, sind grundlegende Probleme immer noch von den Aberglauben der Spieler verdeckt. Laut Richard Jeffrey, "Vor der Mitte des siebzehnten Jahrhunderts, der Begriff wahrscheinlich (Latin probabilis) bedeutete erheblich, und wurde in diesem Sinne, eindeutig, auf Meinung und Handlung angewendet. Eine wahrscheinliche Handlung oder Meinung war eine wie vernünftige Menschen würden sich unter den Umständen verpflichten oder halten. " Insbesondere in rechtlichen Zusammenhängen könnte es jedoch auch auf Aussagen kommen, für die gute Beweise vorliegen. Der sechzehnte Jahrhundert italienische Polymath Gerolamo Cardano zeigte die Wirksamkeit der Definition von Quoten als das Verhältnis von günstigen zu ungünstigen Ergebnissen (was bedeutet, dass die Wahrscheinlichkeit eines Ereignisses durch das Verhältnis von günstigen Ergebnissen zu der Gesamtzahl der möglichen Ergebnisse gegeben wird). Neben der elementaren Arbeit von Cardano stammt die Lehre der Wahrscheinlichkeiten aus der Korrespondenz von Pierre de Fermat und Blaise Pascal (1654). Christiaan Huygens (1657) gab die früheste wissenschaftliche Behandlung des Themas. Jakob Bernoulli's Ars Conjectandi (posthumous, 1713) und Abraham de Moivre's Doctrin of Chances (1718) behandelten das Thema als Zweig der Mathematik. Siehe Ian Hacking's The Emergence of Probability und James Franklins The Science of Conjecture für Geschichten der frühen Entwicklung des Konzepts der mathematischen Wahrscheinlichkeit. Die Fehlertheorie kann auf Roger Cotes's Opera Miscellanea zurückverfolgt werden (posthum, 1722), aber ein Memoir, das von Thomas Simpson 1755 (printed 1756) vorbereitet wurde, hat zunächst die Theorie auf die Diskussion von Beobachtungsfehlern angewandt. Der Nachdruck (1757) dieses Memoirs legt die Axiome fest, die positive und negative Fehler gleichermaßen wahrscheinlich sind und bestimmte zuordenbare Grenzen den Bereich aller Fehler definieren. Simpson diskutiert auch kontinuierliche Fehler und beschreibt eine Wahrscheinlichkeitskurve. Die ersten beiden Fehlergesetze, die beide vorgeschlagen wurden, stammen von Pierre-Simon Laplace. Das erste Gesetz wurde 1774 veröffentlicht und erklärte, dass die Häufigkeit eines Fehlers als exponentielle Funktion der numerischen Größe des Fehlers ausgedrückt werden könnte – unberücksichtigtes Zeichen. Das zweite Fehlergesetz wurde 1778 von Laplace vorgeschlagen und erklärte, die Häufigkeit des Fehlers sei eine exponentielle Funktion des Fehlerquadrats. Das zweite Fehlergesetz wird die normale Verteilung oder das Gauss-Gesetz genannt. " Es ist historisch schwierig, dieses Gesetz Gauss zuzuschreiben, das trotz seiner bekannten Vorstadt diese Entdeckung wahrscheinlich nicht gemacht hatte, bevor er zwei Jahre alt war."DanielBernoulli (1778) führte das Prinzip des maximalen Produkts der Wahrscheinlichkeiten eines Systems mit gleichzeitigen Fehlern ein. Adrien-Marie Legendre (1805) entwickelte die Methode der kleinsten Quadrate und stellte sie in seine Nouvelles méthodes pour la détermination des orbites des comètes (Neue Methoden zur Bestimmung der Orbits der Kometen). In der Ignoranz von Legendre's Beitrag, ein irisch-amerikanischer Schriftsteller, Robert Adrain, Herausgeber von "The Analyst" (1808,) zunächst das Gesetz der Einrichtung des Fehlers, φ (x) = c e - h 2 x 2 , {\displaystyle \phi (x)=ce{-h^{2}x^{ wobei h 1.Er gab zwei Beweise, das zweite ist im Wesentlichen das gleiche wie John Herschels (1850). Gauss gab den ersten Beweis, der 1809 in Europa (dem dritten nach Adrain) bekannt war. Weitere Beweise wurden von Laplace (1810, 1812,) Gauss (1823,) James Ivory (1825, 1826,) Hagen (1837,) Friedrich Bessel (1838,) W.F Donkin (1844, 1856) und Morgan Crofton (1870). Weitere Beiträge waren Ellis (1844,) De Morgan (1864,) Glaisher (1872,) und Giovanni Schiaparelli (1875). Peters' (1856) Formel für r, der wahrscheinliche Fehler einer einzigen Beobachtung, ist bekannt. Im neunzehnten Jahrhundert, Autoren der allgemeinen Theorie waren Laplace, Sylvestre Lacroix (1816,) Littrow (1833,) Adolphe Quetelet (1853,) Richard Dedekind (1860,) Helmert (1872,) Hermann Laurent (1873,) Liagre, Didion und Karl Pearson. Augustus De Morgan und George Boole verbesserten die Darstellung der Theorie. Im Jahr 1906 führte Andrey Markov den Begriff Markov-Ketten ein, die eine wichtige Rolle in stochastischen Prozessen Theorie und seine Anwendungen spielte. Die moderne Wahrscheinlichkeitstheorie, die auf der Messtheorie basiert, wurde 1931 von Andrey Kolmogorov entwickelt. Auf der geometrischen Seite waren die Beiträge zu The Educational Times einflussreich (Miller, Crofton, McColl, Wolstenholme, Watson und Artemas Martin). Siehe integrale Geometrie für weitere Informationen. Theorie Wie andere Theorien ist die Theorie der Wahrscheinlichkeit eine Darstellung ihrer Begriffe in formalen Begriffen, d.h. in Begriffen, die getrennt von ihrer Bedeutung betrachtet werden können. Diese formalen Begriffe werden durch die Regeln der Mathematik und Logik manipuliert, und alle Ergebnisse werden interpretiert oder in die Problem-Domain übersetzt. Es gab mindestens zwei erfolgreiche Versuche, die Wahrscheinlichkeit zu formalisieren, nämlich die Kolmogorov-Formulierung und die Cox-Formulierung. In Kolmogorovs Formulierung (siehe auch Wahrscheinlichkeitsraum) werden Sätze als Ereignisse und Wahrscheinlichkeit als Maß für eine Klasse von Sätzen interpretiert. In Cox's Theorem wird die Wahrscheinlichkeit als primitiv (d.h. nicht weiter analysiert) genommen und die Betonung liegt auf dem Aufbau einer konsistenten Zuordnung von Wahrscheinlichkeitswerten zu Propositionen. In beiden Fällen sind die Wahrscheinlichkeitsgesetze mit Ausnahme technischer Details identisch. Es gibt andere Methoden zur Quantifizierung von Unsicherheiten, wie die Dempster-Shafer-Theorie oder die Möglichkeitstheorie, aber diese sind im Wesentlichen anders und nicht kompatibel mit den meistverstandenen Gesetzen der Wahrscheinlichkeit. Anwendungen Probability Theorie wird im Alltag in der Risikobewertung und Modellierung angewendet. Die Versicherungsindustrie und die Märkte nutzen die Wirtschaftswissenschaft, um die Preisfestsetzung zu bestimmen und Handelsentscheidungen zu treffen. Die Regierungen wenden probabilistische Methoden in der Umweltregulierung, der Anspruchsanalyse und der Finanzregelung an. Ein Beispiel für die Verwendung der Wahrscheinlichkeitstheorie im Aktienhandel ist die Wirkung der wahrgenommenen Wahrscheinlichkeit eines weit verbreiteten Nahen Osten Konflikts auf Ölpreise, die in der Wirtschaft insgesamt rissige Auswirkungen haben. Eine Einschätzung eines Waarenhändlers, dass ein Krieg wahrscheinlicher ist, kann die Preise dieser Ware auf- oder absenden und andere Händler dieser Meinung signalisieren. Dementsprechend werden die Wahrscheinlichkeiten weder unabhängig noch notwendigerweise rational bewertet. Die Theorie der verhaltensbezogenen Finanzen entstand, um die Auswirkungen solcher Gruppen auf die Preisgestaltung, die Politik und den Frieden und den Konflikt zu beschreiben. Neben der finanziellen Bewertung kann die Wahrscheinlichkeit genutzt werden, um Trends in der Biologie (z.B. Krankheitsausbreitung) sowie Ökologie (z.B. biologische Punnett Quadrate) zu analysieren. Wie bei der Finanzierung kann die Risikobewertung als statistisches Instrument zur Berechnung der Wahrscheinlichkeit unerwünschter Ereignisse verwendet werden, und kann bei der Umsetzung von Protokollen helfen, solche Umstände zu vermeiden. Wahrscheinlichkeit wird verwendet, um Spiele der Chance zu entwerfen, so dass Casinos einen garantierten Gewinn machen können, aber bieten Auszahlungen an Spieler, die häufig genug sind, um weiter spielen zu fördern. Eine weitere signifikante Anwendung der Wahrscheinlichkeitstheorie im Alltag ist Zuverlässigkeit. Viele Verbraucherprodukte wie Automobile und Unterhaltungselektronik nutzen die Zuverlässigkeitstheorie im Produktdesign, um die Ausfallwahrscheinlichkeit zu reduzieren. Die Ausfallwahrscheinlichkeit kann die Entscheidungen eines Herstellers auf die Gewährleistung eines Produkts beeinflussen. Das Cache-Sprachmodell und andere statistische Sprachmodelle, die in der natürlichen Sprachverarbeitung verwendet werden, sind auch Beispiele für Anwendungen der Wahrscheinlichkeitstheorie. Mathematische Behandlung (Für eine Liste der in diesem Artikel verwendeten mathematischen Logik-Notation siehe Notation in Wahrscheinlichkeit und Statistik und/oder Liste der Logik-Symbole.) Betrachten Sie ein Experiment, das eine Reihe von Ergebnissen erzeugen kann. Die Erfassung aller möglichen Ergebnisse wird als Probenraum des Experiments bezeichnet, manchmal als Ω {\displaystyle \Omega } bezeichnet.Der Leistungssatz des Probenraums wird durch die Betrachtung aller verschiedenen Sammlungen möglicher Ergebnisse gebildet. Beispielsweise kann das Walzen eines Stempels sechs mögliche Ergebnisse erzielen. Eine Sammlung möglicher Ergebnisse gibt eine ungerade Zahl auf der Düse. Somit ist die Teilmenge {1,3,5} ein Element des Leistungssatzes des Probenraums von Würfelrollen. Diese Sammlungen werden als Veranstaltungen bezeichnet". In diesem Fall ist {1,3,5} das Ereignis, dass die Düse auf eine seltsame Zahl fällt. Wenn die Ergebnisse, die tatsächlich auftreten, in einem bestimmten Ereignis fallen, wird gesagt, dass das Ereignis stattgefunden hat. Eine Wahrscheinlichkeit ist eine Möglichkeit, jedem Ereignis einen Wert zwischen Null und einem zuzuordnen, mit der Voraussetzung, dass das Ereignis aus allen möglichen Ergebnissen (in unserem Beispiel das Ereignis {1,2,3,4,5,6}) einen Wert von einem zugewiesen wird. Um als Wahrscheinlichkeit zu qualifizieren, muss die Zuordnung von Werten die Voraussetzung erfüllen, dass für jede Sammlung von einander exklusiven Ereignissen (Ereignisse ohne gemeinsame Ergebnisse, wie die Ereignisse {1,6} {3,} und {2,4}) die Wahrscheinlichkeit gegeben wird, dass mindestens eines der Ereignisse durch die Summe der Wahrscheinlichkeiten aller einzelnen Ereignisse gegeben wird. Die Wahrscheinlichkeit eines Ereignisses A wird als P (A) {\displaystyle P(A}), p (A ) {\displaystyle p(A}) oder Pr (A ) {\displaystyle \text{Pr}(A) geschrieben. Diese mathematische Definition der Wahrscheinlichkeit kann sich auf unendliche Probenräume und sogar unzählbare Probenräume erstrecken, wobei das Konzept einer Maßnahme verwendet wird. Das Gegenteil oder die Ergänzung eines Ereignisses A ist das Ereignis [nicht A] (d.h. das Ereignis von A nicht auftritt), das oft als A', A c {\displaystyle A',A^{c}, A ̄, A ∁, ¬ A {\displaystyle {\overline A}, A^{\complement sim,\}neg A} bezeichnet wird, oder 1 - P(A). Als Beispiel ist die Chance, auf einer sechsseitigen Matrize nicht sechs zu rollen, 1 – (Tanz des Walzens einer sechs) = 1 - 1 6 = 5 6 {\displaystyle =1-{\tfrac 1 5}{6. Für eine umfassendere Behandlung siehe Ergänzungsveranstaltung. Wenn auf einer einzigen Leistung eines Experiments zwei Ereignisse A und B auftreten, wird dies als Schnitt- oder Gelenkwahrscheinlichkeit von A und B bezeichnet, die als P (A С B) bezeichnet ist {\displaystyle P(A\cap B}) . Unabhängige Veranstaltungen Sind zwei Ereignisse, A und B unabhängig, so beträgt die gemeinsame Wahrscheinlichkeit P (A und B ) = P (A С B ) = P (A ) P (B ). {\displaystyle P(A{\mbox und }B)=P(A\cap B)=P(A)P(B) Wenn z.B. zwei Münzen gekippt werden, dann beträgt die Wahrscheinlichkeit, dass beide Köpfe 1 2 × 1 2 = 1 4 {\displaystyle {\tfrac 1}{2}\Zeiten {\tfrac 1 1{4. Mutual exklusive Veranstaltungen Wenn entweder Ereignis A oder Ereignis B auftreten kann, aber nie beide gleichzeitig, dann werden sie gegenseitig exklusive Ereignisse genannt. Sind zwei Ereignisse gegenseitig ausschließbar, so wird die Wahrscheinlichkeit der beiden Ereignisse als P (A С B ) {\displaystyle P(A\cap B}) und P (A und B ) = P (A С B ) = 0 {\displaystyle P(A{\mbox und }B) = P(A\cap B) = 0 = 0 Sind zwei Ereignisse gegenseitig ausschließbar, so ist die Wahrscheinlichkeit entweder vorhandener Ereignisse als P (A ∪ B ) {\displaystyle P(A\cup B}) und P (A oder B ) = P (A ∪ B ) = P (A ) + P (B ) - P (A iglio B ) = P (A ) + P (B ) - 0 = P (A ) + P (A ) + P (A ){\displaystyle P(A{\mbox oder }B)=P(A\cup B)=P(A)+P(B)-P(A\cap B)=P(A)+P(B)-0=P(A)+P(B} Zum Beispiel ist die Chance, eine 1 oder 2 auf einer sechsseitigen Matrize zu rollen (1 oder 2 ) = P ( 1 ) + P ( 1 1 1{3. Nicht gegenseitig exklusive Veranstaltungen Wenn die Ereignisse nicht gegenseitig ausschließen, dann P (A oder B) = P (A ∪ B ) = P (A ) + P (B ) - P (A und B ) . Zum Beispiel, wenn eine einzelne Karte zufällig aus einem regelmäßigen Kartendeck gezogen wird, beträgt die Chance, ein Herz oder eine Gesichtskarte (J,Q,K) (oder eine, die beides ist) 13 52 + 12 52 - 3 52 = 11 26 {\displaystyle {\tfrac 13 12 3}{52}={\tfrac 11}{26 , da unter den 52 Karten eines Decks, 13 sind Herzen, 12 sind Gesichtskarten, und 3 sind beide: hier sind die Möglichkeiten in den "3 die beide sind" in jedem der "13 Herzen" und die "12 Gesichtskarten enthalten, sondern nur einmal gezählt werden. Bedingte Wahrscheinlichkeit Die bedingte Wahrscheinlichkeit ist die Wahrscheinlichkeit eines Ereignisses A, bei Auftreten eines anderen Ereignisses B. Die bedingte Wahrscheinlichkeit ist P (A ∣ B) {\displaystyle P(A\mid B}) geschrieben und wird "die Wahrscheinlichkeit von A, gegeben B" gelesen. Es wird durch P (A ∣ B ) = P (A С B ) P (B ) definiert. {\displaystyle P(A\mid B)={\frac {P(A\cap B)}{P(B,.\ If P (B) = 0 {\displaystyle P(B)=0} dann P (A ∣ B) {\displaystyle P(A\mid B}) wird durch diesen Ausdruck formal undefiniert. In diesem Fall sind A {\displaystyle A} und B {\displaystyle B} unabhängig, da P (A С B ) = P (A ) P (B ) = 0 {\displaystyle P(A\cap B)=P(A)P(B)=0} ist. Es ist jedoch möglich, eine bedingte Wahrscheinlichkeit für einige Null-Probability-Ereignisse mit einer σ-Algebra solcher Ereignisse zu definieren. Bei einem Beutel von 2 roten Kugeln und 2 blauen Kugeln (insgesamt 4 Kugeln) beträgt die Wahrscheinlichkeit, einen roten Ball zu nehmen 1 / 2 {\displaystyle 1/2}; bei der Einnahme eines zweiten Balls hängt jedoch die Wahrscheinlichkeit davon ab, dass er entweder eine rote Kugel oder eine blaue Kugel ist. Wenn beispielsweise ein roter Ball genommen wurde, dann wäre die Wahrscheinlichkeit, einen roten Ball wieder zu holen 1 / 3 {\displaystyle 1/3}, da nur noch 1 rote und 2 blaue Kugeln übrig geblieben wären. Und wenn zuvor ein blauer Ball genommen wurde, wird die Wahrscheinlichkeit eines roten Balls 2 / 3 {\displaystyle 2/3} sein. Inverse Wahrscheinlichkeit In der Wahrscheinlichkeitstheorie und -applikationen bezieht sich die Bayes'-Regelung auf das Ereignis A 1 {\displaystyle A_{1}, vor (vor) und nach (nach) Konditionierung auf einem anderen Ereignis B {\displaystyle B} .Die Chancen auf A 1 {\displaystyle A_{1} auf das Ereignis A 2 {\displaystyle A_{2} ist einfach das Proplayverhältnis A_{2 Wenn willkürlich viele Ereignisse Eine {\displaystyle A} ist von Interesse, nicht nur zwei, die Regel kann als posterior proportional zu den früheren Zeiten Wahrscheinlichkeit, P (A | B ) a P (A ) P (B | A ) {\displaystyle P(A|B)\propto P(A)P(B|A} geändert werden, wobei das Proportionalitätssymbol bedeutet, dass die linke Seite proportional ist. In dieser Form geht es zurück nach Laplace (1774) und nach Cournot (1843); siehe Fienberg (2005). Siehe Inverse Wahrscheinlichkeit und Bayes' Regel. Zusammenfassung der Wahrscheinlichkeiten Beziehung zu Zufall und Wahrscheinlichkeit in Quantenmechanik In einem deterministischen Universum, basierend auf neutonischen Konzepten, würde es keine Wahrscheinlichkeit geben, wenn alle Bedingungen bekannt waren (Laplaces Dämon), (aber es gibt Situationen, in denen die Empfindlichkeit gegenüber anfänglichen Bedingungen unsere Fähigkeit überschreitet, sie zu messen, also sie kennen). Im Falle eines Roulette-Rads, wenn die Kraft der Hand und die Periode dieser Kraft bekannt sind, wäre die Anzahl, auf der der Ball stoppen würde eine Gewissheit (obwohl als praktische Angelegenheit, dies würde wahrscheinlich nur von einem Roulette-Rad, das nicht genau geebnet wurde - wie Thomas A. Bass' Newtonian Casino offenbart.) Dies nimmt auch Kenntnis von Trägheit und Reibung des Rades, Gewicht, Glätte und Rundheit der Kugel, Variationen der Handgeschwindigkeit während des Drehens und so weiter. Eine probabilistische Beschreibung kann somit nützlicher sein als die neutonische Mechanik zur Analyse der Ergebnisse wiederholter Rollen eines Rouletterades. Physiker stehen in der kinetischen Theorie der Gase in der gleichen Situation, wo das System im Prinzip so komplex ist (mit der Anzahl der Moleküle typischerweise die Größenordnung der Avogadro-Konstante 6.02 x 1023), dass nur eine statistische Beschreibung seiner Eigenschaften möglich ist. Die Wahrscheinlichkeitstheorie ist erforderlich, um Quantenphänomene zu beschreiben. Eine revolutionäre Entdeckung der frühen Physik des 20. Jahrhunderts war der zufällige Charakter aller physikalischen Prozesse, die in subatomaren Skalen auftreten und durch die Gesetze der Quantenmechanik geregelt werden. Die objektive Wellenfunktion entwickelt sich deterministisch, aber nach der Kopenhagener Interpretation befasst sie sich mit Wahrscheinlichkeiten der Beobachtung, das Ergebnis wird durch einen Wellenfunktionszusammenbruch erklärt, wenn eine Beobachtung erfolgt. Der Verlust des Determinismus im Sinne des Instrumentalismus entsprach jedoch nicht der allgemeinen Zustimmung. Albert Einstein bemerkte berühmt in einem Brief an Max Born: "Ich bin überzeugt, dass Gott keine Würfel spielt". Wie Einstein, Erwin Schrödinger, der die Wellenfunktion entdeckte, glaubte Quantenmechanik eine statistische Annäherung an eine zugrunde liegende deterministische Realität. In einigen modernen Interpretationen der statistischen Messmechanik wird die Quantendekohärenz aufgerufen, um das Auftreten subjektiv probabilistischer Versuchsergebnisse zu berücksichtigen. Siehe auch Chance (Disambiguation) Klassenmitgliedschaft Wahrscheinlichkeiten Contingency Equiprobability Heuristics in Urteil und Entscheidungsfindung Wahrscheinlichkeit Theorie Randomness Statistik Schätzungen Theorie Wahrscheinlichkeit Dichte Funktion Paarliche Unabhängigkeit Im GesetzBalance of probabilities Hinweise Referenzen Bibliographie Kallenberg, O. (2005)Probabilistische Symmetrien und Invarianzprinzipien. Springer-Verlag, New York.510 pp.ISBN 0-387-25115-4 Kallenberg, O. (2002) Stiftungen Modern Probability, 2. ed.Springer Series in Statistics.650 pp.ISBN 0-387-95313-2 Olofsson, Peter (2005) Probability, Statistics, and Stochastic Processes, Wiley-Interscience.504 pp. Wahrscheinlichkeit Theorie: Die Logik der Wissenschaft. Vordruck: Washington University, (1996).— HTML-Index mit Links zu PostScript-Dateien und PDF (erste drei Kapitel) Menschen aus der Geschichte der Probability and Statistics (Univ. of Southampton) Wahrscheinlichkeit und Statistiken über die frühere Nutzungsseite (Univ. of Southampton)Earliest Uses of Symbols in Probability and Statistics on Earliest Uses of Various Mathematical Symbols A tutorial on Wahrscheinlichkeit and Bayes' theorem devised for first Year Oxford University students [1] pdf file of An Richard P. Feynmans Vortrag über Wahrscheinlichkeit.