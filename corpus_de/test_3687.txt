Eine Programmiersprache ist eine formale Sprache, die eine Reihe von Strings enthält, die verschiedene Arten von Maschinencodeausgabe erzeugen. Programmiersprachen sind eine Art von Computersprache und werden in der Computerprogrammierung verwendet, um Algorithmen zu implementieren. Die meisten Programmiersprachen bestehen aus Anweisungen für Computer. Es gibt programmierbare Maschinen, die anstelle von allgemeinen Programmiersprachen eine Reihe von spezifischen Anweisungen verwenden. Seit den frühen 1800er Jahren wurden Programme verwendet, um das Verhalten von Maschinen wie Jacquardwebern, Musikboxen und Player-Pianos zu lenken. Die Programme für diese Maschinen (wie z.B. die Scrolls eines Players) erzeugten in Reaktion auf unterschiedliche Eingaben oder Bedingungen kein unterschiedliches Verhalten. Tausende von verschiedenen Programmiersprachen wurden erstellt und jedes Jahr werden mehr erstellt. Viele Programmiersprachen werden in einer zwingenden Form (d.h. als Folge der durchzuführenden Operationen) geschrieben, während andere Sprachen die deklarative Form verwenden (d.h. das gewünschte Ergebnis wird angegeben, nicht wie es zu erreichen ist). Die Beschreibung einer Programmiersprache wird in der Regel in die beiden Komponenten von Syntax (Form) und Semantik (Mittel) aufgeteilt. Einige Sprachen werden durch ein Spezifikationsdokument definiert (z.B. die C-Programmiersprache wird durch eine ISO-Norm vorgegeben), während andere Sprachen (z.B. Perl) eine dominante Implementierung aufweisen, die als Referenz behandelt wird. Einige Sprachen haben beide, mit der Grundsprache, die durch einen Standard definiert wird, und Erweiterungen aus der dominanten Umsetzung. Die Programmiersprachetheorie ist ein Unterfeld der Informatik, das sich mit der Gestaltung, Implementierung, Analyse, Charakterisierung und Klassifizierung von Programmiersprachen befasst. Begriffsbestimmungen Eine Programmiersprache ist eine Notation zum Schreiben von Programmen, die Spezifikationen einer Berechnung oder eines Algorithmus sind. Einige Autoren beschränken den Begriff "Programmiersprache" auf jene Sprachen, die alle möglichen Algorithmen ausdrücken können. Zu den für eine Programmiersprache oft als wichtig erachteten Merkmalen gehören: Funktion und Ziel Eine Computerprogrammiersprache ist eine Sprache, die zum Schreiben von Computerprogrammen verwendet wird, wobei ein Computer eine Art Berechnung oder Algorithmus durchführt und möglicherweise externe Geräte wie Drucker, Festplatten, Roboter usw. steuert. Beispielsweise werden PostScript-Programme häufig von einem anderen Programm erstellt, um einen Computerdrucker oder eine Anzeige zu steuern. Im Allgemeinen kann eine Programmiersprache die Berechnung auf einigen, möglicherweise abstrakten, Maschine beschreiben. Es wird allgemein angenommen, dass eine vollständige Spezifikation für eine Programmiersprache eine Beschreibung, gegebenenfalls idealisiert, einer Maschine oder einem Prozessor für diese Sprache enthält. In den meisten praktischen Zusammenhängen beinhaltet eine Programmiersprache einen Computer; folglich werden Programmiersprachen in der Regel so definiert und untersucht. Programmiersprachen unterscheiden sich von natürlichen Sprachen, indem natürliche Sprachen nur für die Interaktion zwischen Menschen verwendet werden, während Programmiersprachen auch Menschen erlauben, Anweisungen an Maschinen zu kommunizieren. Abstraktionen Programmiersprachen enthalten in der Regel Abstraktionen zur Definition und Manipulation von Datenstrukturen oder zur Steuerung des Ablaufs der Ausführung. Die praktische Notwendigkeit, dass eine Programmiersprache angemessene Abstraktionen unterstützt, wird durch das Abstraktionsprinzip ausgedrückt. Dieses Prinzip wird manchmal als Empfehlung an den Programmierer formuliert, solche Abstraktionen richtig zu nutzen. Ausdruckskraft Die Berechnungstheorie klassifiziert Sprachen durch die Berechnungen, die sie ausdrücken können. Alle Turing-komplete Sprachen können den gleichen Satz von Algorithmen implementieren. ANSI/ISO SQL-92 und Charity sind Beispiele für Sprachen, die nicht Turing komplett sind, aber oft Programmiersprachen genannt werden. Markupsprachen wie XML, HTML oder Troff, die strukturierte Daten definieren, werden in der Regel nicht als Programmiersprachen betrachtet. Programmiersprachen können jedoch die Syntax mit Markupsprachen teilen, wenn eine rechnerische Semantik definiert ist. XSLT ist zum Beispiel eine vollständige Turing-Sprache mit XML-Syntax. Darüber hinaus enthält LaTeX, die meist für die Strukturierung von Dokumenten verwendet wird, auch eine Turing vollständige Teilmenge. Der Begriff Computersprache wird manchmal austauschbar mit Programmiersprache verwendet. Die Nutzung beider Begriffe variiert jedoch unter den Autoren, einschließlich des genauen Rahmens jedes einzelnen. Eine Verwendung beschreibt Programmiersprachen als Teilmenge von Computersprachen. In ähnlicher Weise werden im Computing verwendete Sprachen, die ein anderes Ziel als das Ausdrucken von Computerprogrammen haben, allgemein als Computersprachen bezeichnet. Zum Beispiel werden Markupsprachen manchmal als Computersprachen bezeichnet, um zu betonen, dass sie nicht für die Programmierung verwendet werden sollen. Eine weitere Verwendung bezieht sich auf Programmiersprachen als theoretische Konstrukte zur Programmierung von abstrakten Maschinen und Computersprachen als Teilmenge davon, die auf physikalischen Computern läuft, die über endliche Hardwareressourcen verfügen. John C. Reynolds betont, dass formale Spezifikationssprachen genauso viel Programmiersprachen sind wie die für die Ausführung vorgesehenen Sprachen. Er argumentiert auch, dass textuelle und sogar grafische Eingabeformate, die das Verhalten eines Computers beeinflussen, Programmiersprachen sind, obwohl sie häufig nicht Turing-komplete sind, und bemerkt, dass Unwissenheit der Programmiersprache Konzepte ist der Grund für viele Fehler in Eingabeformaten. Geschichte Sehr frühe Computer, wie Colossus, wurden ohne Hilfe eines gespeicherten Programms programmiert, indem sie ihre Schaltkreise ändern oder Banken von physikalischen Kontrollen einstellen. Etwas später könnten Programme in Maschinensprache geschrieben werden, wo der Programmierer jede Anweisung in numerischer Form schreibt, kann die Hardware direkt ausführen. Beispielsweise kann die Anweisung, den Wert in zwei Speicherplätzen hinzuzufügen, aus 3 Zahlen bestehen: einem Opcode, der den Add-Betrieb wählt, und zwei Speicherplätze. Die Programme wurden in dezimaler oder binärer Form aus Stanzkarten, Papierband, Magnetband eingelesen oder auf Schaltern auf der Frontplatte des Computers eingeschaltet. Maschinensprachen wurden später als Programmiersprachen der ersten Generation (1GL) bezeichnet. Der nächste Schritt war die Entwicklung der sogenannten Programmiersprachen der zweiten Generation (2GL) oder Montagesprachen, die noch eng mit der Befehlssatzarchitektur des spezifischen Computers verbunden waren. Diese dienten dazu, das Programm viel menschlich lesbar zu machen und entlasteten den Programmierer von mühsamen und fehleranfälligen Adressberechnungen. Die ersten hochrangigen Programmiersprachen oder Programmiersprachen der dritten Generation (3GL) wurden in den 1950er Jahren geschrieben. Eine frühe hochrangige Programmiersprache für einen Computer war Plankalkül, die zwischen 1943 und 1945 für das deutsche Z3 von Konrad Zuse entwickelt wurde. Sie wurde jedoch erst 1998 und 2000 umgesetzt. John Mauchly's Short Code, der 1949 vorgeschlagen wurde, war eine der ersten hochrangigen Sprachen, die je für einen elektronischen Computer entwickelt wurden. Im Gegensatz zum Maschinencode stellten Short Code-Anweisungen mathematische Ausdrücke in verständlicher Form dar. Das Programm musste jedoch jedes Mal, wenn es lief, in den Maschinencode übersetzt werden, so dass der Prozess viel langsamer als der entsprechende Maschinencode läuft. An der Universität Manchester entwickelte Alick Glennie Anfang der 1950er Jahre Autocode. Als Programmiersprache benutzte es einen Compiler, um die Sprache automatisch in den Maschinencode umzuwandeln. Der erste Code und Compiler wurde 1952 für den Mark 1 Computer an der Universität Manchester entwickelt und gilt als die erste kompilierte hochrangige Programmiersprache. Der zweite Autocode wurde 1954 für den Mark 1 von R. A. Brooker entwickelt und als "Mark 1 Autocode" bezeichnet. Brooker entwickelte auch einen Autocode für die Ferranti Mercury in den 1950er Jahren in Verbindung mit der Universität Manchester. Die Version für das EDSAC 2 wurde von D. F. Hartley von University of Cambridge Mathematical Laboratory in 1961 entwickelt. Als EDSAC 2 Autocode bekannt, war es eine gerade Entwicklung von Mercury Autocode für lokale Gegebenheiten angepasst und wurde für seine Objektcode-Optimierung und Source-Sprach-Diagnostik, die für die Zeit fortgeschritten wurden, bemerkt. Ein zeitgemäßer, aber separater Entwicklungsfaden, Atlas Autocode wurde für die University of Manchester Atlas 1 Maschine entwickelt. 1954 wurde FORTRAN von John Backus bei IBM erfunden. Es war die erste weit verbreitete hochrangige allgemeine Programmiersprache, um eine funktionelle Umsetzung zu haben, im Gegensatz zu nur einem Design auf Papier. Es ist immer noch eine beliebte Sprache für High-Performance-Computing und wird für Programme verwendet, die Benchmark und die weltweit schnellsten Supercomputer. Eine weitere frühe Programmiersprache wurde von Grace Hopper in den USA entwickelt, genannt FLOW-MATIC. Es wurde im Zeitraum von 1955 bis 1959 für die UNIVAC I in Remington Rand entwickelt. Hopper fand heraus, dass Geschäftsdatenverarbeitung Kunden mit mathematischer Notation unangenehm waren, und Anfang 1955 schrieb sie und ihr Team eine Spezifikation für eine englische Programmiersprache und implementierte einen Prototyp. Der FLOW-MATIC-Compiler wurde Anfang 1958 öffentlich zugänglich und war 1959 im Wesentlichen vollständig. FLOW-MATIC war ein wichtiger Einfluss in der Gestaltung von COBOL, da nur es und seine direkte Nachkommen AIMACO zu der Zeit im eigentlichen Gebrauch waren. Raffinerie Der verstärkte Einsatz von hochrangigen Sprachen führte zu einem Bedarf an Programmiersprachen mit niedrigem Niveau oder Systemprogrammierungssprachen. Diese Sprachen, in unterschiedlichem Grad, bieten Einrichtungen zwischen Montagesprachen und hochrangigen Sprachen. Sie können verwendet werden, um Aufgaben auszuführen, die einen direkten Zugriff auf Hardwareanlagen erfordern, aber immer noch übergeordnete Kontrollstrukturen und Fehlerkontrollen bieten. Die Zeit von den 1960er- bis Ende der 1970er-Jahre brachte die Entwicklung der wichtigsten Sprachparadigmen jetzt im Einsatz: APL führte Array-Programmierung ein und beeinflusste die funktionelle Programmierung. ALGOL verfeinerte sowohl die strukturierte Verfahrensplanung als auch die Disziplin der Sprachspezifikation; der "Revised Report on the Algorithmic Language ALGOL 60" wurde ein Modell dafür, wie spätere Sprachspezifikationen geschrieben wurden. Lisp, 1958 implementiert, war die erste dynamisch eingegebene funktionelle Programmiersprache. In den 1960er Jahren war Simula die erste Sprache, die objektorientierte Programmierung unterstützte; Mitte der 1970er Jahre folgte Smalltalk mit der ersten rein objektorientierten Sprache. C wurde zwischen 1969 und 1973 als System-Programmiersprache für das Unix-Betriebssystem entwickelt und bleibt beliebt. Prolog, entworfen 1972, war die erste logische Programmiersprache. 1978 baute ML auf Lisp ein polymorphes Typsystem, das statisch gemusterte funktionelle Programmiersprachen vorstellte. Jede dieser Sprachen spawned Nachkommen, und die meisten modernen Programmiersprachen zählen mindestens eine von ihnen in ihrer Vorgeschichte. In den 1960er und 1970er Jahren wurde auch eine beträchtliche Debatte über die Verdienste der strukturierten Programmierung geführt, und ob Programmiersprachen dazu bestimmt sein sollten, diese zu unterstützen. Edsger Dijkstra, in einem berühmten 1968 in der Mitteilung der ACM veröffentlichten Brief, argumentierte, dass Goto-Anweisungen von allen "höheren" Programmiersprachen zu beseitigen. Konsolidierung und Wachstum Die 1980er Jahre waren die relativen Konsolidierungen. C+ kombiniert objektorientierte und systemprogrammierung. Die US-Regierung standardisierte Ada, eine von Pascal abgeleitete Programmiersprache, die von Verteidigungsunternehmern verwendet werden soll. In Japan und anderswo wurden große Summen ausgegeben, um die so genannten Sprachen der fünften Generation zu untersuchen, die Logik-Programmierungskonstrukte einschlossen. Die funktionelle Sprachen-Community bewegte sich, um ML und Lisp zu standardisieren. Anstatt neue Paradigmen zu erfinden, haben alle diese Bewegungen die in den vergangenen Jahrzehnten erfundenen Ideen erarbeitet. Ein wichtiger Trend in der Sprachgestaltung für die Programmierung von Großsystemen in den 1980er Jahren war ein verstärkter Fokus auf den Einsatz von Modulen oder großen Organisationseinheiten von Code. Modula-2, Ada und ML entwickelten alle bemerkenswerten Modulsysteme in den 1980er Jahren, die oft mit generischen Programmierkonstrukten verwebt wurden. Das rasante Wachstum des Internets Mitte der 1990er Jahre schaffte Möglichkeiten für neue Sprachen. Perl, ursprünglich ein 1987 erstmals veröffentlichtes Unix-Skripting-Tool, wurde in dynamischen Webseiten häufig. Java kam, um für die serverseitige Programmierung verwendet werden, und Bytecode virtuelle Maschinen wurden wieder beliebt in kommerziellen Einstellungen mit ihrem Versprechen von "Schreiben einmal, Laufen überall" (UCSD Pascal war schon in den frühen 1980er Jahren populär gewesen). Diese Entwicklungen waren nicht grundsätzlich neu, sondern sie waren Verfeinerung vieler existierender Sprachen und Paradigmen (obwohl ihre Syntax oft auf der C-Familie der Programmiersprachen basierte). Die Programmierung der Sprachentwicklung geht weiter, sowohl in der Industrie als auch in der Forschung. Aktuelle Richtungen umfassen Sicherheits- und Zuverlässigkeitsprüfung, neue Arten von Modularität (Mixine, Delegierte, Aspekte) und Datenbankintegration wie Microsofts LINQ. Programmiersprachen der Vierten Generation (4GL) sind Computerprogrammierungssprachen, die eine höhere Abstraktion der internen Computerhardwaredetails als 3GLs bieten sollen. Programmiersprachen der Fünften Generation (5GL) sind Programmiersprachen, die auf der Lösung von Problemen mit Einschränkungen des Programms basieren, anstatt einen von einem Programmierer geschriebenen Algorithmus zu verwenden. Elemente Alle Programmiersprachen haben einige primitive Bausteine für die Beschreibung von Daten und die an sie angewendeten Prozesse oder Transformationen (wie die Addition von zwei Zahlen oder die Auswahl eines Artikels aus einer Sammlung). Diese Primitiven werden durch syntaktische und semantische Regeln definiert, die ihre Struktur bzw. Bedeutung beschreiben. Syntax Die Oberflächenform einer Programmiersprache wird als Syntax bezeichnet. Die meisten Programmiersprachen sind rein textuell; sie verwenden Textsequenzen einschließlich Wörter, Zahlen und Pünktlichkeit, ähnlich wie geschriebene natürliche Sprachen. Auf der anderen Seite gibt es einige Programmiersprachen, die in der Natur grafischer sind, mit visuellen Beziehungen zwischen Symbolen, um ein Programm festzulegen. Die Syntax einer Sprache beschreibt die möglichen Kombinationen von Symbolen, die ein syntaktisch korrektes Programm bilden. Die Bedeutung einer Kombination von Symbolen wird durch Semantik (entweder formal oder schwer codiert in einer Referenz-Implementierung) behandelt. Da die meisten Sprachen textuell sind, spricht dieser Artikel über textuelle Syntax. Die Programmiersprache-Syntax wird in der Regel mit einer Kombination von regulären Ausdrücken (für lexische Struktur) und Backus-Naur-Form (für grammatische Struktur) definiert. Unten ist eine einfache Grammatik, basierend auf Lisp: Diese Grammatik gibt folgendes an: ein Ausdruck ist entweder ein Atom oder eine Liste; ein Atom ist entweder eine Zahl oder ein Symbol; eine Zahl ist eine ungebrochene Folge einer oder mehrerer Dezimalziffern, gegebenenfalls vor einem Plus- oder Minuszeichen; ein Symbol ist ein Buchstabe, gefolgt von Null oder mehreren Zeichen (ohne Weißraum); und eine Liste ist ein abgestimmtes Paar Klammern, mit Null oder mehr Ausdrücken darin. Beispiele für gut ausgebildete Tokensequenzen in dieser Grammatik: 12345, () und (a b c232 (1)). Nicht alle syntaktisch korrekten Programme sind semantisch korrekt. Viele syntaktisch korrekte Programme sind dennoch schlecht formuliert, nach den Regeln der Sprache; und kann (abhängig von der Sprachspezifikation und der Klanglichkeit der Implementierung) zu einem Fehler bei der Übersetzung oder Ausführung führen. In einigen Fällen können solche Programme undefiniertes Verhalten aufweisen. Auch wenn ein Programm in einer Sprache gut definiert ist, kann es noch eine Bedeutung haben, die nicht von der Person bestimmt ist, die es geschrieben hat. Mit der natürlichen Sprache als Beispiel kann es nicht möglich sein, eine Bedeutung einem grammatisch korrekten Satz zuzuordnen oder der Satz kann falsch sein: "Colorless Green Ideen schlafen furious". ist grammatisch gut ausgebildet, hat aber keine allgemein akzeptierte Bedeutung. "John ist ein verheirateter Junggesellen."ist grammatisch gut formuliert, drückt aber eine Bedeutung aus, die nicht wahr sein kann. Das folgende C-Sprachfragment ist syntaktisch korrekt, führt aber nicht semantisch definierte Operationen durch (die Operation *p >> 4 keine Bedeutung für einen Wert mit komplexem Typ und p->im ist nicht definiert, weil der Wert p der Nullzeiger ist: Würde die Typanmeldung auf der ersten Zeile entfallen, würde das Programm während der Zusammenstellung einen Fehler auf der undefinierten Größe p auslösen. Allerdings wäre das Programm noch syntaktisch korrekt, da Typenerklärungen nur semantische Informationen liefern. Die Grammatik, die zur Angabe einer Programmiersprache benötigt wird, kann durch ihre Position in der Chomsky-Hierarchie klassifiziert werden. Die Syntax der meisten Programmiersprachen kann mit einer Type-2 Grammatik angegeben werden, d.h. sie sind kontextfreie Grammatik. Einige Sprachen, einschließlich Perl und Lisp, enthalten Konstrukte, die die Ausführung während der Parsingphase ermöglichen. Sprachen, die Konstrukte haben, die es dem Programmierer ermöglichen, das Verhalten des Parsers zu verändern, machen die Syntaxanalyse zu einem unvorstellbaren Problem und verschwimmen im Allgemeinen die Unterscheidung zwischen Parsing und Ausführung. Im Gegensatz zu Lisp's Makrosystem und Perls BEGIN-Blöcken, die allgemeine Berechnungen enthalten können, sind C-Makros lediglich String-Ersatz und erfordern keine Codeausführung. Semantik Der Begriff Semantik bezieht sich auf die Bedeutung von Sprachen, im Gegensatz zu ihrer Form (Syntax). Statische Semantik Die statische Semantik definiert Einschränkungen der Struktur gültiger Texte, die schwer oder unmöglich sind, in Standard-Syntaktikformalitäten auszudrücken. Für kompilierte Sprachen umfassen statische Semantik im Wesentlichen jene semantischen Regeln, die zu kompilieren Zeit überprüft werden können. Beispiele sind die Überprüfung, dass jede Kennung vor ihrer Verwendung angegeben wird (in Sprachen, die solche Erklärungen erfordern), oder dass die Etiketten auf den Armen einer Fallerklärung deutlich sind. Viele wichtige Einschränkungen dieser Art, wie z.B. die Überprüfung, dass im entsprechenden Kontext (z.B. keine ganze Zahl zu einem Funktionsnamen addiert werden) oder dass unterroutine Anrufe die entsprechende Anzahl und Art von Argumenten aufweisen, können durch die Definition als Regeln in einer Logik, die ein Typsystem genannt wird, durchgesetzt werden. Andere Formen statischer Analysen wie Datenflussanalyse können auch Teil der statischen Semantik sein. Neue Programmiersprachen wie Java und C# haben im Rahmen ihrer statischen Semantik eine bestimmte Zuordnungsanalyse, eine Form der Datenflussanalyse. Dynamische Semantik Nach Angabe der Daten muss die Maschine angewiesen werden, um Operationen auf den Daten durchzuführen. Beispielsweise kann die Semantik die Strategie definieren, mit der Ausdrücke auf Werte ausgewertet werden, oder die Art, in der Steuerstrukturen bedingt Aussagen ausführen. Die dynamische Semantik (auch als Ausführungssemantik bezeichnet) einer Sprache definiert, wie und wann die verschiedenen Konstrukte einer Sprache ein Programmverhalten erzeugen sollen. Es gibt viele Möglichkeiten, die Ausführungssemantik zu definieren. Natürliche Sprache wird oft verwendet, um die Ausführung Semantik von Sprachen, die in der Praxis häufig verwendet werden. Ein beträchtlicher Teil der akademischen Forschung ging in die formale Semantik der Programmiersprachen ein, die es ermöglichen, die Ausführungsssemantik formal festzulegen. Ergebnisse aus diesem Bereich der Forschung haben eine begrenzte Anwendung auf die Programmierung Sprache Design und Implementierung außerhalb der Wissenschaft gesehen. Typ-System Ein Typ-System definiert, wie eine Programmiersprache Werte und Ausdrücke in Typen klassifiziert, wie es diese Typen manipulieren kann und wie sie interagieren. Ziel eines Typ-Systems ist es, ein bestimmtes Maß an Korrektheit in Programmen, die in dieser Sprache geschrieben werden, zu überprüfen und in der Regel durch Erkennung bestimmter fehlerhafter Operationen zu erzwingen. Jedes dezidierbare Typsystem beinhaltet einen Trade-off: Während es viele falsche Programme ablehnt, kann es auch einige richtige, wenn auch ungewöhnliche Programme verbieten. Um diesen Nachteil zu umgehen, haben eine Reihe von Sprachen Typ-Loopholes, in der Regel unkontrollierte Casts, die vom Programmierer verwendet werden können, um explizit einen normalerweise abgeschalteten Betrieb zwischen verschiedenen Typen zu ermöglichen. In den meisten eingegebenen Sprachen wird das Typ-System nur verwendet, um Check-Programme zu tippen, aber eine Reihe von Sprachen, in der Regel funktionale, minder Typen, entlasten den Programmierer von der Notwendigkeit, Typ-Annotationen zu schreiben. Die formale Gestaltung und Untersuchung von Typsystemen ist als Typtheorie bekannt.Typische versus untypische Sprachen Eine Sprache wird eingegeben, wenn die Spezifikation jeder Operation Typen von Daten definiert, auf die die Operation anwendbar ist. Die durch "diesen Text zwischen den Zitaten" dargestellten Daten sind beispielsweise ein String, und in vielen Programmiersprachen, die eine Zahl durch einen String teilen, hat keine Bedeutung und werden nicht ausgeführt. Die ungültige Operation kann erkannt werden, wenn das Programm kompiliert wird (statische Typprüfung) und vom Compiler mit einer Compilationsfehlernachricht zurückgewiesen wird, oder es kann während des laufenden (dynamische Typprüfung) erkannt werden, was eine Laufzeitausnahme zur Folge hat. Viele Sprachen erlauben eine Funktion, die als Ausnahme-Handler bezeichnet wird, um diese Ausnahme zu handhaben und beispielsweise immer -1 als Ergebnis zurückzugeben. Ein spezieller Fall von eingegebenen Sprachen sind die einzelnen Sprachen. Dies sind oft Skript- oder Markup-Sprachen, wie REXX oder SGML, und haben nur einen Datentyp–- Meistens Zeichenfolgen, die sowohl für symbolische als auch für numerische Daten verwendet werden. Eine untypische Sprache, wie die meisten Montagesprachen, erlaubt dagegen, jeden Betrieb auf beliebigen Daten durchzuführen, in der Regel Abläufe von Bits verschiedener Länge. Hochrangige untypische Sprachen sind BCPL, Tcl und einige Sorten von Forth. In der Praxis, während wenige Sprachen von der Typ-Theorie (verifizieren oder ablehnend alle Operationen,) die meisten modernen Sprachen bieten einen Grad der Typisierung. Viele Produktionssprachen bieten Mittel zur Umgehung oder Subvertierung des Typsystems, zur Handelstypsicherheit für eine feinere Kontrolle über die Ausführung des Programms (siehe Casting). Statische versus dynamische Typisierung Bei der statischen Eingabe haben alle Ausdrücke ihre Typen, die vor der Ausführung des Programms ermittelt werden, typischerweise bei der Compile-Zeit. Beispielsweise sind 1 und (2+2) ganze Ausdrücke; sie können nicht an eine Funktion weitergeleitet werden, die einen String erwartet, oder in einer Variable gespeichert werden, die definiert ist, um Daten zu halten. Statisch eingegebene Sprachen können entweder offensichtlich eingegeben oder typenbezogen werden. Im ersten Fall muss der Programmierer an bestimmten Textpositionen explizit Typen schreiben (z.B. an variablen Erklärungen). Im zweiten Fall verweist der Compiler auf die Art der Ausdrücke und Erklärungen auf der Grundlage des Kontexts. Die meisten statisch eingegebenen Mainstream-Sprachen, wie C,+ C# und Java, werden offensichtlich eingegeben. Die vollständige Typinferenz wurde traditionell mit weniger Mainstream-Sprachen, wie Haskell und ML, verbunden. Allerdings unterstützen viele offenkundig eingegebene Sprachen die Teiltypinferenz, z.B. C,+ Java und C# alle minderwertigen Typen in bestimmten begrenzten Fällen. Zusätzlich erlauben einige Programmiersprachen, dass einige Typen automatisch in andere Typen umgewandelt werden; zum Beispiel kann ein Int verwendet werden, wo das Programm einen Float erwartet. Dynamische Typisierung, auch latente Typisierung genannt, bestimmt die Typsicherheit von Operationen zu Laufzeit, d.h. Typen sind mit Laufzeitwerten und nicht mit Textausdrücken verknüpft. Wie bei art-inferred Sprachen benötigen dynamisch eingegebene Sprachen nicht, dass der Programmierer explizite Typannotationen auf Ausdrücken schreibt. Dies kann unter anderem eine einzelne Variable erlauben, auf Werte unterschiedlicher Typen an verschiedenen Punkten der Programmausführung zu verweisen. Typfehler können jedoch erst automatisch erkannt werden, wenn ein Stück Code tatsächlich ausgeführt wird, was möglicherweise das Debuggen erschwert. Lisp, Smalltalk, Perl, Python, JavaScript und Ruby sind alle Beispiele dynamisch eingegebener Sprachen. Schwache und starke Tipping Weak-Eingabe ermöglicht es, einen Wert eines Typs als einen anderen zu behandeln, beispielsweise einen String als Zahl zu behandeln. Dies kann gelegentlich nützlich sein, aber es kann auch einige Arten von Programmfehlern zu kompilieren Zeit und auch zu Laufzeit unentdeckt gehen. Starkes Tippen verhindert diese Programmfehler. Ein Versuch, eine Operation auf der falschen Art von Wert durchzuführen, erhöht einen Fehler. Stark eingegebene Sprachen werden oft als typsicher oder sicher bezeichnet. Eine alternative Definition für "weakly typed" bezieht sich auf Sprachen, wie Perl und JavaScript, die eine Vielzahl von impliziten Typkonvertierungen erlauben. In JavaScript z.B. konvertiert der Ausdruck 2 * x implizit x zu einer Zahl, und diese Umwandlung gelingt auch dann, wenn x null, undefiniert, ein Array oder eine Zeichenfolge von Buchstaben ist. Solche impliziten Konvertierungen sind oft nützlich, können aber Programmierfehler maskieren. Starke und statische Begriffe werden nun allgemein als orthogonale Konzepte betrachtet, aber die Verwendung in der Literatur unterscheidet sich. Einige verwenden den Begriff stark eingegeben, um stark, statisch eingegeben, oder, noch mehr verwirrend, einfach statisch eingegeben zu bedeuten. So wurde C sowohl stark eingegeben als auch schwach, statisch typisiert genannt. Es mag für einige professionelle Programmierer seltsam erscheinen, dass C "schwach, statisch" sein könnte. Beachten Sie jedoch, dass die Verwendung des gattungsgemäßen Zeigers, des Leerzeigers, das Gießen von Zeigern zu anderen Zeigern ermöglicht, ohne dass eine explizite Casting erforderlich ist. Dies ist sehr ähnlich, wie eine Reihe von Bytes auf jede Art von Datentyp in C ohne Verwendung einer expliziten Besetzung, wie (int) oder (char). Standard-Bibliothek und Laufzeitsystem Die meisten Programmiersprachen verfügen über eine zugehörige Kernbibliothek (manchmal als Standard-Bibliothek bekannt, insbesondere wenn sie im Rahmen des veröffentlichten Sprachstandards enthalten ist), die von allen Implementierungen der Sprache konventionell zur Verfügung gestellt wird. Kernbibliotheken umfassen typischerweise Definitionen für häufig verwendete Algorithmen, Datenstrukturen und Mechanismen für Eingabe und Ausgabe. Die Linie zwischen Sprache und Kernbibliothek unterscheidet sich von Sprache zu Sprache. In einigen Fällen können die Sprachdesigner die Bibliothek als eigenständige Einheit aus der Sprache behandeln. Die Kernbibliothek einer Sprache wird jedoch von den Benutzern oft als Teil der Sprache behandelt, und einige Sprachspezifikationen erfordern sogar, dass diese Bibliothek in allen Implementierungen zur Verfügung gestellt wird. In der Tat sind einige Sprachen so konzipiert, dass die Bedeutung bestimmter syntaktischer Konstrukte nicht einmal beschrieben werden kann, ohne sich auf die Kernbibliothek zu beziehen. Beispielsweise wird in Java ein String-Literal als Instanz der java.lang definiert. String-Klasse; ähnlich konstruiert in Smalltalk ein anonymer Funktionsausdruck (ein Block) eine Instanz der BlockContext-Klasse der Bibliothek. Umgekehrt enthält Scheme mehrere zusammenhängende Teilmengen, die ausreichen, um den Rest der Sprache als Bibliotheksmakros zu konstruieren, und so stören die Sprachdesigner nicht einmal, welche Teile der Sprache als Sprachkonstrukte umgesetzt werden müssen und die als Teile einer Bibliothek ausgeführt werden müssen. Design und Implementierung Programmiersprachen teilen Eigenschaften mit natürlichen Sprachen, die mit ihrem Zweck als Fahrzeuge für die Kommunikation verbunden sind, mit einer syntaktischen Form getrennt von ihrer Semantik und zeigen Sprachfamilien in verwandten Sprachen, die sich gegenseitig verzweigen. Als künstliche Konstrukte unterscheiden sie sich aber auch grundlegend von Sprachen, die sich durch Nutzung entwickelt haben. Ein wesentlicher Unterschied ist, dass eine Programmiersprache vollständig beschrieben und studiert werden kann, da sie eine präzise und endliche Definition aufweist. Im Gegensatz dazu haben natürliche Sprachen unterschiedliche Bedeutungen, die ihre Nutzer in verschiedenen Gemeinschaften haben. Obwohl erbaute Sprachen auch künstliche Sprachen sind, die von Grund auf mit einem bestimmten Zweck entworfen wurden, fehlt ihnen die genaue und vollständige semantische Definition, die eine Programmiersprache hat. Viele Programmiersprachen wurden von Grund auf entwickelt, auf neue Bedürfnisse geändert und mit anderen Sprachen kombiniert. Viele sind schließlich in Verwirrung geraten. Obwohl es Versuche gab, eine universelle Programmiersprache zu entwerfen, die allen Zwecken diente, sind alle nicht allgemein als Füllung dieser Rolle akzeptiert worden. Die Notwendigkeit unterschiedlicher Programmiersprachen ergibt sich aus der Vielfalt der Kontexte, in denen Sprachen verwendet werden: Die Programme reichen von winzigen Skripten, die von einzelnen Hobbyisten geschrieben wurden, bis hin zu riesigen Systemen, die von Hunderten von Programmierern geschrieben wurden. Programmierer reichen von Anfängern, die vor allem Einfachheit benötigen, bis hin zu Experten, die sich mit großer Komplexität wohlfühlen können. Programme müssen Geschwindigkeit, Größe und Einfachheit auf Systemen von Mikrocontrollern bis Supercomputern ausgleichen. Programme können einmal geschrieben werden und sich nicht für Generationen ändern, oder sie können sich ständig ändern. Programmierer können sich einfach in ihrem Geschmack unterscheiden: Sie können daran gewöhnt sein, Probleme zu diskutieren und in einer bestimmten Sprache auszudrücken. Ein gemeinsamer Trend in der Entwicklung von Programmiersprachen war, mehr Fähigkeit, Probleme mit einem höheren Grad der Abstraktion zu lösen. Die frühesten Programmiersprachen wurden sehr eng an die zugrunde liegende Hardware des Computers gebunden. Da neue Programmiersprachen entwickelt wurden, wurden Features hinzugefügt, die Programmierer Ideen ausdrücken lassen, die von der einfachen Übersetzung in zugrunde liegende Hardware-Anweisungen entfernt sind. Da Programmierer weniger an die Komplexität des Computers gebunden sind, können ihre Programme mehr Computing mit weniger Aufwand vom Programmierer. Damit können sie mehr Funktionalität pro Zeiteinheit schreiben. Natürliche Sprachprogrammierung wurde vorgeschlagen, die Notwendigkeit einer spezialisierten Sprache für die Programmierung zu beseitigen. Dieses Ziel bleibt jedoch weit entfernt und seine Vorteile sind offen für Diskussionen. Edsger W. Dijkstra nahm die Position, dass die Verwendung einer formalen Sprache wesentlich ist, um die Einführung von sinnlosen Konstrukte zu verhindern und die natürliche Sprachprogrammierung als töricht zu entlassen". Alan Perlis war ähnlich entlassen von der Idee. Hybride Ansätze wurden in Structured English und SQL aufgenommen.Die Designer und Benutzer der Sprache müssen eine Reihe von Artefakte konstruieren, die die Praxis der Programmierung steuern und ermöglichen. Die wichtigsten dieser Artefakte sind die Sprachspezifikation und Umsetzung. Spezifikation Die Spezifikation einer Programmiersprache ist ein Artefakt, mit dem die Sprachanwender und die Implementierungsmitglieder einverstanden sind, ob ein Quellcode ein gültiges Programm in dieser Sprache ist, und wenn ja, was sein Verhalten sein soll. Eine Programmiersprache-Spezifikation kann mehrere Formen annehmen, einschließlich der folgenden: Eine explizite Definition von Syntax, statischer Semantik und Ausführungsssemantik der Sprache. Während Syntax häufig mit einer formalen Grammatik angegeben wird, können semantische Definitionen in natürlicher Sprache (z.B. wie in der C-Sprache) oder in einer formalen Semantik (z.B. in Standard-ML und Scheme-Spezifikationen) geschrieben werden. Beschreibung des Verhaltens eines Übersetzers für die Sprache (z.B. C+ und Fortran-Spezifikationen). Die Syntax und die Semantik der Sprache müssen aus dieser Beschreibung abgeleitet werden, die in natürlicher oder formaler Sprache geschrieben werden kann. Eine Referenz oder Modell-Implementierung, die manchmal in der angegebenen Sprache geschrieben wird (z.B. Prolog oder ANSI REXX). Die Syntax und Semantik der Sprache sind im Verhalten der Referenz-Implementierung explizit. Durchführung Eine Implementierung einer Programmiersprache bietet eine Möglichkeit, Programme in dieser Sprache zu schreiben und sie auf einer oder mehreren Konfigurationen von Hardware und Software auszuführen. Es gibt im Wesentlichen zwei Ansätze zur Programmierung der Sprachumsetzung: Compilation und Interpretation. In der Regel ist es möglich, eine Sprache mit einer Technik zu implementieren. Die Ausgabe eines Compilers kann durch Hardware oder ein Programm, das als Dolmetscher bezeichnet wird, ausgeführt werden. In einigen Implementierungen, die den Dolmetscheransatz nutzen, gibt es keine eindeutige Grenze zwischen Kompilieren und Dolmetschen. Zum Beispiel einige Implementierungen des BASIC-Compiles und führen dann die Quelle zu einem Zeitpunkt aus. Programme, die direkt auf der Hardware ausgeführt werden, laufen in der Regel viel schneller als diejenigen, die in der Software interpretiert werden. Eine Technik zur Verbesserung der Leistungsfähigkeit der interpretierten Programme ist die Just-in-time-Compilation. Hier übersetzt die virtuelle Maschine, kurz vor der Ausführung, die Blöcke von Bytecode, die verwendet werden, um Code zu bearbeiten, für die direkte Ausführung auf der Hardware. Amtssprachen Obwohl die meisten der am häufigsten verwendeten Programmiersprachen vollständig offene Spezifikationen und Implementierungen haben, existieren viele Programmiersprachen nur als proprietäre Programmiersprachen mit der nur von einem einzigen Anbieter verfügbaren Implementierung, die behaupten kann, dass eine solche proprietäre Sprache ihr geistiges Eigentum ist. Proprietäre Programmiersprachen sind allgemein Domain-spezifische Sprachen oder interne Skriptsprachen für ein einzelnes Produkt; einige proprietäre Sprachen werden nur intern innerhalb eines Anbieters verwendet, während andere externen Benutzern zur Verfügung stehen. Einige Programmiersprachen existieren an der Grenze zwischen proprietär und offen; zum Beispiel behauptet Oracle Corporation proprietäre Rechte auf einige Aspekte der Java Programmiersprache, und Microsofts C# Programmiersprache, die offene Implementierungen der meisten Teile des Systems hat, hat auch Common Language Runtime (CLR) als geschlossene Umgebung. Viele proprietäre Sprachen werden trotz ihrer Eigenart weit verbreitet; Beispiele sind MATLAB, VBScript und Wolfram Language. Einige Sprachen können den Übergang von geschlossen zu öffnen, zum Beispiel, Erlang war ursprünglich eine Ericsson interne Programmiersprache. Benutzen Sie Tausende von verschiedenen Programmiersprachen, vor allem im Rechenfeld. Individuelle Softwareprojekte verwenden üblicherweise fünf Programmiersprachen oder mehr. Programmiersprachen unterscheiden sich von den meisten anderen Formen des menschlichen Ausdrucks, indem sie eine größere Präzision und Vollständigkeit erfordern. Wenn Sie eine natürliche Sprache verwenden, um mit anderen Menschen zu kommunizieren, können menschliche Autoren und Lautsprecher mehrdeutig sein und kleine Fehler machen, und immer noch erwarten, dass ihre Absicht verstanden werden. Jedoch, figurativ gesprochen, Computer " tun genau, was sie zu tun," und kann nicht verstehen, welchen Code der Programmierer zu schreiben beabsichtigt. Die Kombination der Sprachdefinition, eines Programms und der Eingaben des Programms muss das externe Verhalten, das bei der Ausführung des Programms auftritt, innerhalb der Domäne der Steuerung dieses Programms vollständig festlegen. Andererseits können dem Menschen Ideen über einen Algorithmus ohne die für die Ausführung erforderliche Präzision unter Verwendung von Pseudocode mitgeteilt werden, die natürliche Sprache mit in einer Programmiersprache geschriebenem Code interleaves. Eine Programmiersprache bietet einen strukturierten Mechanismus zur Definition von Datenstücken und die Operationen oder Transformationen, die auf diesen Daten automatisch durchgeführt werden können. Ein Programmierer nutzt die in der Sprache vorhandenen Abstraktionen, um die an einer Berechnung beteiligten Konzepte darzustellen. Diese Konzepte sind als Sammlung der einfachsten verfügbaren Elemente (genannt Primitiven) dargestellt. Die Programmierung ist der Prozess, mit dem Programmierer diese Primitiven kombinieren, um neue Programme zu erstellen oder bestehende an neue Anwendungen oder eine sich ändernde Umgebung anzupassen. Programme für einen Computer können in einem Batch-Prozess ohne menschliche Interaktion ausgeführt werden, oder ein Benutzer kann Befehle in einer interaktiven Sitzung eines Dolmetschers eingeben. In diesem Fall sind die Befehle einfach Programme, deren Ausführung zusammengekettet ist. Wenn eine Sprache ihre Befehle über einen Dolmetscher (z.B. eine Unix-Shell oder eine andere Befehlszeilen-Schnittstelle) ausführen kann, wird sie als Skriptsprache bezeichnet. Die Bestimmung der am weitesten verbreiteten Programmiersprache ist schwierig, da die Definition der Nutzung durch Kontext variiert. Eine Sprache kann die größere Anzahl von Programmierstunden besetzen, eine andere hat mehr Zeilen von Code, und ein Drittel kann die meisten CPU-Zeit verbrauchen. Einige Sprachen sind für bestimmte Anwendungen sehr beliebt. Zum Beispiel, COBOL ist noch stark im Corporate Data Center, oft auf großen Mainframes; Fortran in wissenschaftlichen und technischen Anwendungen; Ada in Luft- und Raumfahrt, Transport, Militär, Echtzeit- und Embedded-Anwendungen; und C in Embedded-Anwendungen und Betriebssystemen. Andere Sprachen werden regelmäßig verwendet, um viele verschiedene Arten von Anwendungen zu schreiben. Es wurden verschiedene Methoden zur Messung der Sprachbeliebtheit vorgeschlagen, die jeweils einer unterschiedlichen Voreingenommenheit gegenüber dem, was gemessen wird, unterliegen: Zählen der Anzahl der Stellenanzeigen, die die Sprache angeben, die die Anzahl der verkauften Bücher, die die Sprachschätzungen der Anzahl der in der Sprache geschriebenen Codezeilen lehren oder beschreiben, die Sprachen unterschätzen können, die in öffentlichen Recherchen nicht häufig vorkommen, Zählungen von Sprachbezügen (d also auf den Namen der Sprache). Stackify.com berichtete über die zehn beliebtesten Programmiersprachen (in absteigender Reihenfolge durch die allgemeine Popularität): Java, C, C,+ Python, C,# JavaScript, VB .NET, R, PHP und MATLAB. Dialects, Aromen und Implementierungen Ein Dialekt einer Programmiersprache oder einer Datenaustauschsprache ist eine (relativ kleine) Variation oder Erweiterung der Sprache, die ihre Eigenart nicht ändert. Mit Sprachen wie Scheme und Forth können Standards als unzureichend, unzureichend oder unrechtmäßig von den implementierenden angesehen werden, so oft werden sie von der Norm abweichen, einen neuen Dialekt zu machen. In anderen Fällen wird ein Dialekt für die Verwendung in einer Domänensprache, oft einer Submenge, erstellt. In der Lisp-Welt werden die meisten Sprachen, die grundlegende S-Expressions-Syntax und Lisp-ähnliche Semantik verwenden, als Lisp-Dialekte betrachtet, obwohl sie wild variieren, wie sagen, Racket und Clojure. Da es für eine Sprache üblich ist, mehrere Dialekte zu haben, kann es für einen unerfahrenen Programmierer schwierig werden, die richtige Dokumentation zu finden. Die BASIC Programmiersprache hat viele Dialekte. Die Explosion der Forth-Dialekte führte zum Sprichwort: "Wenn Sie einen Forth gesehen haben. Sie haben einen Forth gesehen." Steuerbefreiungen Es gibt kein übergeordnetes Klassifizierungssystem für Programmiersprachen. Eine bestimmte Programmiersprache hat in der Regel keine einzige Vorläufersprache. Sprachen entstehen häufig durch die Kombination der Elemente mehrerer Vorgängersprachen mit neuen Ideen in der Zirkulation zur Zeit. Ideen, die in einer Sprache stammen, werden in einer Familie mit verwandten Sprachen diffundieren und dann plötzlich über familiäre Lücken springen, um in einer ganz anderen Familie zu erscheinen. Die Aufgabe wird weiter dadurch erschwert, dass Sprachen entlang mehrerer Achsen klassifiziert werden können. Zum Beispiel ist Java sowohl eine objektorientierte Sprache (denn sie fördert objektorientierte Organisation) als auch eine gleichzeitige Sprache (denn sie enthält eingebaute Konstrukte zum parallelen Führen mehrerer Threads). Python ist eine objektorientierte Skriptsprache. In breiten Hüben gliedern sich Programmiersprachen in Programmierparadigmen und eine Klassifikation durch bestimmungsgemäße Verwendungsdomäne, wobei allgemeine Programmiersprachen von domainspezifischen Programmiersprachen unterschieden sind. Traditionell wurden Programmiersprachen als Beschreibung der Berechnung in Bezug auf zwingende Sätze betrachtet, d.h. Ausgabebefehle. Diese werden im allgemeinen als zwingende Programmiersprachen bezeichnet. Viele Forschungsarbeiten in Programmiersprachen sind darauf ausgerichtet, die Unterscheidung zwischen einem Programm als eine Reihe von Anweisungen und einem Programm als Behauptung über die gewünschte Antwort zu verschärfen, was das Hauptmerkmal der deklarativen Programmierung ist. Weitere verfeinerte Paradigmen umfassen Prozessprogrammierung, objektorientierte Programmierung, funktionelle Programmierung und logische Programmierung; einige Sprachen sind Hybriden von Paradigmen oder multiparadigmatisch. Eine Montagesprache ist nicht so sehr ein Paradigma als direktes Modell einer zugrunde liegenden Maschinenarchitektur. Die Programmiersprachen können dabei als allgemeiner Zweck betrachtet werden, als System-Programmiersprachen, Skriptsprachen, Domänen-spezifische Sprachen oder gleichzeitige/verteilte Sprachen (oder eine Kombination davon). Einige allgemeine Zielsprachen wurden weitgehend mit Bildungszielen konzipiert. Eine Programmiersprache kann auch durch Faktoren klassifiziert werden, die nicht mit dem Programmierparadigma zusammenhängen. Zum Beispiel, die meisten Programmiersprachen verwenden englische Sprache Schlüsselwörter, während eine Minderheit nicht. Andere Sprachen können als absichtlich esoterisch eingestuft werden oder nicht. Siehe auch Referenzen Weiter lesen = Externe Links ==