In Mathematik und Computer-Algebra ist die automatische Differenzierung (AD,) auch algorithmische Differenzierung, rechnerische Differenzierung, Autodifferenzierung oder einfach Autodiff genannt, eine Reihe von Techniken, um das Derivat einer von einem Computerprogramm spezifizierten Funktion zu bewerten. AD nutzt die Tatsache, dass jedes Computerprogramm, egal wie kompliziert, eine Sequenz von elementaren arithmetischen Operationen (Addition, Subtraktion, Multiplikation, Division, etc.) und elementaren Funktionen (exp, log, sin, cos, etc.) durchführt. Durch wiederholte Anwendung der Kettenregel auf diese Vorgänge können Derivate beliebiger Ordnung automatisch, genau auf die Arbeitsgenauigkeit berechnet und mit höchstens einem kleinen konstanten Faktor mehr Rechenoperationen als das ursprüngliche Programm verwendet werden. Automatische Differenzierung unterscheidet sich von symbolischer Differenzierung und numerischer Differenzierung (die Methode der endlichen Unterschiede). Symbolische Differenzierung kann zu ineffizientem Code führen und der Schwierigkeit, ein Computerprogramm in einen einzigen Ausdruck umzuwandeln, während numerische Differenzierung Rund-off-Fehler in den Diskretisierungsvorgang und die Löschung einleiten kann. Beide klassischen Methoden haben Probleme mit der Berechnung höherer Derivate, wo Komplexität und Fehler zunehmen. Schließlich sind beide klassischen Methoden bei der Berechnung von Teilderivaten einer Funktion gegenüber vielen Eingängen langsam, wie es für Gradienten-basierte Optimierungsalgorithmen erforderlich ist. Automatische Differenzierung löst alle diese Probleme. Die Kettenregel, Vorwärts- und Rückakkumulation Fundamental to AD ist die Zersetzung von Differenzen, die durch die Kettenregel bereitgestellt werden. Für die einfache Zusammensetzung gibt die Kettenregel Üblicherweise werden zwei verschiedene Moden von AD dargestellt, Vorwärtsakkumulation (oder Vorwärtsmodus) und Rückakkumulation (oder Rückwärtsmodus). Vorwärtsakkumulation gibt an, daß man die Kettenregel von innen nach außen traversiert (d.h. zuerst compute d w 1 / d x {\displaystyle dw_{1}/dx und dann d w 2 / d w 1 {\displaystyle dw_{2}/dw_{1 und schließlich d y / d w 2} Auffälliger berechnet die Vorwärtsakkumulation die rekursive Beziehung: d w i d x = d w i d w i - 1 d w i - 1 d x {\displaystyle {\frac dw_{i}{dx}={\frac Dw_{i} dw_{i-1}{dx mit w 3 = y {\displaystyle w_{3}=y, und umgekehrte Akkumulation berechnet die rekursive Beziehung: d y d w i = d y d w i 1 d w i + 1 d w i {\displaystyle} dy{dw_{i}}={\frac dy{dw_{i+1}{\frac dw_{i+1}{dw_{i mit w 0 = x {\displaystyle w_{0}=x . Vorwärtsakkumulation Bei der Vorakkumulation AD fixiert man zunächst die unabhängige Größe, bezüglich welcher Differenzierung durchgeführt wird und berechnet die Ableitung jeder Teilexpression rekursiv. Bei einer Pen-and-Papier-Berechnung bedeutet dies wiederholt die Substitution der Derivate der inneren Funktionen in der Kettenregel: Dies kann als Matrixprodukt von Jacobians auf mehrere Variablen verallgemeinert werden. Im Vergleich zur umgekehrten Akkumulation ist die Vorwärtsakkumulation natürlich und einfach zu implementieren, da der Fluss der abgeleiteten Informationen mit der Reihenfolge der Auswertung zusammenfällt. Jede Variable w wird mit ihrem Derivat ẇ (gespeichert als Zahlenwert, nicht als symbolischer Ausdruck), wie durch den Punkt bezeichnet. Die Derivate werden dann sync mit den Auswerteschritten berechnet und über die Kettenregel mit anderen Derivaten kombiniert. Betrachten Sie als Beispiel die Funktion: Zur Übersichtlichkeit sind die einzelnen Subexpressionen mit den Variablen wi gekennzeichnet. Die Wahl der unabhängigen Variable, auf die Differenzierung durchgeführt wird, betrifft die Samenwerte ẇ1 und ẇ2. Bei Interesse an der Ableitung dieser Funktion gegenüber x1 sollten die Saatgutwerte auf folgende Werte festgelegt werden: Mit den eingestellten Saatgutwerten propagieren die Werte wie gezeigt mit der Kettenregel. Abbildung 2 zeigt eine bildliche Darstellung dieses Verfahrens als Rechendiagramm. Um den Gradienten dieser Beispielfunktion zu berechnen, der die Derivate von f in Bezug auf nicht nur x1, sondern auch x2 benötigt, wird über das Rechendiagramm ein zusätzlicher Sweep mit den Saatwerten w ̇ 1 = 0 durchgeführt; w ̇ 2 = 1 {\displaystyle {\dot w}_{1}=0;{\dot w}_{2}=1 .Die Rechenkomplexität eines Sweeps der Vorwärtsakkumulation ist proportional zur Komplexität des Originalcodes. Die Anhäufung nach vorne ist effizienter als die umgekehrte Anhäufung der Funktionen f : Rn → Rm mit m  » n als nur n Sweeps sind notwendig, verglichen mit m Sweeps für die umgekehrte Akkumulation. Reverse Akkumulierung Bei der umgekehrten Akkumulation AD wird die zu differenzierende abhängige Größe fixiert und das Derivat gegenüber jeder Subexpression rekursiv berechnet. Bei einer Pen-and-Papier-Berechnung wird das Derivat der äußeren Funktionen wiederholt in der Kettenregel substituiert: Bei umgekehrter Akkumulation ist die interessierende Menge die mit einem Balken (wï) bezeichnete Grenze; sie ist eine Ableitung einer gewählten abhängigen Größe in Bezug auf eine Subexpression w: Die umgekehrte Akkumulation durchläuft die Kettenregel von außen nach innen oder im Falle des Rechendiagramms in Abbildung 3 von oben nach unten. Die Beispielfunktion ist skalarbewertet und somit gibt es nur einen Samen für die Derivatberechnung, und zur Berechnung des (zwei-Komponenten-)Gradienten wird nur ein Sweep des Rechendiagramms benötigt. Dies ist nur die Hälfte der Arbeit im Vergleich zur Vorwärtsakkumulation, erfordert aber die Rückakkumulation die Speicherung der Zwischenvariablen wi sowie die Anweisungen, die sie in einer Datenstruktur, die als Wengert-Liste (oder Band) bekannt, erzeugt, die erhebliche Speicher verbrauchen kann, wenn der Rechengraph groß ist. Dies kann teilweise dadurch gemildert werden, daß nur eine Teilmenge der Zwischenvariablen gespeichert und dann die erforderlichen Arbeitsvariablen durch Wiederholung der Auswertungen rekonstruiert werden, eine als Rematerialisierung bekannte Technik. Der Checkpointing wird auch verwendet, um Zwischenzustände zu speichern. Die Operationen zur Berechnung des Derivates mit umgekehrter Akkumulation sind in der folgenden Tabelle dargestellt (Anmerkung der umgekehrten Reihenfolge): Zur Berechnung des Gradienten seiner ursprünglichen Berechnung kann das Datenflussdiagramm einer Berechnung manipuliert werden. Dies geschieht, indem für jeden Primalknoten ein benachbarter Knoten addiert wird, der durch benachbarte Kanten verbunden ist, die die Primalkanten parallel, aber in entgegengesetzter Richtung strömen. Die Knoten im Nebengraphen stellen eine Multiplikation durch die von den Knoten im Primal berechneten Derivate der Funktionen dar. So bewirkt beispielsweise die Hinzufügung im Primal einen Fanout im Nebennächt; das Fanout im Primal bewirkt eine Addition im Nebennächt; eine unary Funktion y = f (x) im Primal bewirkt im Nebennächt xï = ȳ f'(x) usw. Die umgekehrte Akkumulation ist effizienter als die Vorwärtsakkumulation der Funktionen f : Rn → Rm mit m ‡ n als nur m Sweeps sind notwendig, verglichen mit n Sweeps für die Vorwärtsakkumulation. Reverse-Modus AD wurde erstmals 1976 von Seppo Linnainmaa veröffentlicht. Backpropagation von Fehlern in Mehrschicht-Perceptrons, eine Technik beim maschinellen Lernen, ist ein besonderer Fall des Umkehrmodus AD. Jenseits von Vorwärts- und Rückakkumulation Vorwärts- und Rückwärtsakkumulation sind nur zwei (extreme) Wege, die Kettenregel zu durchlaufen. Das Problem der Berechnung eines vollen Jacobian von f : Rn → Rm mit einer minimalen Anzahl von arithmetischen Operationen wird als das optimale Jacobian Akkumulationsproblem (OJA) bekannt, das NP-komplete ist. Zentral zu diesem Beweis ist die Idee, dass algebraische Abhängigkeiten zwischen den lokalen Teilstücken bestehen können, die die Kanten des Diagramms kennzeichnen. Insbesondere können zwei oder mehr Kantenetiketten als gleich erkannt werden. Die Komplexität des Problems ist noch offen, wenn angenommen wird, dass alle Kantenetiketten einzigartig und algebraisch unabhängig sind. Automatische Differenzierung mit doppelten Zahlen Vorwärts-Modus automatische Differenzierung wird durch die Erweiterung der Algebra von realen Zahlen und ein neues arithmetic erreicht. Zu jeder Zahl wird eine zusätzliche Komponente hinzugefügt, um die Ableitung einer Funktion an der Zahl darzustellen, und alle arithmetischen Operatoren werden für die augmentierten Algebra erweitert. Die augmentierte Algebra ist die Algebra mit doppelten Zahlen. Ersetzen Sie jede Zahl x {\displaystyle ,\x} mit der Zahl x + x' ε {\displaystyle x+x'\varepsilon }, wobei x' {\displaystyle x'} eine echte Zahl ist, aber ε {\displaystyle \varepsilon } eine abstrakte Zahl mit der Eigenschaft ε 2 = 0=0\displaystyle \varepsilon {^2} ist. Mit nur diesem, regelmäßig arithmetic gibt und ebenfalls für Subtraktion und Division. Nun können Polynome in dieser augmentierten arithmetischen berechnet werden. Wenn P ( x ) = p 0 + p 1 x + p 2 x 2 + ⋯ + p n x n {\displaystyle P(x)=p_{0}+p_{1}x+p_{2}x{2}+\cdots +p_{n}x{n , dann, wo P (1 ) {\displaystyle P^{(1}) das Derivat von P {\displaystyle P} in Bezug auf sein erstes Argument bedeutet, und x' {\displaystyle x'}, ein Saatgut genannt, kann beliebig gewählt werden. Die neue Arithmetik besteht aus geordneten Paaren, Elementen geschrieben   x , x' ・ {\displaystyle \langle x,x'\rangle }, mit gewöhnlicher Arithmetik auf der ersten Komponente und erster Ordnungsdifferenzierung arithmetic auf der zweiten Komponente, wie oben beschrieben. Die oben genannten Ergebnisse auf Polynomen zu analytischen Funktionen zu erweitern gibt eine Liste der Grundarithmetik und einige Standardfunktionen für die neue Arithmetik: und im Allgemeinen für die Primitive-Funktion g {\displaystyle g}, wobei g u {\displaystyle g_{u} und g v\displaystyle g_{v} die Derivate von g {\displaystyle first g} in Bezug auf seine Argumente sind. Wenn eine binäre grundlegende arithmetische Operation auf gemischte Argumente angewendet wird - das Paar ⟨ u, u' ・ {\displaystyle \langle u,u'\rangle } und die reale Zahl c {\displaystyle c} - die reale Zahl wird zuerst auf ⟩ c gehoben, 0 ・ {\displaystyle \langle c,0\rangle } . Die Ableitung einer Funktion f : R → R {\displaystyle f:\mathbb {R} to\mathbb {R} am Punkt x 0 {\displaystyle x_{0} wird nun durch Berechnung f (⟨ x 0 , 1 ・) {\displaystyle f(\langle x_{0},1\rangle\)} mit dem obigen arithmetic gefunden, was ⟨ f ( x 0 ) , f' ( x 0 ・) ergibt Vektor-Argumente und Funktionen Multivariate Funktionen können mit der gleichen Effizienz und Mechanismen wie univariate Funktionen durch die Annahme eines Richtungs-Derivate-Operators behandelt werden. Das heißt, wenn es ausreicht, y' = MENT f ( x ) ⋅ x' {\displaystyle y'=\nabla f(x)\cdot x'} zu berechnen, das Richtungsderivat y' ε : R n → R m {\displaystyle f:\mathbb {R} {\cHFF} {R} {R}} {\cHFF} {\cHFF}} {\cHFF} {\cHFF} {\cHFF} {\cHFF}} {\cHFF}} {\cHFF}} {\cHFF}} {\cH}} {\c {\cH}}}}} {\cH}}}}}}}}}}}}} {\c}}}}}}}}}}}}}}}}}\cH}}}}}}}}}}}}\c\c\c\c\cH{\cH}\cH}}}}}}}}}}}}}}}}}\c\c\cH}\c\cH}\c\cH}\c\cH{\c\c\cH}}\cH}}}}}}}}}}}}}}}}}}}}}}}\c\cH{\c (R} {^n} in der Richtung x' ε R n {\displaystyle x'\in \mathbb {R} {^n}, dies kann berechnet werden als ( y y 1 , y 1' ・, ... Wenn alle Elemente von MENT f {\displaystyle \nabla f} gewünscht werden, werden n {\displaystyle n} Funktionsauswertungen benötigt. Beachten Sie, dass in vielen Optimierungsanwendungen das Richtderivat tatsächlich ausreichend ist. Hohe Ordnung und viele Variablen Die obige Arithmetik kann zur Berechnung der zweiten Ordnung und der höheren Derivate von Multivariatfunktionen verallgemeinert werden. Die arithmetischen Regeln wachsen jedoch schnell aufwändig: Die Komplexität ist im höchsten Derivatgrad quadratisch. Stattdessen können kegelstumpfförmige Taylor-Polynomalgebra verwendet werden. Die resultierende arithmetische, definiert auf verallgemeinerten Dual-Nummern, ermöglicht eine effiziente Berechnung mit Funktionen, als wären sie ein Datentyp. Sobald das Taylor-Polynom einer Funktion bekannt ist, werden die Derivate leicht extrahiert. Implementierung Forward-mode AD wird durch eine nicht standardmäßige Interpretation des Programms implementiert, in dem reale Zahlen durch Dual-Nummern ersetzt werden, Konstanten mit einem Null-Epsilon-Koeffizienten auf Dual-Nummern angehoben und die numerischen Primitiven auf Dual-Nummern angehoben werden. Diese nicht standardmäßige Interpretation wird in der Regel mit einer von zwei Strategien implementiert: Quellcode-Transformation oder Bediener-Überlastung. Quellcode-Transformation (SCT)Der Quellcode für eine Funktion wird durch einen automatisch generierten Quellcode ersetzt, der Aussagen zur Berechnung der mit den ursprünglichen Anweisungen verknüpften Derivate enthält. Die Quellcodetransformation kann für alle Programmiersprachen implementiert werden, und es ist auch einfacher, dass der Compiler Zeitoptimierungen durchführt.Die Implementierung des AD-Tools selbst ist jedoch schwieriger. Betreiber Überlastung (OO) Betreiber Überlastung ist eine Möglichkeit für Quellcode geschrieben in einer Sprache, die es unterstützt. Objekte für reale Zahlen und elementare mathematische Operationen müssen überlastet werden, um die oben abgebildete augmented arithmetic zu erfüllen. Dies erfordert keine Änderung der Form oder Reihenfolge der Operationen im ursprünglichen Quellcode für die zu differenzierende Funktion, erfordert aber oft Änderungen der Grunddatentypen für Zahlen und Vektoren zur Unterstützung der Überlastung und oft auch die Einführung spezieller Flagging-Operationen. Die Bedienungsüberlastung für die Vorwärtsakkumulation ist einfach zu implementieren und auch für die Rückakkumulation möglich. Allerdings liegen die aktuellen Compiler bei der Optimierung des Codes im Vergleich zur Weiterakkumulation zurück. Die Bedienungsüberlastung kann sowohl für die Vorwärts- als auch für die Rückwärtsakkumulation gut für Anwendungen geeignet sein, bei denen es sich bei den Objekten um Vektoren von realen Zahlen anstatt um Skalare handelt. Dies liegt daran, dass das Band dann Vektoroperationen umfasst; dies kann rechnerisch effiziente Implementierungen erleichtern, bei denen jede Vektoroperation viele Skalaroperationen durchführt. Vector angrenzendt algorithmische Differenzierung (Vector AAD) Techniken können beispielsweise auf durch Monte-Carlo-Simulation berechnete Differenzwerte verwendet werden. Beispiele für operatorüberladende Implementierungen der automatischen Differenzierung in C+ sind die Adept- und Stan-Bibliotheken. Siehe auch differenzierbare Programmierhinweise Referenzen Weiter lesen Rall, Louis B. (1981). Automatische Differenzierung: Techniken und Anwendungen. Vortragshinweise in Informatik.120.Springer.ISBN 978-3-540-10861-0.Griewank, Andreas; Walther, Andrea (2008). Bewertung von Derivaten: Grundsätze und Techniken der Algorithmischen Differenzierung. Andere Titel in Angewandter Mathematik.105 (2. ed.). SIAM.ISBN 978-0-89871-659-7.Archiv aus dem Original auf 2010-03-23.Retrieved 2009-10-21.Neidinger, Richard (2010)." Einführung in die automatische Differenzierung und MATLAB Objektorientierte Programmierung" (PDF). SIAM Review.52 (3): 545–563.CiteSeerX 10.1.1.362.6580.doi:10.1137/080743627.Retrieved 2013-03-15.Naumann, Uwe (2012). Die Kunst der Differenzierung von Computerprogrammen.Software-Environments-tools.SIAM.ISBN 978-1-611972-06-1.Henrard, Marc (2017). Algorithmische Differenzierung in Finanzen Erklärt. Finanztechnik Erklärt. Palgrave Macmillan.ISBN 978-3-319-53978-2. Externe Links www.autodiff.org, Automatische Differenzierung von Parallel-OpenMP-Programmen Automatische Differenzierung, C+-Vorlagen und Photogrammetrie Automatische Differenzierung, Operator Overloading Approach Berechnen analytische Derivate jeder Fortran77, Fortran95 oder C-Programm durch eine webbasierte Schnittstelle Automatische Differenzierung von Fortran-Programmen Beschreibung und Beispielcode für die vorwärts automatische Differenzierung in Scala finmach. Adjoint Algorithmische Differenzierung:Calibration and Implicit Function Theorem C+ Template-basierte automatische Differenzierung Artikel und Implementierung Tangent Source Debuggable Derivatives [1,] Exact First- und Second-Order Greeks by Algorithmic Differenziation [2,] Adjoint Algorithmische Differenzierung einer GPU beschleunigten Anwendung [3,] Fügen Sie Methoden in Computational Finance Software Tool Support für Algorithmische Differenzierungop