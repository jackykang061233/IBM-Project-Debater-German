Datenreinigung oder Datenreinigung ist der Prozess der Aufdeckung und Berichtigung (oder Beseitigung) korrupter oder ungenauer Aufzeichnungen aus einer Aufzeichnung, einem Tisch oder einer Datenbank und bezieht sich auf unvollständige, falsche, unzumutbare oder irrelevante Teile der Daten und ersetzen, ändern oder löschen die verschmutzten oder groben Daten. Datenreinigungen können interaktiv mit Datenschrottwerkzeugen oder als Chargenverarbeitung durch Drehbuchierung durchgeführt werden. Nach der Reinigung sollte ein Datensatz mit anderen ähnlichen Datensets im System übereinstimmen. Die festgestellten oder entfernten Unstimmigkeiten wurden ursprünglich durch Nutzereintrittsfehler, durch Korruption in Übertragung oder Lagerung oder durch unterschiedliche Definitionen ähnlicher Daten in verschiedenen Geschäften verursacht. Datenreinigung unterscheidet sich von der Datenvalidierung insofern, als die Validierung fast unvariabel bedeutet, dass Daten vom System zum Zeitpunkt des Eintritts abgelehnt werden und nicht bei Datensendungen durchgeführt werden. Der tatsächliche Prozess der Datenreinigung kann dazu führen, dass grafische Fehler oder gültige und korrekte Werte gegen eine bekannte Liste von Unternehmen beseitigt werden. Die Validierung kann streng sein (z.B. die Ablehnung einer Anschrift, die keinen gültigen Postcode enthält), oder mit fuzzyer oder ungefährer Abstimmung (z.B. Berichtigung von Aufzeichnungen, die teilweise auf bestehende, bekannte Aufzeichnungen abgestimmt sind). Manche Datenreinigungslösungen werden die Daten durch Überprüfung mit einem validierten Datensatz reinigen. Eine gemeinsame Datenreinigungspraxis ist die Datenverbesserung, bei der Daten durch zusätzliche Informationen vollständiger gemacht werden. Zum Beispiel werden Adressen mit allen Telefonnummern im Zusammenhang mit dieser Adresse geprüft. Datenreinigungen können auch eine Harmonisierung (oder Normalisierung) von Daten beinhalten, die den Prozess der Zusammenführung von Daten von "Verfassungsformaten, Benennungsvereinbarungen und Spalten" und die Umwandlung in ein einheitliches Datenpaket umfasst; ein einfaches Beispiel ist die Ausweitung der Abkürzungen ("st, rd, usw.") auf "Straße, Straße usw."). Motivation falsch, uneinheitliche Daten können zu falschen Schlussfolgerungen und irreführenden Investitionen sowohl auf öffentlicher als auch auf privater Ebene führen. Beispielsweise kann die Regierung die Bevölkerungszählungszahlen analysieren, um zu entscheiden, welche Regionen weitere Ausgaben und Investitionen in Infrastruktur und Dienstleistungen benötigen. In diesem Fall ist es wichtig, Zugang zu zuverlässigen Daten zu haben, um falsche Haushaltsentscheidungen zu vermeiden. In der Geschäftswelt können falsche Daten kosten. Viele Unternehmen nutzen Kundeninformationsdatenbanken, die Daten wie Kontaktinformationen, Adressen und Präferenzen erfassen. Wenn die Adressen nicht kohärent sind, wird das Unternehmen die Kosten für die Weiterverkauf oder sogar den Verlust von Kunden tragen. Datenqualität Hochwertige Daten müssen eine Reihe von Qualitätskriterien erfüllen. Dazu gehören: Gültigkeit: Grad, an dem die Maßnahmen den festgelegten Geschäftsregeln oder -beschränkungen entsprechen (siehe auch Gültigkeit (statistics). Wenn moderne Datenbank-Technologie zur Gestaltung von Daten-Capture-Systemen verwendet wird, ist die Gültigkeit ziemlich einfach zu gewährleisten: Ungültige Daten entstehen vor allem in Erb Kontexten (wo die Beschränkungen nicht in der Software umgesetzt wurden) oder in denen unangemessene Daten-Capture-Technologie verwendet wurde (z.B. Verbreitungsblätter, wo es sehr schwer ist, zu begrenzen, was ein Nutzer für den Eintritt in eine Zelle entscheidet, wenn die Zellerkennung nicht verwendet wird). Datenbeschränkungen fallen in folgende Kategorien: Daten-Typ Constraints – z.B. die Werte in einer bestimmten Spalte müssen von einer bestimmten Datenart sein, z.B. bei Roglean, numerisch (integer oder real), Datum usw. Reichweiten: In der Regel sollten Zahlen oder Daten in einem bestimmten Bereich fallen. Dies sind Mindestwerte und/oder zulässige Höchstwerte. obligatorische Constraints: Bestimmte Spalten können nicht leer sein. Klare Constraints: Ein Bereich oder eine Kombination von Feldern muss über einen Datensatz einzigartig sein. Beispielsweise können keine zwei Personen die gleiche Sozialversicherungsnummer haben. Haftungsbeschränkungen: Die Werte für eine Spalte stammen aus einer Reihe von Einzelwerten oder Codes. Zum Beispiel kann ein Geschlecht der Person Frauen oder Männer sein. Sachzwänge aus dem Ausland: Dies ist der allgemeinere Fall der Mitgliedschaft. Die Werte in einer Spalte werden in einer Spalte einer anderen Tabelle definiert, die einzigartige Werte enthält. In einer US-Steuerzahlerdatenbank ist beispielsweise die staatliche Spalte verpflichtet, einem der von den USA definierten Staaten oder Gebiete zu gehören: die Anzahl der zulässigen Staaten/Territorien ist in einem gesonderten Tabelle aufgeführt. Der Begriff des ausländischen Schlüssels wird aus der Begriffsbestimmung für die Datenbank aufgenommen. Verhaltensmuster: Gelegentlich müssen Textfelder auf diese Weise validiert werden. Telefonnummern können beispielsweise erforderlich sein, um das Muster (999) 999-9999 zu haben. Validierung von Cross-field: Bestimmte Bedingungen, die mehrere Felder verwenden, müssen bestehen. In der Labormedizin muss die Summe der Komponenten der differenzierten weißen Blutkörperchen 100 betragen (da sie alle Prozent sind). In einer Krankenhausdatenbank kann der Zeitpunkt der Ableitung aus dem Krankenhaus nicht früher als das Datum der Zulassung sein. Genauigkeit: Der Grad der Konformität einer Maßnahme an einen Standard oder einen echten Wert - siehe auch Genauigkeit und Präzision. Genauigkeit ist sehr schwierig, durch Datenreinigung im Allgemeinen zu erreichen, weil sie den Zugriff auf eine externe Datenquelle, die den wahren Wert enthält, erfordert: solche „goldenenen Standard“-Daten sind häufig nicht verfügbar. Genauigkeit wurde in einigen Reinigungsumgebungen, insbesondere Kundenkontaktdaten, erreicht, indem externe Datenbanken genutzt werden, die sich an geografische Orte (Stadt und Staat) anlehnen, und auch dazu beitragen, zu überprüfen, ob Straßen innerhalb dieser Reißcodes tatsächlich vorhanden sind. Vollständigkeit: Grad, an dem alle erforderlichen Maßnahmen bekannt sind. Unvollständigkeit ist fast unmöglich, mit Datenreinigungsmethodik zu versehen: Eine kann Fakten, die nicht erfasst wurden, wenn die betreffenden Daten ursprünglich registriert wurden, nicht stören. (In einigen Kontexten, z.B. Interviewdaten, kann es möglich sein, unvollständig zu bestimmen, indem es auf die ursprüngliche Datenquelle zurückgeht, d. h. die Reinterview dieses Themas, aber auch dies gewährleistet nicht den Erfolg aufgrund von Rückrufproblemen - z.B. in einem Interview, um Daten über den Lebensmittelverbrauch zu sammeln, ist nicht zu erwarten, genau das, was vor sechs Monaten zu sehen ist. Im Falle von Systemen, die bestimmte Spalten beharren, darf man sich nicht mit dem Problem befassen, indem sie einen Wert ausweisen, der unbekannt oder fehlt, aber die Lieferung von Standardwerten bedeutet nicht, dass die Daten vollständig gemacht wurden.) Kohärenz: Grad, an dem eine Reihe von Maßnahmen in allen Systemen gleichwertig sind (siehe auch Kohärenz). Inkohärenz kommt zu dem Schluss, dass zwei Daten in den Datensätzen einander widersprechen: z.B. ein Kunde ist in zwei verschiedenen Systemen registriert, da zwei verschiedene aktuelle Adressen vorliegen und nur eine von ihnen korrekt sein kann. Mangelnde Unstimmigkeit ist nicht immer möglich: Es erfordert eine Vielzahl von Strategien - z.B. die Entscheidung, welche Daten vor kurzem registriert wurden, welche Datenquelle wahrscheinlich am zuverlässigsten ist (das letztere Wissen kann sich auf eine bestimmte Organisation beziehen), oder versucht einfach, die Wahrheit durch Prüfung beider Daten (z.B. Anrufung des Kunden). Einheitlichkeit: Der Grad, an dem eine bestimmte Datenmaßnahme mit denselben Maßeinheiten in allen Systemen angegeben wird (siehe auch Maßeinheit). In Datensätzen, die aus verschiedenen Gemeinden zusammengelegt werden, kann das Gewicht entweder in Pfund oder Kilo aufgezeichnet werden und muss in eine einzige Maßnahme mit einem arithmetischen Wandel umgewandelt werden. Die Begriffsintegrität umfasst Genauigkeit, Kohärenz und einige Aspekte der Validierung (siehe auch Datenintegrität), wird jedoch selten von sich selbst in datenreinigen Kontexten genutzt, weil sie unzureichend spezifisch ist.(z.B. „Referential Integrität“ ist ein Begriff, um die Durchsetzung der oben genannten Beschränkungen an der Außenseite zu verweisen. Prozessdatenprüfung: Die Daten werden mit der Verwendung statistischer und Datenbankmethoden zur Erkennung von Unregelmäßigkeiten und Widersprüchen geprüft: Dies zeigt schließlich die Merkmale der Anomalien und ihrer Standorte. Mehrere kommerzielle Software-Pakete werden Ihnen die Zwänge verschiedener Arten (mit einer Sprachkenntnisse, die mit der Standardprogrammsprache, z.B. JavaScript oder visueller Basis) in Einklang stehen) angeben und anschließend einen Code schaffen, der die Daten zur Verletzung dieser Zwänge überprüft. Dieser Prozess wird unter dem Titel "Arbeitsflussspezifikation" und "Durchführung des Arbeitsflusses" erwähnt. " Nutzer, die keinen Zugang zu hochend Reinigungssoftware haben, können auch Mikrocomputer-Datenbankpakete wie Microsoft Access oder File Maker Pro solche Prüfungen auf einer beschränkten Basis durchführen, interaktiv mit wenig oder gar keiner in vielen Fällen erforderlichen Programmierung. Arbeitsbeschreibung: Nachweis und Entfernung von Anomalien werden durch eine Reihe von Operationen auf den als Arbeitsablauf bekannten Daten durchgeführt. Nach der Prüfung der Daten ist es von entscheidender Bedeutung, das Endprodukt hochwertiger Daten zu erreichen. Um einen ordnungsgemäßen Arbeitsablauf zu erreichen, müssen die Ursachen der Unregelmäßigkeiten und Fehler in den Daten eng berücksichtigt werden. Ausführung: In dieser Phase wird der Arbeitsablauf nach der vollständigen Spezifikation ausgeführt und seine Richtigkeit überprüft. Die Ausführung des Arbeitsablaufs sollte effizient sein, selbst bei großen Datenmengen, die unweigerlich einen Abstieg darstellen, weil die Ausführung eines datenreinigen Betriebs rechnerisch teuer sein kann. Post-Verarbeitung und Kontrolle: Nach der Ausführung des Reinigungsvorgangs werden die Ergebnisse überprüft, um die Richtigkeit zu überprüfen. Daten, die während der Ausführung des Arbeitsablaufs nicht korrigiert werden könnten, werden gegebenenfalls manuell berichtigt. Ergebnis ist ein neuer Zyklus im Datenreinigungsprozess, in dem die Daten erneut geprüft werden, um die Spezifikation eines zusätzlichen Arbeitsablaufs zu ermöglichen, um die Daten durch automatische Verarbeitung weiter zu reinigen. Gute Qualitätsquelledaten müssen mit „Datenqualität Kultur“ betrieben werden und müssen an der Spitze der Organisation gestartet werden. Es ist nicht nur eine Frage der Durchführung strenger Validierungskontrollen an den Eingangsbildschirmen, weil fast nichts darüber ist, wie stark diese Kontrollen sind, sie können oft von den Nutzern umgangen werden. Es gibt einen neunstufigen Leitfaden für Organisationen, die die Datenqualität verbessern möchten: Erklärung eines hochrangigen Engagements für eine Datenqualitätskultur-Fortschrittsrenovierung auf der Ebene der Ausgaben zur Verbesserung der Dateneinspeiseausgaben, um die Ausgaben für Integrationsausgaben zu verbessern, um zu verändern, wie Prozesse die Sensibilisierung der End-to-End-Teams für die interdepartale Zusammenarbeit Sensibilisierung der Öffentlichkeit für Exzellenz der Datenqualität fördern und die Datenqualität kontinuierlich verbessern Andere umfassen: Parsing: zur Erkennung von Rechenfehlern. parser entscheidet, ob eine Reihe von Daten innerhalb der erlaubten Datenspezifikation akzeptabel ist. Dies ist ähnlich wie die Art und Weise, wie eine with mit den Sprachen und den Sprachen funktioniert. Datentransformation: Datentransformation ermöglicht die Kartierung der Daten aus ihrem gegebenen Format in das Format, das von der entsprechenden Anwendung erwartet wird. Dies umfasst Wertumrechnungen oder Übersetzungsfunktionen sowie die Normalisierung der numerischen Werte, die den Mindest- und Höchstwerten entsprechen. Dekomplizierende Beseitigung: Du komplizierte Erkennung erfordert einen Algorithmus, um festzustellen, ob Daten doppelte Darstellungen derselben Einheit enthalten. In der Regel werden Daten durch einen Schlüssel geordnet, der doppelte Einträge für eine schnellere Identifizierung einander näher bringen würde. Statistische Methoden: Indem man die Daten anhand der Werte der mittleren, Standardabweichung, Bandbreite oder Cluster-Algorithmen analysiert, ist es möglich, dass ein Experte Werte ermittelt, die unerwartet und damit schädlich sind. Obwohl die Korrektur dieser Daten schwierig ist, da der wahre Wert nicht bekannt ist, kann sie durch die Festlegung der Werte zu einem durchschnittlichen oder anderen statistischen Wert gelöst werden. statistische Methoden können auch verwendet werden, um fehlende Werte zu behandeln, die durch ein oder mehrere plausible Werte ersetzt werden können, die normalerweise durch umfangreiche Datenverstärkungsgorithmen erzielt werden. System Die wesentliche Aufgabe dieses Systems ist es, ein angemessenes Gleichgewicht zwischen der Festsetzung verschmutzter Daten und der Aufrechterhaltung der Daten möglichst nah an den ursprünglichen Daten des Produktionssystems zu finden. Dies ist eine Herausforderung für den Extrakt, den Umbau, den Lastarchitekten. Das System sollte eine Architektur bieten, die Daten, Rekordqualitätsveranstaltungen und die Qualität der Daten im Datenlager reinigen kann. Ein guter Start ist die Durchführung einer gründlichen Datenprofilanalyse, die dazu beitragen wird, die erforderliche Komplexität des Datenreinigungssystems zu definieren und auch eine Vorstellung von der aktuellen Datenqualität im System(en) zu geben. Qualitätsbildschirme Teil des Datenreinigungssystems ist eine Reihe von Diagnosefiltern, die als Qualitätsbildschirme bekannt sind. Jeder führt einen Test im Datenfluss durch, der, wenn er nicht, einen Fehler in der Fehler-Veranstaltungsregelung enthält. Qualitätsbildschirme sind in drei Kategorien unterteilt: Spaltenbildschirme. Prüfen Sie die einzelne Spalte, z.B. für unerwartete Werte wie NULL-Werte, nicht-numerische Werte, die numerisch sein sollten, aus verschiedenen Werten usw. Strukturbildschirme. Diese werden verwendet, um die Integrität verschiedener Beziehungen zwischen den Spalten (normalerweise ausländische/primäre Schlüssel) in denselben oder verschiedenen Tabellen zu testen. Sie werden auch für die Prüfung verwendet, dass eine Gruppe von Spalten nach einer gewissen strukturellen Definition gültig ist, auf die sie sich verpflichten sollte. Geschäftsregelbildschirme. Komplex der drei Tests. Sie testen, um zu sehen, ob Daten, vielleicht über mehrere Tabellen, spezifischen Geschäftsregeln entsprechen. Ein Beispiel könnte sein, dass, wenn ein Kunde als eine bestimmte Art von Kunden gekennzeichnet ist, die Geschäftsregeln, die diese Art von Kunden definieren, eingehalten werden sollten. Wenn ein Qualitätsbildschirm einen Fehler aufweist, kann es entweder den Datenfluss stoppen, die fehlerhaften Daten irgendwo an andere übermitteln als das Zielsystem oder die Daten. Letztere Option wird als beste Lösung angesehen, weil die erste Option erforderlich ist, dass jemand die Angelegenheit jedes Mal manuell bearbeiten muss, und die zweite bedeutet, dass Daten aus dem Zielsystem (Integrität) fehlen und es oft unklar ist, was für diese Daten geschehen sollte. Kritik an vorhandenen Werkzeugen und Prozessen Die meisten Datenreinigungsinstrumente haben Grenzen in der Benutzerfreundlichkeit: Projektkosten: Kosten in der Regel in den hunderttausenden Dollar-Zeit: Die Beherrschung groß angelegter Datenreinigungssoftware ist zeitaufwändige Sicherheit: Quervalidierung erfordert Informationsaustausch, Zugang zu allen Systemen, einschließlich sensibler Altsysteme Fehler-Veranstaltungsschema Die Fehler-Veranstaltungsschema enthält Aufzeichnungen aller Fehlerereignisse, die durch die Qualitätskontrollen verursacht werden. Es besteht aus einer Fehler-Fact-Tabelle mit ausländischen Schlüsseln zu drei Größentabellen, die Datum (wenn), Chargenarbeit (wobei) und Bildschirm (der Fehler verursachte). Sie hat auch Informationen über genau den Zeitpunkt, an dem der Fehler aufgetreten ist, und die Schwere des Fehlers. Außerdem gibt es einen Fehler-Veranstaltungskatalog mit einem ausländischen Schlüssel zum wichtigsten Tisch, der ausführliche Informationen darüber enthält, in welcher Tabelle, Aufzeichnungen und Feld der Fehler aufgetreten sind. Siehe auch Datenverarbeitung Data Mining Iterative proportionale Dokumentation Linkage Single Kunde view Triangulation (Sozialwissenschaft) Links Quelles Han, J. Kamber, M. Data Mining: Konzepte und Techniken, Morgan Kaufmann, 2001.ISBN 1-55860-489-8. Kimball, R. Caserta, J. Data Warehouse ETL-Tool, Haas und Sons, 2004.ISBN 4:645-6757-8.Muller H. Freytag J. Probleme, Methoden und Herausforderungen in den Bereichen umfassende Datenreinigung, Humboldt-Universitat zu Berlin, Deutschland, 2003. Rahm, E. Hongkong, H. Datenreinigung: Probleme und aktuelle Konzepte, Universität Leipzig, Deutschland, 2000. Außenbeziehungen Computerworld: Datenerhebung (Februar 10, 2003) Erhard Rahm, Hong HaiDo: Daten Reinigung: Probleme und aktuelle Konzepte Trinidad Orisha, auch bekannt als Shango, ist eine Syncretic Religion in Trinidad und Tobago und ist der Karibik-Ursprung, ursprünglich aus Westafrika (Yoruba Religion). Trinidad Orisha umfasst Elemente des Geistes Baptismus, und die Nähe zwischen Orisha und dem Geiste Baptismus hat dazu geführt, dass der Begriff "Shango Baptist" verwendet wird, um sich an Mitglieder von oder beiden Religionen zu wenden. Anthropologen James Houk bezeichnete Trinidad Orisha als "Afro-amerikanisches religiöses Komplex", das Elemente vor allem der traditionellen afrikanischen Religion und Yoruba umfasst und einige Elemente des Christentums (Cathicism andernism), des Hinduismus, des Islam (vor allem Sufismus), des Glaubens, des Judentums, der Bahaí und der Trinidad Kabbalah umfasst. Tanzmusik Trinidad Orisha Praxis beinhaltet Call-and-response, begleitet von einem Trio von Trommeln. Orisha-Fälle sind Doppelkopfzylinder, die aus Yoruba-Bembe-Fälern (ähnlich der kubanischen Iyesá-Fälle) abgeleitet werden. Die Trommel, die am niedrigsten ist, ist das Rind oder ein Kongo. Die Federfälschung wird als "zentrales Trommel" bezeichnet. Die kleinsten Trommeln, die am besten in der Schale, sind umele. Die ersten beiden Trommeln werden mit einer einzigen Aufkleber plus Hand kombiniert, während die Umele mit einem Paar von Aufklebern gespielt wird. Endes sind alle Zwänge gestrichen, und das Personal oder die Kiesen. Die Sprache der Lieder wurde als "Trinidad Yoruba" bezeichnet und stammt aus der Sprache Yoruba. Luxemburg Links