Arealdichte ist ein Maß für die Menge an Informationsbits, die auf einer bestimmten Länge von Spur, Fläche oder in einem bestimmten Volumen eines Computerspeichermediums gespeichert werden können. Generell ist eine höhere Dichte wünschenswert, da sie mehr Daten im gleichen physikalischen Raum speichern lässt. Dichte hat daher eine direkte Beziehung zur Speicherkapazität eines bestimmten Mediums. Dichte wirkt sich auch in der Regel auf die Leistung innerhalb eines bestimmten Mediums sowie Preis. Speichergeräteklassen Solid State MediaSolid State Laufwerke verwenden Flash-Speicher, um nichtflüchtige Medien zu speichern. Sie sind die neueste Form von Massenproduktion Speicher und rivalisieren magnetische Plattenmedien. Solid State Media Daten werden in einem Pool von NAND Flash gespeichert. NAND selbst besteht aus sogenannten Floating-Gate-Transistoren. Im Gegensatz zu den in DRAM verwendeten Transistorausführungen, die mehrmals pro Sekunde erfrischt werden müssen, ist NAND-Blitz dazu ausgelegt, seinen Ladezustand auch bei nicht aufgeschaltetem Zustand zu halten. Die höchsten im Handel erhältlichen Kapazitäten sind die Nimbus Data Exadrive© DC-Serien-Laufwerke, die in Kapazitäten von 16TB bis 100TB kommen. Nimbus sagt, dass die 100TB SSD für ihre Größe ein 6:1-platzsparendes Verhältnis über eine nahegelegene HDD-Magnetplattenmedien aufweist Festplattenlaufwerke speichern Daten in der magnetischen Polarisation von kleinen Patches der Oberflächenbeschichtung auf einer Scheibe. Die maximale Flächendichte wird durch die Größe der magnetischen Partikel in der Oberfläche sowie die Größe des zum Lesen und Schreiben der Daten verwendeten Kopfes definiert. 1956 hatte die erste Festplatte, die IBM 350, eine Flächendichte von 2.000 Bit/in2. Seitdem hat die Dichtesteigerung Moore's Law angepasst und erreichte 2014 1 Tbit/in2. Im Jahr 2015 führte Seagate eine Festplatte mit einer Dichte von 1,34 Tbit/in2, mehr als 600 Millionen Mal die der IBM 350. Es wird erwartet, dass die aktuelle Aufzeichnungstechnologie in naher Zukunft auf mindestens 5 Tbit/in2 messbar ist. Neue Technologien wie hitzegestützte magnetische Aufzeichnung (HAMR) und mikrowellengestützte magnetische Aufzeichnung (MAMR) werden entwickelt und sollen eine Erhöhung der magnetischen Flächendichte ermöglichen. Optische Scheibenmedien Optische Scheiben speichern Daten in kleinen Gruben in einer Kunststoffoberfläche, die dann mit einer dünnen Schicht aus reflektierendem Metall bedeckt wird. Kompakte Scheiben (CDs) bieten eine Dichte von ca. 0,90 Gbit/in2, mit Gruben, die 0,83 Mikrometer lang und 0,5 Mikrometer breit sind, in Spuren von 1,6 Mikrometern auseinander angeordnet. DVD-Disks sind im Wesentlichen eine höherdichte CD, mit mehr der Plattenoberfläche, kleinere Boxen (0,64 Mikrometer) und engere Tracks (0,74 Mikrometer), die eine Dichte von etwa 2,2 Gbit/in2 bieten. Einschichtige HD-DVD und Blu-ray-Disks bieten Dichten um 7,5 Gbit/in2 bzw. 12,5 Gbit/in2. Bei der Einführung in 1982 hatten CDs deutlich höhere Dichten als Festplattenlaufwerke, aber Festplattenlaufwerke haben seither viel schneller und verfinsterte optische Medien sowohl in der Flächendichte als auch in der Kapazität pro Gerät. Magnetbänder Der erste Magnetbandantrieb, der Univac Uniservo, aufgenommen mit einer Dichte von 128 Bit/in auf einem halbinch Magnetband, wodurch die Flächendichte von 256 Bit/in2 resultiert. Im Jahr 2015 behaupteten IBM und Fujifilm einen neuen Rekord für die magnetische Bandarealdichte von 1,23 Gbit/in2, während LTO-6, die höchste Dichte Produktionsbandversand im Jahr 2015, eine Flächendichte von 0,84 Gbit/in2 liefert. Forschung Eine Reihe von Technologien versucht, die Dichten bestehender Medien zu übertreffen. IBM zielt darauf ab, ihr Millipede Speichersystem auf 1 Tbit/in2 im Jahr 2007 zu vermarkten, aber die Entwicklung scheint moribund zu sein. Eine neuere IBM-Technologie, Racetrack-Speicher, verwendet eine Reihe von vielen kleinen nanoskopischen Drähten in 3D angeordnet, jede hält zahlreiche Bits zur Verbesserung der Dichte. Obwohl genaue Zahlen nicht erwähnt wurden, wird IBM News Artikel von "100 mal" erhöht. Holographische Speichertechnologien versuchen auch, bestehende Systeme zu leapfrog, aber auch sie haben das Rennen verloren, und werden geschätzt auch 1 Tbit/in2 bieten, mit etwa 250 GB/in2 ist das bisher beste für Nicht-Quantenholographie-Systeme. Andere experimentelle Technologien bieten noch höhere Dichten. Zur Lagerung von 10 Tbit/in2 wurde eine molekulare Polymerspeicherung gezeigt. Bei weitem ist die bisher meiste Art der Speicherspeicherung elektronische Quantenholographie. Durch die Überlagerung von Bildern unterschiedlicher Wellenlänge in das gleiche Hologramm erreichte 2009 ein Stanford-Forschungsteam eine Bitdichte von 35 Bit/Elektron (ca. 3 Exabytes/in2) mit Elektronenmikroskopen und einem Kupfermedium. Im Jahr 2012 wurde DNA erfolgreich als experimentelles Datenspeichermedium verwendet, erforderte jedoch einen DNA-Synthesizer und DNA-Mikrochips für die Transcodierung.Seit 2012 hält DNA den Rekord für hochdichtes Speichermedium. Im März 2017 veröffentlichten Wissenschaftler der Columbia University und des New York Genome Centers eine Methode namens DNA-Brunnen, die eine perfekte Abrufung von Informationen aus einer Dichte von 215 Petabyte pro Gramm DNA erlaubt, 85 % der theoretischen Grenze. Effekte auf die Leistung Mit der bemerkenswerten Ausnahme von NAND Flash-Speicher verbessert die zunehmende Speicherdichte eines Mediums typischerweise die Übertragungsgeschwindigkeit, mit der dieses Medium arbeiten kann. Dies ist am offensichtlichsten, wenn man verschiedene scheibenbasierte Medien betrachtet, bei denen die Speicherelemente über die Oberfläche der Scheibe verteilt sind und unter dem Kopf physisch gedreht werden müssen, um gelesen oder geschrieben zu werden. Höhere Dichte bedeutet mehr Daten bewegt sich unter dem Kopf für eine bestimmte mechanische Bewegung. Beispielsweise können wir die effektive Übertragungsgeschwindigkeit für eine Diskette berechnen, indem wir festlegen, wie schnell sich die Bits unter dem Kopf bewegen. Eine standardmäßige 31⁄2-Zoll-Flopscheibe dreht bei 300 U/min und die innerste Spur ca. 66 mm lang (10,5 mm Radius). Bei 300 U/min beträgt die Lineargeschwindigkeit der Medien unter dem Kopf also etwa 66 mm × 300 U/min = 19800mm/Minute oder 330 mm/s. Entlang dieser Bahn werden die Bits mit einer Dichte von 686 Bit/mm gespeichert, was bedeutet, dass der Kopf 686 Bit/mm × 330 mm/s = 226,380 Bit/s (oder 28,3 KB/s) sieht. Betrachten Sie nun eine Verbesserung des Designs, das die Dichte der Bits verdoppelt, indem die Probenlänge reduziert und den gleichen Spurabstand beibehalten wird. Dies würde die Übertragungsgeschwindigkeit verdoppeln, da die Bits zweimal so schnell unter dem Kopf passieren würden. Frühe Diskettenschnittstellen wurden für 250 kbit/s Transfergeschwindigkeiten konzipiert, wurden aber mit der Einführung der "high density" 1.44 MB (1.440 KB) Disketten in den 1980er Jahren rasch übertroffen. Die große Mehrheit der PCs enthalten Schnittstellen für High-Density-Laufwerke, die mit 500 kbit/s lief statt. Auch diese wurden von neueren Geräten wie dem LS-120 völlig überwältigt, die dazu gezwungen waren, höhere Geschwindigkeitsschnittstellen wie IDE zu verwenden. Obwohl die Wirkung auf die Leistung auf rotierende Medien am offensichtlichsten ist, kommen ähnliche Effekte auch für Festkörpermedien wie Flash RAM oder DRAM ins Spiel. In diesem Fall wird die Leistung in der Regel durch die Zeit definiert, in der die elektrischen Signale durch den Computerbus zu den Chips gelangen, und dann durch die Chips zu den einzelnen Zellen, die zur Speicherung von Daten verwendet werden (jede Zelle hält ein Bit). Eine definierende elektrische Eigenschaft ist der Widerstand der Drähte innerhalb der Chips. Da die Zellgröße abnimmt, wird durch die Verbesserung der Halbleiterfertigung, die zu Moore's Gesetz führte, der Widerstand reduziert und weniger Strom benötigt, um die Zellen zu betreiben. Dies wiederum bedeutet, dass für den Betrieb weniger elektrischer Strom benötigt wird und somit weniger Zeit benötigt wird, um die erforderliche Menge an elektrischer Ladung in das System zu senden. Insbesondere bei DRAM wirkt sich die in einem Zellkondensator zu speichernde Ladungsmenge auch diesmal unmittelbar auf. Da sich die Fertigung verbessert hat, hat sich der Festkörperspeicher hinsichtlich der Leistung dramatisch verbessert. Moderne DRAM-Chips hatten Betriebsgeschwindigkeiten in der Größenordnung von 10 ns oder weniger. Ein weniger offensichtlicher Effekt ist, dass die Anzahl der DIMMs, die benötigt werden, um eine bestimmte Speichermenge zu liefern, abnimmt, was wiederum insgesamt weniger DIMMs in jedem bestimmten Computer bedeutet. Dies führt oft zu einer verbesserten Leistung, da es weniger Busverkehr gibt. Dieser Effekt ist jedoch im allgemeinen nicht linear. Auswirkungen auf die Preis-Speicherdichte haben auch eine starke Auswirkung auf den Speicherpreis, obwohl in diesem Fall die Gründe nicht so offensichtlich sind. Bei scheibenbasierten Medien ist der Primäraufwand die bewegten Teile innerhalb des Antriebs. Dies setzt eine feste untere Grenze, weshalb der durchschnittliche Verkaufspreis für beide großen HDD-Hersteller seit 2007 45–75 US$ beträgt. Das heißt, der Preis für Hochleistungsantriebe ist schnell gefallen, und das ist in der Tat ein Effekt der Dichte. Die höchsten Laufwerke verwenden mehr Platten, im Wesentlichen einzelne Festplatten im Gehäuse. Mit zunehmender Dichte kann die Anzahl der Platten reduziert werden, was zu geringeren Kosten führt. Festplatten werden oft in Bezug auf Kosten pro Bit gemessen. Zum Beispiel lieferte die erste kommerzielle Festplatte, IBMs RAMAC 1957, 3,75 MB für $34,500 oder $9,200 pro Megabyte. 1989 kostet eine 40 MB Festplatte $1200, oder $30/MB. Und im Jahr 2018 verkauften 4 Tb-Laufwerke für $75, oder 1.9¢/GB, eine Verbesserung von 1,5 Millionen seit 1989 und 520 Millionen seit dem RAMAC. Dies ist ohne Anpassung an die Inflation, was die Preise von 1956 bis 2018 neunfach erhöht. Solid-State-Speicher hat einen ähnlichen Kostenabfall pro Bit gesehen.In diesem Fall wird der Aufwand durch die Ausbeute, die Anzahl der in einer Zeiteinheit erzeugten tragfähigen Chips bestimmt. Chips werden in Chargen hergestellt, die auf der Oberfläche eines einzigen großen Siliziumwafers bedruckt werden, der geschnitten wird und nicht arbeitende Proben verworfen werden. Die Herstellung hat im Laufe der Zeit verbesserte Ausbeuten durch die Verwendung größerer Wafer und die Herstellung von Wafern mit weniger Ausfällen. Die untere Grenze auf diesem Prozess ist etwa $1 pro fertigen Chip aufgrund von Verpackungen und anderen Kosten. Der Zusammenhang zwischen Informationsdichte und Kosten pro Bit lässt sich wie folgt darstellen: Ein Speicherchip, der die halbe physikalische Größe hat, bedeutet, dass doppelt so viele Einheiten auf demselben Wafer erzeugt werden können, wodurch der Preis jedes einzelnen halbiert wird. Als Vergleich wurde DRAM zunächst im Handel im Jahr 1971 eingeführt, ein 1 kbit Teil, der etwa 50 $ in großen Chargen kostete, oder etwa 5 Cent pro Bit. ANHANG Mbit-Teile waren 1999 üblich, was etwa 0.00002 Cent pro Bit kostet (20 Mikrozent/Bit). Siehe auch Bekenstein gebundene Bit-Zelle – die Länge, Fläche oder Volumen benötigt, um ein einzelnes Bit Mark Kryder zu speichern, die im Jahr 2009 projiziert, dass, wenn Festplatten mit ihrem dann laufenden Tempo von etwa 40 % pro Jahr weiter vorankommen würden, dann im Jahr 2020 ein zweiplatter, 2,5-Zoll-Disk-Laufwerk würde etwa 40 Terabytes (TB) speichern und ungefähr 40 $ kosten. Gemusterte Medien Schingled magnetische Aufnahme (SMR) == Referenzen ==