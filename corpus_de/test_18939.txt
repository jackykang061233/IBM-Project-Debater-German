Ein Transformator ist ein tiefgreifendes Lernmodell, das den Mechanismus der Aufmerksamkeit annimmt und die Bedeutung jedes Teils der Inputdaten differenziert gewichtet. In erster Linie wird sie im Bereich der natürlichen Sprachverarbeitung (NLP) und in der Computervision (CV) verwendet. Wie die wiederkehrenden Neuralnetze (RNNs) sind Transformatoren für Aufgaben wie Übersetzung und Textzusammenfassung konzipiert, um sequentielle Inputdaten wie die natürliche Sprache zu verarbeiten. Im Gegensatz zu RNNs verarbeiten Transformatoren die Daten jedoch nicht unbedingt. Man sieht vielmehr einen Kontext für jede Position in der Eingangssequenz vor. Wenn die Eingabedaten zum Beispiel eine natürliche Sprachestrafe sind, muss der Transformator den Beginn der Strafe nicht vor Ende verarbeiten. Man stellt vielmehr den Kontext fest, der dem jeweiligen Wort in der Strafe entspricht. Dieses Merkmal ermöglicht mehr Parallelisierung als RNNs und verringert daher die Ausbildungszeiten. Lösungen sind das Modell der Wahl für NLP-Probleme und ersetzen RNN-Modelle wie langfristiges Gedächtnis (LSTM). Durch die zusätzliche Parallelisierung der Fortbildung können größere Datensätze ausgebildet werden als möglich. Dies führte zur Entwicklung von prätrainierten Systemen wie BERT (Bidirektionale Ventil Vertretungen von Transformatoren) und GPT (Generative Pre-trained  Transformationer), die mit großen Sprachdatensätzen, wie etwa Wikipedia Corpus und Common Crawl, ausgebildet wurden und für spezifische Aufgaben verfeinert werden können. Hintergrund vor Transformationsformen, die meisten hochmodernen NLP-Systeme basieren auf gehobenen RNNs, wie z.B. der LSTM und der abgelagerten Einheiten (GRU) mit zusätzlichen Aufmerksamkeitsmechanismen. Flaschen werden auf diesen Schwerpunkttechnologien gebaut, ohne eine RNN-Struktur zu verwenden, wobei darauf hingewiesen wird, dass die Aufmerksamkeitsmechanismen allein der Leistung von RNNs entsprechen können. Sequentielle Verarbeitung Gated RNNs-Prozess holt sich fort und behält einen staatlichen Vektor, der eine Darstellung der Daten enthält, die nach jedem Token zu verzeichnen sind. Um den n dampftextstyle n} zu verkennen, kombiniert das Modell den Staat, der den Satz bis zu den Token n − 1 fasertextstyle n-1} mit den Informationen des neuen Tokens, um einen neuen Staat zu schaffen, der den Satz bis zur Herausgabe des Token n n Memetext n} .Theoretally, die Informationen von einem zuken kann willkürlich weit unter die Sequenz fallen, wenn der Staat auf jeden Punkt weiterverschlüsselt, um Informationen über den Kontext zu verschlüsseln. In der Praxis ist dieser Mechanismus unausgewogen: Das schwindende Bewegungsproblem lässt den Zustand des Modells am Ende eines langen Satzes ohne genaue, mineralierbare Informationen über die Vorwürfe ab. Beton Dieses Problem wurde durch Aufmerksamkeitsmechanismen angegangen. Hinweismechanismen lassen ein Modell aus dem Staat auf jedem vorherigen Punkt entlang der Sequenz ausweisen. Die Aufmerksamkeitsschicht kann allen vorherigen Staaten zugänglich sein und sie nach einer gelernten Maßnahme der Relevancy abwälzen, die relevante Informationen über weiträumige Token enthält. Ein klares Beispiel für den Wert der Aufmerksamkeit ist in der Übersetzung der Sprache, wo es wichtig ist, die Bedeutung eines Wortes in einer Strafe zuzuweisen. In einem englischen Übersetzungssystem hängt das erste Wort des französischen Outputs wahrscheinlich stark von den ersten Wörtern des englischen Inputs ab. In einem klassischen LSTM-Modell, um das erste Wort der französischen Produktion zu erstellen, wird das Modell jedoch nur den staatlichen Vektor des letzten englischen Wortes erhalten. Kurzum, dieser Vektor kann Informationen über den gesamten englischen Satz verschlüsseln und dem Modell alle notwendigen Kenntnisse geben. In der Praxis werden diese Informationen oft von der LSTM nicht erhalten. Ein Aufmerksamkeitsmechanismus kann hinzugefügt werden, um dieses Problem zu lösen: Der Decoder erhält Zugang zu den staatlichen Vektoren jedes englischen Eingangsworts, nicht nur der letzte, und kann die Aufmerksamkeitsgewichte erfahren, die zeigen, wie viel an jedem englischen Input-Staatsvektor teilnehmen können. Bei der Ergänzung zu RNNs erhöhen die Aufmerksamkeitsmechanismen die Leistung. Die Weiterentwicklung der Transformer-Architektur hat ergeben, dass die Aufmerksamkeitsmechanismen selbst kräftig waren und dass eine fortlaufende Weiterverarbeitung der Daten nicht notwendig war, um die Leistungsgewinne von RNNs mit Aufmerksamkeit zu erzielen. Flaschen verwenden einen Aufmerksamkeitsmechanismus ohne RNN, die gleichzeitig alle Token verarbeiten und die Aufmerksamkeitsgewichte zwischen ihnen in verschiedenen Schichten berechnen. Architektur wie frühere Modelle nimmt der Transformator eine kodierende Architektur an. Kodierungsschichten, die den Input nach einer anderen Schicht verarbeiten, während der Decoder aus Entschlüsselungsschichten besteht, die die gleiche Sache für die Produktion von Notebooks tun. Die Funktion der einzelnen Filterschichten ist die Erstellung von Kodizes, die Informationen enthalten, über die Teile der Inputs einander relevant sind. Sie sendet seine Kodizes an die nächste Geberschicht als Input. Jede Decoderschicht ist das Gegenteil, indem alle Kodizes und ihre integrierten Kontextinformationen verwendet werden, um eine Outputsequenz zu erstellen. Um dies zu erreichen, nutzt jede Geber- und Decoderschicht einen Aufmerksamkeitsmechanismus. Für jeden Input wird die Relevanz jedes anderen Inputs berücksichtigt und zieht aus ihnen die Produktion. Jede Decoderschicht hat einen zusätzlichen Aufmerksamkeitsmechanismus, der Informationen aus den Outputs früherer Decoder enthält, bevor die Decoderschicht Informationen aus den Kodierungscodes enthält. Sowohl die Trägerschichten als auch die Decoderschichten verfügen über ein Feed-forward-Neuralnetz für die zusätzliche Verarbeitung der Outputs und enthalten Restverbindungen und Normalisierungsschritte. Größere punktuelle Aufmerksamkeit Die Umwandler-Bausteine werden flächendeckend eingesetzt. Wenn ein Satz in ein Transformationsmodell übergeht, werden die Aufmerksamkeitsgewichte zwischen jedem Token gleichzeitig berechnet. Die Aufmerksamkeitseinheit stellt fest, dass jedes Spielzeug im Kontext integriert wird, das Informationen über das Token selbst enthält und eine gewichtete Kombination anderer relevanter Tinten mit ihrem Gewicht enthält. Für jede Aufmerksamkeitseinheit lernt das Transformationsmodell drei Gewicht matrices; die Abfragegewichte W Memedisplaystyle W_{Q}, die wichtigsten Gewichte W K {\displaystyle W_{K} und die Wertgewichte W V Memedisplaystyle W_{V}. Für jedes Spielzeug i {\displaystyle i} wird das Eingangswort, das x i {\displaystyle x_{i} enthält, mit jedem der drei Gewicht matrices multipliziert, um einen Abfragevektor q i W Q 7.8displaystyle herzustellen. q_{i}=x_{i} Q , ein Schlüsselvektor k i = x i W K {\displaystyle k_{i}=x_{i}W_{K , und ein Wertvektor v i = x i W V {\displaystyle v_{i}=x_{i} Anhand der Abfrage- und Schlüsselvektoren berechnet: das Gewicht einer i j Memedisplaystyle a_{ij} von token i {\displaystyle i} token j {\displaystyle j} ist das Produkt zwischen q i {\displaystyle q_{i} und k dampfstyle k_{j}. Die Aufmerksamkeitsgewichte werden durch die Quadratwurzel der Dimension der Schlüsselvektoren, d k Memedisplaystyle ggioqrt d_{k} , die die Beeinträchtigungen während der Ausbildung stabilisiert, und durch eine Softmax, die die Gewichte normalisiert. Die Tatsache, dass W Q {\displaystyle W_{Q} und W K WELLdisplaystyle W_{K} unterschiedliche matrices es ermöglichen, nicht symmetrisch zu sein: wenn man i {\displaystyle i} an der Verkennung j Memestyle j} (d. h. q i  i k {\displaystyle q_{iccdot k {j} ist groß), bedeutet dies nicht zwangsläufig, dass j KING j} style j KINGstyle j . Die Leistung der Aufmerksamkeitseinheit für das Aufken i {\displaystyle i} ist die gewichtete Summe der Wertträger aller Token, gewichtet durch eine i j {\displaystyle a_{ij}, die Aufmerksamkeit von Token i {\displaystyle i} auf jeden Token. Die Aufmerksamkeitsberechnung für alle Token kann als eine große Matrix berechnet werden, wobei die Softmax-Funktion verwendet wird, die aufgrund von Rechenmatrix-Betriebsoptimierungen, die schnell zu berechnen sind, sinnvoll ist. Die matrices Q Memedisplaystyle Q} , K {\displaystyle K} und V {\displaystyle V} sind definiert als die matrices, in denen die i {\displaystyle i} drei Zeilen sind Vektor q i KINGstyle q_{i} , k i {\displaystyle k_{i} , und v i Memestyle v_{i}. Schwerpunkt ( Q , K , V ) = Weichmax ( Q K T d k ) V displaystyle beginnt {aligned)text{Attention((Q,K,V)= } QK^{\ Mathematik {T} )sqrt d_{k}}}}\right) V\end Mehrseitige Aufmerksamkeit Ein Satz (W Q , W K , W V ) Glühbirnestyle links(W_{Q},W_{K},W_{V.right) matrices wird als Aufmerksamkeitskopf bezeichnet, und jede Schicht in einem Transformationsmodell hat mehrere Aufmerksamkeitsköpfe. Jede Aufmerksamkeit nimmt an den Token teil, die für jeden Token relevant sind, wobei mehrere Aufmerksamkeitsköpfe das Modell für unterschiedliche Definitionen von Bedeutung tun können." Darüber hinaus kann der Einflussbereich, der die Relevanz vertritt, schrittweise in aufeinander folgenden Schichten abgezweigt werden. Viele Transformatoren betonen die relevanten Beziehungen, die für den Menschen sinnvoll sind. Beispielsweise können die Leiter der Aufmerksamkeit in erster Linie am nächsten Wort teilnehmen, während andere vor allem an den Vers zu ihren direkten Gegenständen teilnehmen. Die Berechnungen für jeden Aufmerksamkeitskopf können parallel durchgeführt werden, was eine schnelle Verarbeitung ermöglicht. Die Outputs für die Aufmerksamkeitsschicht werden verdrängt, in die Netzschichten des Feed-forward-Neural-Netzes zu übertragen. MTBEach-Appell besteht aus zwei Hauptkomponenten: einem Selbstattention-Mechanismus und einem Feed-forward-Neuralnetz. Der Selbstattention-Mechanismus akzeptiert Eingabecodes aus dem vorherigen Koeffizienten und bewältigt ihre Relevanz für die Herstellung von Outputcodes. Das Feed-forward-Neural-Netz setzt die einzelnen Produktionsverfahren fort. Diese Outputcodes werden dann an den nächsten Empfänger als Input sowie an die Decoder weitergegeben. Der erste Klärstoff nimmt Positionsinformationen auf und setzt die Eingangssequenz als Input ein, anstatt zu kodieren. Die Positionsinformationen sind für den Transformator erforderlich, um die Reihenfolge der Sequenz zu nutzen, da kein anderer Teil des Transformators dies nutzt. Decoder Jeder Decoder besteht aus drei großen Komponenten: einem Selbstattention-Mechanismus, einem Beobachtungsmechanismus über die Kodifizierungen und einem Feed-forward-Neuralnetz. Der Decoder funktioniert in ähnlicher Weise wie der Koch, aber ein zusätzlicher Aufmerksamkeitsmechanismus wird eingeführt, der stattdessen relevante Informationen aus den von den Kodizes generierten Kodizes enthält. Wie der erste Decoder nimmt die erste Decoder Positionsinformationen auf und setzt die Outputsequenz als Input ein, statt die Kodierung. Der Transformator darf die derzeitige oder künftige Produktion nicht verwenden, um eine Produktion vorherzusagen, so dass die Outputsequenz teilweise verschleiert werden muss, um diesen umgekehrten Informationsfluss zu verhindern. Der letzte Decoder folgt einer endgültigen linearen Transformations- und Softmax-Ebene, um die Output-Probabilities über das Wortschatz zu produzieren. Alternativen Training transformerbasierte Architekturen können teuer sein, vor allem für lange Sätze. Alternative Architekturen umfassen die Reformer (die die Rechenlast von O (N 2 ) Memedisplaystyle O(N2)2)} auf O (N l  N N ) Memestyle O(N\ln N)} ) oder Modelle wie ETC/BigBird (die sie auf O (N ) Memestyle O(N)} reduzieren können) ) , wenn N KINGstyle N} ist die Länge der Sequenz. Hier geht es um lokale, sensibel und umkehrbare Schichten. Ende 2020 wurde ein Benchmark für den Vergleich von Transformationsarchitekturen eingeführt. Betriebsoptimierungen durchlaufen in der Regel halbgewaltetes Lernen mit unkontrollierter Vorausbildung, gefolgt von kontrollierter Feinabstimmung. Umschulungen werden in der Regel auf einem größeren Datensatz als Feinabstimmung durchgeführt, da es nur geringe Verfügbarkeit von etikettierten Schulungsdaten gibt. Aufgaben für die Vorausbildung und Feinabstimmung sind häufig: Sprachmodellierung der nächsten Zustimmungsvorhersage, die Beantwortung von Leseverständnisanalysen paraphrasing Anwendungen Der Transformator hat bei der natürlichen Sprachverarbeitung (NLP) einen großen Erfolg erzielt, zum Beispiel die Aufgaben der maschinellen Übersetzung und Zeitvorhersage. Viele vorbeugende Modelle wie GPT-2, GPT-3, BERT, XLNet und RoBERTa zeigen die Fähigkeit von Transformatoren, eine Vielzahl solcher NLP-bezogener Aufgaben durchzuführen und die Möglichkeit zu nutzen, echte Anwendungen zu finden. Hierzu gehören: maschinelle Übersetzungsdokument Zusammenfassung der Dokumentationsdokument-Generation mit dem Namen "Einigung" (NER) biologische Sequenzanalyse-Video-Ververständnis. Im Jahr 2020 wurde gezeigt, dass die Transformationsarchitektur, insbesondere GPT-2, in die Lage versetzt werden könnte, Schach zu spielen. Flaschen wurden auf die Bildverarbeitung mit wettbewerbsfähigen Ergebnissen mit konvolutionalen Neuralnetzen angewendet. Durchführung Das Transformationsmodell wurde in Standard-Fernbildungsrahmen wie TensorFlow und PyTorch umgesetzt. Transformatoren sind eine Bibliothek, die von Hugging Face produziert wird, die umgestalterbasierte Architekturen und prätrainierte Modelle liefert. Hinweis für die weitere Lesung Hubert Kuhner et al.(2020), "Hyfield Networks is All You Need", "Vorentwurf für ICLR 2021.arXiv:2008.02217; siehe auch Autoren'- Blog Diskussion über die Wirkung einer Umwandlungsschicht als gleichwertig mit einer Aktualisierung von Hopfield, die einen der festen Punkte (repräsentable Muster) eines kontinuierlichen Hopfield-Netzwerks Außenbeziehungen Alexander Rush, The Annotated Transformationer, Harvard NLP-Gruppe, 3 April 2018 näher bringt