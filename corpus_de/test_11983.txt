Adversarial maschinelles Lernen ist eine maschinelle Lerntechnik, mit der versucht wird, Modelle zu katastrophalen Inputs zu liefern. Der häufigste Grund ist, eine Funktionsstörung in einem maschinenlesbaren Lernmodell zu verursachen. Die meisten Maschinen- und Lerntechniken wurden entwickelt, um auf spezifischen Problemsets zu arbeiten, in denen die Ausbildungs- und Testdaten aus dem gleichen statistischen Vertrieb (IID) generiert werden. Wenn diese Modelle auf die reale Welt angewendet werden, können sie Daten liefern, die gegen diese statistische Annahme verstoßen. Diese Daten können so gestaltet werden, dass spezifische Schwachstellen genutzt und die Ergebnisse beeinträchtigt werden. Geschichte In Schnee Sofort (1992) bot der Autor Szenarien der Technologie an, die für einen konversarialen Angriff anfällig waren. In der Nullgeschichte (2010) gibt es ein T-Shirts, das in einer Weise, die ihn für die elektronische Überwachung unsichtbar macht. Nilsh Dalvi und andere wiesen darauf hin, dass lineare Klassenaggregate, die in Spamfiltern verwendet werden, durch einfache „Umfallangriffe“ als Spammer „gute Worte“ in ihre Spam-E-Mails abgelehnt werden könnten. (Arund 2007, einige Spammers haben zufälliges Lärm in "Image Spam" aufgenommen, um OCR-basierte Filter zu besiegen.) Im Jahr 2006 haben Marco Barreno und andere „Kann WeSafe“ veröffentlicht, die eine breite Steueronomie von Angriffen ausschließt. Bis Ende 2013 hoffen viele Forscher weiterhin, dass nicht-lineare Klassentoren (wie z.B. die Unterstützung von Vektor- und Neuralnetzen) robust sein könnten, bis Battista Biggio und andere die ersten schwindenden Angriffe auf solche maschinenlesbare Lernmodelle (2012-2013) gezeigt haben. Im Jahr 2012 haben tiefe Neuralnetze begonnen, Computer-Vision-Probleme zu beherrschen; ab 2014 haben Christian Szegedy und andere gezeigt, dass tiefgreifende Neuralnetze durch Verlierer verdächtigt werden könnten, indem sie erneut einen sprungsbedingten Angriff auf das Handwerk auf färöische Perturbationen verwenden. Kürzlich wurde festgestellt, dass es aufgrund der unterschiedlichen Umweltbehinderungen, die den Effekt von Lärmen abheben, schwieriger ist, in der Praxis zu produzieren. Beispielsweise kann jede kleine Rotation oder leichte Beleuchtung auf einem konversarialen Image die Zweckmäßigkeit zerstören. Beispiele sind beispielsweise Angriffe auf Spam-Filter, bei denen Spam-Nachrichten durch die Verfehlung von schlechten Wörtern oder die Einfügung von guten Worten verschleiert werden; Angriffe auf Computersicherheit, wie z.B. obfuscating-Decode innerhalb von Netzpaketen oder fehlerhafte Signaturerkennung; Angriffe auf biometrische Anerkennung, bei denen gefälschte biometrische Merkmale genutzt werden können, um einen rechtmäßigen Benutzer zu entbinden; oder um die Muster der Benutzer zu Kompromissen zu bringen, die sich an die aktualisierten Merkmale über die Zeit anpassen. Forscher zeigten, dass es möglich war, durch eine Veränderung nur ein einziges iPhone zu entwickeln. Andere 3-D-Drucke eine Spielzeugschildkröte mit einer Struktur, die entwickelt wurde, um Googles Objekt-Identifikationsart einzustufen, unabhängig von dem Blickwinkel, an dem die Schildkröten angesehen wurden. Herstellung der Schildkröten erforderte nur kostengünstige kommerzielle 3D-Drucktechnik. Ein maschinenlesbares Bild eines Hundes wurde gezeigt, wie eine Katze sowohl Computer als auch Menschen aussieht. Laut einer Studie aus dem Jahr 2019 kann der Mensch davon ausgehen, wie die Maschinen für die Kateversarial-Bilder qualifizieren. Forscher entdeckten Methoden, um das Aussehen eines Stop-Zeichens zu überprüfen, das ein autonomes Fahrzeug, das es als Fusion oder Geschwindigkeitsgrenzzeichen eingestuft hat. McAfee griff das frühere Mobileye-System an, das es in der Geschwindigkeitsbegrenzung zu 50 mph verfesselt, indem es einfach einen Zwei-Zoll-Streifen von schwarzen Banden auf ein Geschwindigkeitsgrenzzeichen erweitert. Adversariale Muster auf brillen oder Kleidung, die darauf ausgelegt sind, Gesichtserkennungssysteme oder Lizenzplattenlese zu täuschen, haben zu einer Nischenindustrie von "stealth Streetwear" geführt. Ein konversarialer Angriff auf ein Neuralnetz kann es einem Angriffsgegner ermöglichen, Algorithmen in das Zielsystem umzusetzen. Forscher können auch adversariale Audio-Inputs für verschleierte Befehle für intelligente Assistenten in benign-see Audio erstellen; eine parallele Literatur untersucht die menschliche Wahrnehmung dieser Stimulus. Clustering-Algorithmen werden in Sicherheitsanwendungen verwendet. Resistenz und Computer-Virusanalyse zielen darauf ab, Schadsoftware-Familien zu identifizieren und spezifische Nachweis-Signs zu erstellen. Angriffsmodalitäten Taxonomy Angriffe auf (überwachte) Maschinenlern-Algorithmen wurden in drei Hauptachsen zusammengefasst: Einfluss auf den Klassenprüfer, Sicherheitsverletzungen und ihre Besonderheiten. Einfluss der Klasse: Ein Angriff kann den Einstufungsprüfer beeinflussen, indem er die Einstufungsphase unterbrochen. Vor diesem Hintergrund kann eine Explorationsphase zur Erkennung von Schwachstellen vorausgehen. Die Fähigkeiten des Angreifers könnten durch die Anwesenheit von Datenmanipulation eingeschränkt werden. Sicherheitsverletzung: Ein Angriff kann schädliche Daten liefern, die als legitim eingestuft werden. irrtümliche Daten, die während der Ausbildung geliefert werden, können berechtigte Daten nach der Ausbildung ablehnen. Spezifität: Ein gezielter Angriff Versuch, ein spezifisches Eindringen/Diruption zu ermöglichen. Alternativ führt ein wahlfreier Anschlag zu einem allgemeinen Hemm. Diese Taxonomy wurde in ein umfassenderes Bedrohungsmodell erweitert, das explizite Annahmen über das Ziel der Gegenpartei, das Wissen über das angegriffene System, die Fähigkeit zur Entfaltung der Inputdaten/Systemkomponenten und die Angriffsstrategie ermöglicht. Diese Taxonomy wurde weiter erweitert, um die Abmessungen für Verteidigungsstrategien gegen schädliche Angriffe einzubeziehen. Strategien Nachstehend sind einige der am häufigsten auftretenden Anwendungsszenarien: Evasion Evasion-Angriffe sind die häufigste Art von Angriffen. Spammers und Hacker versuchen zum Beispiel, durch die Verwechslung des Inhalts von Spam und Schadprogrammen zu vermeiden. Proben werden zur Evade-Erkennung geändert; das ist als legitim einzustufen. Dies beinhaltet keinen Einfluss auf die Ausbildungsdaten. Ein klares Beispiel für Evasion ist bildbasierte Spam, in dem der Spam-Gehalt in einem beigefügten Bild eingebettet ist, um die Textanalyse durch Anti-Spam-Filter zu umgehen. Ein weiteres Beispiel für Steuerhinterziehung wird durch spekulierende Angriffe auf biometrische Überprüfungssysteme gegeben. Poisoning Poisoning ist eine konversariale Kontamination von Ausbildungsdaten. Maschinenlernsysteme können mit Daten, die während der Operationen gesammelt wurden, neu ausgebildet werden. Intrusionserkennungssysteme (IDSs) werden häufig mit solchen Daten neu betrieben. Ein Angreifer kann diese Daten vergiften, indem er schädliche Proben während der Operation injiziert, die anschließend die Umschulung stören. Modellfälschung (auch als Modellgewinnung bezeichnet) umfasst eine Querverweise, die ein Black Box-System für maschinelles Lernen vorschreibt, um entweder das Modell zu wiederherstellen oder die darin enthaltenen Daten zu extrahieren. Dies kann Probleme verursachen, wenn die Ausbildungsdaten oder das Modell selbst sensibel und vertraulich sind. Beispielsweise könnte ein Modell, bei dem ein eigenes Aktienhandelsmodell gewonnen werden könnte, das der Adversor für seinen eigenen finanziellen Nutzen verwenden könnte. Konkrete Angriffe Es gibt eine Vielzahl unterschiedlicher Anschläge, die gegen Maschinenlernsysteme verwendet werden können. Viele dieser Arbeiten betreffen sowohl tiefe Lernsysteme als auch traditionelle maschinenlesbare Lernmodelle wie SVM und lineare Regression. Hochrangige Stichprobe dieser Anschlagtypen umfasst: Adversarial Beispiele für Antiangriffe / Angriffsmodelle Inversionsmitgliedschaft Inference Adversarial Beispiele Ein Paradebeispiel bezieht sich auf speziell vorbereitete Inputs, die für den Menschen normal aussehen, aber zu einer Fehlklassifizierung auf ein Modell des maschinenlesbaren Lernens führen. Häufig wird eine Art speziell konzipierter Lärm verwendet, um die Fehlklassen zu eliminieren. Nachstehend sind einige aktuelle Techniken zur Erstellung von Adversarial-Beispielen in der Literatur (um keine erschöpfende Liste zu erstellen). Fehlerbasierte Evasion-Angriffe Schnell-Test-Methode (FGSM)Projektedfall Descent (PGD) Carlini und Wagner (C &W) Angriffe auf Adversarial-Phäsionsangriffe haben einen Multi-Level-Ansatz zum Schutz des maschinenlesbaren Lernens vorgeschlagen. Risikomodellierung – Formalisierung der Angriffeziele und Fähigkeiten im Hinblick auf das Zielsystem. Angriffssimulation – Formalisierung des Optimierungsproblems versucht der Angriffsgegner nach möglichen Angriffsstrategien zu lösen. Folgenabschätzung Erkennung von Lärmschutz (für evasionsbedingten Angriff) Informationenwäsche – Altern Sie die Informationen, die von Adversaries (für Muster, die Angriffe abbauen) erhalten haben Eine Reihe von Verteidigungsmechanismen gegen Steuerhinterziehung, Vergiftung und Privatsphäre wurden vorgeschlagen, darunter: Sichere Lernalgorithmen Multiple Klasseifier Systeme AI-geschriebene Algorithmen. AIs, die das Ausbildungsumfeld erkunden; zum Beispiel in der Bilderkennung, die eine 3D-Umgebung aktiv beschiffen, anstatt eine fixe Reihe von 2D-Bildern zu passiven. Datenschutz: Ladder-Algorithmus für Kaggle-style-Auswahlverfahren Game theoretic Modelle Verminderung der Schulungsdaten Adversarial Training Filter-Impfung Lesen Sie auch externe Links NIPS 2007Workshop zum Thema Maschinenbau in Adversarial Environment for Computer Security AlfaSVMLib – Adversarial Label Flip Angriffe gegen Vectormaschinen Laskov, Pavel; Lippmann, Richard (2010) "Maschinen lernen in konversarialen Umgebungen". Maschinen- und Lernprozess.81 (2): 115–119. doi:10.1007/s10994-010-5207-6.S2CID 12567278.Dagstuhl Perspektiven Workshop zum Thema "Maschinenlernmethoden für Computersicherheit" Workshop über künstliche Intelligenz und Sicherheit (AISec) Serie