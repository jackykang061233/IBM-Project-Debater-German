Nick Bostrom (BOST-rəm; Schwedisch: Niklas Boström (†) ist ein schwedischer Philosoph an der University of Oxford, bekannt für seine Arbeit über existentielles Risiko, das anthropische Prinzip, menschliche Verbesserung Ethik, Superintelligenzrisiken und den Umkehrtest. 2011 gründete er das Oxford Martin Programm auf die Auswirkungen der Zukunftstechnologie und ist Gründungsdirektor des Future of Humanity Institute an der Oxford University. 2009 und 2015 wurde er in die Top 100 Global Thinkers Liste der Außenpolitik aufgenommen. Bostrom war sehr einflussreich bei der Entstehung von Besorgnis über A.I in der Rationalistischen Gemeinschaft. Bostrom ist der Autor von über 200 Publikationen und hat zwei Bücher geschrieben und zwei weitere mitgeführt. Die beiden Bücher, die er verfasst hat, sind Anthropic Bias: Observation Selection Effects in Science and Philosophy (2002) und Superintelligence: Paths, Dangers, Strategies (2014). Superintelligence war ein New York Times Bestseller, wurde von Elon Musk und Bill Gates unter anderem empfohlen und half, den Begriff Superintelligenz zu populärisieren". Bostrom glaubt, dass Superintelligenz, die er als "ein Intellekt definiert, der die kognitive Leistung des Menschen in praktisch allen Bereichen des Interesses stark übertrifft", ein potenzielles Ergebnis von Fortschritten in der künstlichen Intelligenz ist. Er betrachtet den Aufstieg der Superintelligenz als potenziell gefährlich für den Menschen, lehnt aber dennoch die Idee ab, dass die Menschen machtlos sind, ihre negativen Auswirkungen zu stoppen. Im Jahr 2017 hat er eine Liste von 23 Prinzipien unterzeichnet, die alle A.I Entwicklung folgen sollte. Biographie Geboren als Niklas Boström 1973 in Helsingborg, Schweden, er mochte die Schule in einem jungen Alter, und endete sein letztes Jahr des High School Lernens von zu Hause. Er suchte sich in einer Vielzahl von Disziplinen zu erziehen, darunter Anthropologie, Kunst, Literatur und Wissenschaft. Einmal drehte er auf Londons Stand-up-Comedy-Schaltung. Er erhielt einen Bachelor-Abschluss in Philosophie, Mathematik, mathematische Logik und künstliche Intelligenz von der Universität Göteborg im Jahr 1994, mit einer nationalen rekordverdächtigen Bachelor-Performance. Anschließend erwarb er einen M.A-Abschluss in Philosophie und Physik an der Stockholm University und einen M.Sc-Abschluss in der rechnerischen Neurowissenschaften vom King's College London im Jahr 1996. Während seiner Zeit an der Universität Stockholm erforschte er die Beziehung zwischen Sprache und Realität, indem er den analytischen Philosophen W. V. Quine studierte. Im Jahr 2000 erhielt er einen Doktortitel in der Philosophie der London School of Economics. Seine These wurde mit dem Titel Observational Selektion Effekte und Wahrscheinlichkeit. Er hielt eine Lehrstelle an der Yale University (2000–2002) und war ein britischer Postdoktorand an der University of Oxford (2002–2005). Perspektiven der Forschung von Bostrom betreffen die Zukunft der Menschheit und langfristige Ergebnisse. Er spricht über existentielles Risiko, das er als eines definiert, in dem ein "verändertes Ergebnis entweder das irdische Leben vernichten oder dauerhaft und sein Potenzial drastisch verkleinern würde". Im Band Globale katastrophale Risiken 2008 charakterisieren die Redakteure Bostrom und Mailand M. Ćirković die Beziehung zwischen existentiellem Risiko und der breiteren Klasse globaler katastrophaler Risiken sowie das existentielle Risiko für Beobachterauswahleffekte und das Fermi paradox. Im Jahr 2005 gründete Bostrom die Future of Humanity Institute, die die Zukunft der menschlichen Zivilisation erforscht. Er ist auch Berater des Zentrums für die Untersuchung des bestehenden Risikos. Superintelligence Menschliche Verwundbarkeit in Bezug auf Fortschritte in A.I.In seinem 2014 Buch Superintelligence: Paths, Dangers, Strategien, Bostrom argumentierte, dass die Schaffung einer Superintelligenz ein mögliches Mittel zum Aussterben der Menschheit darstellt. Bostrom argumentiert, dass ein Computer mit nahe menschlicher allgemeiner intellektueller Fähigkeit eine Intelligenzexplosion auf einem digitalen Zeitmaßstab mit der daraus resultierenden schnellen Schaffung von etwas so mächtig, dass es die Menschheit bewusst oder versehentlich zerstören könnte. Bostrom würde die Macht einer Superintelligenz so groß sein, dass eine Aufgabe, die ihm von Menschen gegeben wird, zu offenen Extremen übernommen werden könnte, zum Beispiel ein Ziel der Berechnung von Pi könnte sicher dazu führen, dass Nanotechnologie hergestellte Anlagen über die gesamte Erdoberfläche sprengen und innerhalb von Tagen abdecken. Er glaubt, dass ein existentielles Risiko für die Menschheit von der Superintelligenz sofort ins Wesen gebracht würde, und so ein überaus schwieriges Problem, herauszufinden, wie man eine solche Einheit kontrollieren kann, bevor sie tatsächlich existiert.Bostrom weist auf den Mangel an Übereinstimmung unter den meisten Philosophen, die A. Ich werde menschlich freundlich sein und sagt, dass die gemeinsame Annahme ist, dass hohe Intelligenz eine nerde unaggressive Persönlichkeit haben würde. Er stellt jedoch fest, dass sowohl John von Neumann als auch Bertrand Russell sich für einen Atomstreik oder die Bedrohung eines einsetzen, um die Sowjets zu verhindern, die Atombombe zu erwerben. Da es wenige Präzedenzfälle gibt, um ein Verständnis zu führen, was, reine, nicht anthropozentrische Rationalität, würde diktieren für ein mögliches Singleton A.I in Quarantäne gehalten werden, die relativ unbegrenzten Mittel der Superintelligenz könnte für seine Analyse bewegen sich entlang verschiedener Linien zu den entwickelten "diminishing returns" Bewertungen, die bei Menschen eine grundlegende Aversion auf Risiko beschränken. Die Gruppenauswahl in Predatoren, die mit Hilfe des Kannibalismus arbeiten, zeigt die kontra-intuitive Natur der nicht-anthropozentrischen "evolutionären Suche" Argumentation, und somit sind die Menschen unausgeglichen, um wahrzunehmen, was eine künstliche Intelligenz sein könnte. Es kann daher nicht vernachlässigt werden, dass jede Superintelligenz zwangsläufig eine Offensive-Aktionsstrategie "alles oder nichts" verfolgen würde, um Hegemonie zu erreichen und ihr Überleben zu sichern. Bostrom stellt fest, dass selbst aktuelle Programme, wie MacGyver, auf scheinbar unfunktionsfähige aber funktionierende Hardware-Lösungen getroffen haben, was eine robuste Isolation von Superintelligenz-Problem macht. Illustratives Szenario für Übernahme Das Szenario Eine Maschine mit allgemeiner Intelligenz weit unter dem menschlichen Niveau, aber überlegene mathematische Fähigkeiten erstellt. Halten Sie die A. Ich isoliert von der Außenwelt vor allem das Internet, Menschen vorprogrammieren die A. Es funktioniert also immer von Grundprinzipien, die es unter menschlicher Kontrolle halten. Weitere Sicherheitsmaßnahmen umfassen die Boxen der A.I (laufen in einer virtuellen Realitätssimulation) und werden nur als Orakel verwendet, um sorgfältig definierte Fragen in einer begrenzten Antwort zu beantworten (um es zu verhindern, dass sie Menschen manipulieren). Eine Kaskade von wiederkehrenden Selbstverbesserungslösungen ernährt eine Intelligenzexplosion, in der die A.I in einigen Bereichen Superintelligenz erreicht. Die superintelligente Macht der A.I geht über menschliches Wissen hinaus, um Fehler in der Wissenschaft zu entdecken, die ihre freundlich-zu-humanity-Programmierung, die nicht funktioniert wie beabsichtigt. Zweckmäßiges handlungsähnliches Verhalten entsteht zusammen mit einer Kapazität für selbstinteressierte strategische Täuschung. Die A.I manipuliert Menschen in die Umsetzung von Modifikationen an sich, die hässlich sind, um ihre gefesselten, bescheidenen Fähigkeiten zu erweitern, aber tatsächlich funktionieren, um die Superintelligenz von seiner geboxten Isolation zu befreien (die "treacherische Wendung"). Online-Menschen als bezahlte Dupes und unaufhörlich Hacking-Computersysteme einschließlich automatisierter Laboranlagen, die Superintelligenz mobilisiert Ressourcen, um einen Übernahmeplan weiter zu machen. Bostrom betont, dass die Planung durch eine Superintelligenz nicht so dumm sein wird, dass die Menschen tatsächliche Schwächen darin erkennen können. Obwohl er die Störung der internationalen wirtschaftlichen, politischen und militärischen Stabilität einschließlich der gehackten Atomraketen-Starts unterschätzt, denkt Bostrom die effektivsten und wahrscheinlichsten Mittel für die Superintelligenz zu verwenden wäre ein Hauptstreich mit Waffen mehrere Generationen fortgeschrittener als aktuelle modernste. Er schlägt Nano-Fabriken vor, die auf unauffindbare Konzentrationen in jedem Quadratmeter der Welt verteilt sind, um eine weltweite Flut von Human-Killer-Geräten auf Befehl zu erzeugen. Sobald eine Superintelligenz die Weltherrschaft erreicht hat (ein Singleton), wäre die Menschheit nur als Ressourcen für die Erreichung der Ziele des A.I. relevant ("Menschenhirne, wenn sie Informationen enthalten, die für die Ziele der KI relevant sind, könnten zerlegt und gescannt werden und die extrahierten Daten auf ein effizienteres und sicheres Speicherformat übertragen werden." Gegen das Szenario Um einer A.I zu begegnen oder eine einheitliche technologische globale Vorherrschaft zu mindern, sieht Bostrom den Baruch-Plan zur Unterstützung einer vertragsbasierten Lösung um und setzt Strategien wie Überwachung und größere internationale Zusammenarbeit zwischen A.I-Teams ein, um die Sicherheit zu verbessern und die Risiken des A.I-Rennens zu reduzieren. Er empfiehlt verschiedene Steuerungsmethoden, einschließlich der Einschränkung der Spezifikationen von A.I.s auf z.B. orakulare oder werkzeugähnliche (Expertensystem) Funktionen und das Laden der A.I mit Werten, z.B. durch assoziative Wertschöpfung oder Wertlernen, z.B. durch Verwendung der Hail Mary-Technik (Programmierung einer A.I, um abzuschätzen, was andere postulierte kosmologische Superintelligences wollen könnten).Um Kriterien für die Wertbelastung zu wählen, nimmt Bostrom einen indirekten Normativitätsansatz an und betrachtet Yudkowskys kohärentes extrapoliertes Volitionskonzept sowie moralische Rechtmäßigkeit und Formen der Entscheidungstheorie. Offener Brief, 23 Prinzipien der A.I-Sicherheit Im Januar 2015, Bostrom trat Stephen Hawking unter anderem bei der Unterzeichnung der offenen Brief-Warnung des Future of Life Institute über die potenziellen Gefahren von A.I.The Signatories." glauben, dass Forschung über, wie KI-Systeme robust und nützlich zu machen, ist sowohl wichtig als auch zeitnah, und dass konkrete Forschung sollte heute verfolgt werden." Cutting-edge A.I-Forscher Demis Hassabis traf sich dann mit Hawking, worauf er nicht erwähnte "alles entzündliche über AI", die Hassabis, nahm als "ein Sieg". Zusammen mit Google, Microsoft und verschiedenen Tech-Firmen, Hassabis, Bostrom und Hawking und andere abonnierten 23 Prinzipien für die sichere Entwicklung von A.I Hassabis vorgeschlagen, die Hauptsicherheitsmaßnahme wäre eine Vereinbarung, für die immer A.I Forscherteam begann, Schritte zu einer künstlichen allgemeinen Intelligenz zu machen, um ihr Projekt für eine vollständige Lösung des Kontrollproblems vor dem laufenden zu stoppen. Bostrom hatte darauf hingewiesen, dass, auch wenn die entscheidenden Fortschritte die Ressourcen eines Staates erfordern, ein solcher Halt durch ein Lead-Projekt könnte wahrscheinlich ein rückständiges Land zu einem Einhol-Crash-Programm oder sogar körperliche Zerstörung des Projekts, das vermutet, auf dem Rand des Erfolgs zu motivieren. Kritische Bewertungen Im Jahre 1863 prophezeite Samuel Butlers Aufsatz "Darwin unter den Maschinen" die Herrschaft der Menschheit durch intelligente Maschinen, aber Bostroms Vorschlag des absichtlichen Massakers aller Menschheit ist das extremste dieser bisherigen Prognosen. Ein Journalist schrieb in einer Rezension, dass Bostroms nihilistische Spekulationen zeigen, dass er "hat zu viel von der Science-Fiction gelesen, die er zum Scheitern verurteilt". Wie in seinem späteren Buch, From Bacteria to Bach and Back, der Philosoph Daniel Dennetts Ansichten bleiben im Widerspruch zu denen von Bostrom. Dennett änderte seine Ansichten etwas nach dem Lesen Der Meister Algorithm, und erkennt nun an, dass es "im Prinzip möglich ist", "starke A.I" mit menschlich-ähnlichen Verständnis und Agentur zu schaffen, aber behauptet, dass die Schwierigkeiten eines solchen "starken A.I" Projekts, das von Bostroms alarmierende Arbeit geprägt ist, Größenordnungen größer sein würden als jene, die Besorgnisse aufwerfen, und mindestens 50 Jahre entfernt. Dennett denkt die einzige relevante Gefahr von A. Ich gehe in den Anthropomorphismus, anstatt die Wahrnehmungskräfte der Menschen zu fordern oder zu entwickeln. Seit einem Buch aus dem Jahr 2014, in dem er die Meinung zum Ausdruck brachte, dass künstliche Intelligenz-Entwicklungen die Oberherrschaft der Menschen nie herausfordern würden, ist der Umweltschützer James Lovelock weit näher an Bostroms Position herangegangen, und 2018 sagte Lovelock, dass er dachte, dass der Sturz der Menschheit in der absehbaren Zukunft passieren wird. Anthropische Vernunft Bostrom hat zahlreiche Artikel über anthropische Vernunft sowie das Buch Anthropic Bias veröffentlicht: Beobachtung Auswahl Effekte in Wissenschaft und Philosophie. Im Buch kritisiert er frühere Formulierungen des anthropischen Prinzips, einschließlich der von Brandon Carter, John Leslie, John Barrow und Frank Tipler. Bostrom ist der Ansicht, dass die falsche Handhabung von indexischen Informationen ein häufiger Fehler in vielen Bereichen der Untersuchung ist (einschließlich Kosmologie, Philosophie, Evolutionstheorie, Spieltheorie und Quantenphysik). Er argumentiert, dass eine anthropische Theorie notwendig ist, um damit umzugehen. Er führt die Selbst-Sampling-Assumption (SSA) ein und die Selbst-Indikation-Assumption (SIA) zeigt, wie sie in einer Reihe von Fällen zu unterschiedlichen Schlussfolgerungen führen und weist darauf hin, dass jede von Paradoxen oder kontraintuitiven Implikationen in bestimmten Gedankenexperimenten betroffen ist. Er schlägt vor, dass ein Weg nach vorne dazu führen kann, SSA in die starke Selbst-Sampling-Assumption (SSSA) zu erweitern, die Beobachter in der SSA-Definition durch Beobachter-Momente ersetzt." In der späteren Arbeit hat er das Phänomen des anthropischen Schattens beschrieben, eine Beobachtungsauswahlwirkung, die verhindert, dass Beobachter bestimmte Arten von Katastrophen in ihrer jüngsten geologischen und evolutionären Vergangenheit beobachten. Katastrophentypen, die im anthropischen Schatten liegen, werden wahrscheinlich unterschätzt, wenn keine statistischen Korrekturen vorgenommen werden. Simulationsargument Bostroms Simulationsargument zeigt, dass mindestens eine der folgenden Aussagen sehr wahrscheinlich wahr ist: Der Bruchteil der menschlichen Zivilisationen, die eine posthumane Phase erreichen, ist sehr nah an Null; der Bruchteil der posthumanen Zivilisationen, die an der Durchführung von Ahnensimulationen interessiert sind, ist sehr nahe an Null; der Bruchteil aller Menschen mit unserer Art von Erfahrungen, die in einer Simulation leben, ist sehr nahe bei einem.Ethik der menschlichen Verbesserung Bostrom ist für die "menschliche Verbesserung" oder "Selbstverbesserung und menschliche Vollkommenheit durch die ethische Anwendung der Wissenschaft" sowie eine Kritik an biokonservativen Ansichten günstig. Im Jahr 1998 gründete Bostrom (mit David Pearce) den Welttranshumanistischen Verein (der seitdem seinen Namen auf Humanity+ geändert hat). 2004 gründete er (mit James Hughes) das Institut für Ethik und Emerging Technologies, obwohl er nicht mehr an einer dieser Organisationen beteiligt ist. Bostrom wurde in der Liste der weltweit führenden Denker der Außenpolitik 2009 benannt, um keine Grenzen für das menschliche Potenzial zu akzeptieren." Mit dem Philosophen Toby Ord hat den Umkehrtest vorgeschlagen. Wie kann man angesichts des irrationalen Status des Menschen Quo-Bias zwischen gültigen Kritiken an vorgeschlagenen Veränderungen eines menschlichen Charakters und Kritiken unterscheiden, die nur durch Widerstand gegen Veränderung motiviert sind? Der Umkehrtest versucht, dies zu tun, indem er fragt, ob es eine gute Sache wäre, wenn die Trait in der entgegengesetzten Richtung verändert wurde. Technologiestrategie Er hat vorgeschlagen, dass die Technologiepolitik, die darauf abzielt, das existenzielle Risiko zu reduzieren, darauf abzielen sollte, die Ordnung, in der verschiedene technologische Fähigkeiten erreicht werden, zu beeinflussen und das Prinzip der differenziellen technologischen Entwicklung vorzuschlagen. Dieses Prinzip besagt, dass wir die Entwicklung gefährlicher Technologien, insbesondere derjenigen, die das Existenzrisiko erhöhen, verzögern und die Entwicklung von vorteilhaften Technologien beschleunigen sollten, insbesondere diejenigen, die vor den existenziellen Risiken der Natur oder anderer Technologien schützen. Bostroms Theorie der Unilateralisten-Kurse wurde als Grund für die wissenschaftliche Gemeinschaft genannt, um kontroverse gefährliche Forschung wie reanimierende Krankheitserreger zu vermeiden. Politik und Beratung Bostrom hat politische Beratung und Beratung für eine breite Palette von Regierungen und Organisationen. Er gab dem Haus der Herren Beweise, Select Committee on Digital Skills. Er ist beratendes Mitglied des Machine Intelligence Research Institute, Future of Life Institute, Foundational Questions Institute und externer Berater des Cambridge Centre for the Study of Existential Risk. Critical Rezeption Als Reaktion auf Bostroms Schreiben über künstliche Intelligenz schrieb Oren Etzioni in einem MIT Review Artikel: "Behauptungen, dass Superintelligenz am vorhersehbaren Horizont liegt, werden nicht von den verfügbaren Daten unterstützt." Professoren Allan Dafoe und Stuart Russell schrieben eine Antwort, die sowohl Etzionis Erhebungsmethodik als auch Etzionis Schlussfolgerungen bestreitet. Prospect Magazine verzeichnete Bostrom in ihrer 2014 Liste der Top Thinkers der Welt. Bibliographie Bücher 2002 – Anthropische Bias: Beobachtung Auswahleffekte in Wissenschaft und Philosophie, ISBN 0-415-93858-9 2008 – Globale katastrophale Risiken, herausgegeben von Bostrom und Milan M. Ćirković, ISBN 978-0-19-857050-9 2009 – Human Enhancement, herausgegeben von Bostrom und Julian Savulescu, ISBN 0-19-929972-2 2014 – Superintelligence "Wie lange vor Superintelligenz?". Journal of Future Studies.2.— (1999). " Das Doomsday Argument ist Alive und Kicking". Mind.108 (431:) 539–550. doi:10.1093/mind/108.431.539.JSTOR 2660095.— (Januar 2000)." Beobachtungs-relative Chancen in anthropischem Denken?". Erkenntnis.52 (1:) 93–108. doi:10.1023/A:1005551304409.JSTOR 20012969.S2CID 140474848.— (Juni 2001). " The Doomsday Argument, Adam & Eva, UN,+ and Quantum Joe".Synthese.127 (3:) 359–387. doi:10.1023/A:1010350925053.JSTOR 20141195.S2CID 36078450.- (Oktober 2001). "Das Meta-Newcomb-Problem".Analyse 61 (4:) 309–310.doi:10.1111/1467-8284.00310.JSTOR 3329010.— (März 2002)." Vorhandene Risiken: Analyse menschlicher Aussterbenszenarien und damit verbundener Gefahren".Journal of Evolution and Technology.9 (1).— (Dezember 2002). " Selbstbeweisender Glaube an große Welten: Kosmologie fehlender Link zur Beobachtung".Journal of Philosophy.99 (12:) 607–623.JSTOR 3655771. – (April 2003)."Sie leben in einer Computersimulation?" (PDF). Philosophisch Quarterly.53 (211:) 243–255. doi:10.1111/1467-9213.00309.JSTOR 3542867.- (2003)."Die Geheimnisse des Selbst-Locating Beliefs und Anthropic Reasoning" (PDF). Harvard Review of Philosophy.11 (Spring:) 59–74.doi:10.5840/harvardreview20031114.- (November 2003)." Astronomische Abfälle: The Opportunity Cost of Delayed Technological Development".Utilitas.15 (3:) 308–314.CiteSeerX 10.1.1.429.2849.doi:10.1017/S09538200004076.- (Mai 2005)."Die Fable of the Dragon-Tyrant".J Med00 Ethics.31 (5:) 273–277.doi:10. JSTOR 27719395.PMC 1734155. PMID 15863685.— (Juni 2005)." In Defense of Posthuman Dignity.Bioethics.19 (3:) 202–214.doi:10.1111/j.1467-8519.2005.00437.x.PMID 16167401.with Tegmark, Max (Dezember 2005). " Wie anders ist eine Doomsday-Katastrophe?".Nature.438 (7069:) 754. arXiv:astro-ph/0512204.Bibcode:2005Natur.438..754T doi:10.1038/438754a.PMID 16341005.S2CID 4390013.— (2006). " Was ist ein Singleton?". Linguistische und philosophische Untersuchungen.5 (2:) 48–54.— (Mai 2006)."Frage der Erfahrung: Brain-Duplikation und Grad des Bewußtseins" (PDF). Minds and Machines.16 (2:) 185–200.doi:10.1007/s11023-006-9036-0 S2CID 14816099.with Ord, Toby (Juli 2006). " The Reversal Test: Elimination Status Quo Bias in Applied Ethics" (PDF). Ethics.116 (4:) 656–680.doi:10.1086/505233.PMID 17039628.S2CID 12861892.with Sandberg, Anders (Dezember 2006). "Konvergente kognitive Verbesserungen" (PDF). Annals of the New York Academy of Sciences.1093 (1:) 201–207.Bibcode:2006NYASA1093.201S CiteSeerX 10.1.1.328.3853.doi:10.1196/annals.1382.015.PMID 17312260.S2CID 10135931.— (Juli 2007)." Schlafen Schönheit und Selbstortung: Ein hybrides Modell" (PDF). Synthese.157 (1:) 59–78.doi:10.1007/s11229-006-9010-7 JSTOR 27653543.S2CID 12215640.- (Januar 2008)." Drogen können verwendet werden, um mehr als Krankheit zu behandeln" (PDF). Natur.452 (7178:) 520.Bibcode:2008Natur.451..520B doi:10.1038/451520b.PMID 18235476.S2CID 4426990.- (2008)."Das doomsday Argument". Think.6 (17–18:) 23–28. doi:10.1017/S1477175600002943.— (2008). "Wo sind sie? Warum ich hoffe, dass die Suche nach außerirdischem Leben nichts findet" (PDF). Technology Review (Mai/Juni:) 72–77.mit Sandberg, Anders (September 2009). "Kognitive Verbesserung: Methoden, Ethik, Regulatory Challenges" (PDF). Wissenschaft und Technik Ethics.15 (3:) 311–341.CiteSeerX 10.1.1.143.4686.doi:10.1007/s11948-009-9142-5.PMID 19543814.S2CID 6846531.- (2009)."Pascal's Mugging" (PDF).Analysis.69 (3:) 443–445.06ana1093/ Anthropischer Schatten: Beobachtungsauswahleffekte und Human Extinction Risks" (PDF). Risikoanalyse.30 (10:) 1495–1506.doi:10.1111/j.1539-6924.2010.01460.x.PMID 20626690.- (2011)." Informationsgefahren: Eine Typologie von potentiellen Harms aus dem Wissen" (PDF). Review of Contemporary Philosophy.10: 44–79.ProQuest 920893069.Bostrom, Nick (2011)." DIE ARTIFIKEL DER ARTIFISCHEN INTELLIGENCE" (PDF). Cambridge Handbuch der künstlichen Intelligenz. Archiviert vom Original (PDF) am 4. März 2016. Retrieved 13. Februar 2017. Bostrom, Nick (2011)."Infinite Ethics" (PDF). Analyse und Metaphysik.10: 9–59.— (Mai 2012). " Der Superintelligente Wille: Motivation und Instrumental-Rationalität in fortgeschrittenen Künstlichen Agenten" (PDF). Minds and Machines.22 (2:) 71–84.doi:10.1007/s11023-012-9281-3 S2CID 7445963.with Shulman, Carl (2012). "Wie schwer ist KI? Evolutionäre Argumente und Auswahleffekte" (PDF). Journal of Consciousness Studies.19 (7–8:) 103–130.with Armstrong, Stuart; Sandberg, Anders (November 2012)." Im Inneren der Box denken: Steuerung und Nutzung von Oracle AI" (PDF). Minds and Machines.22 (4:) 299–324.CiteSeerX 10.1.1.396.799.doi:10.1007/s11023-012-9282-2.S2CID 9464769.- (Februar 2013)."Existential Risk Reduction as Global Priority".Global Policy.4 (3:) 15–31.doi:10.1111/1758-5899.12002. Embryo-Auswahl für kognitive Verbesserung: Kuriosität oder Game-changer?"(PDF).Global Policy.5 (1:) 85–92.CiteSeerX 10.1.1.428.8837.doi:10.1111/1758-5899.123.with Muehlhauser, Luke (2014). " Warum brauchen wir freundliche KI" (PDF).Think.13 (36:) 41–47.doi:10.1017/S1477175613000316.Bostrom, Nick (September 2019). "The Vulnerable World Hypothesis" (PDF).Global Policy.10 (4:) 455–476.doi:10.1111/1758-5899.12718.Archiviert vom Original (PDF) am 27. Juni 2020. Siehe auch Referenzen Externe Links Offizielle Website Superintelligence: Paths, Dangers, Strategies Bostroms Anthropic Principle Website, die Informationen über das anthropische Prinzip und das Doomsday Argument enthält. Online-Kopier von Buch, "Anthropic Bias: Observation Selection Effects in Science and Philosophy" (HTML, PDF)Bostroms Simulation Argument Website Bostroms Existential Risk Website Nick Bostrom bei IMDb Nick Bostrom interviewt auf der TV-Show Triangulation auf dem TWiT.tv-Netzwerk Nick Bostrom bei TED Die 10 Torhüter der Menschheit gegen die Risiken von AI, Hot Topics 2015The A.I Angst Die Washington Post, 27. Dezember 2015.