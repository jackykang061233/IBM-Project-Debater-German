Grammarische Induktion (oder grammatische Inferenz) ist der Prozess beim maschinellen Erlernen einer formalen Grammatik (in der Regel als Sammlung von Re-Write Regeln oder Produktionen oder alternativ als endliche Zustandsmaschine oder Automatisierung irgendeiner Art) aus einer Reihe von Beobachtungen zu lernen, so dass ein Modell erstellt wird, das die Eigenschaften der beobachteten Objekte berücksichtigt. Generell ist die grammatische Inferenz die Branche des maschinellen Lernens, wo der Instanz-Raum aus diskreten kombinatorischen Objekten wie Strings, Bäumen und Graphen besteht. Grammatikalische Inferenzen haben sich oft sehr auf das Problem des Lernens von endlichen staatlichen Maschinen verschiedener Art konzentriert (siehe Artikel Induktion von regulären Sprachen für Details zu diesen Ansätzen), da es seit den 1980er Jahren effiziente Algorithmen für dieses Problem gab. Seit Anfang des Jahrhunderts wurden diese Ansätze auf das Problem der Inferenz von kontextfreien Grammatiken und reicheren Formalismen erweitert, wie mehrere kontextfreie Grammatiken und parallele multiple kontextfreie Grammatiken. Andere Grammatikklassen, für die eine grammatische Inferenz untersucht wurde, sind kombinatorische Kategorialgramme, stochastische kontextfreie Grammatik, kontextuelle Grammatik und Mustersprachen. Lernmodelle Die einfachste Form des Lernens ist, wo der Lernalgorithmus lediglich eine Reihe von Beispielen erhält, die aus der betreffenden Sprache gezogen werden: Ziel ist es, die Sprache aus Beispielen zu lernen (und selten aus Gegenbeispielen, also beispielsweise, die nicht zur Sprache gehören). Andere Lernmodelle wurden jedoch untersucht. Eine häufig erforschte Alternative ist der Fall, dass der Lernende Mitgliedschaftsanfragen wie im exakten Abfrage-Lernmodell oder minimal adäquatem Lehrermodell von Angluin stellen kann. Methoden Es gibt eine Vielzahl von Methoden für die grammatische Inferenz. Zwei der klassischen Quellen sind Fu (1977) und Fu (1982). Duda, Hart & Stork (2001) widmen sich dem Problem auch einen kurzen Abschnitt und nennen eine Reihe von Referenzen. Im Folgenden wird die von ihnen vorhandene grundlegende Test-und-Fehler-Methode diskutiert. Für Ansätze, insbesondere Unterklassen von regulären Sprachen zu unterziehen, siehe Induktion regelmäßiger Sprachen. Ein neueres Lehrbuch ist de la Higuera (2010), das die Theorie der grammatischen Inferenz von regulären Sprachen und endlichen Staatsautomaten abdeckt. D'Ulizia, Ferri und Grifoni liefern eine Umfrage, die grammatische Inferenzmethoden für natürliche Sprachen untersucht. Die in Abschnitt 8.7 von Duda, Hart & Stork (2001) vorgeschlagene Methode schlägt vor, Grammatikregeln (Produktionen) nacheinander zu erraten und gegen positive und negative Beobachtungen zu testen. Der Regelsatz wird erweitert, um jedes positive Beispiel erzeugen zu können, aber wenn ein bestimmter Regelsatz auch ein negatives Beispiel erzeugt, muss er verworfen werden. Dieser spezielle Ansatz kann als "Hypothesetest" gekennzeichnet werden und trägt eine Ähnlichkeit mit Mitchels Versionsraumalgorithmus. Der Text Duda, Hart & Stork (2001) bietet ein einfaches Beispiel, das den Prozess schön illustriert, aber die Machbarkeit eines solchen ungeführten Trial-and-Eror-Ansatzes für wesentliche Probleme ist zweifelhaft. Grammatische Inferenz durch genetische Algorithmen Grammatische Induktion mit evolutionären Algorithmen ist der Prozess der Entwicklung einer Darstellung der Grammatik einer Zielsprache durch einen evolutionären Prozess. Formale Grammatiken können leicht als Baumstrukturen von Produktionsregeln dargestellt werden, die evolutionären Operatoren unterworfen werden können. Solche Algorithmen stammen aus dem genetischen Programmierparadigma, das von John Koza Pionier war. Andere frühe Arbeiten an einfachen formalen Sprachen nutzten die binäre String-Darstellung genetischer Algorithmen, aber die inhärent hierarchische Struktur der Grammatik, die in der EBNF-Sprache galt, machte Bäume zu einem flexibleren Ansatz. Koza vertreten Lisp Programme als Bäume. Er war in der Lage, Analoga zu den genetischen Operatoren innerhalb der Standard-Set von Baumoperatoren zu finden. Zum Beispiel ist das Überschlagen von Teilbäumen dem entsprechenden Prozess der genetischen Überkreuzung gleichwertig, wobei Teilstrings eines genetischen Codes in ein Individuum der nächsten Generation transplantiert werden. Die Fitness wird gemessen, indem die Ausgabe aus den Funktionen des Lisp-Codes abgetastet wird. Ähnliche Analoga zwischen der strukturierten Lisp-Darstellung und der Darstellung von Grammatik als Bäumen ermöglichten die Anwendung genetischer Programmiertechniken für die Grammatikinduktion. Im Falle der Grammatikinduktion entspricht die Transplantation von Unterbäumen dem Abschlagen von Produktionsregeln, die die Parsierung von Phrasen aus einer Sprache ermöglichen. Der Fitness-Operator für die Grammatik basiert auf einigen Maßen, wie gut es bei der Parsierung einiger Sätze aus der Zielsprache durchgeführt. In einer Baumdarstellung einer Grammatik entspricht ein Endsymbol einer Produktionsregel einem Blattknoten des Baumes. Seine Elternknoten entsprechen einem nicht-terminalen Symbol (z.B. einem Noun-Satz oder einem Verb-Satz) im Regelsatz. Letztendlich könnte der Wurzelknoten einem Satz nicht-terminal entsprechen. Grammatikalische Inferenz durch gierige Algorithmen Wie alle gierigen Algorithmen treffen gierige Grammatik-Inferenzalgorithmen auf iterative Weise Entscheidungen, die in dieser Phase am besten zu sein scheinen. Die getroffenen Entscheidungen betreffen in der Regel Dinge wie die Schaffung neuer Regeln, die Beseitigung bestehender Regeln, die Wahl einer anzuwendenden Regel oder die Verschmelzung einiger vorhandener Regeln. Da es mehrere Möglichkeiten gibt, "die Bühne" und "die beste" zu definieren, gibt es auch mehrere gierige Grammatik-Inferenzalgorithmen. Diese kontextfreien Grammatik-Generierungsalgorithmen entscheiden nach jedem Lesesymbol: Lempel-Ziv-Welch-Algorithmus erzeugt eine kontextfreie Grammatik in einer bestimmten Weise, so dass es notwendig ist, nur die Startregel der erzeugten Grammatik zu speichern. Sequitur und seine Modifikationen. Diese kontextfreien Grammatik-Generierungsalgorithmen lesen zunächst die gesamte vorgegebene Symbolfolge und beginnen dann, Entscheidungen zu treffen: Byte Paar Codierung und seine Optimierungen. Verteiltes Lernen Ein neuerer Ansatz basiert auf dem Verteilungslernen. Algorithmen mit diesen Ansätzen wurden angewandt, um kontextfreie Grammatik und leicht kontextsensitive Sprachen zu lernen und haben sich als richtig und effizient für große Unterklassen dieser Grammatik erwiesen. Das Erlernen von Mustersprachen Angluin definiert ein Muster, das "eine Reihe von konstanten Symbolen aus Σ und variablen Symbolen aus einem disjoint set" sein soll. Die Sprache eines solchen Musters ist der Satz aller seiner unbelasteten Bodeninstanzen, d.h. aller Strings, die durch konsequente Ersetzung seiner variablen Symbole durch unbelastete Zeichenketten konstanter Symbole entstehen. Ein Muster wird als beschreibend für einen endlichen Eingabesatz von Strings bezeichnet, wenn seine Sprache minimal ist (bezogen auf die eingestellte Inklusion) unter allen Mustersprachen, die den Eingabesatz subsumieren. Angluin gibt einen Polynomalgorithmus, um für einen bestimmten Eingabestring-Set alle beschreibenden Muster in einer Variablen x zu berechnen. Zu diesem Zweck baut sie einen Automaten, der alle möglicherweise relevanten Muster repräsentiert; unter Anwendung ausgeklügelter Argumente über Wortlängen, die auf x die einzige Variable sind, kann die Zustandszahl drastisch reduziert werden. Erlebach et al.give eine effizientere Version des Musterlernalgorithmus von Angluin sowie eine parallelisierte Version. Arimura et al. zeigen, dass eine Sprachklasse, die aus begrenzten Strukturen von Mustern gewonnen wird, in polynomischer Zeit gelernt werden kann. Mustertheorie Musterlehre, formuliert von Ulf Grenander, ist ein mathematischer Formalismus, um das Wissen der Welt als Muster zu beschreiben. Es unterscheidet sich von anderen Ansätzen zur künstlichen Intelligenz, indem es nicht beginnt, Algorithmen und Maschinen zu beschreiben, Muster zu erkennen und zu klassifizieren; vielmehr schreibt es einen Vokabular, die Musterkonzepte in präziser Sprache zu artikulieren und neu zu gestalten. Neben dem neuen algebraischen Vokabular war sein statistischer Ansatz neu, um die versteckten Variablen eines Datensatzes mit realen Weltdaten zu identifizieren, anstatt künstliche Stimuli, die damals gemeinsam waren. Legen Sie Vorverteilungen für versteckte Variablen und Modelle für die beobachteten Variablen vor, die die Vertiken eines Gibbs-ähnlichen Graphen bilden. Studieren Sie die Zufalls- und Variabilität dieser Grafiken. Erstellen Sie die Grundklassen der stochastischen Modelle, die durch die Auflistung der Verformungen der Muster angewendet werden. Synthesize (Proben) von den Modellen, nicht nur Signale mit ihm analysieren. Breite in seiner mathematischen Erfassung, Mustertheorie erstreckt sich Algebra und Statistiken, sowie lokale topologische und globale entrope Eigenschaften. Anwendungen Das Prinzip der Grammatikinduktion wurde auf andere Aspekte der natürlichen Sprachverarbeitung angewendet und (unter vielen anderen Problemen) auf semantisches Parsing, natürliches Sprachverständnis, exemplarische Übersetzung, Morphemanalyse und Ortsnamenableitung angewendet. Grammar Induktion wurde auch für verlustfreie Datenkompression und statistische Inferenz über minimale Nachrichtenlänge (MML) und Mindestbeschreibungslänge (MDL) Prinzipien verwendet. Grammar Induktion wurde auch in einigen probabilistischen Modellen des Spracherwerbs verwendet. Siehe auch Künstliches Grammatiklernen#Künstliche Intelligenz Beispielbasierte maschinelle Übersetzung Induktive Programmierung Kolmogorov Komplexität Spracherkennung in der Grenze Geraden Grammatik Syntaktische Mustererkennung Anmerkungen Quellen Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001,) Musterklassifikation (2 ed,.) New York: John Wiley & Sons Fu, King Sun (1982,) Syntactic Pattern Recognition and Applications, Englewood Cliffs, NJ: Prentice-Hall Fu, King Sun (1977), Syntactic Pattern Recognition, Applications, Berlin: Springer-Verlag Horning, James Jay (1969,) A Study of Grammatical Inference (Ph.D Thesis ed.) Stanford: Stanford Universität Informatik Department, ProQuest 302483145 Gold, E. Mark (1967,) Sprachidentifikation in the Limit, 10, Information and Control, pp.447–474, archiviert vom Original auf 2016-08-28, abgerufen 2016-09-04 Gold, E. Mark (1967) Sprachidentifikation in the Limit (PDF,) 10, Information and Control, pp.447–474