Concurrent Computing ist eine Art von Computern, bei denen mehrere Berechnungen gleichzeitig ausgeführt werden – die Überschneidungszeiträume – statt sequential – mit einem Abschluss vor dem nächsten Start. Es handelt sich um ein System – ob ein Programm, ein Computer oder ein Netz – wo für jeden Prozess ein gesonderter Ausführungspunkt oder eine "Ethread of Control" besteht. Ein gleichzeitiges System ist eines, bei dem eine Berechnung vorangehen kann, ohne auf alle anderen Berechnungen warten zu müssen. Concurrent Computing ist eine Form der modularen Programmierung. In ihrem Paradigmen wird eine Gesamtberechnung in Subkomputationen berechnet, die gleichzeitig ausgeführt werden können. Pioneers im Bereich der gleichzeitigen Rechentechnik umfassen Edsger Dijkstra, Per BrZoll Hansen und C.A.R Hoare. Einleitung Das Konzept des ständigen Rechenverfahrens ist häufig mit dem verwandten, aber gesonderten Konzept des Parallel-".s verwechselt, obwohl beide als "multiple Process" bezeichnet werden können, die während derselben Zeit durchgeführt werden. Parallelprozessor ist die Ausführung am gleichen physischen Moment: zum Beispiel bei getrennten Prozessoren einer Multiprozessor-Maschine, mit dem Ziel, die Rechengeschwindigkeiten zu beschleunigen –parallel-). ist für einen (einen Kern) einzigen Prozessor unmöglich, da nur eine Berechnung sofort (während eines einzigen Uhrzyklus) erfolgen kann. Kontrastlich besteht das gleichzeitige Rechen aus Prozessüberschneidungen, aber die Ausführung muss nicht gleichzeitig erfolgen. Hier geht es darum, die Prozesse in der Außenwelt zu modellieren, die gleichzeitig passieren, wie mehrere Kunden, die gleichzeitig einen Server benutzen. Strukturierung von Softwaresystemen, die aus mehreren gleichzeitigen Quellen bestehen, können Teile für die Bewältigung der Komplexität nützlich sein, unabhängig davon, ob die Teile parallel ausgeführt werden können. So können z.B. die laufenden Prozesse an einem Kern durch die Verknüpfung der Ausführungsschritte der einzelnen Prozesse über die Zeitspanne hinweg ausgeführt werden: nur ein Prozess läuft zu einem Zeitpunkt, und wenn er während seiner Ausführung nicht vollständig abgeschlossen ist, wird es pädagogisch, ein anderer Prozess beginnt oder wiederaufgenommen, und später wird der ursprüngliche Prozess wieder aufgenommen. In diesem Sinne sind mehrere Prozesse durch Hinrichtung an einer einzigen Stelle, aber nur ein Prozess wird sofort ausgeführt. Parallele Berechnungen können z.B. durch die Zuweisung jedes Verfahrens an einen separaten Prozessor oder Prozessorkern oder die Verbreitung einer Berechnung über ein Netz erfolgen. Insgesamt könnten die Sprachen, Werkzeuge und Techniken der Parallelprogrammierung jedoch nicht für die laufende Programmierung geeignet sein und umgekehrt. Die genaue Zeit, wann Aufgaben in einem laufenden System ausgeführt werden, hängt von der Planung ab, und die Aufgaben müssen nicht immer gleichzeitig ausgeführt werden. T1 und T2: T1 können vor T2 oder umgekehrt (serial und sequential) T1 und T2 abwechselnd ausgeführt werden (serial und gleichzeitig) T1 und T2 können gleichzeitig (parallel und gleichzeitig) ausgeführt werden. Das Wort sequential wird sowohl als auch parallel als Abkürzung verwendet; wenn dies ausdrücklich Unterscheidungskraft sind, werden gleichzeitige/sequentielle und parallele/seriale als opposing Paar verwendet. Ein Zeitplan, in dem Aufgaben ausgeführt werden, die eine zu einem Zeitpunkt (serily, no Parallelismus), ohne Interleaing (sequent, no conwährung: keine Aufgabe beginnt bis zum Ablauf der vorherigen Aufgabe) auszuführen, wird als Serienplan bezeichnet. Es handelt sich um eine Reihe von Aufgaben, die für die Serienfertigung vorgesehen werden können. Koordinierung des Zugangs zu gemeinsamen Ressourcen Hauptherausforderung bei der Konzipierung von Programmen ist die Währungskontrolle: Gewährleistung der korrekten Sequenzierung der Interaktionen oder Mitteilungen zwischen verschiedenen rechnerischen Hinrichtungen und Koordinierung des Zugangs zu Ressourcen, die bei Hinrichtungen geteilt werden. Potenzielle Probleme sind z.B. Rennbedingungen, Sackgassen und Ressourcenschwund. Man denke zum Beispiel an den folgenden Algorithmus, aus einem Kontrollkonto, das durch die gemeinsame Ressourcenbilanz repräsentiert ist, zurückzuziehen: Ausgleich = 500, und zwei gleichzeitige Gewinde machen die Anrufe zurück (300) und nehmen(350) zurück. Wenn die Linie 3 in beiden Operationen vor der Linie 5 ausgeführt wird, werden beide Operationen feststellen, dass ausgewogenes Verhältnis= Rücknahmebewertungen zu wahren ist und die Ausführung den Rücktrittsbetrag verlängern wird. Da beide Prozesse ihre Rücknahmen durchführen, wird der zurückgezogene Gesamtbetrag mehr als das ursprüngliche Gleichgewicht beenden. Diese Arten von Problemen mit gemeinsamen Ressourcen profitieren von der Verwendung von Währungskontrollen oder von nicht blockierten Algorithmen. Vorteile Zu den Vorteilen des gleichzeitigen Rechens gehören: Mehr Programmdurchführung –parallel Ausführung eines laufenden Programms ermöglicht es, die Anzahl der in einer bestimmten Zeit abgeschlossenen Aufgaben proportional zur Anzahl der Verarbeiter nach dem Gesetz von Gustafson zu erhöhen, die hohe Reaktionsfähigkeit auf Input/Input/Out-intensive Programme vor allem auf Input- oder Output-Operationen abzuwarten. Kontinuierliche Programmierung erlaubt die Zeit, die für eine andere Aufgabe genutzt werden soll. Mehr geeignete Programmstruktur – einige Probleme und Problembereiche sind gut geeignet, um sich als gleichzeitige Aufgaben oder Prozesse anzusehen. Modelle PET-Netze, die 1962 eingeführt wurden, waren ein frühzeitiger Versuch, die Regeln der laufenden Ausführung zu kodifizieren. Datenflusstheorie, die später auf diesen basiert, und Dataflow-Architekturen wurden geschaffen, um die Ideen der Datenflusstheorie physisch umzusetzen. In den späten siebziger Jahren wurden Prozesskalkulien wie Calculus of Communicating Systems (CCS) und Communicating Sequentialprozesse (CSP) entwickelt, um zu ermöglichen, kalkulative Gründe für Systeme, die aus Interaktionskomponenten bestehen, zu erkennen. Das .-calité fügte die Fähigkeit hinzu, dynamische Topologien zu begründen. Input/Out automata wurde 1987 eingeführt. Logiken wie die TLA von Lamport,+ und mathematische Modelle wie Spuren und Spielpläne wurden ebenfalls entwickelt, um das Verhalten der laufenden Systeme zu beschreiben. Software-Transaktionsspeicherkredite aus der Datenbanktheorie das Konzept der Atomtransaktionen und gelten für Speicherzugange. Konsequente Modelle Concurrent Programmiersprachen und Multiprozessor-Programme müssen ein einheitliches Modell aufweisen (auch als Gedächtnismodell bekannt). Das Konsistenzmodell legt Regeln für die Art und Weise fest, wie Transaktionen auf Computerspeichern auftreten und wie Ergebnisse hergestellt werden. Eines der ersten Modelle der Konsistenz war das fortschreitende Konsistenzmodell von Marion Lamport. Sequentielle Konsistenz ist das Eigentum eines Programms, das seine Ausführung als Folgeprogramm die gleichen Ergebnisse hervorbringt. Konkret ist ein Programm folgerichtig konsistent, wenn "die Ergebnisse jeder Ausführung die gleiche sind wie, wenn die Operationen aller Verarbeiter in gewisser Reihenfolge ausgeführt wurden und die Operationen jedes einzelnen Verarbeiters in dieser Reihenfolge in der im Programm genannten Reihenfolge erscheinen. Durchführung einer Reihe unterschiedlicher Methoden können verwendet werden, um parallele Programme durchzuführen, wie z.B. die Durchführung jeder rechnerischen Ausführung als Betriebssystem oder die Durchführung der rechnerischen Prozesse als eine Reihe von Gewinden innerhalb eines einzigen Betriebssystems. Interaktion und Kommunikation In einigen gleichzeitigen Rechensystemen ist die Kommunikation zwischen den gleichzeitigen Komponenten vom Programmträger (z.B. durch Nutzung von Zukunften) verborgen, während in anderen Bereichen ausdrücklich behandelt werden muss. Sondierungskommunikation kann in zwei Klassen unterteilt werden: Gemeinsame Speicherkommunikation Concurrent Komponenten kommunizieren durch eine Änderung des Inhalts der gemeinsamen Speicherstandorte (von Java und C#). In dieser Form der gleichzeitigen Programmierung muss in der Regel eine gewisse Form der Verriegelung (z.B. Mutexes, semaphores oder Monitore) verwendet werden, um zwischen den Fadenn zu koordinieren. Ein Programm, das eine dieser Programme ordnungsgemäß umgesetzt hat, wird als färötliche Sicherheit bezeichnet. Kommunikation Kontinuierliche Komponenten kommunizieren durch den Austausch von Nachrichten (Exmplifizierte durch den Rahmen, Go, Italia, Erlang und chinam). Der Austausch von Nachrichten kann als synchron ausgeführt werden oder kann einen synchronen rendezvous Stil verwenden, in dem der Absender bis zur Empfangsmeldung die Signale erhält. Asynchrone Botschaft kann zuverlässig oder unzuverlässig sein (in manchen Fällen, die als "send and be" bezeichnet werden). Nachrichtenpassing conwährung ist tendenziell wesentlich einfacher als die gemeinsame Währung und wird in der Regel als robustere Form der laufenden Programmierung angesehen. Es gibt eine Vielzahl mathematischer Theorien, um die Signal-Passagiersysteme zu verstehen und zu analysieren, einschließlich des Modells des Schauspielers und verschiedener Prozesse kalkuli. Nachrichtenübermittlungen können effizient über symmetrische Multiverarbeitung mit oder ohne gemeinsame Speicher-Kohäsion durchgeführt werden. Gemeinsamer Gedächtnis und Botschaft von Konwährung haben unterschiedliche Leistungsmerkmale. In der Regel (obwohl nicht immer), ist die proprozessale Speicherüberlastung und der Wechsel von Overhead in einem Übertragungssystem niedriger, aber die Überleitung der Botschaft ist größer als bei einem Verfahrensanruf. Diese Unterschiede werden oft durch andere Leistungsfaktoren überfordert. Geschichte Concurrent Computing entwickelte sich von früheren Arbeiten an Eisenbahnen und Telegraphie, vom 19. und frühen 20. Jahrhundert, und einige Bedingungen für diesen Zeitraum, wie z.B. Man könnte sich mit der Frage befassen, wie mehrere Züge auf dem gleichen Eisenbahnsystem (Vermeidung von Kollisionen und Maximierung der Effizienz) zu handhaben sind und wie mehrere Übertragungen über eine Reihe von Drähten (Verbesserung der Effizienz) wie z.B. über die Zeitspanne Multiplexing (1870s) abgewickelt werden können. In den 60er-Jahren begann die akademische Studie der anhängigen Algorithmen, wobei Dijkstra (1993) mit dem ersten Papier auf diesem Gebiet die gegenseitige Ausgrenzung erkennen und lösen konnte. Prevalence Conwährung ist pervasives Computer, das von kohlenstoffarmen Hardware auf einem einzigen Chip bis zu weltweiten Netzen ausgeht. Beispiele folgen. Programmierungssprache: Kanal Coroutine Futures und Versprechen Betriebssystem: Computer Multitasking, einschließlich kooperativer Multitaskings und vorbeugender Multitasking Timesharing, die die sequentielle Chargenverarbeitung von Arbeitsplätzen mit gleichzeitiger Nutzung eines System-Zyklus ersetzen netzgebundene Systeme werden in der Regel durch ihre Natur betrieben, da sie aus verschiedenen Geräten bestehen. Sprachen, die gleichzeitige Programmierung von Concurrent Programmierungssprachen unterstützen, sind Programmiersprachen, die Sprachaufbauen für Währungen verwenden. Mehrsprachigkeit, Unterstützung für verteiltes Rechen, Nachrichtenübermittlung, gemeinsame Ressourcen (einschließlich gemeinsamer Speicher) oder Zukunftsaussichten. Diese Sprachen werden manchmal als konWährungsorientierte Sprachen oder konWährungsorientierte Programmierungssprachen (COPL) bezeichnet. Heute sind die am häufigsten verwendeten Programmierungssprachen, die spezielle Konwährungsstrukturen aufweisen, Java und C#.Both dieser Sprachen verwenden ein gemeinsames Währungsmodell, das von Monitoren bereitgestellt wird (obwohl nachrichtenpassierende Modelle neben dem zugrunde liegenden gemeinsamen Modell durchgeführt werden können und umgesetzt wurden). Von den Sprachen, die ein nachrichtenpassierendes Währungsmodell verwenden, ist Erlang wahrscheinlich die am häufigsten in der Industrie eingesetzten Sprachen. Viele gleichzeitige Programmiersprachen wurden mehr entwickelt, als Forschungssprachen (z.B. Pict) anstatt als Sprachen für die Produktion. Sprachen wie Erlang, Limbo und chinam haben in den letzten 20 Jahren jedoch unterschiedliche industrielle Verwendungen erlebt. eine nicht erschöpfende Liste von Sprachen, die gleichzeitige Programmierungseinrichtungen verwenden oder anbieten: Ada – Hauptzweck, mit einheimischer Unterstützung für die Erreichung und Überwachung von Konwährung Alef – zusammen mit Gewinden und Nachrichtenübermittlung, für Systemplanung in frühen Versionen des Plans 9 von Bell Alice Labs – Verlängerung der Norm ML, ergänzt die Unterstützung von Konwährungen über künftige Ateji PX – Verlängerungen auf Java mit parallelen Variablen, die von .-calité Axum-Domal inspiriert sind – – auf der Grundlage von Modell und . NETCommon Language Runtime mit einer C-ähnlichen Mischung BMD –FM –Binary modular DataFlow Maschinen C++std::threadC SCO (C Omega) – for research, C,# verwendet asynchrone Kommunikation C# – support concurrent Computing mit Schleusung, Rendite, auch seit Version 5.0 aynynynynyn und absehbare Stichwörter, die Clojure modern, funktionelle Lisp auf der Java-Plattform Concurrent Programme, ähnlich wie Haskell Concurrent Sammlungen (CnC)Avesplicitismus unabhängiger Speichermodell durch Definition von Standard- und Standard-Standards, Erlang – Nutzung asynchroner Botschaft, die mit nichts geteilter FAUSTreal-time-Funktion verfolgt wird, für die Signalverarbeitung, die Zusammenlegung von Zusammenschlüssen über OpenMP oder einen bestimmten Arbeitsplaner Fortran –coarrays und do concurrent sind Teil der Standard Goran 2008 für Systemplanung, mit einem auf CSP Haskell basierenden Modell und parallele Programmierung Hume-Funktion, gleichzeitig für bindende Raum- und Zeiträume, in denen automatische Prozesse beschrieben werden; Aufgaben, ayn-wait, Kanäle. " JavaScript –via Web-Arbeitnehmer, in einem Browserumfeld, Versprechen und Rückschläge. JoCaml – laufender und verteilter Kanal, Ausweitung von OCaml, führt das Zusammenwirken von Prozessen an Java – auf Basis von Java Sprache Joule –dataflow-basierter Kommunikationsmittel durch Botschaft von Joyceconcurrent, Lehre, gebaut auf Concurrent Pascal mit Merkmalen von CSP von Per BrZoll Hansen Lab (grafal, Datenfluss, Funktionen sind Knoten in einem Diagramm, Daten sind zwischen den Knoten; enthält eine objektorientierte Sprache Limborelative Alef, Programmierung für Multifunktionale System (Living System). Systemvariante verlängert, um Parallelismus Modula-2 – für die Systemplanung von N. Wirth als Nachfolger von Pascal mit einheimischer Unterstützung für Koroutinen Modula-3 – ein modernes Mitglied der Algol-Familie mit umfangreicher Unterstützung für Gewinde, Mutexes, Bedingungsvariablen Newsqueakfor Research, mit Kanälen als erstklassige Werte; Vorgänger von Alef chinam-influenced stark durch sequentiale Prozesse (CSP) azzomπa moderne Variante von Milm, die Ideen von Oz-Mex-Mex-Mexiko, Konkursiv-Mex-Mode-Mod-S-Mod-S-S- Oz-S-Sens umfasst. Pict – in erster Linie eine exekutable Umsetzung von Milner's .-calité Raku umfasst Klassen für Faden, Versprechen und Kanäle durch Standard-Chips – Verwendungen von Parallelismus und prozessbasierten Parallelismus Reiauses als synchronisierte Botschaft, die zwischen gemeinsamen und nicht genutzten Gegenständen Red/System für die Systemprogrammierung auf der Grundlage von Rebol Rustfor Systemprogrammierung – unter Verwendung von Nachrichtenpassing mit beweglichen semanten, geteiltem Gedächtnis und gemeinsamem Gedächtnis. Italia – allgemeines Ziel, gemeinsame Programmierungsmuster in einer knappen, eleganten und typssicheren Art von SequenceL – allgemeine Zweckfunktion, die wichtigsten Gestaltungsziele sind einfache Programmierung, Codeklarheit und automatische Parallelisierung für die Leistung auf Multicore-Hardware, und nachweisbar frei von Wettbedingungen SR für Forschung SuperPascal –concurrent, für den Unterricht, auf Concurrent Pascal und Joyce by Per BrZoll Hansencon – für die Entwicklung von TNSDL für Telekommunikation, Verwendung als SMS-Nachrichten (VIE) auf Basis von Standard-Sprachen (VIE) Siehe auch AsynI/O Chuspace Flow-basierte Programmierung Java ConcurrentMap Liste wichtiger Veröffentlichungen in gleichzeitiger, paralleler und verteilter Rechen-Ptolemy-Projekt Rasse § She Sheaf (mathematics)Beschreibungen Quellen Weitere Lesung Dijkstra, E. W. "Lösung eines Problems bei gleichzeitiger Programmierungskontrolle". Mitteilungen der ACM.8 (9): 569. doi:10.1145/365559.365617.S2CID 19357737.Herlihy, Maurice (2008) [2008]. Art der Multiprozessor-Programmierung. Morgan Kaufmann.ISBN, S.123705914. Downey, Allen B. (2005) [2005]. The Little Book of Semaphores (PDF). Green Tea Press.ISBN: Auf der Website 2016-03-04. Retrieved 2009-11-21.Filman, Robert E; Daniel P. Friedman (1984). Koordinierung Computer: Werkzeuge und Techniken für verteilte Software. New York: McGraw-Water.p 370.ISBNUNG DES0-07-022439-1.Leppäinka, Jouni (2008). Eine pragmatische, historisch ausgerichtete Umfrage zur Universalität von Synchronisierungseinheiten (PDF). Universität Oulu. Blumenfeld, Gadi (2006). Synchronisierung Algorithms und Concurrent Programmierung. Pearson / Prentice Hall.p 433.ISBN UV0-13-197259-9. Externe Links Medien im Zusammenhang mit der laufenden Programmplanung bei der Wikimedia Commons Concurrent Systems Virtual Library