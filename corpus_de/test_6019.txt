In der Wahrscheinlichkeitstheorie und -statistik ist Varianz die Erwartung der quadratischen Abweichung einer Zufallsgröße von ihrem Mittelwert. Varianz ist ein Maß für Dispersion, d.h. es ist ein Maß dafür, wie weit sich eine Anzahl von Zahlen aus ihrem Durchschnittswert ausbreitet. Varianz hat eine zentrale Rolle in der Statistik, wo einige Ideen, die es verwenden, beschreibende Statistiken, statistische Inferenz, Hypothesentests, Güte der Passform und Monte Carlo Probenahmen. Varianz ist ein wichtiges Instrument in den Wissenschaften, wo die statistische Analyse der Daten gemeinsam ist. Die Varianz ist das Quadrat der Standardabweichung, das zweite zentrale Moment einer Verteilung, und die Kovarianz der Zufallsvariablen mit sich selbst, und es wird oft durch σ 2 {\displaystyle \sigma {^2}, s 2 {\displaystyle s^2}, Var yon (X)\displaystyle \operatorname {Var} (X}, V} (X) Ein Vorteil der Varianz als Dispersionsmaß ist, dass es gegenüber algebraischer Manipulation besser geeignet ist als andere Dispersionsmaße, wie die erwartete absolute Abweichung; beispielsweise ist die Varianz einer Summe unkorrelierter Zufallsgrößen gleich der Summe ihrer Varianzen. Nachteilig an der Varianz für praktische Anwendungen ist, dass sich ihre Einheiten im Gegensatz zur Standardabweichung von der Zufallsgröße unterscheiden, weshalb die Standardabweichung nach Beendigung der Berechnung häufiger als Maß für Dispersion gemeldet wird. Es gibt zwei verschiedene Konzepte, die beide Varianz genannt werden". Eine, wie oben erläutert, ist Teil einer theoretischen Wahrscheinlichkeitsverteilung und wird durch eine Gleichung definiert. Die andere Varianz ist charakteristisch für eine Reihe von Beobachtungen. Wenn aus Beobachtungen Varianz berechnet wird, werden diese Beobachtungen typischerweise aus einem realen Weltsystem gemessen. Sind alle möglichen Beobachtungen des Systems vorhanden, so wird die berechnete Varianz als Bevölkerungsvarianz bezeichnet. Normalerweise ist jedoch nur eine Untermenge verfügbar, und die daraus berechnete Varianz wird als Mustervarianz bezeichnet. Die aus einer Stichprobe errechnete Varianz wird als Schätzung der Gesamtpopulationsvarianz betrachtet. Es gibt mehrere Möglichkeiten, eine Schätzung der Bevölkerungsvarianz zu berechnen, wie im Abschnitt unten diskutiert. Die beiden Arten von Varianz sind eng miteinander verbunden. Um zu sehen, wie, denken Sie, dass eine theoretische Wahrscheinlichkeitsverteilung als Generator hypothetischer Beobachtungen verwendet werden kann. Werden mit einer Verteilung eine unendliche Anzahl von Beobachtungen erzeugt, so entspricht die aus diesem unendlichen Satz berechnete Probenvarianz dem mit der Verteilungsgleichung berechneten Wert für Varianz. Begriff Die Varianz einer Zufallsvariable X {\displaystyle X} ist der Erwartungswert der quadratischen Abweichung vom Mittelwert von X {\displaystyle X}, μ = E ‡ [ X] {\displaystyle mu=\operatorname (E) (X) Var ‡ (X ) = E {\displaystyle \operatorname (X)=\Operatorname {E} links[(X-\mu {^)2}\rechts] Diese Definition umfasst zufällige Variablen, die durch diskrete, kontinuierliche oder gemischte Prozesse erzeugt werden. Die Varianz kann auch als Kovarianz einer Zufallsvariablen mit sich selbst betrachtet werden: Var ‡ (X ) = Cov ‡ (X, X ) . {\displaystyle \operatorname {Var} (X)=\Operatorname {Cov} (X,X}) Die Varianz entspricht auch dem zweiten Kumulanten einer Wahrscheinlichkeitsverteilung, die X {\displaystyle X} erzeugt.Die Varianz wird typischerweise als Var ‡ (X ) {\displaystyle \operatorname {Var} (X}) oder manchmal als V (X\)\displaystyle V(X}) oder V (X ) {\displaystyle \mathbb {V} bezeichnet. Der Ausdruck für die Varianz kann wie folgt erweitert werden: (X) = E ‡ [ ( X - E ≠ [ X ] ) 2 ] = E (X ) 2 ] = E ‡ [ X 2 ] - 2 E ‡ [ X ] E ‡ [ X ] + E ‡ (X ) 2 = E kennzeichnet [ X 2 ] - E ‡ [ X ] 2 {\displaystyle begin{aligned}\operatorname {Var} (X)&=\Operatorname {E} links[(X-\operatorname{E}X])^{2}\right]\\[4pt]&=\Operatorname {E} links [X^{2}-2X\Operatorname {E} [X]+\Operatorname {E} X]^{2}\right]\\[4pt]&=\Operatorname {E} links[X^{2}\right]-2\operatorname {E} [X] {E} [X]+\Operatorname {E} X]^{2}\[4pt]&=\Operatorname {E} links [X^{2}\right]-\Operatorname {E} [X]^{2}\end{ausgeglichen Mit anderen Worten ist die Varianz von X gleich dem Mittelwert des Quadrats von X minus dem Quadrat des Mittelwertes von X. Diese Gleichung sollte nicht für Berechnungen mit Floating Point arithmetic verwendet werden, weil sie unter katastrophalen Annullierung leidet, wenn die beiden Komponenten der Gleichung in der Größe ähnlich sind. Für andere numerisch stabile Alternativen siehe Algorithmen zur Berechnung der Varianz. Diskutieren Sie zufällige Variable Wenn der Generator der Zufallsvariable X {\displaystyle X} mit der Wahrscheinlichkeits-Massenfunktion x 1 ↦ p 1 , x 2 ↦ p 2 , ... , x n ↦ p n {\displaystyle x_{1}\mapsto p_{1},x_{2}\mapsto p_{2},\ldots ,x_{n}\mapsto p_{n}, then Var ‡ ( X ) = Σ i = 1 n p i ∙ ( x i - μ ) 2 , {\displaystyle \operatorname {Var} (X)=\sum i=1}{n}p_{i}\cdot (x_{i}-\mu {^)2,} wobei μ {\displaystyle \mu } der Erwartungswert ist. Das heißt, μ = Σ i = 1 n p i x i . {\displaystyle \mu =\sum i=1}{n}p_{i}x_{i. (Wenn eine solche diskrete gewichtete Varianz durch Gewichte vorgegeben wird, deren Summe nicht 1 ist, so teilt man sich durch die Summe der Gewichte.) Die Varianz einer Sammlung von n {\displaystyle n} gleichwahrscheinlichen Werten kann als Var ziert (X ) = 1 n Σ i = 1 n ( x i - μ ) 2 {\displaystyle \operatorname {Var} (X)={\frac 1}{n}}\sum i=1}^{n}(x_{i}-\mu {^)2} wobei μ {\displaystyle \mu } der Mittelwert ist. Das heißt, μ = 1 n Σ i = 1 n x i . {\displaystyle \mu {=\frac 1}{n}\sum i=1^{n}x_{i. Die Varianz eines Satzes von n {\displaystyle n} gleichwahrscheinlichen Werten kann äquivalent ausgedrückt werden, ohne sich direkt auf den Mittelwert in Bezug auf quadratische Abweichungen aller Punkte voneinander zu beziehen: Var ≠ (X) = 1 n 2 Σ i = 1 n Σ j = 1 n 1 2 ( x i - x j ) 2 = 1 n 2 ≠ i ( x i - x x j ) 2 . {\displaystyle \operatorname (X)={\frac 1}{n^{2}}\sum ) 1 {_i}\sum j>i}(x_{i}-x_{j})^^^^^2. Absolute zufällige Variable Hat die Zufallsvariable X {\displaystyle X} eine Wahrscheinlichkeitsdichtefunktion f ( x ) {\displaystyle f(x}) und F ( x ) {\displaystyle F(x}) die entsprechende kumulative Verteilungsfunktion, dann Var ≠ (X ) = σ 2 = δ R ( x - μ ) 2 f ( x ) d x = δ R x 2 f ( x ) 2 μ tek R x f ( x ) d x + μ 2 ξ R f ( x ) d x = δ R x 2 d F ( x ) - 2 μ tek R x d F ( x ) + μ 2 tek R d F ( x ) = δ R x 2 d F ( x ) - 2 μ ⋅ μ + μ 2 ⋅ 1 = δ R x 2 d F ( x ) - μ 2 , {\displaystyle start{aligned}\operatorname {Var} (X)=\sigma ^2}&=\int_{\mathbb ,,,μ = δ R x f ( x ) d x = δ R x d F ( x ) . {\displaystyle \mu =\_mathbb {R}xf(x)\,dx=\int {\_mathbb {R} x\,dF(x) In diesen Formeln sind die Integrale bezüglich d x {\displaystyle dx} und d F ( x ) {\displaystyle dF(x}) Lebesgue bzw. Lebesgue–Stieltjes Integrale. Ist die Funktion x 2 f ( x ) {\displaystyle x^{2}f(x) Riemann-integrierbar auf jedem endlichen Intervall [a, b ] ≠ R, {\displaystyle [a,b]\subset \mathbb {R},}, dann Var differenziert (X ∞) = ∞ +style x 2 f (x oper} Beispiele Exponential Distribution Die exponentielle Verteilung mit dem Parameter λ ist eine kontinuierliche Verteilung, deren Wahrscheinlichkeitsdichtefunktion durch f (x) = λ e - λ x {\displaystyle f(x)=\lambda e^_lambda x} im Intervall [0, ∞) gegeben ist. Sein Mittelwert kann als E ≠ = δ ∞ λ x e - λ x d x = 1 λ bezeichnet werden. {\displaystyle \operatorname [X]=\int_0{\infty \}lambda xe^{-\lambda x}\,dx={\frac 1}{\lambda }}}. Mit der Integration durch Teile und der Nutzung des bereits berechneten Erwartungswerts haben wir: E ≠ [X 2 ] = δ 0 ∞ x 2 e - λ x d x = [ - x 2 e - λ x ] ∞ + ∞ 0 ∞ 2 x e - λ x d x = 0) 2 λ E ≠ = 2 λ 2 . {\displaystyle start{aligned}\operatorname {E} links [X^{2}\right]&=\int _0}^{\infty \}lambda ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ {E} [X]\\\&={\frac 2}{\lambda ^2}}.\end{align So wird die Varianz von X von Var ≠ (X ) = E [ X 2 ] - E ‡ [ X ] 2 = 2 λ 2 − ( 1 λ ) 2 = 1 λ 2 . {Var} (X)=\Operatorname {E} links [X^{2}\right]-\Operatorname (E} [X]^{2}={\frac 2}{\lambda ^2}}-\left({\frac 1}{\lambda right){2}={\frac 1}{\lambda {\lambda {}}} {\lambda {\c 1}}}}} {\frac 1}{\lambda {\2}}}}} Faire Würfel Eine faire sechsseitige Würfel kann als diskrete Zufallsvariable X mit den Ergebnissen 1 bis 6 mit jeweils gleicher Wahrscheinlichkeit 1/6 modelliert werden. Der erwartete Wert von X ist ( 1 + 2 + 3 + 4 + 5 + 6) / 6 = 7 / 2. {\displaystyle (1+2+3+4+5+6)/6=7/2.} Daher ist die Varianz von X Var ‡ ( X ) = Σ i = 1 6 1 6 (i - 7 2 ) 2 = 1 6 ( ( - 5 / 2 ) 2 + ( - 3 / 2 ) 2 + ( - 1 / 2 ) 2 + ( 1 / 2 ) 2 + ( 3 / 2 ) 2 + ( 5 / 2 ) 2 style) = 35 12 ≈ 2.92 {\display Start{ausgerichtet}\Operatorname {Var} (X)&=\sum (c) Die allgemeine Formel für die Varianz des Ergebnisses, X, eines n-Seitenstempels ist Var ‡ (X ) = E ‡ (X 2 ) − (E ‡ (X ) ) 2 = 1 n Σ i = 1 n i 2 − ( 1 n Σ\ = 1 n i ) 2 = (n + 1 ) (play n + 1 ) 6 - (n + 1 2 } 2 ) 2 = 2 ) 2 = 2 = 2 . (X^{2}(\)(X)(X)(X)(X){2}(\operatorname {E}(X)){2}\[5pt]&={\frac 1}{n}\sum_i=1{c}{2}{\{\frac 1}{n}{\c}{\c}}{\c}}}{\c}}}}{\c}{\c}}}}{\c}}\c}\c}\c}}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c}\c\c\c\c\c}\c\c\c\c}\c\c\c\c}\c\c\c\c\c\c\c\c\c\c\c}\c}\c\c\c\c\c\c\c\c n+1{2}{2}\right)^{2}\\[4pt]&={\frac n^{2}{12}}\end{align Häufig genutzte Wahrscheinlichkeitsverteilungen Die folgende Tabelle listet die Varianz für einige häufig verwendete Wahrscheinlichkeitsverteilungen auf. Eigenschaften Varianz ist nicht-negativ, da die Quadrate positiv oder Null sind: Var ‡ 0. {\displaystyle \operatorname (X) 0. Die Varianz einer Konstante ist Null. (a) = 0. {\displaystyle \operatorname {Var} (a)=0.} Umgekehrt, wenn die Varianz einer Zufallsgröße 0 ist, ist sie fast sicher eine Konstante. Das heißt, es hat immer den gleichen Wert: Var ≠ (X ) = 0 ∃ ∃ Þ a : P ( X = a ) = 1. {\displaystyle \operatorname {Var} (X)=0\iff \existiert a:P(X=a)=1.} Die Varianz ist im Hinblick auf Änderungen eines Standortparameters unverändert. Wird also allen Werten der Größe eine Konstante hinzugefügt, so ist die Varianz unverändert: (X + a) = Var zähl (X ) . {\displaystyle \operatorname {Var} (X+a)=\Operatorname (X.} Wenn alle Werte durch eine Konstante skaliert werden, wird die Varianz durch das Quadrat dieser Konstante skaliert: Var ‡ (a X ) = a 2 Var ≠ (X ) . {\displaystyle \operatorname {Var} (aX)=a^{2}\Operatorname (X.} Die Varianz einer Summe von zwei zufälligen Variablen wird von Var ‡ (a X + b Y ) = a 2 Var ‡ ( X ) + b 2 Var ‡ ( Y ) + 2 a b Cov ‡ (X, Y ) , {\displaystyle \operatorname {Var} (aX+bY)=a^{2}\Operatorname {Var} (X)+b^{2}\Operatorname (Y)+2ab\,\Operatorname {Cov} (X,Y}) Var (a X - b Y ) = a 2 Var ‡ ( X ) + b 2 Var ‡ ( Y ) - 2 a b Cov ‡ ( X , Y ) , {\displaystyle \operatorname {Var} (aX-bY)=a^{2}\Operatorname {Var} (X)+b^{2}\Operatorname {Var} (Y)-2ab\,\Operatorname {Cov} (X,Y}), wo Cov (X, Y) {\displaystyle \operatorname {Cov} (X,Y}) die Kovarianz ist. Im Allgemeinen wird für die Summe von N {\displaystyle N} zufälligen Variablen { X 1 , ... , X N } {\displaystyle X_{1},\dots ,X_{N\} die Varianz: Var ( Σ i = 1 N X i ) = Σ i , j = 1 N Cov ‡ ( X i , X j ) = Σ i = 1 N Var i ≠ j Cov ‡ ( X i, X j ) .{\displaystyle \operatorname (Var} links(\sum i=1}^{N}X_{i}\right)=\sum _i,j=1}{N}\operatorname (X_{i},X_{j})=\sum _i=1^{N}\Operatorname (X_{i})+\sum {_i\neq j}\operatorname (X_{i},X_{j) Diese Ergebnisse führen zur Varianz einer linearen Kombination wie: Var ‡ ( Σ i = 1 N a i X i) = Σ i, j = 1 N a i a j Cov ‡ ( X i , X j ) = Σ i = 1 N a i 2 Var i ≠ j a i a j Cov ‡ ( X i , X j ) = Σ i = 1 N a i 2 Var ‡ ( X i ) + 2 Σ 1 ≤ i < j ≤ N a i a j Cov ‡ ( X i , X j ) Start{ausgerichtet}\Operatorname (\sum i=1}{N}a_{i}X_{i}\right)&=\sum i,j=1{N}a_{i}a_{j}\operatorname (X_{i},X_{j}\\&=\sum ***_________________________________________________________________________________________ (X_{i}) =j}a_{i}a_{j}\operatorname (X_{i},X_{j}\\&=\sum ***_________________________________________________________________________________________ (X_{i})+2\sum {_1\leq N'a_{i}a_{j\operatorname (X_{i},X_{j}) Wenn die Zufallsvariablen X 1 , ... , X N {\displaystyle X_{1},\dots ,X_{N} so sind, dass Cov ‡ (X i, X j ) = 0 , ê ( i ≠ j ) , {\displaystyle \operatorname {Cov} (X_{i},X_{j})=0\,\ \forall \ (i\neq j,}) dann werden sie als unkorreliert bezeichnet. Es ergibt sich sofort aus dem zuvor angegebenen Ausdruck, dass, wenn die Zufallsvariablen X 1 , ..., X N {\displaystyle X_{1},\dots ,X_{N} unkorreliert sind, die Varianz ihrer Summe gleich der Summe ihrer Varianzen ist, oder, symbolisch ausgedrückt: Var ( Σ i = 1 N X i ) = Σ i = 1 N Var ‡ ( X i ) . {\displaystyle \operatorname (Var} links(\sum i=1}^{N}X_{i}\right)=\sum _i=1^{N}\Operatorname (X_{i}) Da unabhängige Zufallsvariablen immer unkorreliert sind (vgl. Covariance § Uncor relatedness and Independent,) hält die obige Gleichung insbesondere dann, wenn die Zufallsvariablen X1, ..., X n {\displaystyle X_{1},\dots ,X_{n} unabhängig sind. Somit ist die Unabhängigkeit ausreichend, aber für die Varianz der Summe nicht erforderlich, um die Summe der Varianzen gleichzusetzen. Probleme der Finiteness Hat eine Verteilung keinen endlichen Erwartungswert, wie dies bei der Cauchy-Verteilung der Fall ist, so kann die Varianz auch nicht endlich sein. Einige Verteilungen haben jedoch keine endliche Varianz, obwohl ihr erwarteter Wert endlich ist. Ein Beispiel ist eine Pareto-Verteilung, deren Index k {\displaystyle k} 1 < k ≤ 2 erfüllt. 2.} Summe der unkorrelierten Variablen (Bienaymé-Formel)Ein Grund für die Verwendung der Varianz, bevorzugt anderer Dispersionsmassnahmen ist, dass die Varianz der Summe (oder der Unterschied) von unkorrelierten Zufallsvariablen die Summe ihrer Varianzen ist: Var ( Σ i = 1 n X i ) = Σ i = 1 n Var ‡ ( X i ) . {\displaystyle \operatorname (Var} links(\sum i=1}^{n}X_{i}\right)=\sum _i=1^{n}\Operatorname (X_{i}) Diese Aussage wird die Bienaymé Formel genannt und wurde 1853 entdeckt. Es wird oft mit der stärkeren Bedingung gemacht, dass die Variablen unabhängig sind, aber unkorreliert ausreicht. Wenn also alle Größen die gleiche Varianz σ2 haben, dann, da die Division durch n eine lineare Transformation ist, bedeutet diese Formel sofort, dass die Varianz ihres Mittelwertes Var ‡ (X ̄ ) = Var ≠ ( 1 n Σ i = 1 n X i) = 1 n 2 Σ i = 1 n Var ≠ (X i ) = 1 n 2 n σ 2 = σ 2 n . {\displaystyle \operatorname {\overline {X}\right)=\operatorname (Var} links({\frac 1}{n}}\sum i=1}{n}X_{i}\right)={\frac 1}{n^}}}\sum _i=1{n}\operatorname {Var} links(X_{i}\right)={\frac 1}{n^{2}}}n\sigma ^2}={\frac {\sigma ^2}{n. Das heißt, die Varianz des Mittels verringert sich, wenn n zunimmt. Diese Formel für die Varianz des Mittelwertes wird in der Definition des Standardfehlers des Probenmittels verwendet, der im zentralen Grenzwerttheorem verwendet wird. Um die erste Aussage zu beweisen, genügt es, zu zeigen, dass Var ‡ ( X + Y ) = Var ‡ ( X ) + Var ‡ ( Y ) . {\displaystyle \operatorname {Var} (X+Y)=\Operatorname {Var} (X)+\Operatorname (J) Das allgemeine Ergebnis folgt dann durch Induktion. Ausgehend von der Definition, Var ‡ ( X + Y ) = E ≠ [ ( X + Y ) 2 ] - (E ‡ [ X + Y ] ) 2 = E ‡ [ X 2 + 2 X Y + Y 2 ] − ( E ‡ [ X ] + E ‡ [ Y ] ) 2 . {\displaystyle begin{aligned}\operatorname (X+Y)&=\Operatorname {E} links[(X+Y)^{2}\right]-(\Operatorname (E} X+Y)^{2}\\[5pt]&=\Operatorname {E} links[X^{2}+2XY+Y^{2}\right](\operatorname {E} [X]+\Operatorname (E} [Y]) Mit der Linearität des Erwartungsoperators und der Annahme der Unabhängigkeit (oder Unvereinbarkeit) von X und Y vereinfacht sich dies weiter wie folgt: (X + Y ) = E ‡ [ X 2 ] + 2 E 2 + 2 E traf [ X ] E traf [ Y ] + E traf [ Y ] 2 ) = E ‡ [ X 2 ] + E ‡ [ Y 2 ] − E ‡ (X ) 2 - E kennzeichnet [ Y ] 2 = Var zähl ( X ) + Var zähl ( Y ) . {\displaystyle Start{ausgerichtet}\Operatorname {Var} (X+Y)&=\Operatorname {E} links[X^{2}\right]+2\operatorname {E} [XY]+\Operatorname {E} links[Y^{2}\right]-\left(\Operatorname {E} [X]^{2}+2\Operatorname {E} [X] {E} [Y]+\Operatorname (E} Y]^{2}\right)\\[5pt]&=\Operatorname {E} links [X^{2}\right]+\Operatorname {E} links [Y^{2}\right]-\Operatorname {E} [X]^{2}-\Operatorname {E} Y'^{2}\\[5pt]&=\Operatorname {Var} (X)+\Operatorname {Var} (Y)\end{aligned} Summe der korrelierten Variablen mit Korrelation und fester Probengröße Im allgemeinen ist die Varianz der Summe von n Variablen die Summe ihrer Kovarianzen: Var ( Σ i = 1 n X i ) = Σ i = 1 n Σ j = 1 n Cov 1 n Var anstieg (X i ) + 2 Σ 1 ≤ i < j ≤ n Cov sprung (X i, X j ) . {\displaystyle \operatorname (Var} links(\sum i=1}^{n}X_{i}\right)=\sum _i=1{n}\sum _j=1{n}\operatorname (Cov} links(X_{i},X_{j}\right)=\sum _i=1{n}\operatorname {Var} links(X_{i}\right)+2\sum {_1\leq i <j\leq n\operatorname (X_{i},X_{j}\right.) (Anmerkung: Die zweite Gleichheit ergibt sich aus der Tatsache, dass Cov(Xi,Xi) = Var(Xi).) Hier ist Cov ċ ( ⋅ , ⋅ ) {\displaystyle \operatorname {Cov} \(cdot ,\cdot )} die Kovarianz, die für unabhängige Zufallsvariablen Null ist (wenn es vorhanden ist). Die Formel besagt, dass die Varianz einer Summe gleich der Summe aller Elemente in der Kovarianzmatrix der Komponenten ist. Der nächste Ausdruck sagt gleichwertig, dass die Varianz der Summe die Summe der Diagonale der Kovarianzmatrix plus das Zweifache der Summe ihrer oberen dreieckigen Elemente (oder ihrer unteren dreieckigen Elemente) ist, was betont, dass die Kovarianzmatrix symmetrisch ist. Diese Formel wird in der Theorie von Cronbachs alpha in der klassischen Testtheorie verwendet. Wenn also die Varianz der Varianz σ2 und die durchschnittliche Korrelation der einzelnen Variablen ρ ist, dann ist die Varianz ihres Mittelwertes Var ‡ = σ 2 n + n - 1 n ρ σ 2 . {\displaystyle \operatorname {Var} links({\overline X}\right)={\frac {\sigma ^2}{n}+{\frac n-1}{n}}\rho \sigma {^2} Dies bedeutet, dass die Varianz des Mittelwerts mit dem Durchschnitt der Korrelationen zunimmt. Mit anderen Worten, zusätzliche korrelierte Beobachtungen sind nicht so wirksam wie zusätzliche unabhängige Beobachtungen zur Verringerung der Unsicherheit des Mittelwerts. Wenn die Varianzen der Varianzen der Varianzen, z.B. wenn sie normiert sind, dann vereinfacht sich dies zu Var ≠ = 1 n + n - 1 n ρ . {\displaystyle \operatorname (Var} links({\overline X}\right)={\frac 1{n}+{\frac n-1}{n}}\rho .} Diese Formel wird in der Spearman-Brown-Prädiktionsformel der klassischen Testtheorie verwendet. Dies entspricht ρ, wenn n auf Unendlichkeit geht, sofern die durchschnittliche Korrelation konstant bleibt oder auch konvergiert. Für die Varianz des Mittelwerts von normierten Variablen mit gleichen Korrelationen oder konvergierenden mittleren Korrelation haben wir also lim n → ∞ Var ‡ (X ̄ ) = ρ . {\displaystyle \lim {_n\to \infty \}Betreibername {Var} links({\overline {X}\right)=\rho .} Daher ist die Varianz des Mittelwerts einer Vielzahl von standardisierten Größen etwa gleich ihrer mittleren Korrelation. Dies macht deutlich, dass die Stichprobenmittel der korrelierten Variablen im Allgemeinen nicht mit der Bevölkerung zusammentreffen, obwohl das Gesetz der großen Zahlen besagt, dass die Stichprobenmittel für unabhängige Variablen konvergieren werden. I.i.d.mit Stichprobengröße Es gibt Fälle, in denen eine Probe genommen wird, ohne vorher zu wissen, wie viele Beobachtungen nach einigen Kriterien akzeptabel sein werden. In solchen Fällen ist die Stichprobengröße N eine Zufallsgröße, deren Variation zur Variation von X addiert, so dass Var(ΣX) = E(N)Var(X+ Var(N)E2(X), die aus dem Gesetz der Gesamtvarianz folgt. Hat N eine Poisson-Verteilung, so wird E(N) = Var(N) mit Schätzer N = n.So, der Schätzer von Var(ΣX) wird nS2X + nX2 mit Standardfehler(X) =√[(S2X + X2)/n] Matrix-Notation für die Varianz einer linearen Kombination Defin X {\displaystyle X} als Spaltenvektor von n {\displaystyle n} Zufallsvariablen X 1,..., X n {\displaystyle X_{1},\ldots ,X_{n} und c {\displaystyle c} als Spaltenvektor von n {\displaystyle n} {\displaystyle \operatorname (Var} links (c^{\mathsf T}X\right)=c^{\mathsf {T}\Sigma c.} Dies bedeutet, dass die Varianz des Mittelwerts wie (mit einem Spaltenvektor von einem) geschrieben werden kann. Var ( x ̄ ̄) = Var ( 1 n 1' X ) = 1 n 2 1' Σ 1. {\displaystyle \operatorname {\bar {x}\right)=\operatorname (Var} links({\frac 1}{n}}}1'X\right)={\frac 1}{n^{2}}1'\Sigma 1.} Gewichtete Summe der Variablen Die Skalierungseigenschaft und die Bienaymé-Formel, zusammen mit der Eigenschaft der Kovarianz Cov(aX, bY) = über Cov(X, Y) zusammen bedeuten, dass Var (a X ± b Y ) = a 2 Var ‡ + b 2 Var ≠ ( Y ) ± 2 a b Cov ≠ (X, Y ) . {display \opername {Var} (aX\pm bY)=a^{2}\Operatorname {Var} (X)+b^{2}\Operatorname {Var} (Y)\pm 2ab\,\Operatorname {Cov} (X,Y}) Dies bedeutet, dass in einer gewichteten Summe von Variablen die Variable mit dem größten Gewicht in der Varianz der Summe ein unverhältnismäßig großes Gewicht haben wird. Wenn z.B. X und Y unkorreliert sind und das Gewicht von X das zweifache Gewicht von Y beträgt, so wird das Gewicht der Varianz von X das vierfache Gewicht der Varianz von Y betragen. Der vorstehende Ausdruck kann auf eine gewichtete Summe mehrerer Variablen ausgedehnt werden: Var ( Σ i n a i X i ) = Σ i = 1 n a i 2 Var ‡ ( X i ) + 2 Σ 1 ≤ i Σ < j ≤ n a i a j Cov ‡ ( X i , X j ) (Var} links(\sum i}^{n}a_{i}X_{i}\right)=\sum ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ (X_{i})+2\sum {_1\leq ****________________________________________________________________________________________ (X_{i},X_{j) Produkt unabhängiger Variablen Sind zwei Variablen X und Y unabhängig, so wird die Varianz ihres Produktes von Var ≠ (X Y) = [ E ≠ (X ) ] 2 Var ‡ ( Y ) + [ E ≠ ( Y ) ] 2 Var ≠ (X ) + Var ‡ ( X ) Var ≠ ( Y ) angegeben. {\displaystyle \operatorname (XY)=[\Operatorname {E} (X) {Var} (Y)+[\Operatorname (J) {Var} (X)+\Operatorname {Var} (X)\Operatorname {Var} (Y.}) Entsprechend wird unter Verwendung der Grundeigenschaften der Erwartung von Var ≠ = E ≠ (X 2 ) E ‡ (Y 2 ) - [ E ≠ (X ) ] 2 [ E ≠ (Y ) ] 2. {\displaystyle \operatorname {Var} (XY)=\Operatorname {E} links(X^{2}\right)\Operatorname {E} links(Y^{2}\right)-[\operatorname {E} (X)]^{2}[\operatorname {E} (Y)]^{2.} Produkt von statistisch abhängigen Variablen Im allgemeinen, wenn zwei Variablen statistisch abhängig sind, wird die Varianz ihres Produktes durch: Var ‡ (X Y ) = E ‡ [X 2 Y 2 ] - [ E ≠ (X Y ) ] 2 = Cov ‡ ( X 2 , Y 2 ) + E ‡ ( X 2 ) E ‡ ( Y 2 ) − [ E ‡ ( X Y ) ] 2 = Cov ‡ ( X 2 , Y 2 ) + ( Var ≠ X χ ) + [ E 2 ) ( Var ‡ ( Y ) + [ E ‡ ( Y ) ] 2 ) − [ Cov onym ( X , Y ) + E ‡ ( X ) E ‡ ( Y ) ] 2 {\displaystyle beginnen{aligned}\operatorname {Var} (XY)={}&\operatorname (E} links[X^{2}Y^{2}\right]-[\operatorname] {E} (XY)]^{2}\\[5pt]={}&\operatorname {Cov} links(X^{2},Y^{2}\right)+\operatorname {E} (X^{2})\Operatorname {E} links(Y^{2}\right)-[\operatorname {E} (XY)^{2}\\[5pt]={}&\operatorname {Cov} links(X^{2},Y^{2}\right)+\left(\operatorname) (X)+[\Operatorname (E} X) {Var} (Y)+[\operatorname {E} (Y)]^{2}\right)\[5pt]&-[\operatorname {Cov} (X,Y)+\operatorname {E} (X)\operatorname {E} (Y)]^{2}\end{ausgerichtet Zusammensetzung Die allgemeine Formel für die Varianzzersetzung oder das Gesetz der totalen Varianz ist: Wenn X {\displaystyle X} und Y {\displaystyle Y} zwei zufällige Variablen sind und die Varianz von X {\displaystyle X} existiert, dann Var ‡ = E ≠ ( Var ziert [ X ∣ Y ] ) + Var glieder (E ‡ [ X ∣ Y ] ) . {\displaystyle \operatorname {Var} [X]=\Operatorname {E} \(Betreibername {Var} [X\mid Y])+\Betreibername {Var} \(Betreibername (E} (X\mid Y) Die bedingte Erwartung E ‡ X ∣ Y ) {\displaystyle \operatorname {E} (X\mid Y}) von X {\displaystyle X} gegeben Y {\displaystyle Y} und die bedingte Varianz Var ≠ (X ∣ Y ) {\displaystyle \operatorname {Var} (X\mid Y}) können wie folgt verstanden werden. Bei einem bestimmten Wert y der Zufallsvariablen Y gibt es bei dem Ereignis Y = y eine bedingte Erwartung E ≠ (X ∣ Y = y ) {\displaystyle \operatorname {E} (X\mid Y=y}). Diese Menge hängt vom jeweiligen Wert y ab; sie ist eine Funktion g ( y ) = E ≠ ( X ∣ Y = y ) {\displaystyle g(y)=\operatorname {E} (X\mid Y=y}) . Diese gleiche Funktion, die bei der Zufallsvariablen Y ausgewertet wird, ist die bedingte Erwartung E ‡ Y = g ( Y ) . {\displaystyle \operatorname {E} (X\mid Y)=g(Y.} Insbesondere wenn Y {\displaystyle Y} eine diskrete Zufallsvariable ist, die mögliche Werte y 1 , y 2 , y 3 annimmt... {\displaystyle y_{1},y_{2},y_{3}\ldots } mit entsprechenden Wahrscheinlichkeiten p 1 , p 2 , p 3 ..., {\displaystyle p_{1},p_{2},p_{3}\ldots ,}, dann in der Formel für Gesamtvarianz wird der erste Begriff auf der rechten Seite E ( Var)= Σ i p i σ i 2 , {\displaystyle \operatorname {E} \(Betreibername [X\mid Y]=\sum i}p_{i}\sigma _i}^{2, wobei σ i 2 = Var ‡ [ X ∣ Y = y i ] {\displaystyle \sigma _i}^{2}=\Operatorname (Var) .Darüber hinaus wird der zweite Begriff auf der rechten Seite Var ≠ (E ‡ Y ) = Σ i p i i 2 - ( Σ i p i i i ) 2 = Σ i p i i 2 − μ 2 , {\displaystyle \operatorname {Var} \(Betreibername (E} [X\mid Y)=\sum i}p_{i}\mu _i{2}-\left(\sum i}p_{i}\mu_i}\right)^{2}=\sum I'p_{i} ^^^^^^^^^^^^^,}, wo μ i = E [ X ∣ Y = y i ] {\displaystyle \mu {_i}=\Operatorname {E} [X\mid Y=y_{i]} und μ = Σ i p i p i i {\displaystyle \mu =\sum i}p_{i}\mu {_i} .Thus die Gesamtvarianz wird von Var ‡ [X] = Σ i p i σ i 2 + ( Σ i p i μ i 2 - μ 2 ) . {\displaystyle \operatorname {Var} [X]=\sum I'p_{i}\sigma ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ - Ja. Eine ähnliche Formel wird in der Varianzanalyse angewendet, wobei die entsprechende Formel M S total = M S zwischen + M S innerhalb von ; {\displaystyle {\mathit MS}_{\text{total}={\mathit * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * MS}_{\text{within; hier bezieht sich M S {\displaystyle {\mathit {MS} auf das Mittel der Quadrate. Bei der linearen Regressionsanalyse ist die entsprechende Formel M S total = M S regression + M S Rest . {\displaystyle} * ***_{\text{regression}}+{\mathit MS}_{\text{residual. Dies kann auch von der Additivität der Varianzen abgeleitet werden, da die Gesamtwertung (beobachtet) die Summe der vorhergesagten Punktzahl und der Fehlerwertung ist, wobei die letzteren beiden unkorreliert sind. Ähnliche Zersetzungen sind für die Summe von quadratischen Abweichungen möglich (Summe von Quadraten, S S {\displaystyle {\mathit {SS}): S S total = S S zwischen + S S innerhalb , {\displaystyle {\mathit SAMMLUNG ! SS}_{\text{within, S S S total = S regression + S S Rest . {\displaystyle {\mathit SAMMLUNG ! SS}_{\text{residual. Berechnung aus der CDF Die Populationsvarianz für eine nichtnegative Zufallsvariable kann in Bezug auf die kumulative Verteilungsfunktion F unter Verwendung von 2 δ 0 ∞ u (u ) σ ) d u - ( δ ∞ ∞ (u ) d u ) 2 ausgedrückt werden. Merkmale Das zweite Moment einer Zufallsgröße erreicht den minimalen Wert, wenn es um das erste Moment (d.h. Mittelwert) der Zufallsgröße, d.h. eine r g m i n m E ( (X - m) 2 ) = E (X ) (X-m\rechts)=\mathrm {E} links(\left(X-m\right)^{2}\right)=\mathrm {E} (X}) 0.Dies hält auch im mehrdimensionalen Fall. Maßeinheiten Anders als bei der erwarteten absoluten Abweichung weist die Varianz einer Größe Einheiten auf, die das Quadrat der Einheiten der Größe selbst sind. Zum Beispiel wird eine in Metern gemessene Größe eine Varianz in Metern quadratisch gemessen haben. Aus diesem Grund wird die Beschreibung von Datensätzen über ihre Standardabweichung bzw. Wurzelmittelquadratabweichung oft gegenüber der Verwendung der Varianz bevorzugt. Im Würfelbeispiel beträgt die Standardabweichung √2.9 ≈ 1,7, etwas größer als die erwartete absolute Abweichung von 1,5. Die Standardabweichung und die erwartete absolute Abweichung können beide als Indikator für die Streuung einer Verteilung verwendet werden. Die Standardabweichung ist gegenüber algebraischer Manipulation besser geeignet als die erwartete absolute Abweichung, und zusammen mit Varianz und Verallgemeinerungskovarianz wird häufig in theoretischen Statistiken verwendet; die erwartete absolute Abweichung ist jedoch eher robuster, da sie weniger empfindlich auf Ausreißer aus Messanomalien oder einer unausgeglichenenen Schwerverteilung ist. Annäherung der Varianz einer Funktion Das Delta-Verfahren verwendet Taylor-Erweiterungen zweiter Ordnung, um die Varianz einer Funktion einer oder mehrerer Zufallsvariablen anzunähern: siehe Taylor-Erweiterungen für die Momente von Funktionen von Zufallsvariablen. Beispielsweise wird die ungefähre Varianz einer Funktion einer Variablen von Var ≠ (f (X) ≈ (f' (E ‡ [ X] ) ) 2 Var merk [ X ] {\displaystyle \operatorname {Var} links[f(X)\right]\approx\left(f'(\operatorname {E} links[X\right])\right)^{2}\operatorname {Var} links[X\right]} vorausgesetzt, dass f doppelt differenzierbar ist und dass die mittlere und Varianz von X endlich ist. Bevölkerungsvarianz und Stichprobenvarianz Reale Weltbeobachtungen wie die Messungen des gestrigen Regens über den Tag können in der Regel nicht vollständige Sätze aller möglichen Beobachtungen sein, die gemacht werden könnten. Die Varianz, die aus dem endlichen Satz berechnet wird, entspricht in der Regel nicht der Varianz, die aus der vollen Population der möglichen Beobachtungen berechnet worden wäre. Dies bedeutet, dass man den Mittelwert und die Varianz schätzt, die aus einer allwissenden Beobachtungsreihe unter Verwendung einer Schätzgleichung berechnet worden wäre. Der Schätzwert ist eine Funktion der Stichprobe von n Beobachtungen, die ohne Beobachtungsvorspannung aus der gesamten Population potenzieller Beobachtungen gezogen werden. In diesem Beispiel wäre die Probe die Menge der tatsächlichen Messungen des gestrigen Niederschlags von verfügbaren Regenmessstreifen innerhalb der Geographie von Interesse. Die einfachsten Schätzungen für den Bevölkerungsdurchschnitt und die Bevölkerungsvarianz sind einfach der Mittelwert und die Varianz der Probe, die Stichprobenmittel und (unkorrigierte) Probenvarianz – das sind konsistente Schätzungen (sie konvergieren sich mit der Anzahl der Proben auf den richtigen Wert), können aber verbessert werden. Die Schätzung der Bevölkerungsvarianz durch Einnahme der Varianz der Probe ist im Allgemeinen nahezu optimal, kann aber auf zwei Arten verbessert werden. Am einfachsten wird die Probenvarianz als Durchschnitt von quadratischen Abweichungen über den (Proben) Mittelwert berechnet, indem n geteilt wird. Die Verwendung von anderen Werten als n verbessert jedoch den Schätzwert auf verschiedene Weise. Vier gemeinsame Werte für den Nenner sind n, n - 1, n + 1 und n - 1,5: n ist die einfachste (Populationsvarianz der Probe), n - 1 eliminiert Vorspannung, n + 1 minimiert mittleren quadratischen Fehler für die normale Verteilung und n - 1,5 eliminiert meist Vorspannung bei unvoreingenommener Schätzung von Standardabweichungen für die normale Verteilung. Erstens, wenn der allwissende Mittelwert unbekannt ist (und als Probenmittel berechnet wird), dann ist die Probenvarianz ein voreingestellter Schätzer: er unterschätzt die Varianz um einen Faktor (n - 1) / n; Korrektur um diesen Faktor (geteilt durch n - 1 anstelle von n) wird Bessels Korrektur genannt. Der resultierende Schätzer ist unvoreingenommen und wird als (korrigierte) Probenvarianz oder unvoreingenommene Probenvarianz bezeichnet. Wenn z.B. n = 1 die Varianz einer einzigen Beobachtung über den Probenmittel (es selbst) unabhängig von der Bevölkerungsvarianz offensichtlich Null ist. Wird der Mittelwert in anderer Weise bestimmt als aus den gleichen Proben, die zur Abschätzung der Varianz verwendet werden, so entsteht diese Vorspannung nicht und die Varianz kann sicher als die der Proben über den (unabhängig bekannten) Mittelwert geschätzt werden. Zweitens minimiert die Probenvarianz im Allgemeinen keinen mittleren quadratischen Fehler zwischen Probenvarianz und Populationsvarianz. Die Korrektur der Bias macht dies oft schlimmer: Man kann immer einen Skalenfaktor wählen, der besser ausfällt als die korrigierte Probenvarianz, obwohl der optimale Skalenfaktor von der überschüssigen Kurtose der Bevölkerung abhängt (siehe mittlere quadratische Fehler: Varianz) und Bias einführt. Dies besteht immer darin, den unvoreingenommenen Schätzer (geteilt durch eine Zahl größer als n - 1) zu skalieren und ist ein einfaches Beispiel eines Schwindungsschätzers: Man schrumpft den unvoreingenommenen Schätzer gegen Null. Für die normale Verteilung minimiert die Aufteilung durch n + 1 (statt n - 1 oder n) den mittleren quadratischen Fehler. Der resultierende Schätzer ist jedoch vorgespannt und wird als voreingestellte Probenvariation bezeichnet. Bevölkerungsabweichung Im allgemeinen wird die Populationsvarianz einer endlichen Population der Größe N mit den Werten xi angegeben, indem der Bevölkerungsmittel μ = 1 N Σ i = 1 N x i ist. {\displaystyle \mu {=\frac 1 i=1^{N}x_{i. Die Populationsvarianz kann auch mit σ 2 = 1 N 2 Σ i < j ( x i - x j ) 2 = 1 2 N 2 Σ i, j = 1 N ( x i - x j ) 2 berechnet werden. {\displaystyle \sigma ^2}={\frac 1{N^{2}\sum i<j}\left(x_{i}-x_{j}\right)^{2}={\frac 1 {2N{2}}\sum i,j=1}{N}\left(x_{i}-x_{j}\right)^{2. Dies ist wahr, weil die Populationsvarianz mit der Varianz der generierenden Wahrscheinlichkeitsverteilung übereinstimmt. In diesem Sinne kann das Konzept der Bevölkerung auf kontinuierliche zufällige Variablen mit unendlichen Populationen erweitert werden. Probenvarianz Biased Sample Variance In vielen praktischen Situationen ist die wahre Varianz einer Bevölkerung nicht bekannt a priori und muss irgendwie berechnet werden. Beim Umgang mit extrem großen Populationen ist es nicht möglich, jedes Objekt in der Bevölkerung zu zählen, so dass die Berechnung auf einer Probe der Bevölkerung durchgeführt werden muss. Die Probenvarianz kann auch auf die Abschätzung der Varianz einer kontinuierlichen Verteilung aus einer Probe dieser Verteilung angewendet werden. Wir nehmen eine Probe mit Austausch von n Werten Y1, ..., Yn aus der Bevölkerung, wo n < N, und schätzen die Varianz auf der Grundlage dieser Stichprobe. Direkt unter Berücksichtigung der Varianz der Stichprobendaten ergibt sich der Mittelwert der quadratischen Abweichungen: σ Y 2 = 1 n Σ i = 1 n ( Y i - Y ̄ ̄ ) 2 = ( 1 n Σ i = 1 n Y i 2 ) - Y ̄ 2 = 1 n 2 Σ i, j : i < j ( Yi - Y j ) 2 . {\displaystyle \sigma * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * (Y_{i}-{\overline Y}\right)^^{2}=\left({\frac 1}{n}\sum i=1}{n}Y_{i}^{2}\right)-{\overline ^{2}={\frac 1}{n^{2}}}\sum i,j\:\,i<j}\left (Y_{i}-Y_{j}\right)^^^2. Hier bedeutet Y ̄ {\displaystyle {\overline {Y} den Probenmittel: Y ̄ = 1 n Σ i = 1 n Y i . {\displaystyle {\overline ~{\fra{}{n}}\sum ~ i=1^{n}Y_{i. Da die Yi zufällig ausgewählt werden, sind beide Y ̄ {\displaystyle {\overline {Y} und σ Y 2 {\displaystyle \sigma _Y}^{2 zufällige Variablen. Ihre erwarteten Werte können durch Mittelung über das Ensemble aller möglichen Proben ausgewertet werden Die Größe n der Bevölkerung. Für σ Y 2 {\displaystyle \sigma _Y}^{2 ergibt sich folgendes: E [ σ Y 2] = E [1 n Σ i = 1 n ( Y i - 1 n Σ j = 1 n Y j ] 2 ] = 1 n Σ i = 1 n E ‡ k = 1 n Y k Σ] = 1 n Σ i = 1 n Y j + 1 n 2 n E ≠ [ Y j Y k ] + 1 n 2 Σ j = 1 n E ρ [ Y j 2 ] ) = 1 n Σ i = 1 n [ n - 2 n (σ 2 + μ 2 ) - 2 n ( n - 1 ) μ 2 + 1 n 2 n ( n - 1 ) μ 2 + 1 n (σ 2 + μ 2 ) stil)] = n - 1 n σ\ {E} _Y}^{2}&=\Operatorname (E) links[{\fra 1}{n}\sum i=1{n} (Y_{i}-{\frac 1}{n}}\sum j=1}{n}Y_{j}\right)^{2}\right\\\[5pt]&={\frac 1}{n}\sum _i=1^{n}\Operatorname {E} links (Y_{i}{2}-{\fra - Ja. * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 1 **________________________________________________________________ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ (E} links[Y_{i}{2}\right]-{\frac 2}{n}\sum {_j\neq i}\operatorname (E} links[Y_{i}Y_{j}\right]+{\fra 1 ) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ {E} links (Y_{j}Y_{k}\right)+{\fra ***________________________________________________________________________________________ (J) Daher gibt σ Y 2 {\displaystyle \sigma _Y}^{2 eine Schätzung der Bevölkerungsvarianz, die durch einen Faktor von n - 1 n {\displaystyle {\frac n-1}{n vorgespannt ist. Aus diesem Grund wird σ Y 2 {\displaystyle \sigma _Y}^^{2 als vorgespannte Mustervarianz bezeichnet. Unvoreingenommene Stichprobenvarianz Die Korrektur dieser Vorspannung ergibt die unvoreingenommene Probenvarianz, bezeichnet s 2 {\displaystyle s^{2}: s 2 = n - 1 σ Y 2 = n - 1 [ 1 n Σ i = 1 n ( Y i - Y ̄ ̄) 2] = 1 n - 1 Σ i = 1 n ( Y i - Y ̄ ̄ ̄) 2 {\displaystyle S^{2}={\fra n}{n-1}\sigma * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * (Y_{i}-{\overline Y}\right)^{2}\right]={\frac 1 ) (Y_{i}-{\overline Y}\right)^^^^^2 Entweder kann der Schätzer einfach als Mustervarianz bezeichnet werden, wenn die Version durch Kontext bestimmt werden kann. Der gleiche Nachweis gilt auch für Proben aus einer kontinuierlichen Wahrscheinlichkeitsverteilung. Die Verwendung des Begriffs n - 1 wird als Bessel-Korrektur bezeichnet, und es wird auch in der Probenkovarianz und der Probenstandardabweichung (die Quadratwurzel der Varianz) verwendet. Die Quadratwurzel ist eine konkave Funktion und führt somit negative Vorspannungen (von Jensens Ungleichheit), die von der Verteilung abhängig sind, ein und damit die korrigierte Probennormalabweichung (mit Bessels Korrektur) vorgespannt wird. Die unvoreingenommene Schätzung der Standardabweichung ist ein technisch aufwendiges Problem, jedoch ergibt für die Normalverteilung unter Verwendung des Begriffs n - 1,5 ein nahezu unvoreingenommener Schätzwert. Die unvoreingenommene Probenvarianz ist eine U-Statistik für die Funktion (y1, y2) =(y1 - y2)2/2, d.h. sie wird durch Mittelung einer 2-Probenstatistik über 2-Elemente-Untergruppen der Bevölkerung gewonnen. Verteilung der Probenvarianz Als Funktion von Zufallsvariablen ist die Probenvarianz selbst eine Zufallsvariable, und es ist natürlich, seine Verteilung zu studieren. Für den Fall, dass Yi unabhängige Beobachtungen von einer normalen Verteilung sind, zeigt Cochrans Theorem, dass s2 einer skalierten Chi-Quadrat-Verteilung folgt: ( n - 1 ) s 2 σ 2 χ n - 1 2 . Als direkte Folge folgt, dass E ‡ ( s 2 )= σ 2 σ 2 n - 1 χ n - 1 2 ) = σ 2 {E} links({2}\right)=\Operatorname {E} links({\frac {\sigma) ^2}{n-1}\chi _n-1}^{2}\right)=\sigma {^2,} und Var [ s 2 ] = Var anstieg (σ 2 n - 1 χ n - 1 2 ) = σ 4 ( n - 1 ) 2 Var fälschlich ( χ n - 1 2 ) = 2 σ 4 n - 1 . Links: ^2}{n-1}\chi (\chi _n-1}{2}{2}\sigma 4}{(n-1)^{2}}\operatorname {Var} links(\chi_n-1}^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Sind die Yi unabhängig und gleich verteilt, aber nicht notwendigerweise normal verteilt, dann E ċ [ s 2] = σ 2 , Var yon [ s 2 ] = σ 4 n ( κ - 1 + 2 n - 1 ) = 1 n (μ 4 - n - 3 n - 1 σ 4 ) , {displaystyle \operatorname {E} links[s^{2}\right]=\sigma {^2},\quad \operatorname (Var} links[2}\right]={\\ (\sigma ^4}{n}}\left(\kappa -1+{\frac 2}{n-1}\right)={\frac 1}{n}}\left(\mu _4}-{\frac 1}{n}\left(\mu_4}-{\frac 1}{n}}\left(\mu_4}-{\ n-3}{n-1}\sigma {^4}\right,) wobei κ die Kurtose der Verteilung ist und μ4 das vierte zentrale Moment ist. Wenn die Bedingungen des Gesetzes von großen Zahlen für die quadratischen Beobachtungen halten, ist s2 ein konsequenter Schätzer von σ2. Man kann in der Tat erkennen, dass die Varianz des Schätzers asymptotisch auf Null tendiert. Eine asymptotisch äquivalente Formel wurde in Kenney and Keeping (1951:164,) Rose and Smith (2002:264,) und Weisstein (n.d) Samuelsons Ungleichheit Samuelsons gegeben, ist ein Ergebnis, dass Zustände an den Werten gebunden sind, die einzelne Beobachtungen in einer Probe annehmen können, da die Probenmittel- und (voreingenommene) Varianz berechnet wurde. Werte müssen innerhalb der Grenzen y ̄ ± σ Y (n - 1 ) 1 / 2 liegen. {\displaystyle {\bar}\pm \sigma _Y}(n-1)^{1/2. Beziehungen zu den harmonischen und arithmetischen Mitteln Es hat sich gezeigt, dass für eine Probe {yi} positiver realer Zahlen σ y 2 ≤ 2 y max (A - H ) , {\displaystyle \sigma _y}{2}\leq 2y_{\max (A-H), wobei ymax das Maximum der Probe ist, A das arithmetische Mittel ist, H das harmonische Mittel der Probe und σ y 2\display ist. Diese Grenze wurde verbessert, und es ist bekannt, dass die Varianz durch σ y 2 ≤ y max (A - H) ( y max - A) y max - H , {\displaystyle \sigma begrenzt wird *y^{2}\leq {\frac y_{\max }A-H)(y_{\max -A)}{y_{\max -H, σ y 2 ≥ y min (A - H ) (A - y min ) H - y min , {\displaystyle \sigma _y}^{2}\geq {\frac y_{\min }A-H)(A-y_{\min })H-y_{\min }}}}, wobei ymin das Minimum der Probe ist. Prüfungen der Gleichheit der Varianzen Die Prüfung der Gleichheit von zwei oder mehr Varianzen ist schwierig. Die F-Test- und Chi-Quadrattests werden sowohl durch Nicht-Normalität beeinträchtigt als auch zu diesem Zweck nicht empfohlen. Es wurden mehrere nicht parametrische Tests vorgeschlagen: Dazu gehören der Barton–David–Ansari–Freund–Siegel–Tukey-Test, der Capon-Test, der Mood-Test, der Klotz-Test und der Sukhatme-Test. Der Sukhatme-Test gilt für zwei Varianzen und erfordert, dass beide Median bekannt und gleich Null sind. Die Prüfungen Mood, Klotz, Capon und Barton–David–Ansari–Freund–Siegel–Tukey gelten auch für zwei Varianzen. Sie ermöglichen es dem Median, unbekannt zu sein, erfordern aber, dass die beiden Median gleich sind. Der Lehmann-Test ist ein parametrischer Test von zwei Varianzen. Von diesem Test sind mehrere Varianten bekannt. Weitere Tests der Gleichheit der Varianzen sind der Box-Test, der Box-Anderson-Test und der Moses-Test. Um die Gleichheit der Varianzen zu testen, können Probenahmeverfahren, einschließlich Bootstrap und Jackknife, verwendet werden. Geschichte Der Begriff Varianz wurde zuerst von Ronald Fisher in seinem 1918 Papier The Correlation Between Verwandtes on the Supposition of Mendelian Inheritance eingeführt: Der große Körper der verfügbaren Statistiken zeigt uns, dass die Abweichungen einer menschlichen Messung von ihrem Mittelwert sehr genau dem Normalgesetz der Fehler folgen und damit die Variabilität durch die der Quadratwurzel des mittleren Quadratfehlers entsprechende Standardabweichung gleichmäßig gemessen werden kann. Wenn es zwei unabhängige Ursachen der Variabilität gibt, die in einer ansonsten einheitlichen Bevölkerungsverteilung mit Standardabweichungen σ 1 {\displaystyle \sigma {_1} und σ 2 {\displaystyle \sigma {_2} erzeugen können, wird festgestellt, dass die Verteilung, wenn beide Ursachen zusammen wirken, eine Standardabweichung σ 1 2 + σ 2 {\displaystyle {\sqrt {\sigma aufweist _1{2}+\sigma _2}^^^^2. Es ist daher wünschenswert, die Ursachen der Variabilität als Maß für Variabilität mit dem Quadrat der Standardabweichung zu analysieren. Wir werden diese Menge als Varianz bezeichnen. Moment der Trägheit Die Varianz einer Wahrscheinlichkeitsverteilung ist analog zum Trägheitsmoment in der klassischen Mechanik einer entsprechenden Massenverteilung entlang einer Linie, bezogen auf die Rotation um ihren Massenschwerpunkt. Wegen dieser Analogie werden solche Dinge wie die Varianz Momente der Wahrscheinlichkeitsverteilungen genannt. Die Kovarianzmatrix bezieht sich auf das Trägheitsmoment des Tensors für multivariate Verteilungen. Das Trägheitsmoment einer Wolke von n Punkten mit einer Kovarianzmatrix von Σ {\displaystyle \Sigma } wird durch I = n ( 1 3 × 3 tr ≠ ( Σ ) - Σ ) gegeben. {\displaystyle I=n\left(\mathbf) {1} {_3\times 3}\operatorname {tr} \(Sigma \-)Sigma \right.} Dieser Unterschied zwischen Trägheitsmoment in der Physik und in der Statistik ist für Punkte klar, die entlang einer Linie gesammelt werden. Angenommen, viele Punkte sind nahe der x-Achse und verteilt entlang. Die Kovarianzmatrix könnte wie Σ = [10 0 0 0,1 0 0 0 0,1 0 0,1 ] aussehen. {\displaystyle <= <===================================================================================== Das heißt, es gibt die meisten Varianz in der x-Richtung. Physiker würden dies für einen niedrigen Moment um die x-Achse halten, so dass der momentane Inertia-Tensor I = n [ 0,2 0 0 0 10.1 0 0 0 10.1] . {\displaystyle I = n{\begin{bmatrix}0.2&0&0&0\0&10.1&0\0&0&0&0&10.1\c Halbvariante Die Halbvarianz wird in der gleichen Weise berechnet wie die Varianz, aber nur die Beobachtungen, die unter den Mittelwert fallen, sind in der Berechnung enthalten: Es wird manchmal als Maß für das Ausfallrisiko in einem Anlagekontext beschrieben. Für skewede Verteilungen kann die Halbvarianz zusätzliche Informationen liefern, dass eine Varianz nicht. Für Ungleichheiten im Zusammenhang mit der Halbvarianz siehe Chebyshevs Ungleichheit § Semivariancen. Verallgemeinerungen Für komplexe Variablen Ist x {\displaystyle x} eine skalare, komplex bewertete Zufallsvariable, mit Werten in C, {\displaystyle \mathbb {C},}, so ist ihre Varianz E ≠ (x - μ ) (x - μ ) χ χ χ χ χ χ χ ,\displaystyle \operatorname {E} links[(x-style\mu ()x-style\mu () {\displaystyle x.} Diese Varianz ist ein echter Skalar. Für vektorwertierte Zufallsvariablen Als Matrix, wenn X {\displaystyle X} eine vektorbewertete Zufallsvariable ist, mit Werten in R n , {\displaystyle \mathbb {R} {^n,} und als Spaltenvektor gedacht, dann ist eine natürliche Verallgemeinerung der Varianz Epos\\μ ) (X -μ ) T ], {\displaystyle \opername {E} links Das Ergebnis ist eine positive semi-definite quadratische Matrix, die allgemein als Varianz-Kovarianz-Matrix (oder einfach als Kovarianz-Matrix) bezeichnet wird. Ist X {\displaystyle X} eine vektor- und komplexwertige Zufallsvariable, mit Werten in C n , {\displaystyle \mathbb {c} {^n,} dann ist die Kovarianzmatrix E ≠ (X - μ ) (X - μ ) † , {\displaystyle \operatorname {E} links[(X-\mu (rechts)X-\m\ {\displaystyle X.} Diese Matrix ist auch positiv semi-definit und quadratisch. Als Skalar Eine weitere Verallgemeinerung der Varianz für vektorbewertete Zufallsvariablen X {\displaystyle X}, die zu einem Skalarwert statt in einer Matrix führt, ist die verallgemeinerte Varianzdete (C ) {\displaystyle \det(C}), die Determinante der Kovarianzmatrix. Die verallgemeinerte Varianz kann gezeigt werden, dass sie mit der multidimensionalen Streuung von Punkten um ihren Mittelwert zusammenhängt. Eine andere Verallgemeinerung ergibt sich durch die Berücksichtigung des Euclideschen Abstandes zwischen der Zufallsgröße und ihrem Mittelwert. Dies führt zu E ≠ [ (X - μ ) T (X - μ ) ] = tr ‡ (C ) , {\displaystyle \operatorname {E} links[(X-\mu {\^)operatorname {T} (}X-\mu )right]=\operatorname {tr} (C,}), die die Spur der Kovarianzmatrix ist. Siehe auch Bhatia–Davis inequality Koeffizient der Variation Homoscedasticity Maßnahmen für statistische Dispersion Popoviciu Ungleichheit auf Varianzen Arten der Varianz Korrelation Entfernungsvarianz Erklärte Varianz Pooled variance == Referenzen ===