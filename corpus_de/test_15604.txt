Die Informationstheorie ist die wissenschaftliche Untersuchung der Quantifizierung, Speicherung und Kommunikation digitaler Informationen. Das Feld wurde von den Werken von Harry Nyquist und Ralph Hartley in den 1920er Jahren und Claude Shannon in den 1940er Jahren grundlegend etabliert. Das Feld ist am Schnittpunkt der Wahrscheinlichkeitstheorie, Statistik, Informatik, statistische Mechanik, Informationstechnik und Elektrotechnik. Eine wichtige Maßnahme in der Informationstheorie ist Entropie. Die Entropie quantifiziert die Höhe der Unsicherheit, die am Wert einer zufälligen Variable oder dem Ergebnis eines zufälligen Prozesses beteiligt ist. Zum Beispiel bietet die Identifizierung des Ergebnisses eines fairen Münzen-Flips (mit zwei ebenso wahrscheinlichen Ergebnissen) weniger Informationen (untere Entropie) als die Angabe des Ergebnisses von einer Rolle eines Stempels (mit sechs ebenso wahrscheinlichen Ergebnissen). Einige andere wichtige Maßnahmen in der Informationstheorie sind gegenseitige Informationen, Kanalkapazität, Fehlerexponenten und relative Entropie. Wichtige Teilfelder der Informationstheorie sind die Quellcodierung, algorithmische Komplexitätstheorie, algorithmische Informationstheorie, informationstheoretische Sicherheit und endliche Block-Länge-Informationstheorie. Anwendungen grundlegender Themen der Informationstheorie umfassen verlustfreie Datenkompression (z.B. ZIP-Dateien), verlustige Datenkompression (z.B. MP3s und JPEGs) und Kanalcodierung (z.B. für DSL). Seine Auswirkungen waren entscheidend für den Erfolg der Voyager Missionen in den tiefen Raum, die Erfindung der kompakten Scheibe, die Machbarkeit von Mobiltelefonen und die Entwicklung des Internets. Die Theorie hat auch Anwendungen in anderen Bereichen gefunden, einschließlich statistische Inferenz, Kryptographie, Neurobiologie, Wahrnehmung, Linguistik, die Evolution und Funktion von molekularen Codes (Bioinformatik,) thermische Physik, molekulare Dynamik, Quanten-Computing, schwarze Löcher, Informations-Retrieval, Nachrichtensammlung, Plagiat-Erkennung, Mustererkennung, Anomalie-Erkennung und sogar Kunsterstellung. Überblick Informationstheorie untersucht die Übertragung, Verarbeitung, Extraktion und Nutzung von Informationen. Abstrakt können Informationen als Auflösung der Unsicherheit betrachtet werden. Im Falle der Kommunikation von Informationen über einen lauten Kanal wurde dieses abstrakte Konzept 1948 von Claude Shannon in einem Papier mit dem Titel A Mathematical Theory of Communication formalisiert, in dem Informationen als eine Menge von möglichen Nachrichten gedacht werden, und das Ziel ist, diese Nachrichten über einen lauten Kanal zu senden, und den Empfänger die Nachricht mit geringer Fehlerwahrscheinlichkeit rekonstruieren zu lassen, trotz des Rauschkanals. Das Hauptergebnis von Shannon, das Noisy-Channel-Codierungtheorem, zeigte, dass die Informationsrate, die aymptotisch erreichbar ist, in der Grenze vieler Kanalanwendungen gleich der Kanalkapazität ist, eine nur von der Statistik des Kanals abhängige Menge, über den die Nachrichten gesendet werden. Die Coding-Theorie beschäftigt sich mit der Suche nach expliziten Methoden, sogenannten Codes, zur Steigerung der Effizienz und zur Verringerung der Fehlerrate der Datenkommunikation über laute Kanäle bis in die Nähe der Kanalkapazität. Diese Codes können grob in Datenkompression (Source-Codierung) und Fehlerkorrektion (Kanal-Codierung)-Techniken unterteilt werden. Im letzteren Fall dauerte es viele Jahre, die Methoden zu finden, die Shannons Arbeit erwies sich als möglich. Eine dritte Klasse von Informationstheorie-Codes sind kryptographische Algorithmen (beide Codes und Ciphers). Konzepte, Methoden und Ergebnisse aus der Codierung Theorie und Informationstheorie sind weit verbreitet in der Kryptographie und Kryptanalyse. Siehe das Artikelverbot (Einheit) für eine historische Anwendung. Historischer Hintergrund Im Bell System Technical Journal im Juli und Oktober 1948 veröffentlichte Claude E. Shannons klassisches Papier "A Mathematical Theory of Communication" in der Zeitschrift Bell System Technical Journal. Vor diesem Artikel wurden in Bell Labs begrenzte informationstheoretische Ideen entwickelt, die implizit Ereignisse gleicher Wahrscheinlichkeit annehmen. Harry Nyquists 1924 Papier, Bestimmte Faktoren, die Telegraph affecting Geschwindigkeit, enthält eine theoretische Schnittquantifizierung Intelligenz und die "Liniengeschwindigkeit", mit der sie durch ein Kommunikationssystem übertragen werden kann, wobei die Beziehung W = K log m (Reichsel Boltzmanns Konstante) gegeben ist, wobei W die Geschwindigkeit der Nachrichtenübertragung ist, m die Anzahl der verschiedenen Spannungspegel ist, die bei jedem Zeitschritt gewählt werden sollen, und K eine Konstante ist. Ralph Hartleys 1928-Papier, Übertragung von Informationen, verwendet die Wortinformation als messbare Menge, was die Fähigkeit des Empfängers widerspiegelt, eine Folge von Symbolen von jeder anderen zu unterscheiden, so dass die Information als H = log Sn = n log Sn, wo S die Anzahl der möglichen Symbole war, und n die Anzahl der Symbole in einer Übertragung.Die Einheit der Information war daher die Dezimalstelle, die seitdem manchmal als Einheit oder Maßstab oder Maß der Information die hartley in seiner Ehre genannt wurde. Alan Turing im Jahr 1940 nutzte ähnliche Ideen als Teil der statistischen Analyse des Bruchs des deutschen Zweiten Weltkriegs Enigma Chiffren. Viele der Mathematik hinter der Informationstheorie mit Ereignissen verschiedener Wahrscheinlichkeiten wurden für das Gebiet der Thermodynamik von Ludwig Boltzmann und J. Willard Gibbs entwickelt. Verbindungen zwischen der informationstheoretischen Entropie und der thermodynamischen Entropie, einschließlich der wichtigen Beiträge von Rolf Landauer in den 1960er Jahren, werden in Entropy in der Thermodynamik und Informationstheorie untersucht. In Shannons revolutionärer und bahnbrechender Zeitung, deren Arbeit bis Ende 1944 in Bell Labs im Wesentlichen abgeschlossen war, stellte Shannon erstmals das qualitative und quantitative Kommunikationsmodell als statistischer Prozess vor, der der Informationstheorie zugrunde liegt und mit der Behauptung öffnet: "Das grundlegende Problem der Kommunikation ist die Wiedergabe an einem Punkt, entweder genau oder annähernd, eine an einem anderen Punkt ausgewählte Nachricht. " Mit ihm kamen die Ideen der Informationsentropie und Redundanz einer Quelle, und ihre Relevanz durch die Quelle Codierung Theorem; die gegenseitige Information, und die Kanalkapazität eines lauten Kanals, einschließlich der Verheißung der perfekten verlustfreien Kommunikation durch den noisy-Kanal Codierung Theorem; das praktische Ergebnis des Shannon-Hartley-Gesetzes für die Kanalkapazität eines Gaussian-Kanals; sowie das Bit - eine neue Weise. Informationstheorie basiert auf Wahrscheinlichkeitstheorie und -statistik. Die Informationstheorie betrifft sich oft mit Informationen der mit Zufallsvariablen verbundenen Verteilungen. Wichtige Informationsmengen sind Entropie, ein Informationsmaß in einer einzigen zufälligen Variablen und gegenseitige Informationen, ein gemeinsames Informationsmaß zwischen zwei zufälligen Variablen. Die ehemalige Menge ist eine Eigenschaft der Wahrscheinlichkeitsverteilung einer Zufallsgröße und gibt eine Grenze der Rate, mit der durch unabhängige Proben mit der gegebenen Verteilung erzeugte Daten zuverlässig komprimiert werden können. Letzteres ist eine Eigenschaft der gemeinsamen Verteilung von zwei zufälligen Variablen und ist die maximale Rate der zuverlässigen Kommunikation über einen lauten Kanal in der Grenze von langen Blocklängen, wenn die Kanalstatistik durch die gemeinsame Verteilung bestimmt wird. Die Wahl der logarithmischen Basis in den folgenden Formeln bestimmt die verwendete Einheit der Informationsentropie. Eine gemeinsame Informationseinheit ist das Bit, basierend auf dem binären Logarithm. Weitere Einheiten umfassen den nat, der auf dem natürlichen logarithm basiert, und die dezimale Ziffer, die auf dem gemeinsamen Logarithm basiert. Im folgenden wird ein Ausdruck des Formulars p log p durch Konvention als gleich Null betrachtet, wenn p = 0. Dies ist gerechtfertigt, weil lim p → 0 + p log ζ = 0 {\displaystyle \lim {_p\rightarrow 0+}p\log p=0} für jede logarithmische Basis. Entropie einer Informationsquelle Basierend auf der Wahrscheinlichkeits-Massenfunktion jedes zu kommunizierenden Quellsymbols wird die Shannon-Entropie H in Einheiten von Bits (pro Symbol) durch H = - Σ i p i log 2 ≠ (p i) gegeben. {\displaystyle H=-\sum i}p_{i}\log 2}(p_{i), wobei pi die Wahrscheinlichkeit des Auftretens des i-ten möglichen Wertes des Quellsymbols ist. Diese Gleichung gibt die Entropie in den Einheiten der Bits (pro Symbol), weil sie ein Logarithmus der Basis 2 verwendet, und diese Basis-2 Maß der Entropie wurde manchmal als Shannon zu seiner Ehre genannt. Die Entropie wird auch häufig mit dem natürlichen Logarithmus berechnet (Basis e, wo e Eulers Nummer ist), der eine Messung der Entropie in Nats pro Symbol erzeugt und die Analyse manchmal vereinfacht, indem es die Notwendigkeit, zusätzliche Konstanten in die Formeln einzubeziehen. Andere Basen sind auch möglich, aber weniger häufig verwendet. Beispielsweise wird ein Logarithmus der Basis 28 = 256 eine Messung in Bytes pro Symbol erzeugen, und ein Logarithmus der Basis 10 wird eine Messung in Dezimalziffern (oder hartleys) pro Symbol erzeugen.Intuitiv ist die Entropie HX einer diskreten Zufallsgröße X ein Maß für die dem Wert X zugeordnete Unsicherheit, wenn nur ihre Verteilung bekannt ist. Die Entropie einer Quelle, die eine Sequenz von N-Symbolen aussendet, die unabhängig und identisch verteilt (iid) sind N ⋅ H-Bits (per Nachricht von N-Symbolen). Sind die Quelldatensymbole identisch verteilt, aber nicht unabhängig, so wird die Entropie einer Nachricht der Länge N kleiner als N ⋅ H sein. Wenn man 1000 Bits (0s und 1s) sendet und der Wert jedes dieser Bits dem Empfänger (eine bestimmte Größe mit Sicherheit) vor der Übertragung bekannt ist, ist klar, dass keine Information übertragen wird. Wenn jedoch jedes Bit unabhängig gleichwahrscheinlich 0 oder 1 ist, wurden 1000 Shannons von Informationen (mehr häufig Bits genannt) übertragen. Zwischen diesen beiden Extremen können Informationen wie folgt quantifiziert werden. Wenn X {\displaystyle \mathbb {X} der Satz aller Nachrichten {x1, ..., xn} ist, dass X sein könnte, und p(x) die Wahrscheinlichkeit von einigen x ε X {\displaystyle x\in \mathbb {X} } ist, dann wird die Entropie, H, von X definiert: H (X) = E X [ I (x) ] = p {\displaystyle H(X)=\mathbb {E} {_X}[I(x)]=-\sum {_x\in \mathbb {X}p(x)\log p(x}) (Hier ist I(x) die Selbstinformation, die der Entropiebeitrag einer einzelnen Nachricht ist, und E X {\displaystyle \mathbb {_X} ist der erwartete Wert.) Eine Eigenschaft der Entropie ist, dass sie maximiert wird, wenn alle Nachrichten im Nachrichtenraum äquiprobierbar sind p(x=) 1/n; d.h. am unvorhersehbaren, in welchem Fall H(X)=log n. Der besondere Fall der Informationsentropie für eine Zufallsvariable mit zwei Ergebnissen ist die binäre Entropiefunktion, die üblicherweise zur logarithmischen Basis 2 genommen wird, also die Shannon (Sh) {\displaystyle H_{\mathrm {b}(p)=-p\log {_2}p-(1-p)\log {_2}(1-p) Gemeinsame Entropie Die gemeinsame Entropie von zwei diskreten Zufallsvariablen Xand Y ist lediglich die Entropie ihrer Paarung:(X, Y.) Dies bedeutet, dass, wenn X und Y unabhängig sind, ihre gemeinsame Entropie die Summe ihrer einzelnen Entropie ist. Wenn z.B. (X, Y) die Position eines Schachstücks darstellt - X die Reihe und Y die Säule, dann die gemeinsame Entropie der Reihe des Stückes und die Säule des Stückes wird die Entropie der Position des Stückes sein. H (X, Y ) = E X, Y [ − log ‡ p ( x , y ) ] = − Σ x , y p ( x , y ) log p ( x , y ) {\displaystyle H(X,Y)\mathbb {E} {_X,Y}[-log p(x,y)]=-\sum{_x,Y] Trotz ähnlicher Vorstellung sollte die gemeinsame Entropie nicht mit Kreuzentropie verwechselt werden. Bedingte Entropie (Equivokation) Die bedingte Entropie oder bedingte Unsicherheit von X gegebener Zufallsvariable Y (auch die Äquivokation von X um Y genannt) ist die durchschnittliche bedingte Entropie über Y: H (X | Y ) = E Y[ H ( X | y ) ] = - Σ y ε y p ( y ) Σ x ε x p ( x | y ) log ‡ p ( x | y ) = - Σ x , y p ( x , y ) log p ( x | y ) . {\displaystyle H(X|Y)=\mathbb (E} {_Y}[H(X|y)]=-\sum {_y\in Y}p(y)\sum {_x\in X}p(x|y)\log p(x|y)=-\sum {_x,y}p(x,y)\log p(x|y}) Da die Entropie auf einer zufälligen Variable oder auf einer bestimmten Variablen konditioniert werden kann, sollte darauf geachtet werden, diese beiden Definitionen der bedingten Entropie nicht zu verwechseln, deren früherer häufiger verwendet wird. Eine grundlegende Eigenschaft dieser Form der bedingten Entropie ist, dass: H (X | Y ) = H (X, Y ) - H (Y ) . {\displaystyle H(X|Y)=H(X,Y)-H(Y}.\ Mutual information (transinformation)Mutual information misst die Menge der Information, die durch Beobachtung einer anderen über eine Zufallsvariable gewonnen werden kann. Es ist wichtig in der Kommunikation, wo es verwendet werden kann, um die Menge der zwischen gesendeten und empfangenen Signalen geteilten Informationen zu maximieren. Die gegenseitige Information von X relativ zu Y ist gegeben durch: I (X; Y) = E X, Y [S I ( x , y )] = Σ x, y p ( x , y ) log kenn p (x , y ) p ( y ) p ( y ) {\displaystyle I(X;Y)=\mathbb {E} X,Y}(SI) Eine grundlegende Eigenschaft der gegenseitigen Informationen ist, dass I (X; Y) = H (X) - H (X | Y ) . {\displaystyle I(X;Y)=H(X)-H(X.Y}.,\ Das heißt, wissend Y, können wir einen Durchschnitt von I(X; Y)-Bits in der Kodierung X speichern, verglichen mit dem nicht wissenden Y. Mutual information is symmetric: I (X; Y) = I (Y; X) = H (X) + H (Y) - H (X, Y) = I (X;Y) = I(Y;X) = I(Y;X)= Gegenseitige Informationen können als durchschnittliche Kullback-Leibler-Divergenz (Informationsgewinn) zwischen der posterior Wahrscheinlichkeitsverteilung von X bei dem Wert von Y und der vorherigen Verteilung auf X: I (X; Y) = E p ( y ) [ D K L (p (X | Y = y ) p (X ) ) ] angegeben werden.{\displaystyle I(X;Y)=\mathbb {E} p(y)}[D_{\mathrm {KL} p(X|Y=y)\.p(X]. Mit anderen Worten ist dies ein Maß dafür, wie sich im Durchschnitt die Wahrscheinlichkeitsverteilung auf X ändern wird, wenn wir den Wert von Y erhalten. Dies wird oft als Abweichung vom Produkt der Randverteilungen zur tatsächlichen gemeinsamen Verteilung neu berechnet: I (X ; Y ) = D K L (p (X, Y ) gebildet p (X ) p (Y ) ) ; Y ) ; Kullback–Leibler Divergenz (Informationsgewinn)Die Kullback–Leibler Divergenz (oder Information Divergenz, Informationsgewinn oder relative Entropie) ist eine Möglichkeit, zwei Distributionen zu vergleichen: eine wahre Wahrscheinlichkeitsverteilung p (X) {\displaystyle p(X}) und eine beliebige Wahrscheinlichkeitsverteilung q (X) {\displaystyle q(X}). Wenn wir Daten in einer Weise komprimieren, die q (X) annimmt {\displaystyle q(X}) ist die Verteilung, die einigen Daten zugrunde liegt, wenn in Wirklichkeit p (X) {\displaystyle p(X}) die richtige Verteilung ist, die Kullback-Leibler Divergenz die Anzahl der durchschnittlichen zusätzlichen Bits pro Datum, die für die Kompression erforderlich ist. Es ist also definiert D K L (p (X ) φ (X ) ) = Σ x ε X - p ( x ) log ‡ q ( x ) - Σ x ε X - p ( x ) log p ( x ) = Σ x ε X p ( x ) log ∂ p ( x ) q ( x ) . {\displaystyle D_{\mathrm {KL}p(X)\|q(X))=\sum {_x\in X}-p(x)\log {q(x)}\,-\,\sum {_x\in X}-p(x)\log p(x)}=\sum {_x\in X}p(x)\log\frac Obwohl es manchmal als "Abstandsmetrik" verwendet wird, ist KL-Divergenz keine wahre Metrik, da sie nicht symmetrisch ist und die Dreiecksungleichheit nicht erfüllt (macht es halbquasimetrisch). Eine weitere Interpretation der KL-Divergenz ist die "unnötige Überraschung", die von einem Vorherigen aus der Wahrheit eingeführt wird: Nehmen Sie an, eine Zahl X wird zufällig aus einem diskreten Satz mit Wahrscheinlichkeitsverteilung p (x) gezogen werden {\displaystyle p(x}). Wenn Alice die wahre Verteilung p ( x ) {\displaystyle p(x}) kennt, während Bob glaubt (hat a prior) dass die Verteilung q ( x ) {\displaystyle q(x}) ist, dann wird Bob mehr überrascht sein als Alice, im Durchschnitt, wenn man den Wert von X sieht. Die KL Divergenz ist der (objektive) Erwartungswert von Bobs (subjektive) surprisal minus Alices surprisal, gemessen in Bits, wenn das Log in Basis ist 2. Auf diese Weise kann das Ausmaß, in dem Bobs Vorhergehenden falsch ist, quantifiziert werden, in Bezug darauf, wie " unnötig überrascht" es erwartet wird, ihn zu machen.andere Mengen Weitere wichtige Informations-Theoretische Mengen umfassen Rényi entropy (eine Verallgemeinerung der Entropie), Differenzentropie (eine Verallgemeinerung der Informationsmengen zu kontinuierlichen Verteilungen), und die bedingten gegenseitigen Informationen. Coding Theorie Coding Theorie ist eine der wichtigsten und direkten Anwendungen der Informationstheorie. Sie kann in die Quellcodierungstheorie und Kanalcodierungstheorie unterteilt werden. Anhand einer statistischen Datenbeschreibung quantifiziert die Informationstheorie die Anzahl der Bits, die zur Beschreibung der Daten benötigt werden, was die Informationsentropie der Quelle ist. Datenkompression (Quellencodierung): Es gibt zwei Formulierungen für das Kompressionsproblem: verlustlose Datenkompression: die Daten müssen exakt rekonstruiert werden; verlustige Datenkompression: nimmt Bits an, die zur Rekonstruktion der Daten benötigt werden, innerhalb eines vorgegebenen Treueniveaus, gemessen durch eine Verzerrungsfunktion. Diese Untermenge der Informationstheorie wird als Rate-Distortion-Theorie bezeichnet. Fehlerkorrekturcodes (Kanalcodierung:) Während die Datenkompression so viel Redundanz wie möglich entfernt, fügt ein Fehlerkorrekturcode genau die richtige Art von Redundanz (d.h. Fehlerkorrektur) hinzu, die benötigt wird, um die Daten effizient und treu über einen lauten Kanal zu übertragen. Diese Spaltung der Codierungstheorie in Kompression und Übertragung ist durch die Informationsübertragungstheoreme oder Source-Channel-Trenntheorems gerechtfertigt, die die Verwendung von Bits als universelle Währung für Informationen in vielen Kontexten rechtfertigen. Diese Theoreme halten jedoch nur in der Situation, in der ein sendender Benutzer einem empfangenden Benutzer kommunizieren möchte. In Szenarien mit mehr als einem Sender (der Mehrzweckkanal), mehr als einem Empfänger (der Sendekanal) oder zwischengeschalteten Helfern (der Relaiskanal) oder mehr allgemeine Netzwerke kann die Kompression gefolgt von der Übertragung nicht mehr optimal sein. Netzwerkinformationstheorie bezieht sich auf diese multiagent Kommunikationsmodelle. Quelle Theorie Jeder Vorgang, der aufeinanderfolgende Nachrichten erzeugt, kann als Informationsquelle betrachtet werden. Eine speicherlose Quelle ist eine, in der jede Nachricht eine unabhängige, identisch verteilte Zufallsvariable ist, während die Eigenschaften von Ergonomie und Stationarität weniger restriktive Einschränkungen auferlegen. Alle diese Quellen sind stochastisch. Diese Begriffe werden in ihrem eigenen Recht außerhalb der Informationstheorie gut untersucht. Rate Information Rate ist die durchschnittliche Entropie pro Symbol. Für speicherlose Quellen ist dies lediglich die Entropie jedes Symbols, während es im Falle eines stationären stochastischen Prozesses r = lim n → ∞ H ist (X n  of X n - 1 , X n - 2 , X n - 3 , ... ;\displaystyle r=\lim {_n\ zu \infty H(X_{n}|}{n}-1 Für den allgemeineren Fall eines nicht unbedingt stationären Prozesses ist die durchschnittliche Rate r = lim n → ∞ 1 n H (X 1 , X 2 , ... X n ) ; {\displaystyle r=\lim {_n\to \infty {}\frac 1}{n}H(X_{1},X_{2},\dots X_{n})); das ist gemeinsames Für stationäre Quellen ergeben diese beiden Ausdrücke das gleiche Ergebnis. Die Informationsrate ist definiert als r = lim n → ∞ 1 n I (X 1 , X 2 , ... X n ; Y 1 , Y 2 , ... Y n ) ; 1{n}I(X_{1},X_{2},\dots X_{n};Y_{1},Y_{2},\dots Y_{n};Es ist in der Informationstheorie üblich, von der Rate oder Entropie einer Sprache zu sprechen. Dies ist beispielsweise dann sinnvoll, wenn die Informationsquelle englische Prosa ist. Die Rate einer Informationsquelle hängt mit ihrer Redundanz zusammen und wie gut sie komprimiert werden kann, dem Thema Quellcodierung. Kanalkapazität Kommunikation über einen Kanal ist die primäre Motivation der Informationstheorie. Die Kanäle führen jedoch oft nicht zu einer exakten Rekonstruktion eines Signals, Lärm, Schweigezeiten und andere Formen der Signalverfälschung verschlechtern oft die Qualität. Betrachten Sie den Kommunikationsprozess über einen diskreten Kanal. Im folgenden wird ein einfaches Verfahrensmodell dargestellt: → Nachricht W Encoder f n → E n c o d e d s e q u e n c e X n Kanal p ( y | x ) → ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ }\hline text{Encoder}\f_{n}\\hline End{array){\xrightarrow[{\mathrm {Encoded \atop Sequenz} )X^{n}{\begin{array}{\c}\hline text{ Channel}\p(y|x\\hline end{array}{\xrightarrow[{\mathrm {\begin{array}{|c }\hline text{ Entscheider: '\g_{n}\\hline end{array}{\xrightarrow\{\mathrm {Estimed \atop message} {\}hat {W}Dabei stellt X den Raum der gesendeten Nachrichten dar und Y den Raum der während einer Zeiteinheit empfangenen Nachrichten über unseren Kanal. Lassen Sie p(y|x) die bedingte Wahrscheinlichkeitsverteilungsfunktion von Y gegeben X sein. Wir werden p(y|x) als ein inhärentes festes Eigentum unseres Kommunikationskanals betrachten (die die Natur des Rauschens unseres Kanals darstellen). Dann wird die gemeinsame Verteilung von X und Y vollständig durch unseren Kanal und durch unsere Wahl von f(x) die marginale Verteilung von Nachrichten, die wir wählen, um über den Kanal zu senden bestimmt. Unter diesen Einschränkungen möchten wir die Informationsrate oder das Signal maximieren, die wir über den Kanal kommunizieren können. Die geeignete Maßnahme hierfür ist die gegenseitige Information, und diese maximale gegenseitige Information wird als Kanalkapazität bezeichnet und wird durch: C = max f I (X; Y) gegeben. Diese Kapazität hat die folgende Eigenschaft, die mit der Kommunikation mit der Informationsrate R verbunden ist (wobei R in der Regel Bits pro Symbol ist). Für jede Informationsrate R < C und Kodierungsfehler ε > 0 gibt es für groß genug N einen Code von Länge N und Rate ≥ R und einen Decodierungsalgorithmus, so dass die maximale Wahrscheinlichkeit des Blockfehlers ≤ ε ist; d.h. es ist immer möglich, mit willkürlich kleinem Blockfehler zu übertragen. Zusätzlich für jeden Satz R > C, es ist unmöglich, mit willkürlich kleinen Blockfehler zu übertragen. Die Kanalcodierung beschäftigt sich damit, solche nahezu optimalen Codes zu finden, mit denen Daten über einen lauten Kanal mit einem kleinen Kodierungsfehler in einer Geschwindigkeit nahe der Kanalkapazität übertragen werden können. Kapazität bestimmter Kanalmodelle Ein kontinuierlicher analoger Kommunikationskanal, der dem Gaussschen Rauschen ausgesetzt ist – siehe Shannon-Hartley theorem. Ein binärer symmetrischer Kanal (BSC) mit Überkreuzwahrscheinlichkeit p ist ein binärer Eingang, binärer Ausgangskanal, der das Eingabebit mit Wahrscheinlichkeit p umkippt. Das BSC hat eine Kapazität von 1 - Hb(p)-Bits pro Kanalgebrauch, wobei Hb die binäre Entropiefunktion zum Basis-2 Logarithm ist: Ein binärer Löschkanal (BEC) mit Löschwahrscheinlichkeit p ist ein binärer Eingang, ternärer Ausgangskanal. Die möglichen Kanalausgänge sind 0, 1 und ein drittes Symbol e als Löschung bezeichnet. Die Löschung stellt einen vollständigen Informationsverlust über ein Eingabebit dar. Die Kapazität des BEC beträgt 1 - p Bit pro Kanaleinsatz. Kanäle mit Speicher und gerichteten Informationen In der Praxis haben viele Kanäle Speicher. Der Kanal wird zum Zeitpunkt i {\displaystyle i} durch die bedingte Wahrscheinlichkeit P ( y i | x i, x i - 1 , x 1 - 2 ,... x 1 , y i - 1 , y 1 - 2 ,... , y 1 ) gegeben. {\displaystyle P(y_{i}|x_{i},x_{i-1},x_{1-2},...,x_{1},y_{i-1},y_{1-2},...,y_{1) Es ist oft bequemer, die Notation x i = ( x i, x i - 1 , x 1 - 2 ,..., x 1 ) {\displaystyle x{i}=(x_{i},x_{i-1},x_{1-2},...,x_{1) zu verwenden und der Kanal zu P ( y i | x i , y i - 1 ) .(\displaystyle P(y_{i}|x{i},y^{i-1.) In einem solchen Fall wird die Kapazität durch die gegenseitige Informationsrate gegeben, wenn kein Feedback zur Verfügung steht und der direkte Informationssatz für den Fall, dass entweder Feedback vorliegt oder nicht (wenn kein Feedback vorliegt, entspricht die gerichtete Information j den gegenseitigen Informationen.) Anwendungen auf andere Bereiche Intelligenz verwendet und Geheimhaltung Anwendungen Informationen theoretische Konzepte gelten für Kryptographie und Kryptanalyse. Turings Informationseinheit, das Verbot, wurde im Ultra-Projekt verwendet, bricht den deutschen Enigma-Maschinencode und beschleunigt das Ende des Zweiten Weltkriegs in Europa. Shannon selbst definierte ein wichtiges Konzept, das jetzt als Einstadtdistanz bezeichnet wurde. Basierend auf der Redundanz des Klartexts versucht es, eine minimale Menge an Ciphertext zu geben, die notwendig ist, um eine einzigartige Entschlüsselung zu gewährleisten. Die Informationstheorie führt uns dazu, zu glauben, dass es viel schwieriger ist, Geheimnisse zu bewahren, als es zuerst erscheinen könnte. Ein brutaler Kraftangriff kann Systeme auf Basis asymmetrischer Schlüsselalgorithmen oder auf am häufigsten verwendeten Methoden symmetrischer Schlüsselalgorithmen (manchmal geheime Schlüsselalgorithmen genannt) wie Block-Ciphers brechen. Die Sicherheit aller derartigen Verfahren kommt derzeit aus der Annahme, dass kein bekannter Angriff sie in praktischer Zeit brechen kann. Die informationstheoretische Sicherheit bezieht sich auf Methoden wie das einmalige Pad, das nicht anfällig für solche brutalen Angriffe sind. In solchen Fällen kann die positive bedingte gegenseitige Information zwischen dem Klartext und dem Ciphertext (im Schlüssel) eine ordnungsgemäße Übertragung gewährleisten, während die bedingungslose gegenseitige Information zwischen dem Klartext und dem Ciphertext Null bleibt, was zu absolut sicheren Kommunikationen führt. Mit anderen Worten, ein Eavesdropper wäre nicht in der Lage, seine oder ihre Vermutung des Klartextes zu verbessern, indem er Wissen über den Chiffretext, aber nicht über den Schlüssel gewinnt. Wie in jedem anderen kryptographischen System muss jedoch darauf geachtet werden, selbst informationstheoretisch sichere Methoden korrekt anzuwenden; das Projekt Venona konnte die einmaligen Pads der Sowjetunion aufgrund ihrer unangemessenen Wiederverwendung von Schlüsselmaterial knacken. Pseudorandom Nummer Generation Pseudorandom Nummer Generatoren sind weit verbreitet in Computer-Sprachbibliotheken und Anwendungsprogrammen. Sie sind, fast universell, ungeeignet für kryptographische Verwendung, da sie nicht die deterministische Natur der modernen Computerausrüstung und Software verlassen. Eine Klasse von verbesserten Zufallszahlengeneratoren wird als kryptographisch sichere Pseudozufallszahlen-Generatoren bezeichnet, aber auch sie erfordern zufällige Samen extern an die Software, wie beabsichtigt zu arbeiten. Diese können über Extraktoren erhalten werden, wenn sie sorgfältig durchgeführt werden. Das Maß für eine ausreichende Zufallszahl in Extraktoren ist min-entropy, ein Wert, der mit Shannon entropy durch Rényi entropy zusammenhängt; Rényi entropy wird auch bei der Beurteilung der Zufallsbildung in kryptographischen Systemen verwendet. Die Unterschiede zwischen diesen Maßnahmen bedeuten zwar, dass eine zufällige Variable mit hoher Shannon-Entropie für den Einsatz in einem Extraktor und so für Kryptographie-Anwendungen nicht unbedingt zufriedenstellend ist. Seismische Exploration Eine frühe kommerzielle Anwendung der Informationstheorie war auf dem Gebiet der seismischen Ölexploration. Die Arbeit in diesem Bereich ermöglichte es, das unerwünschte Geräusch vom gewünschten seismischen Signal abzustreifen und zu trennen. Informationstheorie und digitale Signalverarbeitung bieten eine wesentliche Verbesserung der Auflösung und Bildklarheit gegenüber früheren analogen Methoden. Semiotik Semiotiker Doede Nauta und Winfried Nöth betrachteten beide Charles Sanders Peirce als eine Theorie der Informationen in seinen Arbeiten auf Semiotik geschaffen. Nauta definierte semiotische Informationstheorie als die Studie "die internen Prozesse der Codierung, Filterung und Informationsverarbeitung. "Konzepte aus der Informationstheorie wie Redundanz und Code-Kontrolle wurden von Semiotikern wie Umberto Eco und Ferruccio Rossi-Landi verwendet, um Ideologie als eine Form der Nachrichtenübertragung zu erklären, wobei eine dominante soziale Klasse ihre Botschaft mit Zeichen ausgibt, die einen hohen Redundanzsgrad aufweisen, so dass nur eine Nachricht unter einer Auswahl konkurrierender entschlüsselt wird. Verschiedene Anwendungen Die Informationstheorie hat auch Anwendungen in der Gambling- und Informationstheorie, schwarzen Löchern und Bioinformatik. Siehe auch Anwendungen Geschichte Hartley, R.V.L Geschichte der Informationstheorie Shannon, C.E Zeitlinie der Informationstheorie Yockey, H.P Theorie Konzepte Referenzen Das klassische Werk Weitere Zeitschriftenartikel Lehrbücher zur Informationstheorie Weitere Bücher MOOC zur Informationstheorie Raymond W. Yeung, "Informationstheorie" (The Chinese University of Hong Kong) Externe Links Informationen, Encyclopedia of Mathematics, EMS Press, 2001 [1994] Lambert F. L. (1999,) "Shuffled Cards, Messy Desks, and Disorderly Dorm Rooms - Beispiele für Entropy Erhöhe?Unsinn!" Journal of Chemical Education IEEE Information Theory Society und ITSOC Monographen, Umfragen und Bewertungen