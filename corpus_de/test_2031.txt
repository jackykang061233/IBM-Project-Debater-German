In künstlichen neuronalen Netzen definiert die Aktivierungsfunktion eines Knotens den Ausgang dieses Knotens bei Eingabe oder Eingabe. Als digitales Netzwerk von Aktivierungsfunktionen, die je nach Eingang EIN (1) oder AUS (0,) sein können, ist eine standardmäßig integrierte Schaltung zu erkennen. Dies ist dem linearen Perceptron in neuronalen Netzen ähnlich. Mit nur nichtlinearen Aktivierungsfunktionen können solche Netzwerke jedoch nichttriviale Probleme nur mit einer geringen Anzahl von Knoten berechnen und solche Aktivierungsfunktionen werden als Nichtlinearitäten bezeichnet. Klassifizierung von Aktivierungsfunktionen Die häufigsten Aktivierungsfunktionen können in drei Kategorien unterteilt werden: Gratfunktionen, radiale Funktionen und Faltfunktionen. Ridge-Aktivierungsfunktionen Ridge-Funktionen sind multivariate Funktionen, die auf eine lineare Kombination der Eingangsgrößen einwirken. Häufig verwendete Beispiele sind: Lineare Aktivierung: φ (v ) = a + v' b {\displaystyle \phi (\mathbf {v} =)a+\mathbf {v} '\mathbf {b} , ReLU-Aktivierung: φ (v ) = max (0, a + v' b ) 0 {\displaystyle \phi (\mathbf {v} =)1_{a+\mathbf {v} '\mathbf {b} >0}, Logistische Aktivierung: φ (v ) = ( 1 + exp kennzeichnet - a - v' b ) ) - 1\displaystyle \phi (\mathbf {v} =)1+\exp( Diese Funktion ist in ihrer einfachsten Form binär, d.h. entweder das Neuron feuert oder nicht. Die Funktion sieht aus wie φ (v ) = U (a + v' b ) {\displaystyle \phi (\mathbf {v} =)U(a+\mathbf {v} '\mathbf {b} )}, wobei U {\displaystyle U} die Heaviside-Schrittfunktion ist. Mit einer Linie positiver Steigung kann die mit zunehmendem Eingangsstrom auftretende Erhöhung der Zündrate reflektiert werden. Eine solche Funktion wäre der Form φ (v ) = a + v' b {\displaystyle \phi (\mathbf {v} =)a+\mathbf {v} '\mathbf {b} . Neuronen können auch nicht schneller als eine bestimmte Rate feuern, motivieren sigmoid Aktivierungsfunktionen, deren Reichweite ein endliches Intervall ist. Radiale Aktivierungsfunktionen In RBF-Netzen werden eine spezielle Klasse von Aktivierungsfunktionen, die als radiale Basisfunktionen (RBFs) bekannt sind, eingesetzt, die als universelle Funktions-Aktuatoren äußerst effizient sind. Diese Aktivierungsfunktionen können viele Formen annehmen wie: Gaussian: φ (v ) = exp ‡ ( − ε v - c ected 2 2 σ 2 σ 2 ) {\displaystyle ,\phi (\mathbf {v} )(-{\frac {fra\mathbf} - Ja. Multiquadratics: φ (v ) = zusammengestellt von 2 + a 2 {\displaystyle ,\phi (\mathbf {v} {=)\sqrt {|\mathbf {v} - Ja. |\2}+a^{2 wobei c {\displaystyle \mathbf {c} der Vektor, der das Funktionszentrum repräsentiert, und ein {\displaystyle a} und σ {\displaystyle \sigma } die Ausbreitung des Radius beeinflussen. Folding AktivierungsfunktionenFolding Aktivierungsfunktionen werden in den Pooling-Schichten in konvolutionalen neuronalen Netzwerken und in Ausgabeschichten von mehrstufigen Klassifikationsnetzwerken umfassend eingesetzt. Diese Aktivierungen führen eine Aggregation über die Eingänge, wie z.B. das mittlere, minimale oder maximale. Bei der Multiclass-Klassifikation wird häufig die Softmax-Aktivierung verwendet. Vergleich von Aktivierungsfunktionen Es gibt zahlreiche Aktivierungsfunktionen. Hinton et al.s Halbnalpapier 2012 zur automatischen Spracherkennung verwendet eine logistische sigmoid Aktivierungsfunktion. Die seminale 2012 AlexNet Computer Vision Architektur verwendet die ReLU Aktivierung Funktion, wie die seminale 2015 Computer Vision Architektur ResNet. Das seminale Sprachverarbeitungsmodell BERT 2018 verwendet eine glatte Version der ReLU, der GELU. Neben ihrer empirischen Leistung weisen Aktivierungsfunktionen auch unterschiedliche mathematische Eigenschaften auf: Nichtlinear Ist die Aktivierungsfunktion nichtlinear, so kann sich ein zweischichtiges neuronales Netz als universeller Funktions-Azimator erweisen. Dies ist als Universal Approximation Theorem bekannt. Die Identitätsaktivierungsfunktion erfüllt diese Eigenschaft nicht. Wenn mehrere Schichten die Identitätsaktivierungsfunktion nutzen, ist das gesamte Netzwerk einem einschichtigen Modell äquivalent. Reichweite Wenn der Bereich der Aktivierungsfunktion endlich ist, neigen Gradienten-basierte Trainingsmethoden zu stabileren, da Musterpräsentationen nur begrenzte Gewichte signifikant beeinflussen. Wenn die Reichweite unendlich ist, ist Training in der Regel effizienter, weil Musterpräsentationen die meisten Gewichte erheblich beeinflussen. Im letzteren Fall sind typischerweise kleinere Lernraten erforderlich. Kontinuierlich differenzierbar Diese Eigenschaft ist wünschenswert (ReLU ist nicht kontinuierlich differenzierbar und hat einige Probleme mit Gradienten-basierter Optimierung, aber es ist immer noch möglich), um Gradienten-basierte Optimierungsverfahren zu ermöglichen. Die binäre Schritt-Aktivierungsfunktion ist bei 0 nicht differenzierbar, und sie differenziert für alle anderen Werte auf 0, so dass Gradienten-basierte Verfahren damit keinen Fortschritt machen können. Diese Eigenschaften beeinflussen weder die Leistung entscheidend noch sind sie die einzigen mathematischen Eigenschaften, die nützlich sein können. So eignet sich der streng positive Bereich des softplus zur Vorhersage von Varianzen in variierenden Autocodierern. Die folgende Tabelle vergleicht die Eigenschaften mehrerer Aktivierungsfunktionen, die Funktionen einer Falte x aus der vorherigen Schicht oder Schichten sind: ^ Hier ist σ {\displaystyle \sigma } die logistische Funktion^. Die folgende Tabelle listet Aktivierungsfunktionen auf, die keine Funktionen einer einzelnen Falte x aus der vorherigen Schicht oder Schichten sind: ^ Hier ist δ i j {\displaystyle \delta {_ij} die Kronecker delta^. Beispielsweise könnte j {\displaystyle j} durch die Anzahl der Kerne der vorherigen neuronalen Netzwerkschicht iterieren, während i {\displaystyle i} durch die Anzahl der Kerne der aktuellen Schicht iteriert. Siehe auch Logistische Funktion Gleichrichter (Nural-Netzwerke)Stabilität (Lerntheorie)Softmax-Funktion == Referenzen ==