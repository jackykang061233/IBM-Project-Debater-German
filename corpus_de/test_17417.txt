Deepfakes (ein Hafenmanteau von "Deep Learning" und Fälschung) sind synthetische Medien, in denen eine Person in einem vorhandenen Bild oder Video mit einer anderen Person ersetzt wird. Obwohl der Akt von Förmstoffen nicht neu ist, bremst er mächtige Techniken aus dem maschinellen Lernen und künstlichen Intelligenz, um visuelle und audiovisuelle Inhalte mit hohem Potenzial zu manipulieren oder zu generieren. Die wichtigsten maschinellen Lernmethoden, die zur Schaffung von Tieffakes verwendet werden, basieren auf einem tiefen Lernen und umfassen die Ausbildung generativer Neuralnetzarchitekturen wie Autoencoders oder generativer Adversarialnetze (GAN). Deepfakes haben breite Aufmerksamkeit für ihre Verwendung in prominenten pornographischen Videos, neuvenge porn, gefälschte Nachrichten, Hetze und Finanzbetrug geweckt. Dies hat die Reaktion sowohl der Industrie als auch der Regierung auf die Feststellung und Begrenzung ihrer Nutzung geschuldet. Geschichte Photomanipulation wurde im 19. Jahrhundert entwickelt und in Kürze auf Bewegungsbilder angewendet. Technologie hat sich während des 20. Jahrhunderts stetig verbessert und schneller mit dem digitalen Video. Deepfake-Technologie wurde von Forschern in akademischen Einrichtungen entwickelt, die in den 90er Jahren beginnen, und später von Amateuren in Online-Gemeinschaften. Kürzlich wurden die Methoden von der Industrie angenommen. wissenschaftliche Forschung im Zusammenhang mit tiefenfakes liegt vor allem im Bereich der Computervision, einem Teilbereich der Informatik. Ein frühes wegweisendes Projekt war das 1997 veröffentlichte Video-Reformprogramm, das bestehende Videomaterial einer Person veränderte, die darlegte, dass sie die in einem anderen Audio-Track enthaltenen Wörter missbraucht. Es war das erste System, um diese Art der Gesichtsreanimation vollständig zu automatisieren, und es nutzte so maschinelle Lerntechniken, um Verbindungen zwischen den von einem Video betroffenen Geräuschen und der Form des Gesichts herzustellen. zeitgenössische akademische Projekte haben sich auf die Schaffung realistischer Videos und die Verbesserung der Techniken konzentriert. Mit dem im Jahr 2017 veröffentlichten "Synthesizing Obama"-Programm wird das Videomaterial des ehemaligen Präsidenten Barack Obama geändert, um ihm die in einem separaten Audio-Track enthaltenen Worte zu präsentieren. Das Projekt stellt einen wesentlichen Forschungsbeitrag für seine Photorealistische Technik zur Synthese von Mundformen aus Audio vor. Das im Jahr 2016 veröffentlichte Gesichts2Face-Programm moduliert Videoaufnahmen von Gesicht einer Person, um sie zu beschreiben, die Gesichtsausdrücke einer anderen Person in Echtzeit. Im Rahmen des Projekts wird die erste Methode zur Echtzeit-Einführung von Gesichtsausdrücken mit einer Kamera aufgeführt, die keine Tiefe erfasst, so dass es möglich ist, die Technik mit gemeinsamen Verbraucherkameras durchzuführen. Juni 2018 veröffentlichten Forscher an der University of California in Berkeley ein Papier mit der Einführung einer falschen Tanz-App, die den Eindruck von Masterful Tanzfähigkeit mit AI erzeugen kann. Dieses Projekt erweitert die Anwendung von tiefen Geförderern auf den gesamten Körper; frühere Arbeiten konzentrierten sich auf den Kopf oder Teile des Gesichts. Forscher haben auch gezeigt, dass sich die Tiefenfakes in andere Bereiche, wie z.B. die Manipulation medizinischer Bilder, ausweiten. In dieser Arbeit wurde gezeigt, wie ein Angriffsmann in der 3D CT-Scan des Patienten automatisch Lungenkrebs injizieren oder entfernen kann. Das Ergebnis war so überzeugend, dass es drei Radiologisten und eine hochmoderne Lungenkrebserkennungs-Identität verfälscht. Um die Bedrohung zu demonstrieren, haben die Autoren den Angriff auf ein Krankenhaus erfolgreich in einem weißen Hassdurchdringtest durchgeführt. Laut einer im Mai 2020 veröffentlichten Studie von tiefenfakes gibt es einen Zeitplan dafür, wie die Einrichtung und die Aufdeckung vertieft werden können. In der Umfrage wird festgestellt, dass sich die Forscher auf die Lösung der folgenden Herausforderungen der tiefgreifenden Gründung konzentrieren: Generalisierung. Qualitativ hochwertige Tieffakes werden oft durch Schulungen in Stunden von Videos des Ziels erreicht. Diese Herausforderung besteht darin, den Umfang der Ausbildungsdaten, die zur Erstellung hochwertiger Bilder erforderlich sind, zu minimieren und die Durchführung von ausgebildeten Modellen für neue Identitäten (ungebunden während der Ausbildung) zu ermöglichen.. Ausbildung. Schulung eines beaufsichtigten Modells kann hochwertige Ergebnisse liefern, erfordert jedoch Daten Paarung. Dies ist der Prozess, Beispiele für Inputs und ihre gewünschten Outputs für das Modell zu finden. Datenpaarung ist mühsam und unpraktisch bei der Ausbildung auf mehreren Identitäten und Gesichtsverhalten. Manche Lösungen umfassen Selbstkontrollierte Schulungen (unter Verwendung von Rahmen aus demselben Video), die Nutzung von nichtgeschädigten Netzen wie Fahrrad-GAN oder die Manipulation von Netzanbindungen. Identitätsverlagerung. Hier ist die Identität des Fahrers (d. h. der Schauspieler, der das Gesicht in einer Neuregelung kontrolliert) teilweise auf das entstehende Gesicht übertragen. Manche vorgeschlagenen Lösungen umfassen Aufmerksamkeitsmechanismen, wenig ergebnisorientiertes Lernen, Entflechtung, Grenzumrechnungen und Anschlussverbindungen. Dinge. Liegt ein Teil des Gesichts mit einer Hand, Haare, Gläsern oder einem anderen Punkt, dann kann es vorkommen, dass Artefakten auftreten. Ein gemeinsamer Okclusion ist ein geschlossener Mund, der den Mund und die Zähne verbirgt. Manche Lösungen umfassen Image-Segmentierung während der Ausbildung und In-painting. Temporal Kohärenz. In Videos, die tiefgefroren sind, können Artefakte wie z.B. Hörer und Schitter auftreten, weil das Netzwerk keinen Kontext der vorherigen Rahmen hat. Manche Forscher bieten diesen Kontext oder verwenden neuartige zeitliche Kohärenzverluste, um Realismus zu verbessern. Wie sich die Technologie verbessert, verringert sich der Eingriff. Insgesamt dürften sich die tieferen Auswirkungen auf Medien und Gesellschaft, Medienproduktion, Medienvertretungen, Medienpublikum, Geschlecht, Recht und Regulierung sowie Politik ergeben. Amateurentwicklung Ende 2017 stammte der Begriff „Atfakes“ aus einem belarussischen Nutzer mit dem Namen „Alumfakes“. Er, aber auch andere in der Woiwoll-Gemeinschaft r/deepfakes, teilten sie mit; viele Videos beteiligten sich an den Körper der Schauspielerin in pornographischen Videos, während nicht-pornographische Inhalte viele Videos mit dem Gesicht des Schauspielers Nicolas Christi in verschiedenen Filmen enthalten. Andere Online-Gemeinschaften sind weiterhin, einschließlich der Gemeinden, die keine Pornographie besitzen, wie r/SFWdeepfakes (kurz „Sicherheit für Arbeit tieffakes“), in denen die Mitglieder der Gemeinschaft tiefgreifende Beispiele für prominente, Politiker und andere in nicht-pornographischen Szenarien teilen. Andere Online-Gemeinschaften teilen weiterhin Pornographie auf Plattformen, die keine Tiefseepornographie verboten haben. Kommerzielle Entwicklung Januar 2018 wurde ein proprietärer Desktop-App namens gefälschteApp gestartet. Diese App ermöglicht es den Nutzern, Videos mit ihren Gesichtern, die miteinander vertauscht werden, leicht zu erstellen und zu teilen. Seit 2019 wurde der gefälschteApp durch offene Alternativen wie Gesichtsswap, Befehlslinie-basierte DeepFaceLab und Web-basierte Apps wie DeepfakesWeb.com Großunternehmen beginnen auch mit tiefenfakes. Die mobile App Riesen Momo hat den Antrag Zao gestellt, der es den Nutzern ermöglicht, ihr Gesicht auf Fernseh- und Filmclips mit einem einzigen Bild zu überlasten. Der japanische AI-Unternehmen DatenGrid hat eine umfassende Körpertiefstfake geschaffen, die eine Person von Kratzern schaffen kann. Sie beabsichtigen, diese für Mode und Kleidung zu verwenden. Audio tiefefakes und AI-Software, die nach 5 Sekunden Hörzeit auch tiefgreifende und geruchende menschliche Stimmen erkennen können. Im März 2020 wurde eine mobile tiefenfake-App gestartet. Es war die erste App für die Erstellung von prominenten tiefenfake-Videos von Mobiltelefonen. Klare Deepfakes-Technologie kann nicht nur verwendet werden, um Botschaften und Handlungen anderer zu strukturieren, sondern auch zur Wiederbelebung des Erblassers verwendet werden. Kim KIT hat am 29. Oktober 2020 ein Video ihres verstorbenen Vaters Robert KIT veröffentlicht; das Gesicht im Video von Robert Xiao wurde mit tiefefake Technogy geschaffen. Dieses Hologramm wurde vom Unternehmen Kaleida erstellt, wo sie eine Kombination aus Leistung, Bewegungsverfolgung, SFX, VFX und DeepFake-Technologien in ihrer Hologramm-Erstellung verwenden. Joaquin Oliver, Opfer des Parkland-Schattens, wurde auch mit der Tiefseetechnik wiederaufgenommen. Olivers Eltern haben sich im Namen ihrer Organisation NonProfit Change the Ref zusammengeschlossen, mit McCann Health, um dieses tieffake Video zu produzieren, das für die waffensichere Wahlkampagne plädiert. In dieser tiefen Botschaft zeigt sie Joaquin ermutigende Zuschauer zur Stimmabgabe. Techniken Deepfakes stützen sich auf eine Art von Neuralnetz namens autoencoder. Es handelt sich um einen Koeffizienten, der ein Bild zu einem wenigerdimensionalen latenten Raum und einem Decoder verringert, der das Bild von der verspäteten Darstellung wiederbelebt. Deepfakes nutzt diese Architektur, indem sie einen universellen Kodizes besitzt, der eine Person im Spätraum verschlüsselt. Die verspätete Vertretung enthält wesentliche Merkmale über ihre Gesichtseigenschaften und Körperposten. Dies kann dann mit einem speziell für das Ziel ausgebildeten Modell entkoppelt werden. Dies bedeutet, dass die detaillierten Informationen des Ziels auf die zugrunde liegenden Gesichts- und Körpermerkmale des Original-Videos, die im Spätraum vertreten sind, überdacht werden. Eine populäre Modernisierung dieser Architektur bringt dem Decoder ein generatives Adversarial-Netz auf. A GAN setzt in diesem Fall den Decoder und einen Disriminator in einer konversarialen Beziehung. Der Generator schafft neue Bilder von der verspäteten Darstellung des Ausgangsmaterials, während der Diskriminator versucht, festzustellen, ob das Bild erzeugt wird oder nicht. Dies führt dazu, dass der Generator Bilder hervorbringt, die die Realität extrem gut verlaufen, und eventuelle Mängel würden von der Diskriminatorin erfasst. Beide Algorithmen verbessern ständig in einem Nullsummenspiel. Dies erschwert die Bekämpfung, da sie ständig weiterentwickelt werden; jeder Fehler ist bestimmt, kann korrigiert werden. Anwendungen Blackmail Deepfakes können verwendet werden, um Blackmail-Materialien zu erzeugen, die ein Opfer verunreinigen. Da die Fälschungen nicht zuverlässig von echten Materialien unterschieden werden können, können Opfer von tatsächlichem Blackmail jetzt geltend machen, dass die wahren Arten gefälscht sind und ihnen plausible Verwechsbarkeit gewährt werden. Es geht darum, die Glaubwürdigkeit bestehender Blackmail-Materialien, die die Loyalität an Blackmailers eliminieren und die Kontrolle von Blackmailer zerstören, nicht zu vernachlässigen. Dieses Phänomen kann als "schwarze E-Mail-Inflation" bezeichnet werden, da es reale Blackmail abwertet und es wertlos macht. Es ist möglich, die Hardware für den Rohstoffabbau mit einem kleinen Softwareprogramm umzuwandeln, um diese Blackmail-Inhalte für alle Themen in riesigen Mengen zu generieren, die Versorgung mit gefälschten schwarzen Postinhalten unnötig und in sehr skalierbarer Weise zu fördern. Laut einem Bericht des US-Kongressionsforschungsdienstes kam es zu dem Schluss, dass die tieferen Helfer oder diejenigen, die Zugang zu Verschluss- oder Einflusszwecken haben, in den Genuss von Listeninformationen kommen könnten. Pornographie Viele tiefgreifende Eingriffe im Internet sind eine Pornographie von Menschen, oft weibliche prominente, deren Gleichwertigkeit in der Regel ohne Zustimmung verwendet wird. Deepfakepornographie, die im Jahr 2017 im Internet an erster Stelle liegt, vor allem in Reddit. Laut einem im Oktober 2019 veröffentlichten Bericht der niederländischen Cybersicherheit Startup Deeptrace schätzten 96% aller Tiefstfakes im Internet pornographische Daten. Als erstes, das die Aufmerksamkeit genommen hat, war die in mehreren Artikeln enthaltene Karen Qley Tieffake. Andere prominente pornographische tiefefakes waren von verschiedenen anderen prominenten Persönlichkeiten. Im Oktober 2019 waren die meisten der Tiefseethemen im Internet britische und amerikanische Schauspielerin. Rund ein Viertel der Themen sind Südkorea, die meisten von K-pop-Spitze. Ein weiteres Problem besteht darin, dass tiefgefroren wird, um Kinderpornographie zu erzeugen (virtuelles Material für sexuellen Missbrauch von Kindern). Juni 2019 wurde ein heruntergeladener Windows- und Linux-Anwendungen namens DeepNude veröffentlicht, der neurale Netze, insbesondere physikalische Adversarial-Netze, nutzte, um Kleidung aus Bildern von Frauen zu entfernen. Die App hatte sowohl eine bezahlte als auch eine nicht bezahlte Version, die bezahlte Version kostet 50. Juni entfernten die Urheber den Antrag und erstatteten die Verbrauchern. Politik Deepfakes wurden verwendet, um bekannte Politiker in Videos zu verfehlen. In separaten Videos wurde das Gesicht des argentinischen Präsidenten Mauricio Macri durch das Gesicht von Adolf Hitler ersetzt, und das Gesicht von Angela Merkel wurde durch Donald Trump ersetzt. Juni 2018, Jordanien Peele hat mit Buzz Feed zusammengearbeitet, um einen tiefgreifenden Einsatz von Barack Obama mit der Stimme von Peele zu schaffen; er diente als Ankündigung des öffentlichen Dienstes, das Bewusstsein für tiefe Färben zu schärfen. Im Januar 2019 hat die Fox-Tochter KCPQ während seiner Oval Office-Adresse eine tiefefake von Trump durchgefeuert und seine Aussehen und Hautfarbe (und anschließend ein für das Video verantwortlicher Mitarbeiter entschärft). Mai 2020 Parlamentswahlkampagne benutzte die Delhi Horatiya Janata-Partei ähnliche Technologie, um eine Version einer Englisch-Sprach-Kampagne-Werbung durch ihren Führer Manoj Tiwari zu verteilen, die in Haryanvi übersetzt wurde. Eine Stimme wurde von einem Schauspieler bereitgestellt, und AI, die mit Video von Tiwari Reden ausgebildet wurde, wurde verwendet, um das Video auf die neue Stimme zu drücken. Ein Teilnehmermitglied bezeichnete es als positive Nutzung der tiefenfake-Technologie, die es ihnen erlaubte, "das Zielpublikum zu verfolgen, auch wenn der Kandidat nicht die Sprache des Wählers spricht. „In April 2020 veröffentlichte die belgische Zweigniederlassung der Exinction-Regime ein tiefgreifendes Video des belgischen Premierministers Sophie Wilmès auf Facebook. Das Video förderte einen möglichen Zusammenhang zwischen Entwaldung und COVID-19. Mehr als 100 000 Meinungen innerhalb von 24 Stunden hatten sie viele Kommentare erhalten. Auf der Facebook-Seite, auf der das Video erschienen ist, haben viele Nutzer das tiefefake Video als wirklich interpretiert. Bruno Sartori hat Politiker wie Jair Bolsonaro und Donald Trump geteilt. Juni 2019 veranstaltete der US-amerikanische Geheimausschuss Anhörungen über die mögliche schädliche Nutzung von Tieffakes zu Wahlen. Art im März 2018 veröffentlichte der multidisziplinäre Künstler Joseph Ayerle die Videoartwork Un'emozione per sempre 2.0 (Englischer Titel: The Italian Game). Der Künstler arbeitete mit Deepfake-Technologie, um eine AI- Schauspielerin zu schaffen, eine synthetische Version von 80s Filmstar Ornella Muti, die von 1978 bis 2018. Das Massachusetts Institute of Technology hat dieses Werk in der Studie "Kreatives Geschick" erwähnt. Der Künstler nutzte Ornella Mutis Zeitreise, um die Generierung von Reflexionen zu erkunden, während er auch Fragen zur Rolle der Provokation in der Welt der Kunst untersuchte. Für die technische Umsetzung benutzte Ayerle Fotomodell-Reaktoren. Das Programm ersetzt das Gesicht von Jochens durch eine von Ornella Muti berechnete AI. Infolgedessen hat die AI- Schauspielerin das Gesicht der italienischen Schauspielerin Ornella Muti und des Körpers von Jochen. Maßnahmen Es gab Spekulationen darüber, dass tiefgefroren wird, um digitale Akteure für künftige Filme zu schaffen. Digital gebaute und alternde Menschen wurden bereits vor Filmen verwendet, und die Tieffakes könnten in naher Zukunft neue Entwicklungen unterstützen. Deepfake-Technologie wurde bereits von Fans genutzt, um sich in bestehende Filme zu integrieren, wie etwa die Einfügung von Harry Fords jungen Gesicht auf Han Solos Gesicht in Solo: Eine Star Wars-Geschichte und ähnliche Techniken wie die von tiefenfakes verwendeten Techniken wurden für das Verhalten von Prinzessin Leia in Jamie One verwendet. Disney hat sich in zunehmendem Maße auf die Tiefsee-Technologie ausgewirkt und hat seine visuellen Effekte durch die Hochlösung Deepfake-Swapstechnik verbessert. Disney verbesserte ihre Technologie durch progressive Schulungen zur Identifizierung von Gesichtsausdrücken, der Umsetzung eines Gesichts-Shaping-Spots und deren Optimierung, um die Produktion zu stabilisieren und zu verfeinern. Diese hoch auflösende Tiefsee-Technologie wird in die Film- und Fernsehproduktion umgesetzt – sie spart erhebliche Betriebskosten in Betrieb und Produktion. Disneys Tiefsee-Generationsmodell kann AI-genetische Medien in einer Entschließung von 1024 x 1024 produzieren, die viel größer ist und realistischere Ergebnisse hervorbringt als gemeinsame Modelle, die Medien mit einer Auflösung von 256 x 256 produzieren. Disney hat auch mit dieser Technologie die Möglichkeit, tote Akteure und Figuren mit einem schnellen und einfachen Gesichtstausch wieder zu beleben; Disney kann nun die Zeichen für Fans wiederversichern und wiederfinden. Internet meme Im Jahr 2020 hat sich ein Internet-Meme entwickelt, um Videos von Menschen zu erstellen, die den Chorus von "Baka Mitai" (ばかみたい,) ein Lied aus dem Spiel Yakuza 0 in der Video-Reihe Yakuza. In der Serie ist der Sänger in einem Quiz-Minispiel sung. Die meisten Iterationsarten dieses Meme verwenden ein Video, das von Nutzern Dobbsyrules hochgeladen wird, die den Song als Muster ausdrücken. Social Media Deepfakes hat begonnen, in populären sozialen Medienplattformen, vor allem durch Zao, eine chinesische tiefenfake App zu sehen, die es den Nutzern ermöglicht, ihre eigenen Gesichter auf die Zeichen von Filmen und Fernsehbildern wie Romeo + Juliet und Game of Mythes zu ersetzen. Die App sah sich ursprünglich mit der Prüfung ihrer invasiven Nutzerdaten- und Datenschutzpolitik konfrontiert, nachdem das Unternehmen eine Erklärung abgegeben hat, in der es die Politik ändern würde. Facebook kündigte im Januar 2020 an, dass es neue Maßnahmen einführte, um dies auf ihren Plattformen zu bekämpfen. Der Congressional Research Service nannte nicht spezifizierte Nachweise, dass ausländische Geheimdienste intensiv genutzt werden, um soziale Medienkonten zu erstellen, um Personen mit Zugang zu Verschlusssachen zu rekrutieren. Sock Marionetten Deepfake-Fotos können genutzt werden, um sock Marionetten, nicht vorhandene Personen, die sowohl online als auch in traditionellen Medien aktiv sind. Mit einer offensichtlich nicht vorhandenen Person namens Oliver Taylor, deren Identität als Universitätsstudent im Vereinigten Königreich beschrieben wurde, scheint eine tiefefake-Fotos zusammen entstanden. Oliver Taylor persona hat in mehreren Zeitungen Meinungstexte vorgelegt und war in Online-Medien tätig, die einen britischen Rechtswissenschaftler und seine Frau als "terroristische Sympathisatoren" angreifen. Im Jahr 2018 hatte der Wissenschaftler eine internationale Aufmerksamkeit in Israel gegen NSO, einem Überwachungsunternehmen, im Namen von Menschen in Mexiko, die angeblich Opfer der NSO-Telefon Hacking-Technologie waren, gezeigt. Reuters konnte nur Aufnahmen für Oliver Taylor finden, und seine Universität hatte keine Aufzeichnungen für ihn. Viele Experten waren sich darin einig, dass sein Foto eine tiefefake ist. Mehrere Zeitungen haben seine Artikel nicht zurückgezogen oder aus ihren Websites entfernt. Es ist befürchtet, dass solche Techniken ein neuer Kampfboden in der Uninformation sind. Sammlungen von tiefenfake Fotos von nicht vorhandenen Menschen in sozialen Netzwerken wurden ebenfalls als Teil der israelischen Propaganda eingesetzt. In der Facebook-Seite "Zionist Frühjahr" wurden Fotos von nicht vorhandenen Personen zusammen mit ihren Testimonies vorgestellt, um zu erklären, warum sie ihre linken Politik verlassen haben, um das richtige Auslaufen zu nutzen, und die Seite enthielt auch zahlreiche Posten des Premierministers Israel Benjamin Netanyahu und seines Sohns und anderer israelischer Rechtsquellen. Die Fotos scheinen durch "menschliche Bildsynthese"-Technologie, Computersoftware, die Daten von Fotos von echten Menschen benötigt, um ein realistisches, kombiniertes Bild einer nicht vorhandenen Person zu erstellen. In vielen Fällen war der Grund für das Ersetzen des politischen Rechts der Grund für das Erlernen einer angeblichen Anstiftung zu Gewalt gegen den Premierminister. Legale israelische Rundfunkanstalten übertragen dann das Zeugnis dieser nicht vorhandenen Person auf der Grundlage der Tatsache, dass sie online geteilt wurden. Auch wenn die Rundfunkanstalten solche Menschen nicht finden konnten, erläuterten die Rundfunkanstalten „Waren ist der Ursprung?" Andere Facebook-Fälle – Profile von fiktiven Personen, die angeblich solche Anstiftungen gegen den Rechtssträngerführer enthielten, als Reaktion darauf, dass der Premierminister beschwert hat, dass es ein Grundstück zur Ermordung gab. Betrugsdelikte Audio tiefefakes wurden im Rahmen von Sozial-Engineering-Betrugs eingesetzt, die Menschen ins Denken münden, die sie von einem vertrauenswürdigen Einzelnen erhalten. Im Jahr 2019 war ein US-Unternehmenschef von Energieunternehmen über das Telefon besorgniserregend, als er bestellt wurde, 220.000 € in ein ungarisches Bankkonto von einem Einzelnen zu transferieren, der die Audio-Atfake-Technologie benutzte, um die Stimme des Vorstands der Muttergesellschaft zu wahren.Glaubwürdigkeit und Authentizität Obwohl gefälschte Fotos lange vorhanden sind, sind die Fälschungsbilder schwieriger geworden, und das Vorhandensein von tiefefakes erhöht die Schwierigkeit, Videos als echte oder nicht. AI-Forschunger Alex Champandard hat gesagt, dass die Menschen wissen sollten, wie schnell die Dinge mit der tiefenfake-Technologie beschädigt werden können, und dass das Problem nicht ein technisches, sondern eine Lösung durch Vertrauen in Information und Journalismus ist. Deepfakes können auf Defame, Impersonat und Verbreitung von Informationen verhebelt werden. Hauptfall ist, dass die Menschheit in ein Alter fallen könnte, in dem sie nicht mehr bestimmt werden kann, ob ein mittlerer Inhalt der Wahrheit entspricht. Computer-Wissenschaftsprofessor Hao Li der University of Southern California stellt ebenfalls fest, dass die für böswilligen Einsatz, wie gefälschte Nachrichten, geschaffenen tiefen Färsen noch schädlicher sein werden, wenn nichts unternommen wird, um das Bewusstsein für die Tiefsee-Technologie zu verbreiten. Li beabsichtige, dass echte Videos und tiefe Färsen in den ersten Monaten des Jahres ab Oktober 2019 unzerstörbar werden, weil sie in künstlichen Erkenntnissen und Computerbildern rasch vorankommen. Ehemaliger Google-Betrug czar Shuman G we Majorumder, hat einen Bereich der „gesellschaftlichen Anliegen“ genannt und erklärt, dass sie sich unweigerlich zu dem Punkt entwickeln, in dem sie automatisch generiert werden können, und ein individueller könnte diese Technologie verwenden, um Millionen von tiefenfake-Videos zu produzieren. Die Folgen eines Tiefsees reichen nicht aus, um das gesamte Regierungssystem zu schwächen; dennoch ist die Lage, einzelne Unternehmen enorm zu schädigen. Dies ist, weil die tiefen Färsen oft auf einen einzelnen ausgerichtet sind, und/oder ihre Beziehungen zu anderen in Hoffnung auf die Schaffung eines so wichtigen Ausdrucks, der die öffentliche Meinung oder Überzeugung beeinflussen kann. Dies kann durch eine tiefgreifende Sprachphierung geschehen, die Audio zur Herstellung gefälschter Telefongespräche oder Gesprächen missbraucht. Eine weitere Methode der Verwendung von Deepfake ist eine private Anmerkung, die die Medien missbraucht, um Einzelpersonen zu vermitteln, die negative Kommentare zu übermitteln. Microsoft gab im September 2020 bekannt, dass sie ein Software-Tool für die Deepfake-Erkennung entwickeln. Beispiele Ereignisse Deepfakes des nordkoreanischen Führers Kim Jong-un und des russischen Präsidenten Vladimir Putin wurden ebenfalls von einer nichtparteiischen Advocacy-Gruppe RepresentUs erstellt. Mit diesen tiefgreifenden Worten, dass die Einmischung dieser Führer in die US-Wahlen zu Lasten der Demokratie der Vereinigten Staaten führen würde; der Handel zielte auch darauf ab, die Amerikaner zu schockieren, wie die fragile Demokratie ist und wie Medien und Nachrichten den Weg des Landes unabhängig von der Glaubwürdigkeit erheblich beeinflussen können. Letztere enthielten jedoch eine abschließende Stellungnahme, in der sie darlegten, dass das Material nicht wirklich war, und die kommerziellen Unternehmen haben letztlich aufgrund von Ängsten und Empfindlichkeiten in Bezug auf die Art und Weise, wie die Amerikaner reagieren können, keine Luft. Ein Clip aus der Rede von Nancy Pelosi im Center for American Progress vom 22. Mai 2019 wurde abgeschwächt, neben dem Spiel, um es so zu machen, als ob sie getrunken wurde; Kritiker weisen jedoch darauf hin, dass dies keine tiefe Bedrohung ist. Donald Trump Eine tiefefake von Donald Trump wurde leicht geschaffen, basierend auf einem Skit Jimmy Fallon auf der Show NBC’s The To Nacht. In diesem Skit (aired 4 May 2016) hat Jimmy Fallon als Donald Trump gelobt und sich tendenziell an einem Telefongespräch mit Barack Obama beteiligt, der ihn in einer Weise widerlegte, die ihn über seinen primären Gewinn in Indiana verwundere. Mai 2019 wurde ein Tiefsee von Donald Trump (aus diesem Skit) geschaffen. In dieser tiefen Fake wurde das Gesicht von Jimmy Fallon in Donald Trump umgewandelt (Audio blieb gleich). Dieses tiefefake-Video wurde auf YouTube von dem Gründer von Derpfakes mit einer Absichtserklärung hochgeladen. Barack Obama Tieffake US-Akteur Jordanien Peele, BuzzFeed und Generikahersteller haben eine tiefe Unterstützung von Barack Obama (auf YouTube am 17 Apr 2018) geschaffen, die Barack Obama aufsah und Donald Trump nannte. Peeles Stimme und Mund wurden in Obamas Stimme und Gesicht umgewandelt und gehandhabt. Ziel dieses Videos war es, die gefährlichen Folgen und die Macht von tiefenfakes darzustellen, und wie tiefe Färsen jeden sagen können. Positive Auswirkungen möglicher positiver Innovationen haben sich neben der zunehmenden Beliebtheit und der Entstehung von tiefgreifenden Innovationen ergeben. So können beispielsweise Videos für die Unternehmensausbildung unter Verwendung von Tiefstämmen und ihren Stimmen erstellt werden. Ein Beispiel hierfür ist Synthesia, die die Tieffake-Technologie mit Avataren nutzt, um personalisierte Videos zu erstellen. Antworten auf soziale Medienplattformen Twitter ergreift aktive Maßnahmen zum Umgang mit synthetischen und manipulierten Medien auf ihrer Plattform. Um die Verbreitung von Informationen zu verhindern, stellt Twitter eine Mitteilung zu den Gesprächen vor, die vereinzelte Medien und/oder tiefgreifende Inhalte enthalten, die den Zuschauern signalisieren, dass die Medien manipuliert werden. Es wird auch ein Warnhinweis geben, das den Nutzern, die auf der Retweeting, dem Liking oder dem Gespräch mit dem Gespräch planen, erscheint. Twitter wird auch darauf hinarbeiten, den Nutzern einen Link neben dem Gespräch zu liefern, der sich über die manipulierten oder synthetischen Medien, die mit einem Twitter- oder glaubwürdigen Nachrichtenartikel über das damit verbundene Thema, als Debunking-Aktion, auszeichnet. Twitter verfügt auch über die Möglichkeit, alle mit tiefgreifenden oder manipulierten Medien, die die Sicherheit der Nutzer beeinträchtigen könnten, zu entfernen. Twitter hat Nutzer, die in Zusammenarbeit mit ihnen interessiert sind, gebeten, auf der Suche nach tiefgreifenden und manipulierten Medien zu arbeiten (d. h. 27 November 2020). Facebook Facebook hat Anstrengungen unternommen, um die Schaffung von Tiefstämmen zu fördern, um den Stand der Technik zur Erkennung von Tieren zu entwickeln. Facebook war der prominente Partner bei der Bewältigung des Problems der Deepfake-Erkennung (DFDC), der im Dezember 2019 an 2114 Teilnehmer teilnahm, die mehr als 35000 Modelle generierten. Die besten Modelle mit höchster Nachweisgenauigkeit wurden für ähnliche Ausmaße und Unterschiede analysiert; diese Erkenntnisse sind Bereiche, die für eine weitere Forschung von Interesse sind, um die Erkennungsmodelle von Tiefseearten zu verbessern und zu verfeinern. Facebook hat auch detailliert ausgeführt, dass die Plattform Medien, die mit künstlichen Erkenntnissen erzeugt werden, um die Rede eines Einzelnen zu ändern. Medien, die zur Änderung der Reihenfolge oder des Kontextes von Wörtern in einer Botschaft veröffentlicht wurden, würden jedoch auf der Website bleiben, aber als falsch gekennzeichnet, da sie nicht durch künstliche Intelligenz erzeugt wurde. Nachweis Die meisten der wissenschaftlichen Forschung in der Umgebung von Deepfake zielen darauf ab, die Videos zu entdecken. Die beliebteste Technik ist die Verwendung von Algorithmen, die denen ähneln, die zum Bau der tiefen Fake verwendet werden, um sie zu erkennen. Indem man die Muster in der Art und Weise anerkennt, wie tieffakes geschaffen werden, kann der Algorithmus in der Lage sein, differenzierte Unstimmigkeiten aufzuholen. Forscher haben automatische Systeme entwickelt, die Videos für Fehler wie irreguläre Isoliermuster von Beleuchtung untersuchen. Diese Technik wurde auch kritisiert, um ein "Moving Goal post" zu schaffen, wo immer die Algorithmen für die Erkennung verbessert werden, so tun die tiefen. Die Herausforderung der Deepfake-Erkennung, die von einer Koalition führender Technologieunternehmen ausgerichtet ist, hofft, die Technologie für die Identifizierung geminderter Inhalte zu beschleunigen. Im Oktober 2020 veröffentlichte ein Team an der University of Buffalo ein Papier, in dem sie ihre Lichttechnik in den Augen derjenigen, die sich um die Einlagerung von Tiefstständen mit einem hohen Erfolg, auch ohne die Verwendung eines AI-Identifikationsinstruments, mindestens zum Zeitpunkt der Zeit herausstellen. Andere Techniken verwenden Whirlpool, um die Quelle der Medien zu überprüfen. Videos müssen über den Unterhändler überprüft werden, bevor sie auf sozialen Medienplattformen gezeigt werden. Mit dieser Technologie würden nur Videos aus vertrauenswürdigen Quellen genehmigt, wodurch die Verbreitung möglicherweise schädlicher Tiefsee-Medien verringert würde. Digitale Unterzeichnung aller Video- und Bildaufzeichnungen durch Kameras und Videokameras, einschließlich Smartphone-Kameras, wurde vorgeschlagen, tiefgreifend zu bekämpfen. Das ermöglicht es, jedes Foto oder Video wieder auf seinen ursprünglichen Besitzer zurück zu bringen, der zur Verfolgung von Unidenten verwendet werden kann. Mit der Internet-Reaktion seit 2017 veröffentlichte die Vizepräsidentin eine Reihe von Artikeln, die Nachrichten im Zusammenhang mit der Tiefseepornographie abdecken. Am 31. Januar 2018 begann Gfycat mit der Abschaffung aller Tiefstämme aus seiner Website. Am 7. Februar 2018 wurde das Unterreden r/deepfake wegen der politischen Verletzung der „unfreiwilligen Pornographie“ verboten. Im gleichen Monat erklärten die Vertreter von Twitter, dass sie Konten aussetzen würden, die Verdacht auf Entsendung von nicht-verbrauchsrelevanten Tiefseeinhalten haben. Chat-Site-Ausbruch hat in der Vergangenheit Maßnahmen gegen tiefe Färsen getroffen und hat einen allgemeinen Standpunkt gegen tiefe Färkte eingenommen. Google fügte im September 2018 „unfreiwillige synthetische pornographische Bilderry“ in seine Verbotsliste ein, die es jedem ermöglicht, den Block der Ergebnisse, die ihre gefälschten Akts zeigen, zu fordern. Im Februar 2018 sagte Kerstin, dass es auf der Website tieffake Videos verbieten würde, weil sie als „Nicht-Konvergenz-Inhalte“ angesehen wird, die gegen ihre Dienstleistungen verstoßen. Sie erklärten auch früher zu Mashable, dass sie Inhalte, die als tiefe Färben gekennzeichnet sind, herunternehmen werden. Schriftsteller aus Ricoh von Buzzfeed News berichteten, dass die Suche nach tiefenfakes auf Pornhub noch mehrere neuere Tiefsee-Videos zurückgekehrt ist. Facebook hat zuvor erklärt, dass sie keine tiefgreifenden Ausbrüche aus ihren Plattformen entfernen würden. Die Videos werden stattdessen als Fälschung durch Dritte gedrängt und haben dann eine geringere Priorität in den Futtermitteln der Nutzer. Diese Antwort wurde im Juni 2019 nach einer tiefen Fake mit einem 2016 auf Facebook und Twitter verteilten Video von Mark Sugarberg ausgelöst. Legale Antwort In den Vereinigten Staaten gibt es einige Antworten auf die Probleme, die durch tiefe Färben verursacht wurden. Im Jahr 2018 wurde das geräuscharme Prohibition Act in den USA eingeführt, und im Jahr 2019 wurde das DEEPF N Accountability Act im Repräsentantenhaus eingeführt. Mehrere Staaten haben auch Rechtsvorschriften über Tiefstfakes eingeführt, darunter Virginia, Texas, Kalifornien und New York. Juni 2019 in Kalifornien Gavin Newsom unterzeichnete in Gesetzesversammlungen Nr. 602 und Nr. 730. Versammlungsgesetz Nr. 602 bietet Einzelpersonen, die von sexuell expliziten tiefenfake-Inhalten bestimmt sind, ohne deren Zustimmung mit einer Klage gegen den Urheber des Inhalts. Versammlungsgesetz Nr. 730 verbietet den Vertrieb von bösartigen tiefenfake Audio- oder visuellen Medien, die auf einen Kandidaten für öffentliches Amt innerhalb von 60 Tagen nach ihrer Wahl abzielen. November 2019 China kündigte an, dass tiefflächliche und andere synthetische gefälschte Materialien eine klare Mitteilung über ihre Fälschung ab 2020 tragen sollten. Nichtbefolgung könnte als Straftat angesehen werden, die Cyberspace Administration of China auf ihrer Website erklärt hat. Die chinesische Regierung scheint das Recht zu behalten, sowohl Nutzer als auch Online-Videoplattformen zu verfolgen, die die Vorschriften nicht einhalten. Im Vereinigten Königreich können die Hersteller von Tiefseematerial für Belästigungen verfolgt werden, aber es ist aufgerufen, eine bestimmte Kriminalität zu vertiefen; in den Vereinigten Staaten, wo die Gebühren so unterschiedlich wie Identitätsdiebstahl, Cyberstalking und Revenge porn verfolgt wurden, wurde auch der Begriff einer umfassenderen Satzung diskutiert. In Kanada hat die Einrichtung der Kommunikationssicherheit einen Bericht veröffentlicht, der darauf hinwies, dass tiefgreifende Eingriffe in die kanadische Politik, insbesondere um Politiker und Wähler Einfluss zu nehmen. Infolgedessen gibt es in Kanada mehrere Möglichkeiten, sich mit tiefgreifenden Angriffen zu befassen, wenn sie von ihnen angestrebt werden. Antwort von DARPA Die Verteidigung Advanced Research Projects Agency (DARPA) hat ein Projekt finanziert, bei dem Einzelpersonen mit dem Ziel konkurrieren, AI-gewebte Videos, Audio- und Bilder sowie automatisierte Werkzeuge für die Erkennung dieser tiefen Färben zu erstellen. DARPA hat noch Anstrengungen unternommen, um ein "Proposers Day" für ein Projekt zu starten, das mit dem Programm für die epidemiologische Kriminalisierung verbunden ist, bei dem Forscher die virale Verbreitung von AI-manipierten Medien verhindern. DARPA und das Programm für kreative Forensics arbeiten ebenfalls zusammen, um diese AI-manipierten Medien durch Maßnahmen zu erkennen, die sich auf die Ausbildung von Computern konzentrieren, um den gemeinsamen Sinn zu nutzen. DARPA hat auch ein Programm Media Forensics (MediFor) entwickelt, um den zunehmenden Schaden zu mindern, den tiefen und AI-geförderten Medien verursachen. Dieses Programm zielt nicht nur darauf ab, tiefe Färben zu erkennen, sondern auch Informationen darüber, wie die Medien geschaffen wurden. DARPA verfolgt das Ziel, die sich daraus ergebende Rolle von tiefenfakes und ihren Einfluss auf die Entscheidungsfindung zu berücksichtigen. In der populären Kultur Die Ausgabe des Analogmagazins von 1986 veröffentlichte das Romanette Picaper von Jack Wodhams. Ihr Grundstück revolviert rund um digital verstärkte oder digital erzeugte Videos, die von qualifizierten Hackern produziert werden, die unkontrollierte Rechtsanwälte und politische Zahlen bedienen. Film 1987 Der langwierige Mann mit Arnold Schwarzenegger stellt eine autokratische Regierung dar, die Computer an die digitale Stelle bringt, um die Gesichter von Akteuren mit denen, die es wünschen, zu ersetzen, um sie zu neutralisieren. In der technisch-thriller A Philosophical-Untersuchung von Philip Kerr, Wittgenstein, dem Hauptcharakter und einem seriellen Killer, wird sowohl eine ähnliche Software wie Deepfake als auch eine virtuelle Realität, die mit einem Katar der weiblichen Polizei zu tun hat, verwendet, Isadora Jakowicz, die ihm zugewiesen wird. Der Filmanstieg Sun aus dem Jahr 1993, in dem Sean Connery und Tex Snipes einen anderen Charakter darstellt, zeigt, dass eine Computerscheibe digitale Veränderungen der persönlichen Identität bewirkt, um einen Wettbewerber zu erschweren. Tieffake-Technologie ist Teil des Grundstücks der BBC One Drama The Capture 2019. Die Serie folgt dem britischen Ex-soldier Shaun Emery, der beschuldigt wird, seine Barrister zu verletzen und zu entziehen. sachkundiger arztierter Videofilm wird verwendet, um ihn zu beschaffen und die Polizei, die ihn untersucht, zu irreführen. Al Davis vs. Johnson – Die Abbildungsstruktur dieses Dokuments 2021, ein Teil der 30 von ESPN für 30 Dokumentarfilmreihe, verwendet tiefgreifende Versionen der beiden zentralen Zeichen des Films, die beiden verstorbenen –Al Davis, die im Besitz der Las Vegas-Francs während des zehnten Teams in Seattle und Los Angeles sowie Pete Rozelle sind, die häufig mit Davis zusammentreffen. Siehe auch Facial wayGANSynthetic Media virtueller Akteur Identitätsersatz Technologie Hyperrektivitätsreferenzen Externe Links Sasse, Ben (19 Oktober 2018). " Diese neue Technologie könnte die amerikanische Politik in ein Endspin übertragen. Stellungnahmen. Washington Post.Retrieved 10 Juli 2019. Audio/Spoof Audiotektion Challenge (ASVspoof) Deepfake Nachweis Challenge (DFDC)