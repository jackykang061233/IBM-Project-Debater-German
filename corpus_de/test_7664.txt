Langer Kurzzeitgedächtnis (LSTM) ist eine künstliche neurale Netzwerkarchitektur (RNN), die im Bereich des Tiefenlernens verwendet wird. Im Gegensatz zu Standard-feedforward neuronalen Netzwerken verfügt LSTM über Feedback-Verbindungen. Es kann nicht nur einzelne Datenpunkte (z.B. Bilder) sondern auch ganze Datensequenzen (z.B. Sprache oder Video) verarbeiten. Beispielsweise ist LSTM für Aufgaben wie unsegmentierte, vernetzte Handschrifterkennung, Spracherkennung und Anomalieerkennung im Netzverkehr oder IDS (Intrusionserkennungssysteme) anwendbar. Eine gemeinsame LSTM-Einheit besteht aus einer Zelle, einem Eingangsgate, einem Ausgangsgate und einem Vergessensgate. Die Zelle erinnert Werte über beliebige Zeitintervalle und die drei Gates regeln den Informationsfluss in und aus der Zelle. LSTM-Netzwerke sind gut geeignet, Vorhersagen basierend auf Zeitreihendaten zu klassifizieren, zu verarbeiten und zu machen, da es Verzögerungen von unbekannter Dauer zwischen wichtigen Ereignissen in einer Zeitreihe geben kann. LSTMs wurden entwickelt, um mit dem verschwindenden Gradientenproblem umzugehen, das beim Training traditioneller RNs auftreten kann. Die relative Unempfindlichkeit auf Lückenlänge ist ein Vorteil von LSTM über RNs, versteckte Markov-Modelle und andere Sequenzerwerbsmethoden in zahlreichen Anwendungen. Ide Theorie, Klassik (oder Vanille) RNNs können beliebige langfristige Abhängigkeiten in den Eingabesequenzen verfolgen. Das Problem mit Vanille RNs ist rechnerisch (oder praktisch) in der Natur: Beim Training einer Vanille RNN mit Rückverbreitung können die rückverschobenen Langzeitgradienten (d.h. sie können zu Null neigen) oder explodieren (d.h. sie können zu Unendlichkeit neigen) wegen der an dem Prozess beteiligten Berechnungen, die endliche Präzisionszahlen verwenden. RNNs mit LSTM-Einheiten lösen das verschwindende Gradientenproblem teilweise, da LSTM-Einheiten Gradienten auch unverändert fließen lassen. LSTM-Netzwerke können jedoch noch unter dem explodierenden Gradientenproblem leiden. Varianten In den folgenden Gleichungen stellen die Kleinbuchstaben Vektoren dar. Matrices W q {\displaystyle W_{q} und U q {\displaystyle U_{q} enthalten jeweils die Gewichte der Eingangs- und wiederkehrenden Verbindungen, wobei das Subskript q {\displaystyle {_q} entweder das Eingangsgate i {\displaystyle i} sein kann, das Ausgangsgate o\displaystyle o} ist, das vergessene Gate f {\displaystyle f} oder die Speicherzelle In diesem Abschnitt verwenden wir also eine "Vector Notation". So ist z.B. c t ε R h {\displaystyle c_{t}\in \mathbb {R} {^h} nicht nur eine Zelle einer LSTM-Einheit, sondern enthält h {\displaystyle h} LSTM-Einheitszellen. LSTM mit einem vergessenen Tor Die kompakten Formen der Gleichungen für den Vorlauf einer LSTM-Einheit mit vergessenem Gate sind: f t = σ g ( W f x t + U f h t - 1 + b f ) i t = σ g ( W i x t + U i h t - 1 + b i ) o t = σ g ( W o x t + U o h t - 1 + b o ) c ~ t = σ c ( W c x t + U c h t - 1 + b c ) c t = f t Ÿ c t - 1 + i t ∘ t ∘ t t σ h (c t ) {\displaystyle begin{align}f_{t}&=\sigma g) g) g) c}_{t}&=\sigma (W_{c}x_{t}+U_{c}h_{t-1}+b_{c}\c_{t}&=f_{t}\crc ... ~ ***________ \sigma h}(c_{t})\end{justiert, wobei die Anfangswerte c 0 = 0 {\displaystyle c_{0}=0 und h 0 = 0 {\displaystyle h_{0}=0 und der Bediener ≠ {\displaystyle \circ } das Hadamard-Produkt (elementweise Produkt) bezeichnet. Das Subskript t {\displaystyle t} indexiert den Zeitschritt. Variablen x t ε R d {\displaystyle x_{t}\in \mathbb {R} {^d}: Eingabevektor in die LSTM-Einheit f t ε (0, 1 ) h {\displaystyle f_{t}\in (0,1)}^h ε (0, 1 ) h {\displaystyle i_{t}\in (0,1)}{h : Input/update gate's Aktivierungsvektor o t ε (0 , 1 ) h {\displaystyle o_{t}\in (0,1)}^ Zelleneingang Aktivierungsvektor c t ε R h {\displaystyle c_{t}\in \mathbb {R} {^h} :Zellzustandsvektor W ε R h × d {\displaystyle W\in \mathbb {R} {^h\Zeiten d}, U ε R h × h {\displaystyle U\in \mathbb {R}\h\Zeiten h} und b ε R h Aktivierungsfunktionen σ g {\displaystyle \sigma {_g} : sigmoid Funktion. σ c {\displaystyle \sigma {_c}: hyperbolische Tangentenfunktion. σ h {\displaystyle \sigma {_h}: hyperbolische Tangentenfunktion oder, wie das Peephole LSTM-Papier vorschlägt, σ h ( x ) = x {\displaystyle \sigma {_h}(x)=x . Peephole LSTM Die Figur rechts ist eine grafische Darstellung einer LSTM-Einheit mit Peephole-Verbindungen (d.h. einem Peephole LSTM). Peephole-Verbindungen ermöglichen es den Gates, auf den konstanten Fehlerkarussell (CEC) zuzugreifen, dessen Aktivierung der Zellzustand ist. h t - 1 {\displaystyle h_{t-1} wird nicht verwendet, anstelle der meisten Stellen wird c t - 1 {\displaystyle c_{t-1} verwendet. f t = σ g ( W f x t + U f c t - 1 + b f ) i t = σ g ( W i x t + U i c t - 1 + b i ) o t = σ g ( W o x t + U o c t - 1 + b o ) c t - 1 + i t g) g) g) ... In den Warenkorb \sigma h}(c_{t})\end{alignJedes der Gates kann als Standard-Neuron in einem Feed-Forward (oder Multilayer)-Neuralnetzwerk gedacht werden: das heißt, sie berechnen eine Aktivierung (unter Verwendung einer Aktivierungsfunktion) einer gewichteten Summe. i t, o t {\displaystyle i_{t},o_{t und f t {\displaystyle f_{t} stellen die Aktivierungen des jeweiligen Eingabe-, Ausgabe- und Vergessensgates dar, zum Zeitpunkt t {\displaystyle t} . Die 3 Austrittspfeile aus der Speicherzelle c {\displaystyle c} zu den 3 Toren i, o {\displaystyle i,o} und f {\displaystyle f} stellen die Peephole-Verbindungen dar. Diese Peephole-Verbindungen bezeichnen tatsächlich die Beiträge der Aktivierung der Speicherzelle c {\displaystyle c} zum Zeitpunkt t - 1 {\displaystyle t-1}, d.h. den Beitrag von c t - 1 {\displaystyle c_{t-1} (und nicht c t {\displaystyle c_{t}, wie das Bild vorschlagen kann). Mit anderen Worten, die Gates i, o {\displaystyle i,o} und f {\displaystyle f} berechnen ihre Aktivierungen zum Zeitpunkt t {\displaystyle t} (d.h. i t, o t {\displaystyle i_{t},o_{t und f t\displaystyle t_{t} style auch unter Berücksichtigung der Aktivierung der Speicherzelle c} Die kleinen Kreise mit einem × {\displaystyle \times } Symbol stellen eine elementweise Multiplikation zwischen seinen Eingängen dar. Die großen Kreise, die eine S-förmige Kurve enthalten, stellen die Anwendung einer differenzierbaren Funktion (wie die Sigmoidfunktion) auf eine gewichtete Summe dar. Peephole convolutional LSTM Peephole convolutional LSTM. Der χ {\displaystyle *} bezeichnet den Faltungsoperator. f t = σ g ( W f χ t + U f χ h t - 1 + V f † c t - 1 + b f ) i t = σ g ( W i χ t + U i χ h t - 1 + V i ≠ c t - 1 + b i ) c t = f t ≠ c t - 1 + i t ∘ σ c ( W c χ t + U c υ h t - 1 + b c ) o t = σ g ( W o υ x t + U o υ h t - 1 + V o t + b o ) h t = o t σ h ( c t ig) (W_{f}) c_{t-1}+b_{f}\i_{t}&=\sigma ) c_{t-1}+b_{i}\c_{t}&=f_{t}\circ c_{t-1} (W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c}\o_{t}&=\sigma ) c_{t}+b_{o}\h_{t}&=o_{t}\circ \sigma h}(c_{t})\end{align Training Ein RNN mit LSTM-Einheiten kann auf einer Reihe von Trainingssequenzen unter Verwendung eines Optimierungsalgorithmus, wie Gradientenabstieg, mit Backpropagation über die Zeit zur Berechnung der während des Optimierungsprozesses benötigten Gradienten trainiert werden, um jedes Gewicht des LSTM-Netzwerks im Verhältnis zur Ableitung des Fehlers (an der Ausgangsschicht des LSTM-Netzwerks) in Bezug auf entsprechendes Gewicht zu ändern. Ein Problem bei der Verwendung von Gradientenabstieg für Standard-RNs ist, dass Fehlergradienten mit der Zeitverzögerung zwischen wichtigen Ereignissen exponentiell schnell verschwinden. Dies ist auf lim n → ∞ W n = 0 {\displaystyle \lim {_n\to \infty W^{n}=0, wenn der spektrale Radius von W {\displaystyle W} kleiner als 1 ist. Bei LSTM-Einheiten bleibt jedoch bei Rückwärtsfahren von Fehlerwerten aus der Ausgangsschicht der Fehler in der Zelle der LSTM-Einheit. Dieses "Error-Carousel" liefert kontinuierlich Fehler an jedes der Tore der LSTM-Einheit, bis sie lernen, den Wert abzuschneiden. CTC-Score-Funktion Viele Anwendungen verwenden Stacks von LSTM RNs und trainieren sie durch verbindungsseitige zeitliche Klassifikation (CTC), um eine RNN-Gewichtsmatrix zu finden, die die Wahrscheinlichkeit der Labelsequenzen in einem Trainingsset bei den entsprechenden Eingabesequenzen maximiert. CTC erreicht sowohl Ausrichtung als auch Erkennung. Alternativen Manchmal kann es von Vorteil sein, ein LSTM durch Neuroevolution oder durch politische Gradientenmethoden zu trainieren, insbesondere wenn es keinen Lehrer gibt (d.h. Trainingsetiketten). Erfolg Es gab mehrere erfolgreiche Geschichten von Training, in einer nicht-supervised Mode, RNNs mit LSTM-Einheiten. Im Jahr 2018, Bill Gates nannte es einen “großen Meilenstein in der Entwicklung künstlicher Intelligenz”, als Bots von OpenAI entwickelt wurden Menschen im Spiel von Dota 2 zu schlagen. OpenAI Five besteht aus fünf unabhängigen, aber koordinierten neuronalen Netzwerken. Jedes Netzwerk wird durch eine Policy Gradienten-Methode trainiert, ohne Lehrer zu betreuen und enthält eine einschichtige, 1024-Einheit Long-Short-Term-Memory, die den aktuellen Spielzustand sieht und Aktionen über mehrere mögliche Aktionsköpfe ausgibt. 2018 trainierte OpenAI ein ähnliches LSTM durch politische Gradienten, um eine humanähnliche Roboterhand zu steuern, die physische Objekte mit beispielloser Geschicklichkeit manipuliert. 2019 nutzte DeepMinds Programm AlphaStar einen tiefen LSTM-Kern, um sich beim komplexen Videospiel Starcraft II zu übertreffen. Dies wurde als bedeutender Fortschritt in Richtung Künstlicher Allgemeine Intelligenz angesehen. Anwendungen Anwendungen von LSTM umfassen: Timeline of development 1995-1997: LSTM wurde von Sepp Hochreiter und Jürgen Schmidhuber vorgeschlagen. Durch die Einführung von Constant Error Carousel (CEC)-Einheiten beschäftigt sich LSTM mit dem verschwindenden Gradientenproblem. Die erste Version des LSTM-Blocks umfasste Zellen, Eingabe- und Ausgabegates.1999: Felix Gers und sein Berater Jürgen Schmidhuber und Fred Cummins führten das vergessene Gate (auch "keep gate" genannt) in die LSTM-Architektur ein, wodurch der LSTM seinen eigenen Zustand zurücksetzte. 2000: Gers & Schmidhuber & Cummins fügte Peephole-Verbindungen (Verbindungen von der Zelle zu den Toren) in die Architektur ein. Zusätzlich wurde die Ausgangsaktivierungsfunktion weggelassen. 2009: Ein LSTM-basiertes Modell gewann den ICDAR-Anerkennungswettbewerb. Drei solcher Modelle wurden von einem Team unter der Leitung von Alex Graves eingereicht. Eines war das genaueste Modell im Wettbewerb und eine andere war die schnellste. 2013: LSTM-Netzwerke waren ein wichtiger Bestandteil eines Netzwerks, das auf dem klassischen TIMIT natürlichen Sprachdatensatz eine Rekordrate von 17,7% Phonemfehler erreichte. 2014: Die Welt Cho et al.put forward eine vereinfachte Variante namens Gated recurrent unit (GRU).2015: Google startete mit einem LSTM zur Spracherkennung auf Google Voice. Nach dem offiziellen Blog-Post, das neue Modell geschnitten Transkriptionsfehler um 49 % 2016:Google begann mit einem LSTM, um Nachrichten in der Allo-Konversations-App vorzuschlagen. Im selben Jahr veröffentlichte Google das Google Neural Machine Translation System für Google Translate, das LSTMs verwendet, um Übersetzungsfehler um 60% zu reduzieren. Apple in seinen weltweiten Entwicklern bekannt gegeben Konferenz, dass es mit dem LSTM für den Schnelltyp im iPhone und für Siri.Amazon veröffentlicht Polly, die die Stimmen hinter Alexa erzeugt, mit einem bidirektionalen LSTM für die Text-zu-Sprach-Technologie. 2017: Facebook hat täglich rund 4,5 Milliarden automatische Übersetzungen mit langfristigen Speichernetzwerken durchgeführt. Forscher der Michigan State University, der IBM Research und der Cornell University veröffentlichten eine Studie in der Knowledge Discovery and Data Mining (KDD) Konferenz. Ihre Studie beschreibt ein neuartiges neuronales Netzwerk, das auf bestimmten Datensätzen besser arbeitet als das weit verbreitete langfristige neuronale Netzwerk. Microsoft meldete eine Erkennungsgenauigkeit von 94,9% auf dem Switchboard corpus mit einem Vokabular von 165.000 Wörtern. Der Ansatz verwendet "dialog session-basierte langfristige Speicher".2019: Forscher der Universität Waterloo schlug eine verwandte RNN-Architektur, die kontinuierliche Fenster der Zeit darstellt. Es wurde mit den Legendre-Polynomen abgeleitet und übertrifft die LSTM auf einigen speicherbezogenen Benchmarks. Auf dem in Large Text Compression Benchmark stieg ein LSTM-Modell auf den dritten Platz. Siehe auch Referenzen Externe Links Recurrent Neural Networks mit über 30 LSTM-Arbeiten von Jürgen Schmidhuber's Group bei IDSIA Gers, Felix (2001). "Long Short-Term Memory in recurrent Neural Networks" (PDF). Doktorarbeit. Gers, Felix A.; Schraudolph, Nicol N.; Schmidhuber, Jürgen (Aug 2002). " Lernen Sie präzises Timing mit LSTM-Rezidive-Netzwerken" (PDF). Journal of Machine Learning Research. 3: 115–143.Abidogun, Olusola Adeniyi (2005). Data Mining, Fraud Detection and Mobile Telecommunications: Call Pattern Analysis with Unsupervised Neural Networks. Masterarbeit (Thesis). University of the Western Cape.hdl:11394/249.Archiviert (PDF) aus dem Original am 22. Mai 2012.Original mit zwei Kapiteln zur Erläuterung wiederkehrender neuronaler Netzwerke, insbesondere LSTM. Monner, Derek D.; Reggia, James A. (2010). " Ein generalisierter LSTM-ähnlicher Trainingsalgorithmus für neurale Netzwerke zweiter Ordnung" (PDF). Neural Networks.25 (1): 70–83. doi:10.1016/j.neunet.2011.07.003.PMC 32173.PMID 21803542. Hochperformende Erweiterung von LSTM, die auf einen einzigen Knotentyp vereinfacht wurde und willkürliche Architekturen Dolphin, R. "LSTM Networks - A Ausführliche Erklärung" trainieren kann. Artikel.Herta, Christian. "Wie man LSTM in Python mit Theano implementiert". Tutorial