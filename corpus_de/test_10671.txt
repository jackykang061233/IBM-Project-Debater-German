In der Computerwissenschaft ist die algorische Effizienz ein Eigentum eines Algorithmus, der sich auf die Menge der vom Algorithmus verwendeten Rechenmittel bezieht. Ein Algorithmus muss analysiert werden, um seine Ressourcennutzung zu bestimmen, und die Effizienz eines Algorithmus kann anhand der Nutzung unterschiedlicher Ressourcen gemessen werden. Algorithmieeffizienz kann als Vergleich zur Ingenieurproduktivität für einen wiederholten oder kontinuierlichen Prozess angesehen werden. Höchste Effizienz wollen wir die Ressourcennutzung minimieren. Allerdings können verschiedene Ressourcen wie Zeit und Raumkomplex nicht direkt verglichen werden, so dass zwei Algorithmen als effizienter angesehen werden, hängt oft davon ab, welche Effizienz gemessen wird. So sind beispielsweise Blasenart und Einschüchterungen beide Algorithmen, um eine Liste von Gegenständen von kleinsten bis größten zu erstellen. Legt die Liste in der Zeit, die proportional zur Anzahl der Quadratkilometer ( O ( n 2 ) Memestyle \skriptstyle SSO mathematisch O).(n^{2 Oright) , siehe große O-Bekanntheit), erfordert aber nur eine kleine Menge zusätzlichen Gedächtnis, die in Bezug auf die Länge der Liste ( 1 ) Memetext \skriptstyle \skriptstyle {O((1\right) ) . Tim vereint die Liste in der Zeit linearithmie (proportional bis zu einer Menge seines Logarithm) in der Länge der Liste ( O ( n Log ⁡ n ) Memetext \skript \style Meme mathematisch {O\left(n\log n\right)}, verfügt aber über eine Voraussetzung für lineares Gedächtnis in der Länge der Liste ( O ( n ) \skript \style \style \style \style {O\n\n\n\n\n\n\n\right)], wenn eine große Bandbreite, eine große Bandbreite ist. Hintergrund Die Bedeutung der Effizienz in Bezug auf die Zeit wurde von Ada Lovelace in 1843 als Anwendung auf die mechanische Analyse von Charles Babbage hervorgehoben: "In fast allen Berechnungen ist eine große Vielfalt an Regelungen für die Nachfolge der Prozesse möglich, und verschiedene Überlegungen müssen die Auswahl unter ihnen für die Zwecke einer Berechnungsmaschine beeinflussen. Ein wesentliches Ziel ist es, zu entscheiden, dass die Regelung, die sich tendenziell auf ein Minimum reduzieren wird, die für die Fertigstellung der Berechnung erforderlich ist, stark beschränkt war, sowohl durch die Geschwindigkeit der Vorgänge als auch die Menge der verfügbaren Speicher. In einigen Fällen wurde festgestellt, dass es einen Raum-Time-off gab, bei dem eine Aufgabe entweder durch Verwendung eines schnellen Algorithmus, der viel Arbeitsspeicher verwendet, oder durch Verwendung eines langsameren Algorithmus, der sehr wenig Arbeitsspeicher verwendet hat, bewältigt werden könnte. Nach dem technischen Ausstieg sollte dann der schnellste Algorithmus verwendet werden, der in der verfügbaren Erinnerung passt. moderne Computer sind deutlich schneller als die frühen Computer und verfügen über einen viel größeren Speicher (Gigabyte statt Kilobytes). Donald Knuth betonte jedoch, dass die Effizienz immer noch ein wichtiges Thema ist: „In etablierten Ingenieursdisziplinen wird eine Verbesserung von 12 % nicht als marginal angesehen, und ich glaube, dass der gleiche Standpunkt im Software-Engineering bestehen sollte“. Ein Algorithmus wird als effizient angesehen, wenn sein Ressourcenverbrauch, auch als Rechenkosten bekannt, auf oder unter einem akzeptablen Niveau liegt. Kurz gesagt, annehmbare Mittel: Es wird in angemessener Menge Zeit oder Raum auf einem verfügbaren Computer laufen, in der Regel als Funktion der Eingangsgröße. Seit den 1950er Jahren haben sich die Computer sowohl in der verfügbaren Rechenleistung als auch in der verfügbaren Speichermenge dramatisch erhöht, so dass die aktuellen annehmbaren Werte vor 10 Jahren inakzeptabel gewesen wären. Dank der ungefähren Verdoppelung der Computerleistung alle zwei Jahre können Aufgaben, die für moderne Smartphones und eingebettete Systeme akzeptabel sind, vor 10 Jahren unannehmbar ineffizient gewesen sein. Computerhersteller bringen häufig neue Modelle vor, oft mit höherer Leistung. Softwarekosten können sehr hoch sein, so dass in einigen Fällen die einfache und günstigste Möglichkeit, höhere Leistung zu erzielen, nur einen schnelleren Computer kaufen könnte, sofern sie mit einem vorhandenen Computer kompatibel ist. Es gibt viele Möglichkeiten, wie die von einem Algorithmus verwendeten Ressourcen gemessen werden können: die beiden am häufigsten getroffenen Maßnahmen sind Geschwindigkeit und Gedächtnisnutzung; andere Maßnahmen könnten Übertragungsgeschwindigkeit, vorübergehende Festplattennutzung, Stromverbrauch, Gesamtbeteiligungskosten, Reaktionszeit auf externe Stimulus usw. umfassen. Viele dieser Maßnahmen hängen von der Größe des Beitrags zum Algorithmus ab, d.h. von der Menge der zu verarbeitenden Daten. Sie können auch von der Art und Weise abhängig sein, in der die Daten gespeichert sind; zum Beispiel werden einige Arten von Algorithmen schlecht auf Daten angewendet, die bereits sortiert sind oder in umgekehrter Reihenfolge geordnet sind. In der Praxis gibt es andere Faktoren, die die Effizienz eines Algorithmus beeinflussen können, wie Anforderungen an Genauigkeit und/oder Zuverlässigkeit. Wie weiter unten beschrieben, kann die Art und Weise, wie ein Algorithmus implementiert wird, auch einen erheblichen Einfluss auf die tatsächliche Effizienz haben, obwohl viele Aspekte dieser Art mit Optimierungsproblemen zusammenhängen. theoretische Analyse In der theoretischen Analyse von Algorithmen ist die übliche Praxis, ihre Komplexität im asymptotischen Sinne zu schätzen. Donald Knuth's Big O notation, die die Komplexität eines Algorithmus als Funktion der Größe des Inputs n \style \style n} .Big O notation ist eine asymptotische Maßnahme der Funktionskomplexität, wo f ( n ) = O (n } } \skript n\style f\style f\style f\style f\style \style n\style n\style n\style \style \g) \line \t \g \de \t \g \de (n \g). Diese Schätzung kann irreführend sein, wenn n Memetextstyle \skriptstyle n} klein ist, ist aber in der Regel hinreichend genau, wenn n {\textstyle \skriptstyle n} groß ist, da die Notation alsymptotic ist. Beispielsweise kann die Größe der Blase schneller sein als die Fusionsart, wenn nur einige Elemente sortiert werden sollen; die Umsetzung wird jedoch voraussichtlich die Leistungsanforderungen für eine kleine Liste erfüllen. In der Regel interessieren sich die Programmierer für Algorithmen, die effizient auf große Eingangsgrößen ausgestaltet sind, und die Zusammenlegungsart wird für Listen der Länge in den meisten datenintensiven Programmen bevorzugt. Manche Beispiele für große O-Zulassungen, die auf die asymptotische Zeit Komplexität von Algorithmen angewendet werden, sind: Benchmarking: Messung der Leistung Für neue Versionen von Software oder Vergleiche mit wettbewerbsfähigen Systemen werden Benchmarks manchmal verwendet, die die Abmessung von Algorithmen relativer Leistung unterstützen. Wenn beispielsweise ein neuer Art-Algorithmus hergestellt wird, kann es mit seinen Vorgängern verglichen werden, um sicherzustellen, dass mindestens es effizient ist wie vor mit bekannten Daten, wobei alle funktionalen Verbesserungen berücksichtigt werden. Benchmarks können von Kunden verwendet werden, wenn verschiedene Produkte von alternativen Lieferanten verglichen werden, um zu bewerten, welche Produkte ihren spezifischen Anforderungen in Bezug auf Funktionalität und Leistung am besten entsprechen. Konkret konkurrieren im Hauptrahmen bestimmte herstellereigene Artenprodukte von unabhängigen Softwareunternehmen wie Synsorts mit Produkten der großen Anbieter wie IBM für Geschwindigkeit. Manche Benchmarks bieten die Möglichkeit, eine Analyse zu erstellen, in der die relative Geschwindigkeit der verschiedenen zusammengestellten und interpretierten Sprachen zum Beispiel verglichen wird, und das Computer Language Benchmarks Game vergleicht die Umsetzung typischer Programmprobleme in mehreren Programmiersprachen. Selbst die Erstellung von Benchmarks für "do it selbst" kann die relative Leistung der verschiedenen Programmsprachen unter Verwendung einer Vielzahl von Benutzerkriterien nachweisen. Dies ist ganz einfach, da Christopher W. Cowell-Shah zum Beispiel „Nine Language Performance Roundup“ zeigt. Umsetzungsprobleme können auch Auswirkungen auf die Effizienz haben, wie die Wahl der Programmierungssprache oder die Art und Weise, wie der Algorithmus tatsächlich definiert ist, oder die Wahl eines Sammlers für eine bestimmte Sprache oder die verwendeten Erstellungsmöglichkeiten oder sogar das Betriebssystem. In vielen Fällen kann eine Sprache, die von einem Dolmetscher implementiert wird, viel langsamer sein als eine Sprache, die von einem Sammler implementiert wird. Lesen Sie die Artikel über die Zusammenstellung und Auslegung von Sprachen. Es gibt andere Faktoren, die sich auf Zeit- oder Raumfragen auswirken könnten, die jedoch außerhalb der Kontrolle eines Programmanbieters liegen könnten; dies umfassen Datenangleichung, Datengraularität, Herz-Kohärenz, Müllabfuhr, pädagogische Parallelismus, Multi-threading (auf Hardware- oder Software-Ebene), gleichzeitige Multi-tasking- und Subroutin-Anrufe. Manche Prozessoren verfügen über Fähigkeiten für die Vektorverarbeitung, die eine einzige Anleitung zum Betrieb auf mehreren Betriebsleitern ermöglichen; sie können oder können für einen Programmierer oder Teamer nicht einfach sein, diese Fähigkeiten zu nutzen. Algorithms, die für die sequentielle Verarbeitung bestimmt sind, müssen vollständig umgestaltet werden, um die Parallelverarbeitung zu nutzen, oder sie könnten leicht umgestaltet werden. Parallele und verteilte Rechner wachsen in den späten 2010er Jahren an Bedeutung, werden mehr Investitionen in effiziente High-Level- APIs für parallele und verteilte Rechensysteme wie CUDA, TensorFlow, Hadoop, OpenMP und MPI getätigt. Ein weiteres Problem, das bei der Programmierung auftreten kann, besteht darin, dass die Verarbeiter, die mit der gleichen Anleitung (wie x86-64 oder ARM) kompatibel sind, auf unterschiedliche Weise eine Anleitung durchführen können, so dass die Anweisungen, die bei einigen Modellen relativ schnell sind, auf andere Modelle relativ langsam sein können. Dies stellt oft Herausforderungen für die Optimierung von Sammlern dar, die über ein großes Wissen über die spezifische CPU und andere Hardware verfügen müssen, die über das Zusammenstellungsziel verfügen, um ein Leistungsprogramm optimal zu optimieren. Im Extremfall kann ein Gebilder dazu gezwungen werden, Anweisungen, die nicht auf einer Zusammenstellungszielplattform unterstützt werden, zu erlassen, um einen Code zu erstellen oder eine externe Bibliothek zu verknüpfen, die ansonsten auf dieser Plattform unwiderruflich ist, auch wenn er in der Hardware auf anderen Plattformen aufbaut und effizienter wird. In eingebetteten Systemen ist dies oft der Fall in Bezug auf schwimmende Arithmetic, wo kleine und kostengünstige Mikro-Betreiber oft keine Hardware-Unterstützung für schwimmende arithmetic benötigen und daher rechnerisch teure Software- Routinen benötigen, um schwimmende Punktberechnungen zu erstellen. Maßnahmen zur Ressourcennutzung werden in der Regel als Funktion der Größe des Inputs n Memedisplaystyle \skriptstyle {n} angegeben. Die beiden häufigsten Maßnahmen sind: Zeit: Wie lange dauert der Algorithmus vollständig? Raumfahrt: Wie viel Arbeitsspeicher (typisches RAM) vom Algorithmus benötigt wird? Dies hat zwei Aspekte: die Menge des Gedächtnisses, der durch den Code (Nebenraumnutzung) benötigt wird, und die Menge des Gedächtnisses, der für die Daten, auf denen der Code tätig ist (intrinsische Raumnutzung). Computer, deren Macht von einer Batterie (z.B. Laptops und Smartphones) oder für sehr lange/große Berechnungen (z.B. Supercomputer) geliefert wird, sind andere Maßnahmen von Interesse: Direkter Stromverbrauch: direkt benötigte Macht, um den Computer zu bedienen. Indirekter Stromverbrauch: Kraft für Kühlung, Beleuchtung usw. Seit 2018 wächst der Stromverbrauch als wichtige Parameter für rechnerische Aufgaben aller Arten und auf allen Ebenen, von eingebetteten Internet der Dinge bis zu System-on-Chip-Geräten bis zu Server-Betrieben. Dieser Trend wird häufig als grünes Rechen bezeichnet. Weniger gemeinsame Maßnahmen zur Recheneffizienz können auch in einigen Fällen relevant sein: Übertragungsgröße: Bandbreite könnte ein begrenzter Faktor sein. Datenkompression kann verwendet werden, um die zu übermittelnden Datenmengen zu reduzieren. Darstellung eines Bildes oder Bildes (z.B. Google-Logo) kann dazu führen, dass Zehntausende von Byten (48K in diesem Fall) im Vergleich zu sechs Durchbrüchen für den Text Google übertragen werden. Dies ist wichtig für I/O gebundene Rechenaufgaben. Außenraum: Raum, der auf einer Scheibe oder einem anderen externen Speichergerät benötigt wird; dies könnte für eine vorübergehende Lagerung sein, während der Algorithmus ausgeführt wird, oder es könnte eine langfristige Lagerung sein, die für die künftige Referenz erforderlich ist. Reaktionszeit (Länge): Dies ist besonders wichtig in einer Echtzeit-Anwendung, wenn das Computersystem schnell auf einige externe Ereignisse reagieren muss. Gesamtkosten des Eigentums: insbesondere wenn ein Computer einem bestimmten Algorithmus gewidmet ist. Zeittheorie Analyze den Algorithmus, in der Regel mit Zeit komplexer Analyse, um eine Schätzung der laufenden Zeit als Funktion der Datengröße zu erhalten. Das Ergebnis wird in der Regel mit großer O-Beobachtung ausgedrückt. Dies ist nützlich für den Vergleich von Algorithmen, insbesondere wenn eine große Menge von Daten verarbeitet werden soll. Mehr detaillierte Schätzungen sind erforderlich, um die Leistung von Algorithmen zu vergleichen, wenn die Datenmenge klein ist, obwohl dies wahrscheinlich weniger wichtig ist. Algorithms, die parallele Verarbeitung beinhalten, können schwieriger zu analysieren sein. Praxis Verwendung eines Benchmarks zur Zeit der Verwendung eines Algorithmus. Viele Programmiersprachen verfügen über eine verfügbare Funktion, die die Verwendung von CPU-Zeiten ermöglicht. Langfristige Algorithmen könnten auch von Interesse sein. Ergebnisse sollten in der Regel über mehrere Tests durchschnittlich durchgeführt werden. Run-basiertes Profil kann sehr empfindlich auf die Hardware-Konformität und die Möglichkeit anderer Programme oder Aufgaben sein, die gleichzeitig in einem mehrjährigen und multiprogrammierenden Umfeld laufen. Diese Art des Tests hängt auch stark von der Auswahl einer bestimmten Programmierungssprache, der Sammlung und der Gestaltung von Optionen ab, so dass Algorithmen im Vergleich zu den gleichen Bedingungen umgesetzt werden müssen. Raumfahrt In diesem Abschnitt geht es um die Nutzung von Speicherressourcen (Register, Cache, RAM, virtuelles Gedächtnis, sekundäres Gedächtnis), während der Algorithmus ausgeführt wird. Analyse der Zeitanalyse oben, Analyse des Algorithmus, in der Regel mit einer Analyse der Raumkomplexität, um eine Schätzung des zukunftsfähigen Speichers als Funktion als Größe der Inputdaten zu erhalten. Das Ergebnis wird in der Regel mit großer O-Beobachtung ausgedrückt. Es gibt bis zu vier Aspekte der Speichernutzung: Höhe des Gedächtnisses, um den Code für den Algorithmus zu halten. Höhe der für die Eingabedaten erforderlichen Speicher. Höhe des Speichers, der für alle Outputdaten benötigt wird. Manche Algorithmen wie die Sortierung, die häufig die Inputdaten zurücklegen und brauchen keinen zusätzlichen Raum für Outputdaten. Dieses Eigentum wird als In-place-Betrieb bezeichnet. Die Menge des Gedächtnisses, der während der Berechnung als Arbeitsraum benötigt wird. Hierzu gehören lokale Variablen und jede von Routinen, die in einer Berechnung genannt werden, benötigte stapelbare Fläche; dieser Poolraum kann für Algorithmen, die recursive Techniken verwenden, signifikant sein. Frühe elektronische Computer und frühe Heimcomputer hatten relativ kleine Mengen an Arbeitsspeichern. Zum Beispiel hatte die elektronische Verzögerungsspeicherautomation von 1949 (EDSAC) einen maximalen Arbeitsspeicher von 1024 17-bit- Wörtern, während der Computercomputer von 1980 bis 1980 ursprünglich mit 1024 8-bit mit einem Arbeitsspeicher zusammenkam. In den späten 2010er Jahren ist es typisch für persönliche Computer, zwischen 4 und 32 GB RAM zu verfügen, eine Erhöhung von über 300 Millionen Mal so viel Speicher. Laufende Computer können relativ große Mengen an Speicher haben (möglicherweise JPY), so dass sie einen Algorithmus in eine begrenzte Menge Gedächtnis drücken müssen, ist viel weniger ein Problem als es zu sein hat. Jedoch kann das Vorhandensein von vier verschiedenen Kategorien von Gedächtnis bedeuten: Prozessorregister, die schnellste Computerspeichertechnik mit dem geringsten Speicherumfang. Die meisten direkten Berechnungen zu modernen Computern finden sich in Registern mit Quelle und Zielort, bevor sie erforderlichenfalls auf den Cache, den Hauptspeicher und den virtuellen Speicher aktualisiert werden. Auf einem Prozessorkern gibt es in der Regel Hunderte von Bytes oder weniger Registrierungsverfügbarkeit, obwohl eine Registrierungsdatei mehr physische Register enthalten kann als in der Unterrichtsform definierte architektonische Register. Kühlspeicher ist das zweite schnellste und zweite kleinste Gedächtnis in der Speicherhierarchie. Kühlschränke sind in CPUs, Druckern, Festplattenantrieben und externen Peripheriegeräten vorhanden und werden in der Regel in statischem RAM umgesetzt. Speicherfilter sind multi-Levelig; niedrigere Ebenen sind größer, langsamer und in der Regel zwischen Prozessorkernen in Multicore-Verarbeitern geteilt. Zur Verarbeitung von Betrieben in Cache muss eine Verarbeitungsstelle die Daten aus dem Cache, die Tätigkeit in Registern ausüben und die Daten zurück auf den Cache schreiben. Mit einer Geschwindigkeit von etwa 2-10-mal langsamer wird dies mit der Arithmetic Logik-Einheit oder einer schwimmenden Einheit des L1 Caches vergleichbar. Es ist etwa zehnmal langsamer, wenn ein L1-Chole fehlt, und es muss von und nach dem L2-Cache abgerufen werden, und eine weitere zehnfache Verlangsamung, wenn ein L2-Chole fehlt, und es muss von einem L3-Cache, falls vorhanden, abgerufen werden. Main Physical Memory wird am häufigsten in dynamischen RAM (DRAM) umgesetzt. Hauptspeicher ist viel größer (typischer Gigabyte gegenüber 88 Megabytes) als ein L3-Prozessor, mit Lesen und Verschreibungen in der Regel 10-100 mal langsamer. Seit 2018 wird RAM zunehmend als CPU oder Druckerspeicher auf dem Chip von Prozessoren implementiert. virtuelles Gedächtnis wird am häufigsten in Bezug auf die sekundäre Lagerung wie eine harte Scheibe umgesetzt und ist eine Erweiterung auf die Speicherhierarchie, die viel größere Lagerfläche, aber viel größere Verspätung hat, in der Regel etwa 1000 Mal langsamer als ein Cache für einen Wert in RAM. Obwohl ursprünglich motiviert, den Eindruck höherer Speichermengen zu schaffen als wirklich verfügbar, ist das virtuelle Gedächtnis bei der zeitgenössischen Nutzung für den Zeitraum-Handel und die Nutzung virtueller Maschinen wichtiger. Klärchen, die aus dem Hauptspeicher verfehlt werden, werden als Seitenfehler bezeichnet, und es werden enorme Leistungsstrafen für Programme verhängt.Ein Algorithmus, deren Gedächtnis in die Cache-Speicher passt, wird viel schneller sein als ein Algorithmus, der in der Hauptspeicher passt, der wiederum sehr viel schneller ist als ein Algorithmus, der auf virtuelles Gedächtnis zurückgreifen muss. Da es sich hierbei um eine Ersatzpolitik handelt, ist eine äußerst wichtige Voraussetzung für Hochleistungsrechner, da es sich um die Programmierung und die Datenangleichung handelt. Um das Problem noch weiter zu erschweren, haben einige Systeme bis zu drei Stufen des Cachespeichers mit unterschiedlichen effektiven Geschwindigkeiten. unterschiedliche Systeme werden unterschiedliche Mengen dieser verschiedenen Arten von Gedächtnis haben, so dass die Auswirkungen des Speicherbedarfs von einem System auf ein anderes variieren können. In den frühen Tagen des elektronischen Datenverarbeitungswesens, wenn ein Algorithmus und seine Daten nicht in den Hauptspeicher passen würden, konnte der Algorithmus nicht verwendet werden. Heutzutage scheint der Einsatz virtueller Speicher viel Gedächtnis zu bieten, aber zu den Leistungskosten. Kommt ein Algorithmus und seine Daten in die Kläranlage, so kann eine sehr hohe Geschwindigkeit erreicht werden; in diesem Fall wird der Raum auch dazu beitragen, die Zeit zu minimieren. Dies ist der Grundsatz der Lokalität und kann in die lokale Charakterisierung von Referenz, räumlicher Lokalisierung und zeitlicher Lokalität unterteilt werden. Ein Algorithmus, der sich nicht vollständig in die KL-Speicher passt, der aber die lokale Charakterisierung zeigt, kann recht gut funktionieren. Kritiker des aktuellen Stands der Programmierung David May FRS ein britischer Computerwissenschaftler und derzeit Professor für Informatik an der Universität Bristol und Gründer und CTO von XMOS Semiconductor sind der Ansicht, dass eine der Probleme darin besteht, dass die Rechtsvorschriften der Moore zur Lösung von Ineffizienzen in den Griff bekommen. Er hat eine Alternative zum US-Recht (Bürgerrecht) entwickelt: Softwareeffizienz halves alle 18 Monate, was das Moore-Gesetz vom Mai in den Zustand bringt: In allgegenwärtigen Systemen können die ausgeführten Anweisungen das Batterieleben verdoppeln und große Datensets eröffnen große Möglichkeiten für bessere Software und Algorithmen: Die Verringerung der Zahl der Operationen von N x N bis N x Log(N) hat dramatische Wirkung, wenn N groß ist ... für N = 30 Milliarden, diese Änderung ist genauso gut wie 50 Jahre technologischer Verbesserungen. Software Autor Adam N. Rosenburg in seinem Blog "Das Versagen des digitalen Computers" hat den aktuellen Stand der Programmierung in der Nähe des "Software-Veranstaltungshorizonts" beschrieben (allgemeines Ende der fiktiven "Shoe-Veranstaltung", erläutert von Douglas Adams in seinem Leitfaden für das Aktienbuch. Er schätzt, dass seit den 1980er-Jahren ein Produktivitätsverlust von 70 dB oder "99.99 Prozent seiner Fähigkeit, die Ware zu liefern," gegeben ist, als Arthur C. Clarke im Jahr 2001 gegenüber dem Computer HAL 9000 in seinem Buch 2001:A Weltraum-Atlantik wies er darauf hin, wie sehr kleine und leistungsfähige Computer seien, aber wie enttäuschend die Computer-Programmierung geworden sei. Wettbewerb für die besten Algorithmen Folgende Wettbewerbe fordern Beiträge für die besten Algorithmen auf, die auf einigen willkürlichen Kriterien basieren, die von den Richtern festgelegt wurden: Wirtes Magazin Lesen Sie auch die Analyse von Algorithmen – das Know-how, um die Ressourcen zu bestimmen, die von einem Algorithmus Arithmetic codes benötigt werden – eine Form der variablen entropy-Kodierung zur effizienten Datenverschlüsselung – eine Datenstruktur, die effizienter mit Patricia Bäume oder Judys Arrays Benchmarks eingesetzt werden kann – eine Methode zur Messung vergleichender Ausführungszeiten in bestimmten Fällen beste, schlechte und durchschnittliche Fall – Überprüfungen für die Schätzung von Ausführungszeiten in drei Szenarios – eine einfache und effiziente Methode zur Suche nach verschiedenen Rechenbereichen – eine Technik zur Reduzierung von Unterrichtspfaden, Größe des Maschinencodes, (und oft auch Gedächtnis) Komparison von Programmierungsparadigmen – spezifische Leistungserwägungen Compileroptimierung – eine Optimierung der Cautomatischen Rechenleistung für mathematische Anwendungen im Bereich der Rechenleistung von Computern – Computer-Komprimierungsparameter – eine Kombination von Datenübertragungs-Daten und -speichern, die die die Datenstruktur verbessern Vergleich von Algorithmen Speculative Ausführung oder Eager Ausführung Zweigvorhersage Super-threading Hyper-threading-Loging-Codes – ähnlich der virtuellen Methode-Tabelle oder der branchenspezifischen Tabelle – mit dynamischen Stellen für die Versendung von Verweisen auf Außenbeziehungen des Boyer-Moore-Algorithmus(Java Applet) „Wie Algorithmen gestalten unsere Welt“. TED (Konference) Gespräch von Kevin Slavin. Konzepte zur algorischen Effizienz in High-schools