Die Hauptkomponenten einer Ansammlung von Punkten in einem realen Koordinatenraum sind eine Folge von p \{displaystyle p}-Einheitsvektoren, wobei der i \{displaystyle i} -th-Vektor die Richtung einer Linie ist, die die Daten am besten orthogonal zu den ersten i - 1 \{displaystyle i-1}-Vektoren passt. Hier wird eine am besten passende Linie als eine definiert, die den durchschnittlichen quadratischen Abstand von den Punkten zur Linie minimiert. Diese Richtungen stellen eine orthonormale Grundlage dar, in der unterschiedliche Einzelabmessungen der Daten linear unkorreliert sind. Die Hauptkomponentenanalyse (PCA) ist der Prozess der Berechnung der Hauptkomponenten und deren Verwendung, um eine Änderung der Basis auf den Daten durchzuführen, manchmal nur die ersten wenigen Hauptkomponenten zu verwenden und den Rest zu ignorieren. PCA wird in der explorativen Datenanalyse und zur Herstellung von Vorhersagemodellen verwendet. Es wird häufig für die Dimensionsreduktion verwendet, indem jeder Datenpunkt auf nur die ersten wenigen Hauptkomponenten projiziert wird, um tiefere Dimensionsdaten zu erhalten und dabei möglichst viel der Datenvariation zu erhalten. Die erste Hauptkomponente kann äquivalent als eine Richtung definiert werden, die die Varianz der projizierten Daten maximiert. Die i \{displaystyle i} -th Hauptkomponente kann als Richtung orthogonal zu den ersten i - 1 \{displaystyle i-1} Hauptkomponenten genommen werden, die die Varianz der projizierten Daten maximieren. Von beiden Zielen kann gezeigt werden, dass die Hauptkomponenten Eigenvektoren der Kovarianzmatrix der Daten sind. So werden die Hauptkomponenten oft durch Eigendekomposition der Datenkovarianzmatrix oder Singularwertzersetzung der Datenmatrix berechnet. PCA ist die einfachste der echten eigenvektorbasierten Multivariatanalysen und ist eng mit der Faktoranalyse verbunden. Die Faktoranalyse beinhaltet typischerweise mehr Domain-spezifische Annahmen über die zugrunde liegende Struktur und löst Eigenvektoren einer etwas anderen Matrix. PCA ist auch mit der canonischen Korrelationsanalyse (CCA) verbunden. CCA definiert Koordinatensysteme, die die Kreuzkovarianz zwischen zwei Datensätzen optimal beschreiben, während PCA ein neues orthogonales Koordinatensystem definiert, das die Varianz in einem einzigen Datensatz optimal beschreibt. Auch robuste und L1-norm-basierte Varianten der Standard-PCA wurden vorgeschlagen. Die Geschichte PCA wurde 1901 von Karl Pearson erfunden, als Analogie zum Hauptachsentheorem in der Mechanik; sie wurde später von Harold Hotelling in den 1930er Jahren unabhängig entwickelt und benannt. Je nach Anwendungsgebiet wird sie auch als diskrete Karhunen-Loève-Transformation (KLT) in der Signalverarbeitung bezeichnet, die Hotellerie-Transformation in multivariater Qualitätskontrolle, richtige orthogonale Zersetzung (POD) in der Maschinenbau, Singularwert-Eigenzersetzung (SVD) von X (im letzten Quartal des 19. Jahrhunderts erfunden), Eigenwertzersetzung (EVD) von XTX in linearer Algebra, Faktoranalyse (für eine Intuition PCA kann als Einbau eines p-dimensionalen Ellipsoids zu den Daten gedacht werden, wobei jede Achse des Ellipsoids eine Hauptkomponente darstellt. Ist eine Achse des Ellipsoids klein, so ist auch die Varianz entlang dieser Achse klein. Um die Achsen des Ellipsoids zu finden, müssen wir zunächst den Mittelwert jeder Variablen vom Datensatz subtrahieren, um die Daten um den Ursprung zu zentrieren. Dann berechnen wir die Kovarianzmatrix der Daten und berechnen die Eigenwerte und entsprechende Eigenvektoren dieser Kovarianzmatrix. Dann müssen wir jeden der orthogonalen Eigenvektoren normalisieren, um sie in Einheitsvektoren zu verwandeln. Sobald dies geschehen ist, kann jeder der zueinander orthogonalen Einheitseigenvektoren als Achse des an den Daten angebrachten Ellipsoids interpretiert werden. Diese Wahl der Basis wird unsere Kovarianz-Matrix in eine diagonalisierte Form mit den diagonalen Elementen verwandeln, die die Varianz jeder Achse darstellen. Der Anteil der Varianz, die jeder Eigenvektor repräsentiert, kann berechnet werden, indem der dem Eigenvektor entsprechende Eigenwert durch die Summe aller Eigenwerte geteilt wird. Details PCA ist definiert als eine orthogonale lineare Transformation, die die Daten zu einem neuen Koordinatensystem transformiert, so dass die größte Varianz durch eine skalare Projektion der Daten auf der ersten Koordinaten (genannte erste Hauptkomponente), die zweite größte Varianz auf der zweiten Koordinaten und so weiter zu liegen kommt. Betrachten Sie eine n × p \{displaystyle n\times p}-Datenmatrix, X, mit spaltenweisem Null-Imperium (das Probenmittel jeder Spalte wurde auf Null verschoben), wobei jede der n Zeilen eine andere Wiederholung des Experiments darstellt, und jede der p-Säulen gibt eine bestimmte Art von Merkmal (z.B. die Ergebnisse eines bestimmten Sensors). , n = n Erste Komponente Um die Varianz zu maximieren, muss also der erste Gewichtsvektor w(1) w ( 1 ) = arg ≠ max gefunden w zusammengestellt = 1 { Σ i (t 1 ) (i ) 2 } = arg } \{displaystyle \mathbf {w} _({1)}=\arg \max _\{ Verteidigen Sie sich \Vert 1}\,\left\{\sum_{i}(t_{1})_{(i){2}\right\}=\arg \max _\{\{ Verteidigen Sie sich \Vert =1}\,\left\{\sum _{i}\left(\mathbf {x} _({i)}\cdot \mathbf {w} rechts)^{2}\right Entsprechendes Schreiben in Matrixform ergibt w ( 1 ) = arg ≠ max gefunden w = 1 = arg = arg = max. 1 , t X , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t , t ! Recht\ _=1\left\{\left\|\\\mathbf {Xw} Recht\}{2}\right\}=\arg \max _{left\|\\\\mathbf {w} In den Warenkorb Da w(1) als Einheitsvektor definiert worden ist, erfüllt es äquivalent auch w ( 1 ) = arg fra max { w T x t w t w t w t w t w t w } \ n \ n \ n \ n \ n \ n ^{mathsf {T}\mathbf X}^{\mathf {T}\mathbf {Xw}}{\mathbf {w} \^{mathsf {T}\mathbf {w} rechts Die zu maximierende Menge kann als Rayleigh Quotient anerkannt werden. Ein Standardergebnis für eine positive semidefinite Matrix wie XTX ist, dass der maximal mögliche Quotient der größte Eigenwert der Matrix ist, der auftritt, wenn w der entsprechende Eigenvektor ist. Bei gefundenem w(1) kann dann die erste Hauptkomponente eines Datenvektors x(i) als Score t1(i) =x(i) ∙ w(1) in den transformierten Koordinaten oder als entsprechender Vektor in den Originalvariablen {x(i) ∙ w(1}) w(1) angegeben werden. Weitere Komponenten Die k-te Komponente kann durch Subtraktion der ersten k - 1 Hauptkomponenten aus X: X ^ k = X - Σ s = 1 k - 1 X w (s ) w (s ) T \{displaystyle \mathbf \{hat {X} {X} -\sum _s=1}^{k-1}\mathbf {X} \mathbf {w} _({s)}\mathbf {w} _(s)}^^{\mathsf {T} und dann den Gewichtsvektor zu finden, der die maximale Varianz aus dieser neuen Datenmatrix w (k) = a r g m a x w ermittelt = 1 χ χ ^ k w , 2 } = arg *{left\|\\\mathbf} Recht\|=1}\left\{\left\|\\mathbf \{hat {X}\k}\mathbf {w} In den Warenkorb \^{mathsf {T}\mathbf \{hat (X) (X) {\mathbf} ^{T}\mathbf Es stellt sich heraus, dass dies die übrigen Eigenvektoren von XTX ergibt, wobei die Maximalwerte für die Menge in Klammern durch ihre entsprechenden Eigenwerte gegeben sind. So sind die Gewichtsvektoren Eigenvektoren von XTX. Die k-te Hauptkomponente eines Datenvektors x(i) kann daher als Score tk(i) =x(i) ⋅ w(k) in den transformierten Koordinaten oder als entsprechender Vektor im Raum der Originalvariablen {x(i) ∙ w(k) w(k) angegeben werden, wobei w(k) der k-te Eigenvektor von XTX ist. Die vollständige Hauptkomponentenzersetzung von X kann daher als T = X W \{displaystyle \mathbf {T} =\mathbf {X} \mathbf {W} angegeben werden, wobei W eine p-by-p-Matrix von Gewichten ist, deren Spalten die Eigenvektoren von XTX sind. Die Transpose von W wird manchmal als Weiß- oder Sphäretransformation bezeichnet. Spalten von W multipliziert mit der quadratischen Wurzel entsprechender Eigenwerte, d.h. mit den Varianzen skalierte Eigenvektoren werden in PCA oder in der Faktoranalyse Belastungen genannt. Kovarianzen XTX selbst können als proportional zur empirischen Probenkovarianzmatrix des Datensatzes XT erkannt werden. Die Probenkovarianz Q zwischen zwei der verschiedenen Hauptkomponenten über den Datensatz ist gegeben durch: Q ( P C (j ) , P C (k ) ) a ( X w (j ) ) T (X w (k ) ) )= w (j ) T X T X w (k ) = w (j ) T λ (k ) w (k ) = λ (k ) w ( j ) T w (k) \{displaystyle start{ausgeglichen}Q(\mathrm {PC} _({j)},\mathrm {PC}}) und\propto (\mathbf {X} \mathbf {w}\j)}{\\\\mathsf}(\mathbf {X} \mathbf} {f}} (X} \^{mathsf {T}}\mathbf {X} \mathbf {w} _({k)}\&=\mathbf {w}_(j)}}\\m Eigenwert Das Produkt in der Endzeile ist daher Null; es gibt keine Probenkovarianz zwischen verschiedenen Hauptkomponenten über den Datensatz. Eine weitere Möglichkeit, die Hauptkomponententransformation zu charakterisieren, ist daher die Transformation zu Koordinaten, die die empirische Probenkovarianzmatrix diagonalisieren. In Matrixform kann die empirische Kovarianzmatrix für die Originalvariablen Q a X T X = W Λ W T \{displaystyle \mathbf {Q} \propto \mathbf {X} \^{mathf {T}}\mathbf {X} =\mathbf {W} \mathbf {\Lambda} \mathbf W}^{\mathf {T} Die empirische Kovarianzmatrix zwischen den Hauptkomponenten wird EPMATHMARKEREP Die Transformation T =X W stellt einen Datenvektor x(i) aus einem ursprünglichen Raum von p-Variablen in einen neuen Raum von p-Variablen, die über den Datensatz unkorreliert sind, ab. Allerdings müssen nicht alle Hauptkomponenten gehalten werden. Das Halten nur der ersten L Hauptkomponenten, hergestellt durch die Verwendung nur der ersten L Eigenvektoren, gibt die verkürzte Transformation T L = X W L \{displaystyle \mathbf {T} _{L}=\mathbf {X} \mathbf {W} _{L}, wo die Matrix TL jetzt n Zeilen hat, aber nur L Spalten. Mit anderen Worten lernt PCA eine lineare Transformation t = W L T x , x ε R p, t ε R L , \{displaystyle t=W_{L}^{\mathsf {T}x,x\in R^{p},t\in R^{L}, wobei die Spalten von p ×L Matrix W L \{displaystyle W_{L} eine orthogonale Basis bilden Durch die Konstruktion, aller transformierten Datenmatrizen mit nur L-Spalten, maximiert diese Score-Matrix die Varianz in den ursprünglichen Daten, die erhalten wurden, während die Minimierung des gesamten quadratischen Rekonstruktionsfehlers T W T - T L T L T 2 \{displaystyle |\mathbf {T} \mathbf {W} ^{T}-\mathbf {T} (W) T}\|_{2}^{2 oder φ X - X L wurde gefunden 2 2 \{displaystyle |\mathbf {X} -\mathbf {X} L}\|_{2}{2 . Eine solche Dimensionsreduktion kann ein sehr nützlicher Schritt zur Visualisierung und Verarbeitung von hochdimensionalen Datensätzen sein, wobei noch so viel der Varianz im Datensatz wie möglich beibehalten wird. So findet z.B. die Auswahl von L = 2 und die Beibehaltung nur der ersten zwei Hauptkomponenten die zweidimensionale Ebene durch den hochdimensionalen Datensatz, in dem die Daten am weitesten verbreitet sind, so dass, wenn die Daten Cluster enthalten, diese auch am weitesten verbreitet werden können und daher am sichtbarsten in einem zweidimensionalen Diagramm dargestellt werden können; während bei der Auswahl zweier Richtungen durch die Daten (oder zwei der ursprünglichen Variablen) wesentlich weniger stark voneinander verteilt sein können. Ebenso ist bei der Regressionsanalyse, je größer die Anzahl der zulässigen erläuternden Variablen ist, umso größer ist die Wahrscheinlichkeit, das Modell zu überarbeiten und daraus Schlussfolgerungen zu ziehen, die sich nicht auf andere Datensätze verallgemeinern. Ein Ansatz, insbesondere wenn es starke Korrelationen zwischen verschiedenen möglichen erläuternden Variablen gibt, ist, sie auf einige Hauptkomponenten zu reduzieren und dann die Regression gegen sie zu führen, ein Verfahren, das als Hauptkomponentenregression bezeichnet wird. Die Abmessung kann auch dann sinnvoll sein, wenn die Variablen in einem Datensatz laut sind. Enthält jede Spalte des Datensatzes unabhängige, gleich verteilte Gausssche Geräusche, so enthalten die Spalten von T ebenfalls gleich verteilte Gausssche Geräusche (eine solche Verteilung ist invariant unter den Wirkungen der Matrix W, die als hochdimensionale Rotation der Koordinatenachsen gedacht werden kann). Bei mehr der Gesamtvarianz, die sich in den ersten wenigen Hauptkomponenten im Vergleich zur gleichen Rauschvarianz konzentriert, ist die proportionale Wirkung des Rauschens geringer - die ersten wenigen Komponenten erreichen ein höheres Signal-Rausch-Verhältnis. PCA kann somit dazu führen, dass ein Großteil des Signals in die ersten wenigen Hauptkomponenten konzentriert wird, die sinnvollerweise durch Dimensionsreduktion erfasst werden können, während die späteren Hauptkomponenten durch Rauschen dominiert werden können und so ohne großen Verlust entsorgt werden. Ist der Datensatz nicht zu groß, kann die Bedeutung der Hauptkomponenten mit parametrischem Bootstrap getestet werden, als Hilfe bei der Bestimmung, wie viele Hauptkomponenten zu halten sind. Zersetzung des Wertes Die Hauptkomponententransformation kann auch einer anderen Matrix-Faktorisierung zugeordnet sein, deren Einzelwertzersetzung (SVD) von X, X = U Σ W T \{displaystyle \mathbf {X} =\mathbf {U} \mathbf \{Sigma } \mathbf {T} ist hier als n-by-p Rechteckdiagonalmatrix von positiven Zahlen σ(k) In Bezug auf diese Faktorisierung kann die Matrix XTX X T X = W Σ T U T U Σ W T = W Σ T Σ T Σ T = W Σ W T = W Σ ^ 2 W T \{displaystyle start{align}\mathbf {X} ^{T}\mathbf (X} &=\mathbf {W} \mathbf \{Sigma } \^{mathf {T}\mathbf {U} \^{mathsf {T}}\mathbf {U} \mathbf \{Sigma} \mathbf {W} \^{mathsf {T}\&=\mathbf {W} \mathbf \{Sigma } \^{mathf {T}\mathbf \{Sigma} \mathbf {W} \^{mathsf {T}\&=\mathbf ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ {W} \^{mathsf T}\end{ausgeglichen, wobei Σ ^ \{displaystyle \mathbf \{hat \{Sigma }} die quadratische Diagonalmatrix mit den Singularwerten von X und den überschüssigen Nullen, die abgeschnitten sind, die zufriedenstellende Σ ^ 2 = Σ T Σ \{displaystyle \mathbf \{hat \{Sigma ^{2 =\mathbf \{Sigma } \^{mathsf {T}}\mathbf \{Sigma } .Vergleich mit der Eigenvektorfaktorisierung von XTX stellt fest, dass die rechten Einzelvektoren W von X den Eigenvektoren von XTX entspricht, während die Einzelwerte σ(k) von X \{displaystyle \mathbf {X} gleich der Quadratwurzel der Eigenwerte λ(k) von XTX sind. Mit der einzigartigen Wertzerlegung kann die Score-Matrix T = X W = U Σ W T W = U Σ \{displaystyle start{aligned}\mathbf {T} &=\mathbf {X} \mathbf {W} &=\mathbf {U} \mathbf \{Sigma } \mathbf {W} \^{mathsf {T}\mathbf {W} &=\mathbf {U} \mathbf \{Sigma } \end{sf}} \end{Sing{ed}, so dass jede Spalte von T mit einem der linken Wert angegeben wird Diese Form ist auch die polare Zersetzung von T. Effiziente Algorithmen existieren, um den SVD von X zu berechnen, ohne die Matrix XTX bilden zu müssen, so dass die Berechnung der SVD jetzt die Standardmethode ist, eine Hauptkomponentenanalyse aus einer Datenmatrix zu berechnen, es sei denn, nur eine Handvoll Komponenten sind erforderlich. Wie bei der Eigendekomposition kann eine verkürzte n × L-Score-Matrix TL erhalten werden, indem nur die ersten L größten Singularwerte und deren Singularvektoren berücksichtigt werden: T L = U L Σ L = X W L \{displaystyle \mathbf {T} {L}=\mathbf (U) (Sigma) Die Abkürzung einer Matrix M bzw. T unter Verwendung einer gekürzten Einzelwertzerlegung ergibt auf diese Weise eine gekürzte Matrix, die die nächstmögliche Matrix des Ranges L zur ursprünglichen Matrix ist, im Sinne der Differenz zwischen den beiden mit der kleinsten möglichen Frobenius-Norm, einem als Eckart-Young-Theorem bekannten Ergebnis [1936]. Weitere Überlegungen Bei einer Reihe von Punkten im Euclideschen Raum entspricht die erste Hauptkomponente einer Linie, die das mehrdimensionale Mittel durchläuft und die Summe der Quadrate der Abstände der Punkte aus der Linie minimiert. Die zweite Hauptkomponente entspricht dem gleichen Konzept, nachdem alle Korrelation mit der ersten Hauptkomponente von den Punkten abgezogen worden ist. Die Einzelwerte (in Σ) sind die Quadratwurzeln der Eigenwerte der Matrix XTX. Jeder Eigenwert ist proportional zum Teil der Varianz (genauer der Summe der quadratischen Abstände der Punkte aus ihrem mehrdimensionalen Mittel), die jedem Eigenvektor zugeordnet ist. Die Summe aller Eigenwerte ist gleich der Summe der quadratischen Abstände der Punkte aus ihrem mehrdimensionalen Mittelwert. Die PCA dreht im wesentlichen den Punktsatz um ihr Mittel, um die Hauptkomponenten auszurichten. Dies bewegt sich so viel der Varianz wie möglich (unter Verwendung einer orthogonalen Transformation) in die ersten Dimensionen. Die Werte in den übrigen Dimensionen sind daher eher klein und können mit minimalem Informationsverlust (siehe unten) fallen. Auf diese Weise wird oft PCA zur Dimensionsreduktion eingesetzt. PCA unterscheidet sich von der optimalen orthogonalen Transformation zum Halten des Subraums, der die größte Varianz aufweist (wie oben definiert). Dieser Vorteil kommt jedoch zum Preis größerer Rechenanforderungen, wenn er beispielsweise und gegebenenfalls auf die diskrete Cosinustransformation, insbesondere auf die DCT-II, die einfach als DCT bezeichnet wird, verglichen wird. Nichtlineare Dimensionsreduktionstechniken sind meist rechnerisch anspruchsvoller als PCA. PCA ist empfindlich auf die Skalierung der Variablen. Wenn wir nur zwei Variablen haben und die gleiche Mustervarianz haben und positiv korreliert sind, dann wird die PCA eine Drehung um 45° und die Gewichte (sie sind die Kosinen der Rotation) für die beiden Variablen bezüglich der Hauptkomponente gleich sein. Wenn wir jedoch alle Werte der ersten Variablen um 100 multiplizieren, dann wird die erste Hauptkomponente mit einem kleinen Beitrag aus der anderen Variable fast gleich sein, während die zweite Komponente nahezu mit der zweiten Originalvariablen ausgerichtet wird. Dies bedeutet, dass PCA, wenn die verschiedenen Größen unterschiedliche Einheiten (wie Temperatur und Masse) aufweisen, eine etwas willkürliche Analysemethode ist.(Differente Ergebnisse würden erzielt werden, wenn man beispielsweise Fahrenheit anstatt Celsius verwendet.) Pearsons Originalpapier wurde mit dem Titel "On Lines and Planes of Closest Fit to Systems of Points in Space" – "in space" impliziert physischen Euclidean Raum, wo solche Bedenken nicht auftreten. Eine Möglichkeit, die PCA weniger willkürlich zu machen, ist die Verwendung von Variablen, die so skaliert sind, dass eine Einheitsvarianz vorliegt, indem die Daten standardisiert werden und damit die Autokorrelationsmatrix anstelle der Autokovarianzmatrix als Basis für PCA verwendet wird. Diese komprimiert (oder expandiert) jedoch die Schwankungen in allen Dimensionen des Signalraums zur Einheitsvarianz. Eine mittlere Subtraktion (a.k.a."mittlere Zentrierung)" ist zur Durchführung der klassischen PCA erforderlich, um sicherzustellen, dass die erste Hauptkomponente die Richtung der maximalen Varianz beschreibt. Wird keine mittlere Subtraktion durchgeführt, so kann die erste Hauptkomponente vielmehr mehr oder weniger dem Mittelwert der Daten entsprechen. Ein Mittelwert von Null wird benötigt, um eine Basis zu finden, die den mittleren quadratischen Fehler der Näherung der Daten minimiert. Eine Mittelzentrierung ist unnötig, wenn eine Hauptkomponentenanalyse auf einer Korrelationsmatrix durchgeführt wird, da die Daten nach der Berechnung von Korrelationen bereits zentriert werden. Korrelationen stammen aus dem Cross-Produkt von zwei Standard-Scores (Z-Scores) oder statistischen Momenten (daher der Name: Pearson Product-Moment Correlation). Siehe auch den Artikel von Kromrey & Foster-Johnson (1998) zu "Mean-centering in Moderated Regression: Much Ado About Nothing". PCA ist eine beliebte Primärtechnik in der Mustererkennung. Es ist jedoch nicht für Klassentrennbarkeit optimiert. Es wurde jedoch verwendet, um den Abstand zwischen zwei oder mehr Klassen zu quantifizieren, indem die Massenmitte für jede Klasse im Hauptkomponentenraum berechnet und die Euclide-Distanz zwischen dem Massenzentrum von zwei oder mehr Klassen gemeldet wird. Die lineare diskriminierende Analyse ist eine Alternative, die für die Klassentrennbarkeit optimiert ist. Tabelle der Symbole und Abkürzungen Eigenschaften und Einschränkungen von PCA PropertiesEinige Eigenschaften von PCA umfassen: 1: Für jede ganze Zahl q, 1 ≤ q ≤ p, die orthogonale lineare Transformation y = B' x \{displaystyle y=\mathbf {B'} x} wobei y \{displaystyle y} ein q-Element Vektor ist und B' \{displaystyle \mathbf {B'} } eine (q × p) Matrix ist und Σ y = B' Σ B \{displaystyle \mathbf \{Sigma} *{y}=\mathbf {B'} \mathbf \{Sigma } \mathbf {B} ist die Varianz-Kovarianz-Matrix für y \{displaystyle y} .Dann die Spur von Σ y \{displaystyle \mathbf \{Sigma } _{y}, denoted tr Σ y Σ A q \{displaystyle \mathbf {B} =\mathbf {A} _{q}, wobei A q \{displaystyle \mathbf {A} _{q} aus den ersten q Spalten von A \{displaystyle \mathbf {A} besteht ( B' \Proplaystyle (\mathbf {B} } ist die Transposition von B' Betrachten Sie erneut die orthonormale Transformation y = B' x \{displaystyle y=\mathbf {B'} x} mit x , B, A \{displaystyle x,\mathbf {B},\mathbf {A} und Σ y \{displaystyle \mathbf \{Sigma } _{y} wie zuvor definiert. Dann wird tr ‡ ( Σ y ) \{displaystyle \operatorname {tr} (\mathbf \{Sigma } _{y}) durch Einnahme von B = minimiert A q χ , \{displaystyle \mathbf {B} =\mathbf {A} _q}^*, wobei A q χ \{displaystyle \mathbf {A} _q}^* aus den letzten q Spalten von A \{displaystyle \mathbf {A} PCs besteht.Die statistische Implikation dieser Eigenschaft ist, dass die letzten PCs nicht übrig sind Weil diese letzten PCs Varianzen so klein wie möglich haben, sind sie in ihrem eigenen Recht nützlich. Sie können dazu beitragen, unkontrollierte, nahkonstante lineare Zusammenhänge zwischen den Elementen von x zu erkennen, und sie können auch bei der Regression nützlich sein, bei der Auswahl einer Teilmenge von Variablen aus x und bei der Outlier-Erkennung. Objekt 3: (Spektralzersetzung von Σ) Σ = λ 1 α 1 α 1' +  of + λ p α p' \{displaystyle \mathbf \{Sigma } =\lambda *{1}\alpha *{1}\alpha _{1}'+\cdots +\lambda (p}\alpha) (p}\alpha) *{p} Bevor wir die Nutzung betrachten, betrachten wir zunächst diagonale Elemente, Var ‡ ( x j ) = Σ k = 1 P (x_{j})=\sum _k=1}^{P}\lambda ^^^ Dann, vielleicht die wichtigste statistische Implikation des Ergebnisses ist, dass wir nicht nur die kombinierten Varianzen aller Elemente von x in abnehmende Beiträge durch jeden PC zersetzen können, sondern wir können auch die gesamte Kovarianzmatrix zu Beiträgen λ k α k α k ' \{displaystyle \lambda _{k}\alpha _{k}\alpha _{k}' von jedem PC zerlegen. Obwohl nicht streng abnehmend, werden die Elemente von λ k α k α k ' \{displaystyle \lambda _{k}\alpha _{k}\alpha _{k}' tendenziell kleiner werden, da k \{displaystyle k} zunimmt, da λ k α k α k' \{displaystyle \lambda _{k}\alpha _{k} α k \{displaystyle \alpha _{k} , während die Elemente von α k \{displaystyle \alpha _{k} tendenziell um die gleiche Größe wegen der Normalisierungszwänge zu bleiben: α k' α k = 1 , k = 1 , ..., p \{displaystyle \alpha _{k}'\alpha _{k}=1,k=1,\dots ,p} . Einschränkungen Wie oben erwähnt, hängen die Ergebnisse von PCA von der Skalierung der Variablen ab. Dies kann durch Skalierung jedes Merkmals durch seine Standardabweichung geheilt werden, so dass man mit dimensionslosen Merkmalen mit einheitlicher Varianz endet. Die Anwendbarkeit von PCA wie oben beschrieben wird durch bestimmte (aufgeschlossene) Annahmen, die in ihrer Ableitung gemacht werden, begrenzt. Insbesondere kann PCA lineare Korrelationen zwischen den Merkmalen erfassen, scheitert jedoch, wenn diese Annahme verletzt wird (siehe Abbildung 6a in der Referenz). In einigen Fällen können die Koordinatentransformationen die Linearitätsannahme wiederherstellen und dann PCA angewendet werden (siehe Kernel PCA). Eine weitere Einschränkung ist der Mittel-Removal-Prozess vor dem Aufbau der Kovarianzmatrix für PCA. In Feldern wie der Astronomie sind alle Signale nichtnegativ, und der Mittel-Removal-Prozess wird das Mittel einiger astrophysischer Belichtungen zu Null zwingen, was folglich unphysische negative Flussläufe erzeugt, und die Vorwärtsmodellierung muss durchgeführt werden, um die wahre Größe der Signale wiederherzustellen. Als alternatives Verfahren ist eine nichtnegative Matrixfaktorisierung, die sich nur auf die nichtnegativen Elemente in den Matrizen konzentriert, die für astrophysische Beobachtungen gut geeignet ist. Weitere Informationen zu Relation zwischen PCA und nicht-negativer Matrix Factorization. PCA ist von Nachteil, wenn Daten vor der Anwendung von PCA nicht standardisiert wurden. PCA transformiert Originaldaten in Daten, die für die Hauptkomponenten dieser Daten relevant sind, was bedeutet, dass die neuen Datenvariablen nicht in gleicher Weise interpretiert werden können, wie die Originale waren. Sie sind lineare Interpretationen der ursprünglichen Variablen. Auch, wenn PCA nicht richtig durchgeführt wird, gibt es eine hohe Wahrscheinlichkeit von Informationsverlust. PCA basiert auf einem linearen Modell. Hat ein Datensatz ein in ihm verborgenes Muster, das nichtlinear ist, so kann PCA die Analyse tatsächlich in die komplette entgegengesetzte Richtung des Fortschritts lenken. Forscher der Kansas State University entdeckten, dass der Stichprobenfehler in ihren Experimenten die Vorspannung der PCA-Ergebnisse beeinflusste. " Wenn die Anzahl der Subjekte oder Blöcke kleiner als 30 ist und/oder der Forscher an PC's über die erste hinaus interessiert ist, kann es besser sein, zuerst für die serielle Korrelation zu korrigieren, bevor PCA durchgeführt wird". Die Forscher von Kansas State fanden auch heraus, dass PCA "seriös voreingenommen werden könnte, wenn die Autokorrelationsstruktur der Daten nicht korrekt behandelt wird". PCA und Informationstheorie Dimensionsreduktion verliert im Allgemeinen Informationen. PCA-basierte Dimensionalitätsreduktion neigt dazu, diesen Informationsverlust unter bestimmten Signal- und Rauschmodellen zu minimieren. Unter der Annahme, dass x = s + n, \{displaystyle \mathbf {x} = +\mathbf {n}, d.h., dass der Datenvektor x \{displaystyle \mathbf {x} die Summe des gewünschten Informations-tragenden Signals s \{displaystyle \mathbf {s} und eines Rauschsignals n \{displaystyle \mathbf {n} ist, kann man zeigen, dass PCA für die Dimensionsreduktion optimal sein kann, von einem Informations-the n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = n = \{displaystyle I(\mathbf {x} ;\mathbf {s} )-I(\mathbf {y} ;\mathbf {s} } Die Optimität der PCA wird auch erhalten, wenn das Rauschen n \{displaystyle \mathbf {n} iid und mindestens mehr Gaussian (in Bezug auf die Kullback-Leibler Divergenz) als das informationstragende Signal s \{displaystyle \mathbf {s} } ist. Im allgemeinen verliert auch wenn das obige Signalmodell hält, PCA seine informationstheoretische Optimität, sobald das Rauschen n \{displaystyle \mathbf {n} abhängig wird. Computing PCA mit der KovarianzmethodeDas folgende ist eine detaillierte Beschreibung der PCA mit der Kovarianzmethode (siehe auch hier) im Gegensatz zur Korrelationsmethode. Ziel ist es, einen gegebenen Datensatz X der Dimension p auf einen alternativen Datensatz Y kleinerer Dimension L zu transformieren. Entsprechend suchen wir die Matrix Y, wobei Y die Karhunen-Loève-Transformation (KLT) der Matrix X ist: Y = K L T { X } \{displaystyle \mathbf {Y} =\mathbb {KLT} {\mathbf} {X} Organisieren Sie den DatensatzAngeben Sie haben Daten mit einer Reihe von Beobachtungen von p-Variablen, und Sie möchten die Daten reduzieren, so dass jede Beobachtung mit nur L-Variablen, L <p beschrieben werden kann. Nehmen Sie weiter an, dass die Daten als Set von n Datenvektoren x 1 ... x n \{displaystyle \mathbf {x} _{1}\ldots \mathbf {x} _{n} mit jeder x i \{displaystyle \mathbf {x} _{i} eine einzige gruppierte Beobachtung der p-Variablen darstellt. Schreiben Sie x 1 ... x n \{displaystyle \mathbf {x} _{1}\ldots \mathbf {x} _{n} als Zeilenvektoren, jeweils mit p Elementen. Legen Sie die Zeilenvektoren in eine einzelne Matrix X der Abmessungen n × p.Kalkulieren Sie das empirische MittelFinden Sie das empirische Mittel entlang jeder Spalte j = 1, ..., p. Die berechneten Mittelwerte in einen empirischen mittleren Vektor u der Abmessungen p × ANHANG U j = 1 n = 1 n = 1 n X i i n ***______________________________________________________________________________________ Berechnen Sie die Abweichungen von der mittleren Subtraktion ist ein integraler Bestandteil der Lösung, um eine Hauptkomponentenbasis zu finden, die den mittleren quadratischen Fehler der Annäherung der Daten minimiert. So gehen wir weiter, indem wir die Daten wie folgt zentrieren: Subtraktieren Sie den empirischen mittleren Vektor u T \{displaystyle \mathbf {u} ^{T} aus jeder Zeile der Datenmatrix X. Speichern Sie mittlere subtrahierte Daten in der n × p Matrix B. B = X - h u T \{displaystyle \mathbf {B} =\mathbf {X} -\mathbf {h} h_{i}=1\,\qquad \qquad \text{für }i=1,\ldots ,n} In einigen Anwendungen kann jede Variable (Spalte B) auch mit einer Varianz von 1 skaliert werden (siehe Z-Score). Dieser Schritt beeinflusst die berechneten Hauptkomponenten, macht sie jedoch unabhängig von den zur Messung der verschiedenen Größen verwendeten Einheiten. Die Kovarianzmatrix finden Finden Sie die p × p empirische Kovarianz-Matrix C aus der Matrix B: wobei Ψ \{displaystyle *} der Konjugat-Transpose-Operator ist. Wenn B vollständig aus realen Zahlen besteht, was in vielen Anwendungen der Fall ist, ist die "Konjugat-Transpose" die gleiche wie die regelmäßige Transpose. Die Begründung für die Verwendung von n - 1 anstelle von n zur Berechnung der Kovarianz ist die Korrektur von Bessel. Finde die Eigenvektoren und Eigenwerte der Kovarianzmatrix Berechnen Sie die Matrix V von Eigenvektoren, die die Kovarianzmatrix C diagonalisiert: wobei D die Diagonalmatrix von Eigenwerten von C ist. Dieser Schritt wird typischerweise die Verwendung eines computerbasierten Algorithmus zur Berechnung von Eigenvektoren und Eigenwerten beinhalten. Diese Algorithmen sind als Teilkomponenten der meisten Matrix-Algebra-Systeme, wie SAS, R, MATLAB, Mathematica, SciPy, IDL (Interactive Data Language), oder GNU Octave sowie OpenCV leicht verfügbar. Matrix D wird als p × p Diagonalmatrix ausgebildet, wobei der j. Eigenwert der Kovarianzmatrix C und Matrix V, ebenfalls der Dimension p × p, p Spaltenvektoren enthält, jede der Länge p, die die p Eigenvektoren der Kovarianzmatrix C darstellen. Die Eigenwerte und Eigenvektoren werden bestellt und gepaart.Der jth-Eigenwert entspricht dem jth-Eigenvektor. Matrix V bezeichnet die Matrix der rechten Eigenvektoren (im Gegensatz zu linken Eigenvektoren). Im allgemeinen muss die Matrix der rechten Eigenvektoren nicht die (Konjugat) Transpose der Matrix der linken Eigenvektoren sein. Die Eigenvektoren und Eigenwerte neu anordnen Sortieren Sie die Spalten der Eigenvektormatrix V und der Eigenwertmatrix D in der Reihenfolge des abnehmenden Eigenwerts. Achten Sie darauf, die richtigen Paarungen zwischen den Spalten in jeder Matrix aufrechtzuerhalten. Berechnung des kumulativen Energiegehalts für jeden Eigenvektor Die Eigenwerte stellen die Verteilung der Energie der Quelldaten unter jedem der Eigenvektoren dar, wobei die Eigenvektoren eine Grundlage für die Daten bilden. Der kumulative Energiegehalt g für den j. Eigenvektor ist die Summe des Energiegehalts über alle Eigenwerte von 1 bis j: g j = Σ k = 1 j D k k für j = 1 , ... k=1{j}D_{kk}\qquad \text{für }j=1,\dots ,p} Wählen Sie eine Teilmenge der Eigenvektoren als Basisvektoren aus Speichern Sie die ersten L-Säulen von V als p ×L-Matrix W: Verwenden Sie den Vektor g als Leitfaden bei der Auswahl eines geeigneten Wertes für L. Ziel ist es, einen Wert von L so klein wie möglich zu wählen, während ein relativ hoher Wert von g prozentual erreicht wird. Beispielsweise können Sie L so wählen, dass die kumulative Energie g über einer bestimmten Schwelle liegt, wie 90 Prozent. Wählen Sie in diesem Fall den kleinsten Wert von L so aus, dass die Daten auf die neue Basis projiziert werden Die projizierten Datenpunkte sind die Zeilen der Matrix Das heißt, die erste Spalte von T \{displaystyle \mathbf {T} ist die Projektion der Datenpunkte auf die erste Hauptkomponente, die zweite Spalte ist die Projektion auf die zweite Hauptkomponente, etc. Ableitung von PCA mit der Kovarianzmethode Lassen Sie X ein d-dimensionaler Zufallsvektor sein, der als Spaltenvektor exprimiert wird. Ohne Verlust der Allgemeinheit, vorausgesetzt X hat null Mittel. Wir möchten eine d × d orthonormale Transformationsmatrix P finden ( Ψ ) \{displaystyle (\ast )}, so dass PX eine diagonale Kovarianzmatrix hat (d.h. PX ist ein Zufallsvektor mit allen seinen verschiedenen Komponenten paarweise unkorreliert). Eine schnelle Berechnung unter der Annahme von P \{displaystyle P} waren einheitliche Erträge: cov ‡ ( P X ) = E [ P X ( P X ) χ χ χ χ ) = E [ P X X χ P χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ χ = P E χ χ χ χ χ χ χ χ χ χ χ [ X X χ χ] P χ = P cov ‡ (X ) P − 1 \{displaystyle begin{aligned}\operatorname {cov} (PX)&=\operatorname {E} [PX~(PX)^{*}\&=\Operatorname [PX~X^{*}P^{*}\\&=P\operatorname {E} [XX^{*}P^{*\\&=P\operatorname {cov} (X)P^{-1}\\\\end{ausgeglichen Daher ( Ψ ) \{displaystyle (\ast )} hält, wenn und nur, wenn cov ≠ (X ) \{displaystyle \operatorname {cov} (X}) diagonalisierbar waren durch P \{displaystyle P} .Dies ist sehr konstruktiv, da cov(X) garantiert ist eine nicht-negative definite Matrix und somit wird durch einige einheitliche Matrix diagonalisierbar. Kovarianzfreie Berechnung Bei praktischen Implementierungen, insbesondere bei hochdimensionalen Daten (groß p), wird das naive Kovarianzverfahren selten angewandt, da es aufgrund hoher Rechen- und Speicherkosten der expliziten Bestimmung der Kovarianzmatrix nicht effizient ist. Der covarianzfreie Ansatz vermeidet die np2-Operationen der expliziten Berechnung und Speicherung der Kovarianzmatrix XTX, anstatt eine von matrixfreien Methoden zu verwenden, beispielsweise basierend auf der Funktion, die das Produkt XT(X r) auf Kosten von 2np-Operationen bewertet. Iterative Berechnung Eine Möglichkeit, die erste Hauptkomponente effizient zu berechnen, ist im folgenden Pseudo-Code dargestellt, für eine Datenmatrix X mit Nullmittel, ohne jemals ihre Kovarianzmatrix zu berechnen. r = ein Zufallsvektor der Länge p r = r / Norm(r) c-Zeiten: s = 0 (ein Vektor der Länge p) für jede Zeile x in X s = s + (x ⋅ r) x λ = rTs // λ ist der Eigenwertfehler | = λ r - s| r = s / Norm(s) Austritt, wenn Fehler < Toleranzrückgabe λ, R Dieser Power Iterationsalgorithmus berechnet einfach den Vektor XT(X r), normalisiert und stellt das Ergebnis in r zurück.Der Eigenwert wird von rT (XTX) r angenähert, was der Rayleigh-Quotient auf dem Einheitsvektor r für die Kovarianzmatrix XTX ist. Ist der größte Einzelwert von dem nächstgrößten gut getrennt, so gelangt der Vektor r nahe an der ersten Hauptkomponente von X innerhalb der Anzahl der relativ zu p kleinen Iterationen c zu den Gesamtkosten 2cnp. Die Strom- Iterations-Konvergenz kann beschleunigt werden, ohne die geringen Kosten pro Iteration mit fortschrittlicheren matrixfreien Methoden, wie dem Lanczos-Algorithmus oder dem Locally Optimal Block Precondition Conjugate Gradient (LOBPCG) Verfahren spürbar zu opfern. Nachfolgende Hauptkomponenten können einzeln über Ablenkung oder gleichzeitig als Block berechnet werden. Im ersten Ansatz beeinflussen Ungenauigkeiten in bereits berechneten groben Hauptkomponenten die Genauigkeit der nachfolgend berechneten Hauptkomponenten additiv und erhöhen damit den Fehler bei jeder neuen Berechnung. Letzterer Ansatz im Block-Power-Verfahren ersetzt Einzelvektoren r und s durch Blockvektoren, Matrizen R und S.Jede Spalte von R nähert sich einer der führenden Hauptkomponenten, während alle Spalten gleichzeitig iteriert werden. Die Hauptberechnung ist die Auswertung des Produktes XT(X R). Implementiert zum Beispiel in LOBPCG eliminiert eine effiziente Blockierung die Anhäufung der Fehler, ermöglicht die Verwendung von hochrangigen BLAS-Matrix-Produktfunktionen und führt typischerweise zu einer schnelleren Konvergenz im Vergleich zur Einvektor-Einzel-Technik. Das NIPALS-Verfahren Nichtlineare iterative Teil-Mindest-Quadren (NIPALS) ist eine Variante der klassischen Leistungs iteration mit Matrix-Deflation durch Subtraktion, die zur Berechnung der ersten Komponenten in einer Haupt- oder Teil-Mindest-Quadrats-Analyse durchgeführt wird. Für sehr hochdimensionale Datensätze, wie sie in den *omics (z.B. Genomics, Metabolomics) erzeugt werden, ist es in der Regel nur erforderlich, die ersten wenigen PCs zu berechnen. Der nichtlineare iterative Teil-Mindest-Quadrat (NIPALS)-Algorithmus aktualisiert iterative Annäherungen an die führenden Punkte und Belastungen t1 und r1T durch die Leistungs iteration multipliziert auf jeder Iteration durch X auf der linken und auf der rechten Seite, d.h. die Berechnung der Kovarianz-Matrix wird vermieden, ebenso wie bei der matrixfreien Implementierung der Leistungs iterationen auf XTX, basierend auf der Funktion rX( Die Matrix-Deflation durch Subtraktion wird durch Subtraktion des äußeren Produkts, t1r1T von X aus der deflatierten Restmatrix, die zur Berechnung der nachfolgenden führenden PCs verwendet wird, durchgeführt. Für große Datenmatrizen oder Matrizen, die einen hohen Grad an Spaltenkollinearität haben, leidet NIPALS an Verlust der Orthogonalität von PCs aufgrund von in jeder Iteration und Matrix-Deflation durch Subtraktion akkumulierten Maschinenpräzisionsabrundungsfehlern. Ein Gram–Schmidt re-orthogonalizationalgorithmis beantragte sowohl die Punkte als auch die Belastungen bei jedem Iterationsschritt, um diesen Verlust der Orthogonalität zu beseitigen. NIPALS auf Einvektor-Multiplikationen können nicht von hochrangigen BLAS profitieren und führt zu einer langsamen Konvergenz für gebündelte führende Einzelwerte – diese Mängel werden in ausgefeilteren matrixfreien Blocklösern, wie dem Locally Optimal Block Precondition Conjugate Gradient (LOBPCG)-Verfahren, behoben. Online/Sequentielle Schätzung In einer Online- oder Streamingsituation mit Daten, die Stück für Stück ankommen, anstatt in einer einzigen Charge gespeichert zu werden, ist es sinnvoll, eine Schätzung der PCA-Projektion vorzunehmen, die sequentiell aktualisiert werden kann. Dies kann effizient erfolgen, erfordert aber verschiedene Algorithmen. PCA und qualitative Variablen In PCA ist es üblich, qualitative Variablen als Zusatzelemente einzuführen. Beispielsweise wurden viele quantitative Größen an Pflanzen gemessen. Für diese Pflanzen stehen einige qualitative Variablen zur Verfügung, wie beispielsweise die Art, zu der die Pflanze gehört. Diese Daten wurden für quantitative Variablen PCA unterzogen. Bei der Analyse der Ergebnisse ist es natürlich, die Hauptkomponenten mit der qualitativ variablen Spezies zu verbinden. Dazu werden folgende Ergebnisse erzeugt. Identifizierung, auf den Faktorebenen, der verschiedenen Arten, beispielsweise mit verschiedenen Farben. Darstellung auf den Faktorebenen der Schwerpunkte der Pflanzen der gleichen Art. Für jeden Schwerpunkt und jede Achse, p-Wert, um die Bedeutung der Differenz zwischen dem Schwerpunkt und Ursprung zu beurteilen. Diese Ergebnisse werden so genannt, dass eine qualitative Variable als Ergänzungselement eingeführt wird. Dieses Verfahren ist detailliert und Husson, Lê & Pagès 2009 und Pagès 2013. Wenige Software bieten diese Option automatisch an. Dies ist der Fall von SPAD, dass historisch, nach der Arbeit von Ludovic Lebart, der erste war, um diese Option vorzuschlagen, und das R-Paket FactoMineR. Anwendungen Quantitative Finanzierung In quantitativer Finanzierung kann die Hauptkomponentenanalyse direkt auf das Risikomanagement von Zinsderivatportfolios angewendet werden. Der Handel mit mehreren Swap-Instrumenten, die in der Regel eine Funktion von 30–500 anderen Marktquoten-Swap-Instrumenten sind, soll auf gewöhnlich 3 oder 4 Hauptkomponenten reduziert werden, die den Kurs der Zinssätze auf Makrobasis darstellen. Die Konvertierung von Risiken, die als solche zu Faktorbelastungen (oder Multiplikatoren) zu vertreten sind, bietet Bewertungen und Verständnis darüber hinaus, dass sie nur kollektiv Risiken für einzelne 30-500 Eimer anzeigen. In ähnlicher Weise wurde PCA auch auf Aktienportfolios angewendet, sowohl auf Portfoliorisiko als auch auf Risikorendite. Eine Anwendung besteht darin, das Portfoliorisiko zu reduzieren, wobei die Zuordnungsstrategien auf die "Hauptportfolios" anstelle der zugrunde liegenden Aktien angewandt werden. Zweitens soll die Portfoliorendite verbessert werden, indem die wichtigsten Komponenten die Aktien mit Upside-Potential auswählen. Neurowissenschaften Eine Variante der Hauptkomponentenanalyse wird in der Neurowissenschaft verwendet, um die spezifischen Eigenschaften eines Stimulus zu identifizieren, die die Wahrscheinlichkeit des Erzeugens eines Aktionspotentials erhöhen. Diese Technik wird als spike-triggerierte Kovarianzanalyse bezeichnet. In einer typischen Anwendung präsentiert ein Experimentator einen weißen Rauschprozess als Reiz (in der Regel entweder als sensorischer Eingang zu einem Testobjekt, oder als direkt in das Neuron eingespritzter Strom) und erfasst dadurch eine vom Neuron erzeugte Wirkungspotentiale oder Spikes. Vermutlich machen bestimmte Merkmale des Stimulus das Neuron wahrscheinlicher zu spike. Um diese Eigenschaften zu extrahieren, berechnet der Experimentator die Kovarianzmatrix des spitzentriggerierten Ensembles, den Satz aller Reize (definiert und über ein endliches Zeitfenster, typischerweise in der Größenordnung von 100 ms), die unmittelbar einem Spieß vorausgingen. Die Eigenvektoren der Differenz zwischen der spike-triggerierten Kovarianzmatrix und der Kovarianzmatrix des vorherigen Reizensembles (der über das gleiche Zeitfenster definierte Satz aller Reize) geben dann die Richtungen im Raum der Reize an, entlang denen sich die Varianz des spike-triggerierten Ensembles am meisten von der des vorherigen Reizensembles unterscheidet. Konkret entsprechen die Eigenvektoren mit den größten positiven Eigenwerten den Richtungen, entlang denen die Varianz des spike-triggerierten Ensembles die größte positive Veränderung gegenüber der Varianz des Vorstehenden zeigte. Da es sich dabei um die Richtungen handelt, in denen die Variation des Reizes zu einer Spitze führte, sind sie oft gute Annäherungen der gesuchten relevanten Reizeigenschaften. In der Neurowissenschaften wird PCA auch verwendet, um die Identität eines Neurons aus der Form seines Wirkungspotentials zu erkennen. Spike-Sortung ist ein wichtiges Verfahren, weil extrazelluläre Aufzeichnungstechniken oft Signale von mehr als einem Neuron aufnehmen. Bei der Spike-Sortung verwendet man zunächst PCA, um die Dimensionalität des Aktionspotentials-Wellenformen zu reduzieren, und führt dann Clusteranalyse durch, um spezifische Aktionspotentiale mit einzelnen Neuronen zu verknüpfen. Die PCA als Dimensionsreduktionstechnik eignet sich besonders zur Erkennung koordinierter Aktivitäten großer neuronaler Ensembles. Es wurde bei der Bestimmung von kollektiven Variablen, also Ordnungsparametern, bei Phasenübergängen im Gehirn verwendet. Beziehung zu anderen Methoden Korrespondenzanalyse Korrespondenzanalyse (CA) wurde von Jean-Paul Benzécri entwickelt und ist konzeptuell ähnlich wie PCA, aber skaliert die Daten (die nicht-negativ sein sollten), so dass Zeilen und Spalten gleichwertig behandelt werden. Es wird traditionell auf Kontingenztabellen angewendet. CA zersetzt die dieser Tabelle zugeordnete Chi-Quadrat-Statistik in orthogonale Faktoren. Weil CA eine beschreibende Technik ist, kann es auf Tabellen angewendet werden, für die die chiquared Statistik angemessen ist oder nicht. Mehrere Varianten von CA sind verfügbar, einschließlich detrended Korrespondenzanalyse und kanonische Korrespondenzanalyse. Eine spezielle Erweiterung ist eine multiple Korrespondenzanalyse, die als Gegenstück der Hauptkomponentenanalyse für kategorische Daten angesehen werden kann. Die Hauptkomponentenanalyse erzeugt Variablen, die lineare Kombinationen der ursprünglichen Variablen sind. Die neuen Variablen haben die Eigenschaft, dass die Variablen alle orthogonal sind. Die PCA-Transformation kann als Vorverarbeitungsschritt vor der Clusterung hilfreich sein. PCA ist ein varianz-fokussierter Ansatz, der die Gesamtvarianz der Varianz reproduzieren will, bei dem Komponenten sowohl die gemeinsame als auch die einzigartige Varianz der Variablen widerspiegeln. PCA ist im allgemeinen für die Zwecke der Datenreduktion bevorzugt (d.h. das Übersetzen von variablem Raum in optimalen Faktorraum) aber nicht, wenn das Ziel ist, das latente Konstrukt oder Faktoren zu erkennen. Die Faktoranalyse ist ähnlich wie die Hauptkomponentenanalyse, in dieser Faktoranalyse sind auch lineare Kombinationen von Variablen enthalten. Anders als bei PCA ist die Faktoranalyse ein korrelationsfokussierter Ansatz, der die Interkorrelationen zwischen Variablen reproduzieren will, bei dem die Faktoren "die gemeinsame Varianz von Variablen repräsentieren, ohne eindeutige Varianz". In Bezug auf die Korrelationsmatrix entspricht dies der Fokus auf die Erläuterung der außerdiagonalen Begriffe (d.h. geteilte Kovarianz), während PCA sich auf die Erläuterung der Begriffe konzentriert, die auf der Diagonal sitzen. Als Nebenergebnis neigt PCA jedoch auch beim Versuch, die on-diagonalen Begriffe wiederzugeben, dazu, die außerdiagonalen Korrelationen relativ gut zu passen. Die Ergebnisse der PCA und der Faktoranalyse sind in den meisten Situationen sehr ähnlich, aber das ist nicht immer der Fall, und es gibt einige Probleme, bei denen die Ergebnisse deutlich unterschiedlich sind. Bei der Erfassung der Datenstruktur (d.h. latente Konstrukte oder Faktoren) oder der Kausalmodellierung wird in der Regel eine Faktoranalyse verwendet. Wenn das Faktormodell falsch formuliert wird oder die Annahmen nicht erfüllt sind, dann ergibt die Faktoranalyse fehlerhafte Ergebnisse. K-Means Clustering Es wurde behauptet, dass die von den Cluster-Indikatoren vorgegebene entspannte Lösung von k-Means-Clustering durch die Hauptkomponenten gegeben wird und der durch die Hauptrichtungen aufgespannte PCA-Subraum mit dem Cluster-Centroid-Subraum identisch ist. Diese PCA ist jedoch eine nützliche Entspannung der k-Means-Clustering war kein neues Ergebnis, und es ist einfach, Gegenbeispiele zu der Aussage zu entdecken, dass der Cluster-Centroid-Unterraum durch die Hauptrichtungen überspannt wird. Nichtnegative Matrix-Faktorisierung Nichtnegative Matrix-Faktorisierung (NMF) ist ein Dimensionsreduktionsverfahren, bei dem nur nichtnegative Elemente in den Matrizen verwendet werden, was daher eine vielversprechende Methode in der Astronomie ist, in dem Sinne, dass astrophysische Signale nichtnegativ sind. Die PCA-Komponenten sind orthogonal zueinander, während die NMF-Komponenten alle nicht-negativ sind und daher eine nicht-orthogonale Basis konstruiert. In PCA wird der Beitrag jeder Komponente auf der Höhe ihres entsprechenden Eigenwertes, der der fraktionierten Restvarianz (FRV) bei der Analyse von empirischen Daten entspricht, geordnet. Für NMF werden seine Komponenten nur auf der Basis der empirischen FRV-Kurven platziert. Die restlichen fraktionierten Eigenwertdiagramme, d.h. 1 - Σ i = 1 k λ i / Σ j = 1 n λ j \{displaystyle 1-\sum _i=1^{k}\lambda _i}{\Big }/\sum _j=1}^{n}\lambda _{j} in Abhängigkeit von der Komponentennummer k \{displaystyle k} bei insgesamt n \{displaystyle n} Komponenten, für PCA hat ein flaches Plateau, wo keine Daten erfasst werden, um das quasi-statische Rauschen zu entfernen, dann fallen die Kurven schnell als Anzeige und Die FRV-Kurven für NMF sinken kontinuierlich, wenn die NMF-Komponenten sequentiell aufgebaut sind, was die kontinuierliche Erfassung von quasi-statischem Rauschen anzeigt; dann konvergieren sie auf höhere Ebenen als PCA, was die weniger übergeordnete Eigenschaft von NMF anzeigt. Ikonographie von Korrelationen Oft ist es schwierig, die Hauptkomponenten zu interpretieren, wenn die Daten viele Variablen verschiedener Herkunft enthalten, oder wenn einige Variablen qualitativ sind. Dies führt den PCA-Anwender zu einer zarten Eliminierung mehrerer Variablen. Wenn Beobachtungen oder Variablen einen übermäßigen Einfluss auf die Richtung der Achsen haben, sollten sie entfernt und dann als Ergänzungselemente projiziert werden. Darüber hinaus ist es erforderlich, die Proxyitäten zwischen den Punkten nahe der Mitte der Faktorebene zu interpretieren.Die Ikonographie von Korrelationen, im Gegenteil, die keine Projektion auf ein Achsensystem ist, hat diese Nachteile nicht. Wir können daher alle Variablen behalten. Das Prinzip des Diagramms besteht darin, die "bemerkbaren" Korrelationen der Korrelationsmatrix durch eine feste Linie (positive Korrelation) oder gestrichelte Linie (negative Korrelation) zu unterstreichen. Eine starke Korrelation ist nicht bemerkenswert, wenn sie nicht direkt ist, sondern durch die Wirkung einer dritten Variable verursacht. Umgekehrt können schwache Korrelationen bemerkenswert sein". Wenn beispielsweise eine Größe Y von mehreren unabhängigen Variablen abhängt, sind die Korrelationen von Y mit jedem von ihnen schwach und dennoch bemerkenswert. Verallgemeinerungen Ein besonderer Nachteil der PCA besteht darin, dass die Hauptkomponenten üblicherweise lineare Kombinationen aller Eingangsgrößen sind. Sparse PCA überwindet diesen Nachteil, indem lineare Kombinationen gefunden werden, die nur wenige Eingangsgrößen enthalten. Es erweitert die klassische Methode der Hauptkomponentenanalyse (PCA) zur Reduzierung der Dimensionalität von Daten durch Hinzufügen von Sparsity Constraint auf die Eingangsvariablen. Es wurden mehrere Ansätze vorgeschlagen, darunter ein Regressionsrahmen, ein konvexes Relaxations-/Semidefinite-Programmierungsgerüst, ein generalisiertes Power-Methoden-Framework, ein alternierendes Maximierungs-Framework vorwärts gierige Suche und genaue Methoden mit branchen-und-gebundenen Techniken, Bayesian Formulierungs-Framework. Die methodischen und theoretischen Entwicklungen der Sparse PCA sowie ihre Anwendungen in wissenschaftlichen Studien wurden kürzlich in einem Umfragepapier geprüft. Nichtlinearer PCAMost der modernen Methoden zur nichtlinearen Dimensionsreduktion finden ihre theoretischen und algorithmischen Wurzeln in PCA oder K-Means. Pearsons ursprüngliche Idee war, eine gerade Linie (oder Ebene) zu nehmen, die "die beste Passform" zu einer Reihe von Datenpunkten sein wird. Trevor Hastie erweiterte dieses Konzept, indem er Hauptkurven als natürliche Erweiterung für die geometrische Interpretation von PCA vorschlug, die explizit eine Vielzahl für die Daten-Approximation konstruiert und anschließend die Punkte darauf projiziert, wie dies Fig. Siehe auch den elastischen Kartenalgorithmus und die hauptsächliche geodätische Analyse. Eine weitere populäre Verallgemeinerung ist Kernel PCA, die der PCA entspricht, die in einem wiederkehrenden Kernel Hilbert-Raum mit einem positiven bestimmten Kernel durchgeführt wird. Im multilinearen Subraum-Erlernen wird PCA auf multilineare PCA (MPCA) verallgemeiert, die Funktionen direkt aus Tensor-Darstellungen extrahiert. MPCA wird durch die Durchführung von PCA in jedem Modus des Tensor iterativ gelöst. MPCA wurde auf Gesichtserkennung, Gangerkennung usw. angewendet. MPCA wird weiter auf unkorrelierte MPCA, nicht-negative MPCA und robuste MPCA erweitert. Die N-Wege-Hauptkomponentenanalyse kann mit Modellen wie Tucker Zersetzung, PARAFAC, Multiple Factor Analysis, Co-Inertia Analysis, STATIS und DISTATIS durchgeführt werden. Robuste PCA Während PCA die mathematisch optimale Methode (wie bei der Minimierung des quadratischen Fehlers) findet, ist es dennoch empfindlich gegenüber Ausreißern in den Daten, die große Fehler erzeugen, etwas, das das Verfahren versucht, zuerst zu vermeiden. Daher ist es üblich, Ausreißer vor dem Computing PCA zu entfernen. In einigen Kontexten können jedoch Ausreißer schwer zu identifizieren sein. Beispielsweise ist bei Data Mining Algorithmen wie Korrelations-Clustering die Zuordnung von Punkten zu Clustern und Ausreißern bisher nicht bekannt. Eine kürzlich vorgeschlagene Verallgemeinerung der PCA auf Basis einer gewichteten PCA erhöht die Robustheit, indem Datenobjekte aufgrund ihrer geschätzten Relevanz unterschiedliche Gewichte zugewiesen werden. Auch ausreißfeste Varianten der PCA wurden auf Basis von L1-Norm-Formulierungen (L1-PCA) vorgeschlagen. Robuste Hauptkomponentenanalyse (RPCA) durch Zersetzung in Low-Rank- und Sparse-Matrizen ist eine Modifikation der PCA, die in Bezug auf grob beschädigte Beobachtungen gut funktioniert. Ähnliche Techniken Unabhängige Komponentenanalyse Unabhängige Komponentenanalyse (ICA) richtet sich an ähnliche Probleme wie die Hauptkomponentenanalyse, findet aber additiv trennbare Komponenten statt aufeinanderfolgende Approximationen. Netzwerk-Komponenten-Analyse Angesichts einer Matrix E \{displaystyle E} versucht es, es in zwei Matrizen zu zersetzen, so dass E = A P \{displaystyle E=AP} .Ein wesentlicher Unterschied von Techniken wie PCA und ICA ist, dass einige der Einträge von A \{displaystyle A} auf 0 beschränkt sind. Hier wird P \{displaystyle P} als regulatorische Schicht bezeichnet. Während im allgemeinen eine solche Zersetzung mehrere Lösungen haben kann, beweisen sie, daß, wenn die folgenden Bedingungen erfüllt sind : A \{displaystyle A} hat vollen Spaltenstand Jede Spalte von A \{displaystyle A} muss mindestens L - 1 \{displaystyle L-1} Nullen haben, wobei L \{displaystyle L} die Anzahl der Spalten von A \{displaystyle A} ist (oder alternativ die Anzahl der Zeilen von P \{displaystyle P}). Die Begründung für dieses Kriterium ist, dass, wenn ein Knoten aus der regulatorischen Schicht zusammen mit allen damit verbundenen Ausgangsknoten entfernt wird, das Ergebnis noch durch eine Konnektivitätsmatrix mit vollem Spaltenrang gekennzeichnet werden muss. P \{displaystyle P} muss den vollen Zeilenstand haben. dann ist die Zersetzung einzigartig bis zur Multiplikation durch einen Skalar. Diskriminante Analyse Komponentenanalyse Diskriminante Analyse von Hauptkomponenten (DAPC) ist eine multivariate Methode, die verwendet wird, um Cluster genetisch verwandter Individuen zu identifizieren und zu beschreiben. Genetische Variation wird in zwei Komponenten unterteilt: Variation zwischen Gruppen und innerhalb von Gruppen, und es maximiert das ehemalige. Lineare Diskriminanten sind lineare Kombinationen von Allelen, die die Cluster am besten trennen. Befürchtungen, dass die meisten zu dieser Diskriminierung beitragen, sind daher diejenigen, die die am stärksten unterschiedlichen Gruppen sind. Die Beiträge von Allelen zu den von DAPC identifizierten Gruppierungen können es erlauben, Regionen des Genoms zu identifizieren, die die genetische Divergenz zwischen Gruppen treiben In DAPC werden zunächst Daten mittels einer Hauptkomponentenanalyse (PCA) transformiert und anschließend Cluster mittels diskriminierender Analyse (DA) identifiziert. Ein DAPC kann auf R mit dem Paket Adegenet realisiert werden.(mehr Info: https://adegenet.r-forge.r-project.org) Software/Source-Code ALGLIB - eine C+ und C#-Bibliothek, die PCA und gekürzte PCA Analytica implementiert – Die eingebaute EigenDecomp-Funktion berechnet Hauptkomponenten. ELKI – beinhaltet PCA für Projektion, einschließlich robuster Varianten von PCA, sowie PCA-basierte Clustering Algorithmen. Gretl – Hauptkomponentenanalyse kann entweder über den pca Befehl oder über die princomp() Funktion durchgeführt werden. Julia – Unterstützt PCA mit der pca-Funktion im MultivariateStats-Paket KNIME – Eine java-basierte nodal arranging software for Analysis, in diesem die Knoten PCA, PCA compute, PCA Apply, PCA inverse machen es leicht. Mathematica – Ergänzt die Hauptkomponentenanalyse mit dem Befehl PrincipalComponents sowohl mit Kovarianz- als auch Korrelationsmethoden. MathPHP – PHP-Mathematik-Bibliothek mit Unterstützung für PCA. MATLAB Statistik Toolbox – Die Funktionen princomp und pca (R2012b) geben den Hauptkomponenten, während die Funktionspcares die Reststoffe und rekonstruierte Matrix für eine niedrigere PCA-Annäherung liefern. Matplotlib –Python-Bibliothek haben ein PCA-Paket im .mlab-Modul. mlpack – Bietet eine Implementierung der Hauptkomponentenanalyse in C++. NAG Library – Die Hauptkomponentenanalyse erfolgt über die g03aa-Routine (verfügbar in beiden Fortran-Versionen der Bibliothek). NMath – proprietäre numerische Bibliothek mit PCA für das .NET Framework. GNU Octave – Freie Software-Computational-Umgebung meist kompatibel mit MATLAB, die Funktion princomp gibt die Hauptkomponente. OpenCV Oracle Datenbank 12c – Implementiert über DBMS_DATA_MINING.SVDS_SCORING_MODE durch Angabe des Einstellwertes SVDS_SCORING_PCA Orange (Software) – Integriert PCA in seiner visuellen Programmierumgebung. PCA zeigt ein Scree-Plot (Grad der erklärten Varianz), wo der Benutzer interaktiv die Anzahl der Hauptkomponenten auswählen kann. Origin – Enthält PCA in seiner Pro-Version. Qlucore – Kommerzielle Software zur Analyse von multivariaten Daten mit sofortiger Reaktion mit PCA.R – Kostenloses statistisches Paket, die Funktionen princomp und prcomp können für die Hauptkomponentenanalyse verwendet werden; prcomp verwendet Singular Value Zersetzung, die in der Regel eine bessere numerische Genauigkeit liefert. Einige Pakete, die PCA in R implementieren, beinhalten, aber nicht beschränkt auf: ade4, vegan, ExPosition, dimRed und FactoMineR. SAS – proprietäre Software; zum Beispiel Scikit-learn – Python-Bibliothek für maschinelles Lernen, die PCA, Probabilistic PCA, Kernel PCA, Sparse PCA und andere Techniken im Zersetzungsmodul enthält. Weka – Java-Bibliothek für maschinelles Lernen, die Module für die Berechnung von Hauptkomponenten enthält. Siehe auch Referenzen Weitere Lesung Jackson, J.E (1991). Leitfaden für Hauptkomponenten (Wiley). Jolliffe, I. T. (1986). Hauptkomponentenanalyse. Springer Series in Statistics.Springer-Verlag.pp.487.CiteSeerX 10.1.1.149.8828.doi:10.1007/b98835.ISBN 978-0-387-95442-4.Jolliffe, I. T. (2002). Hauptkomponentenanalyse. Springer Series in Statistik. New York: Springer-Verlag.doi:10.1007/b98835.ISBN 978-0-387-95442-4.Husson François, Lê Sébastien & Pagès Jérôme (2009). Exploratory Multivariate Analysis nach Beispiel mit R. Chapman & Hall/CRC Die R-Serie, London. 224p.ISBN 978-2-7535-0938-2 Pagès Jérôme (2014). Mehrere Faktoranalyse nach Beispiel mit R. Chapman & Hall/CRC Die R-Serie London 272 p Externe Links Universität Kopenhagen Video von Rasmus Bro auf YouTube Stanford University Video von Andrew Ng auf YouTube Ein Tutorial zur Hauptkomponentenanalyse Ein Laieneintrag zur Hauptkomponentenanalyse auf YouTube (ein Video von weniger als 100 Sekunden). StatQuest: Hauptkomponentenanalyse (PCA) auf YouTube deutlich erklärt Siehe auch die Liste der Software-Implementierungen