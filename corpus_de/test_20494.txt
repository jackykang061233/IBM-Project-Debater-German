Im Zusammenhang mit künstlichen Neuralnetzen ist die Aktivierungsfunktion (Recification Linear Unit) eine Aktivierungsfunktion, die als positiver Teil ihres Arguments definiert ist: f ( x ) = x + = x max ( 0 , x ) faserstilstyle f(x)=x^{+}=\max(0,x,x, wo x der Beitrag zu einem Neuron ist. Dies ist auch als Rampenfunktion bekannt und entspricht der Halbwellenkorrektur im Elektrotechnik. Diese Aktivierungsfunktion hat begonnen, im Zusammenhang mit der visuellen Merkmalsgewinnung in hierarchischen Neuralnetzen zu zeigen, die in den späten 60er Jahren beginnen. später wurde argumentiert, es habe starke biologische Motivationen und mathematische Rechtfertigungen. 2011 wurde festgestellt, dass es eine bessere Schulung tieferer Netze ermöglicht, verglichen mit den weit verbreiteten Aktivierungsfunktionen vor 2011, z.B. der logistischen Sigmoid (die von der Wahrscheinlichkeitstheorie inspiriert ist; siehe logistische Regression) und ihrer praktischen Gegenleistung, dem Hyperbolic tangent. Im Jahr 2017 ist der Empfänger die beliebteste Aktivierungsfunktion für tiefe Neuralnetze. lineare Einheiten finden Anwendungen in der Computervision und Redeerkennung mit tiefen Neuralnetzen und rechnerischen Neurowissenschaften. Vorteile Sparte Aktivierung: Beispielsweise werden in einem zufällig paraphierten Netz nur etwa 50 % der versteckten Einheiten aktiviert (mit einer nicht-zero-Produktion). Bessere propagation: Weniger schwindende Bewegungsprobleme im Vergleich zu sigmoidalen Aktivierungsfunktionen, die in beiden Richtungen zufriedenstellend sind. Leistungsfähige Berechnung: Nur Vergleiche, Addition und Multiplikation. Größen-Invariant: max ( 0 , a x ) = ein max ( 0 , x ) für eine ≥ 0 {\displaystyle(0,ax)=a\max(0,x) Haushaltstext für }a\geq 0} Die .Rectation-Funktionen wurden genutzt, um spezielle Erregungs- und unspezifische Hemmnisse in der neuralen abstrakten Pyramide zu trennen, die in einer beaufsichtigten Weise geschult wurde, um mehrere Computer-Visionsaufgaben zu lernen. 2011 wurde gezeigt, dass die Verwendung des Rekrutierers als Nicht-linearität genutzt wurde, um die Ausbildung tief beaufsichtigter Neuralnetze zu ermöglichen, ohne dass unkontrollierte Vorausbildung erforderlich ist. lineare Einheiten, verglichen mit sigmoid Funktion oder ähnlichen Aktivierungsfunktionen, ermöglichen eine schnellere und effektive Ausbildung der tiefen Neuralarchitekturen auf großen und komplexen Datensätzen. Potenzielle Probleme, die nicht unterschiedslos sind; es ist jedoch anderswo, und der Wert des Derivate auf Null kann willkürlich gewählt werden, um 0 oder 1 zu sein. Keine Nullcenter. Ungebunden. Problem: ReLU (Recification Linear Unit) Neuronen können manchmal in Staaten gedrängt werden, in denen sie für im Wesentlichen alle Vorleistungen nicht erwerbstätig werden. In diesem Staat flüchtet sich kein Verlust durch den Neuron, so dass der Neuron in einem ständigen inaktiven Staat und Ernährung festhält. Dies ist eine Form des schwindenden Bewegungsproblems. In einigen Fällen können viele Neuronen in einem Netzwerk in toten Staaten gefangen werden, was die Modellkapazität wirksam verringert. Dieses Problem entsteht in der Regel, wenn die Lernrate zu hoch ist. Man kann durch die Verwendung von Lecky ReLUs anstelle abgemildert werden, die eine kleine positive Steigung für x  0 0 aber die Leistung verringert. Linear Le Leaky ReLU Leaky ReLUs ermöglichen eine kleine, positive Kluft, wenn die Einheit nicht aktiv ist. f ( x ) = { x, wenn x0,00 0 , 0,01 x sonst). 7.8displaystyle f(x)= cubegin{cases}x & Logtext{if x>0,\\ 0,0x & Logtext {otherwise}}.\end{cases Parametrische ReLU Parametric ReLUs (PReLUs) nehmen diese Idee weiter an, indem sie den Grad der Leckage in einen Parameter einführt, der zusammen mit den anderen Neuralnetzparametern f ( x ) = { x x, wenn x) 0 , a x sonst. 7.8displaystyle f(x)= cubegin{cases}x & Logtext{if x>0,\\ax & Logtext{otherwise}}.\end{cases Hinweis darauf, dass dies für einen ≤ 1 entspricht f ( x ) = max ( x , a x ) Memedisplaystyle f(x)=\max(x,ax} und damit ein Verhältnis zu max-Netzen. Nichtlineare Varianten Gaussssian Irr Linear Unit (GELU) GELU ist eine reibungslose Annäherung an den Replizator. Man verfügt über eine nicht monotonische „Brümp“ bei x  0 0 und dient als Standardaktivierung für Modelle wie BERT. f ( x ) = x )  f  x ( x ) HANAstyle f(x)=x\cdot \Phi (x)}, wo .(x) die kumulative Verteilungsfunktion der normalen Verteilung darstellt. Diese Aktivierungsfunktion wird am Anfang dieses Artikels dargestellt. SiLUThe SiLU (Sigmoid Linear Unit) ist eine weitere reibungslose Angleichung, die zunächst im GELU-Papier eingeführt wurde. f ( x ) = x ) sigmoid ⁡ ( x ) faserstyle f(x)=x\cdot \operatorname {sigmoid} (x) Eine reibungslose Angleichung an den Retifier ist die analytische Funktion f ( x ) = ln . ( 1 + x ) , Memedisplaystyle f(x)=\ln(1+e^{x),}, die als Softplus oder glatte ReLU-Funktion bezeichnet wird. großen Negativ x Memestyle x} ist es etwa e x displaydisplaystyle e^{x} so knapp über 0, während für große positive x Kaffeestyle x} über x + e − x Kaffeestyle x+e^{-x} so knapp über x Memestyle x}. Kennzeichnend für k Memedisplaystyle k} kann sein: f ( x ) = ln ⁡ ( 1 + e k x ) k {\displaystyle f(x)= cufrac ggioln links(1+e^{kx}\right)}{k Derivat von Softplus ist die logistische Funktion. Ab der parametrischen Version, f ′ ( x ) = e k x 1 + e k x = 1 + e − k x {\displaystyle f'(x)= Finanzfrac e^{kx11+e=kx== sracfrac 1}{1+e^{-kx Die logistische sigmoid-Funktion ist eine reibungslose Angleichung des Derivat-Derivate, der Heaviside-Phase. Die multivariable Generalisierung von Single-variable Softplus ist das LogSumExp mit dem ersten Argument Null: L S E 0 + ⁡ ( x 1 , ... , x n ) := LSE  0 ( 0 , x 1 , ... , x n ) = Log . ( 1 + x 1 +  + + e x n n ) . KINGstyle \operator LSE_{0} (^+x_{1},\dots ,x_{n}):=\operator {LSE} (0,x_{1},\dots ,x_{n})=\log\left(1+e^{x_{1}}+\cdots+e^{x_{n}}\right). LogSumExp Funktion ist LSE  ( ( x 1 , ... , x n ) = Log ⁡ (e x 1 + ⋯ + e x n ) , Memestyle \operator {LSE} (x_{1},\dots ,x_{n})=\log links(e^{x_{1}}x__x_{x__x__x__x__x_}}\x_}}\x_{n}}\]) und seine maxmax-Methode ist die weiche Ausgangsposition mit der allgemeinen Geschwindigkeit. LogSumExp und Softmax werden im Maschinenlernen verwendet. ELU Exponential lineare Einheiten versuchen, die mittleren Aktivierungen näher an Null zu bringen, was das Lernen beschleunigt. ELUs können eine höhere Klassifikationsgenauigkeit als ReLUs. f ( x ) = { x, wenn x > 0 , a (e x ) ansonsten , JPYstyle f(x)= OLbegin{cases}x & Logtextif x}0,\\a\left(e^{x}-1\right) & Logtextson[otherwise{,\end{cases, in denen ein Memestyle a} eine Hyper-Parameter ist, um ein Null-Parameter zu erhalten, und ein ≥ 0, ist ein \q}. Der ELU kann als eine reibungslose Version einer umgestalteten ReLU (SReLU) angesehen werden, die die Form f ( x ) = max ( , x ) {\displaystyle f(x)=\max(-a,x} hat die gleiche Auslegung eines Geschäftsbereichs Glühbirnestyle a}. Siehe auch Softmax Funktion Sigmoid-Funktion Tobit-Modell (Deep Learning) Referenzen / <f>