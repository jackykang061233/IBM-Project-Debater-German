In der künstlichen Intelligenz ist ein intelligenter Agent (IA) alles, was seine Umgebung wahrnimmt, eigenständig Handlungen durchführt, um Ziele zu erreichen und seine Leistung mit Lernen zu verbessern oder Wissen zu nutzen. Sie können einfach oder komplex sein - ein Thermostat wird als Beispiel für ein intelligentes Mittel betrachtet, wie es ein Mensch ist, wie jedes System, das der Definition entspricht, wie ein Unternehmen, ein Zustand oder ein Biom. Intelligente Mittel werden oft schematisch als abstraktes Funktionssystem ähnlich einem Computerprogramm beschrieben. Forscher wie Russell & Norvig (2003) betrachten zielgerichtetes Verhalten als das Wesen der Intelligenz; ein normativer Agent kann mit einem Begriff aus Wirtschaft, "rational agent" geliehen markiert werden. In diesem rational-action Paradigma besitzt ein IA ein internes Modell seiner Umwelt. Dieses Modell kapselt alle Überzeugungen des Agenten über die Welt. Der Agent hat auch eine "objektive Funktion", die alle Ziele der IA verkapselt. Ein solcher Agent ist darauf ausgelegt, jeden Plan zu erstellen und auszuführen, der nach Abschluss den erwarteten Wert der Zielfunktion maximiert. Ein Verstärkungslerner kann eine "Reward-Funktion" haben, die es den Programmierern ermöglicht, das gewünschte Verhalten der IA zu gestalten, und das Verhalten eines evolutionären Algorithmus wird durch eine "Fitness-Funktion" geformt. Abstract-Beschreibungen von intelligenten Agenten werden manchmal als abstrakte intelligente Agenten (AIA) bezeichnet, um sie von ihrer realen Weltumsetzung als Computersysteme, biologische Systeme oder Organisationen zu unterscheiden. Einige autonome intelligente Agenten sind darauf ausgelegt, ohne menschliche Eingriffe zu funktionieren. Da intelligente Agenten beliebter werden, gibt es zunehmend rechtliche Risiken. Intelligente Agenten in der künstlichen Intelligenz sind eng mit Agenten in der Wirtschaft verbunden, und Versionen des intelligenten Agentenparadigmas werden in der kognitiven Wissenschaft, Ethik, der Philosophie der Praxis sowie in vielen interdisziplinären sozio-kognitiven Modellierung und Computer-Sozialsimulationen untersucht. Intelligente Agenten sind auch eng mit Software-Agenten verbunden (ein autonomes Computerprogramm, das Aufgaben im Auftrag der Nutzer durchführt). In der Informatik ist ein intelligenter Agent ein Software-Agent, der einige Intelligenz hat, zum Beispiel autonome Programme, die für den Bediener-Hilfe oder Datenbergbau verwendet werden (manchmal als Bots bezeichnet) werden auch als "intelligente Agenten" bezeichnet. Definitionen und MerkmaleNach Nikola Kasabov (1998) sollten IA-Systeme folgende Merkmale aufweisen: Akkommodieren Sie neue Problemlösung Regeln inkremental Adapt online und in Echtzeit Sind in der Lage, sich in Bezug auf Verhalten, Fehler und Erfolg zu analysieren. Lernen und verbessern durch Interaktion mit der Umwelt (Bevölkerung) Schnell aus großen Datenmengen lernen Speicherbasierte Exemplar-Speicher und Abrufkapazitäten Haben Parameter für kurze und langfristige Speicher, Alter, Vergessen, etc. Padgham & Winikoff (2005) stimmt zu, dass sich ein intelligenter Agent in einer Umgebung befindet und (in einer zeitgemäßen, aber nicht unbedingt Echtzeit) auf Umweltänderungen reagiert. Intelligente Agenten müssen aber auch gezielt Ziele flexibel und robust verfolgen. Optional Desiderata enthalten, dass der Agent rational ist, und dass der Agent in der Lage sein, die Analyse der Glaubens-Desir-Intention. Einige Definitionen des 20. Jahrhunderts charakterisieren einen Agenten als ein Programm, das einem Benutzer hilft oder das im Auftrag eines Benutzers handelt. Die einflussreiche AIMA (2009) definiert einen Agenten als "alles, was als die Wahrnehmung seiner Umgebung durch Sensoren angesehen werden kann und diese Umgebung durch Aktoren aufgreift", und charakterisiert Intelligenz als die Fähigkeit, nach bestimmten idealen Standards für Rationalität zu handeln. Zielfunktion Einige Agenten können eine explizite "Gate-Funktion" zugeordnet werden; ein Agent wird als intelligenter betrachtet, wenn es konsequent Aktionen durchführt, die seine programmierte Zielfunktion erfolgreich maximieren. Die "Gallenfunktion" verkapselt alle Ziele, die der Agent antreibt, zu handeln; bei rationalen Agenten verkapselt die Funktion auch die akzeptablen Kompromisse zwischen der Erreichung von Konfliktzielen.(Terminologie variiert; zum Beispiel versuchen einige Agenten, eine "Nutzungsfunktion" zu maximieren oder zu minimieren, "objektive Funktion" oder "Verlustfunktion".) Das theoretische und unbestrittene AIXI-Design ist ein maximal intelligenter Agent in diesem Paradigma; in der realen Welt wird die IA jedoch durch endliche Zeit- und Hardwareressourcen eingeschränkt, und Wissenschaftler konkurrieren, Algorithmen herzustellen, die bei Benchmark-Tests mit real-world-Hardware progressiv höhere Ergebnisse erzielen können.Systeme, die nicht traditionell als Agenten betrachtet werden, wie Wissensrepräsentationssysteme, werden manchmal in das Paradigma subsumiert, indem sie als Agenten, die ein Ziel haben, (zum Beispiel) Fragen so genau wie möglich zu beantworten; das Konzept einer Aktion wird hier erweitert, um den Akt einer Antwort auf eine Frage zu umfassen. Als zusätzliche Erweiterung können mimicry-getriebene Systeme als Agenten gerahmt werden, die eine "goale Funktion" optimieren, basierend darauf, wie eng die IA das gewünschte Verhalten nachahmen kann. In den generativen adversarialen Netzwerken der 2010er Jahre versucht ein Encoder"/"Generator-Komponente, menschliche Textkomposition zu mimieren und zu improvisieren. Der Generator versucht, eine Funktion zu maximieren, die verkapselt, wie gut es einen antagonistischen Vorhersager täuschen kann"/"Diskriminator-Komponente. Während GOFAI-Systeme oft eine explizite Zielfunktion annehmen, kann das Paradigma auch auf neuronale Netzwerke und auf evolutionäres Computing angewendet werden. Verstärktes Lernen kann intelligente Agenten erzeugen, die in einer Weise wirken, die eine "Reward-Funktion" maximieren soll. Manchmal, anstatt die Belohnungsfunktion direkt gleich der gewünschten Benchmark-Bewertungsfunktion zu setzen, werden maschinelle Lernprogrammierer Belohnungsform verwenden, um zunächst die Maschine Belohnungen für inkrementelle Fortschritte beim Lernen zu geben. Yann LeCun erklärte im Jahr 2018, dass "die meisten der Lernalgorithmen, mit denen die Menschen kommen, im Wesentlichen darin bestehen, eine gewisse objektive Funktion zu minimieren." AlphaZero Schach hatte eine einfache objektive Funktion; jeder Sieg gezählt als +1 Punkt, und jeder Verlust gezählt als -1 Punkt. Eine objektive Funktion für ein selbstfahrendes Auto müsste komplizierter sein. Evolutionäres Computing kann intelligente Agenten entwickeln, die scheinen, um eine "Fitness-Funktion" zu maximieren, die beeinflusst, wie viele Nachkommen jeder Agent verlassen darf. Struktur der Mittel Ein einfaches Agentenprogramm kann mathematisch als Funktion f(genannte "Agent-Funktion)" definiert werden, die jede mögliche Wahrnehmungssequenz auf eine mögliche Wirkung abbildet, die der Agent ausführen kann oder auf einen Koeffizienten, Feedback-Element, Funktion oder Konstante, die eventuelle Handlungen beeinflusst: f : P ++ → A {\displaystyle f:P^{\ast}\rightarrow A} Agent-Funktion ist ein abstraktes Konzept, da es verschiedene Prinzipien der Entscheidungsfindung wie Berechnung des Nutzens von einzelnen Optionen, Ableitung über Logikregeln, Fuzzy-Logik, etc. einschließen könnte. Der Programmagent statt, kartiert jede mögliche Wahrnehmung einer Aktion. Wir verwenden den Begriff Wahrnehmung, um auf die Wahrnehmungseingänge des Agenten zu jedem gegebenen Zeitpunkt zu verweisen. In den folgenden Figuren ist ein Agent alles, was als die Wahrnehmung seiner Umgebung durch Sensoren angesehen werden kann und über Aktuatoren auf diese Umgebung einwirkt. Architekturen Weiss (2013) definiert vier Klassen von Agenten: Logic-basierte Agenten – in denen die Entscheidung über welche Handlungen durch logische Abzüge getroffen wird; Reactive Agenten – in denen die Entscheidungsfindung in einer Form der direkten Zuordnung von Situation zu Handlung umgesetzt wird; Belief-Desire-Intention Agenten – in denen die Entscheidungsfindung von der Manipulation von Datenstrukturen abhängt, die die Überzeugungen, Wünsche und Absichten des Agents darstellen; und schließlich Layered-Beschlusses. Generell kann ein Mittel durch Trennen des Körpers in die Sensoren und Aktoren aufgebaut werden, so dass es mit einem komplexen Wahrnehmungssystem arbeitet, das die Beschreibung der Welt als Eingabe für einen Regler und Befehle an den Aktuator ausgibt. Eine Hierarchie von Controller-Schichten ist jedoch oft notwendig, um die für Low-Level-Aufgaben gewünschte sofortige Reaktion und die langsame Begründung über komplexe High-Level-Ziele auszugleichen. Klassen Russell & Norvig (2003) Gruppenagenten in fünf Klassen basierend auf ihrem Grad der wahrgenommenen Intelligenz und Fähigkeit: einfache Reflexmittel modellbasierte Reflexmittel zielbasierte Agenten nutzbringende Agenten Lernmittel Einfache Reflexmittel Einfache Reflexmittel wirken nur auf der Grundlage der aktuellen Wahrnehmung und ignorieren den Rest der Wahrnehmungsgeschichte. Die Agentenfunktion basiert auf der Bedingungsregel: "wenn Bedingung, dann Aktion". Diese Agent-Funktion gelingt nur, wenn die Umgebung vollständig beobachtbar ist. Einige Reflexmittel können auch Informationen über ihren aktuellen Zustand enthalten, die es ihnen ermöglichen, Zustände, deren Aktuatoren bereits ausgelöst werden, außer Acht zu lassen.Unendliche Schlaufen sind oft für einfache Reflexmittel, die in teilbeobachtbaren Umgebungen arbeiten, unvermeidbar. Hinweis: Wenn der Agent seine Handlungen randomisieren kann, kann es möglich sein, aus unendlichen Schleifen zu entkommen. Modellbasierte Reflexmittel Ein modellbasierter Agent kann teilweise beobachtbare Umgebungen behandeln. Sein aktueller Zustand wird innerhalb des Agenten gespeichert, der eine Art von Struktur, die den Teil der Welt beschreibt, die nicht gesehen werden kann. Dieses Wissen über "wie die Welt funktioniert" nennt man ein Modell der Welt, also den Namen "model-based agent". Ein modellbasiertes Reflexmittel sollte eine Art internes Modell beibehalten, das von der Wahrnehmungsgeschichte abhängt und dadurch zumindest einige der unbeobachteten Aspekte des aktuellen Zustands reflektiert. Die Wahrnehmung von Geschichte und Wirkung auf die Umwelt kann durch das interne Modell bestimmt werden. Es wählt dann eine Aktion in der gleichen Weise wie Reflexmittel. Ein Agent kann auch Modelle verwenden, um das Verhalten anderer Agenten in der Umgebung zu beschreiben und vorherzusagen. Goal-basierte Agenten Goal-basierte Agenten erweitern die Fähigkeiten der modellbasierten Agenten weiter, indem sie Torinformationen verwenden. Zielinformationen beschreiben Situationen, die wünschenswert sind. Dies bietet dem Agenten eine Möglichkeit, unter mehreren Möglichkeiten zu wählen, die einen Zielzustand erreicht. Suche und Planung sind die Unterfelder künstlicher Intelligenz, die sich auf die Suche nach Handlungssequenzen, die die Ziele des Agenten erreichen, stützen. Utility-basierte Agenten Goal-basierte Agenten unterscheiden sich nur zwischen Zielstaaten und Nicht-Gate-Staaten. Es ist auch möglich, ein Maß dafür zu definieren, wie wünschenswert ein bestimmter Zustand ist. Diese Maßnahme kann durch die Verwendung einer Gebrauchsfunktion erreicht werden, die einen Zustand zu einem Maß für die Nutzbarkeit des Zustands abbildet. Eine allgemeinere Leistungsmaßnahme sollte einen Vergleich verschiedener Weltstaaten ermöglichen, je nachdem, wie gut sie die Ziele des Agenten erfüllten. Der Begriff Dienstprogramm kann verwendet werden, um zu beschreiben, wie glücklich der Agent ist. Ein rationaler utility-basierter Agent wählt die Aktion, die das erwartete Nutzen der Aktionsergebnisse maximiert - das heißt, was der Agent erwartet, um im Durchschnitt, angesichts der Wahrscheinlichkeiten und Nutzen jedes Ergebnisses abzuleiten. Ein utilitaristischer Agent muss seine Umwelt modellieren und verfolgen, Aufgaben, die eine große Menge an Forschung über Wahrnehmung, Darstellung, Argumentation und Lernen beteiligt haben. Lernmittel Lernen hat den Vorteil, dass es den Agenten ermöglicht, zunächst in unbekannten Umgebungen zu arbeiten und kompetenter zu werden, als sein erstes Wissen allein erlauben könnte. Die wichtigste Unterscheidung ist zwischen dem "Lernelement", das für Verbesserungen verantwortlich ist, und dem "Leistungselement", das für die Auswahl externer Aktionen verantwortlich ist. Das Lernelement nutzt Feedback des Kritikers, wie der Agent tätig ist und entscheidet, wie das Leistungselement oder der Schauspieler künftig besser verändert werden soll. Das Leistungselement ist das, was wir zuvor als der gesamte Agent betrachtet haben: es nimmt Wahrnehmungen an und entscheidet über Handlungen. Die letzte Komponente des Lernmittels ist der "Problemgenerator". Es ist verantwortlich für den Vorschlag von Aktionen, die zu neuen und informativen Erfahrungen führen. In seinem Buch, Superhuman Creators, Al Byrd beschreibt die konzeptionelle Architektur für einen künstlichen Agenten genannt eine kontofähige Kriegsdienststelle (ACE.) Ein ACE ist ein zielorientiertes KI-System, das ein autobiographisches Gedächtnis seiner Vergangenheit und Gegenwart und seine Zukunftspläne hat, das es erlaubt, sein Verhalten in einer sozial akzeptablen Weise, in realen und vorgestellten Situationen, zu beschränken und zu rechtfertigen. Der Autor argumentiert, dass eine Einheit mit dieser Architektur immer für ihr Verhalten verantwortlich sein wird und in gewissenhafter Weise handeln wird. Hierarchien der Agenten Um ihre Funktionen aktiv wahrnehmen zu können, werden heute in der Regel intelligente Agenten in einer hierarchischen Struktur gesammelt, die viele "Subagents" enthält. Intelligente Sub-Agenten verarbeiten und führen niedrigere Level-Funktionen aus. Gemeinsam schaffen der intelligente Agent und Subagents ein komplettes System, das schwierige Aufgaben oder Ziele mit Verhaltensweisen und Antworten, die eine Form von Intelligenz anzeigen, erfüllen kann. Anwendungen Intelligente Agenten werden als automatisierte Online-Assistenten eingesetzt, wo sie die Bedürfnisse der Kunden wahrnehmen, um individualisierte Kundendienste durchzuführen. Ein solcher Agent kann grundsätzlich aus einem Dialogsystem, einem Avatar, sowie einem Expertensystem bestehen, um dem Benutzer spezifische Expertise zu bieten. Sie können auch verwendet werden, um die Koordination von Menschengruppen online zu optimieren.Hallerbach et al. diskutierte die Anwendung von agenturbasierten Ansätzen zur Entwicklung und Validierung von automatisierten Fahrsystemen über einen digitalen Zwilling der Fahrzeug-Untertest- und mikroskopischen Verkehrssimulation auf Basis unabhängiger Agenten. Waymo hat eine multiagente Simulationsumgebung Carcraft entwickelt, um Algorithmen für selbstfahrende Autos zu testen. Es simuliert Verkehrsinteraktionen zwischen menschlichen Fahrern, Fußgängern und automatisierten Fahrzeugen. Das Verhalten der Menschen wird von künstlichen Agenten nachgeahmt, die auf Daten des realen menschlichen Verhaltens basieren. Die Grundidee der Verwendung von agentenbasierter Modellierung, um selbstfahrende Autos zu verstehen, wurde bereits 2003 diskutiert. Siehe auch Software Agent Cognitive Architectures Cognitive Radio – ein praktisches Feld für die Implementierung Cybernetics, Computer Science Data Mining Agent Embodied agent Federated search – die Fähigkeit von Agenten zur Suche heterogene Datenquellen mit einem einzigen vocabulären Fuzzy Agenten – IA implementiert mit adaptive fuzzy logic GOAL Agent Programmiersprache Intelligenz Intelligent System JACK Intelligent Agents Multi-agent System und Multiple-Agenten-System Weitere Referenzen Russell, Stuart J;. Norvig, Peter (2003). Künstliche Intelligenz: Ein moderner Ansatz (2. ed.). Upper Saddle River, New Jersey: Prentice Hall.Kapitel 2.ISBN 0-13-790395-2.Franklin, Stan; Graesser, Art (1996). " Ist es ein Agent oder nur ein Programm?:Eine Taxonomie für autonome Agenten" (PDF). Proceedings of the Third International Workshop zu Agent Theorien, Architekturen und Sprachen. Springer-Verlag. Archiviert vom Original (PDF) am 29. Mai 2015. Kasabov, N. (1998)." Einführung: Hybride intelligente adaptive Systeme". International Journal of Intelligent Systems.13 (6:) 453–454.doi:10.1002/(SICI)1098-111X(199806)13:6<453:AID-INT1>3.0.CO;2-K Weiss, G. (2013). Multiagente Systeme (2. ed.). Cambridge, MA: MIT Drücken. ISBN 978-0-262-01889-0. Externe Links Coneural