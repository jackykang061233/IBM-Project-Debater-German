Eine selbstorganisierende Karte (SOM) oder selbstorganisierende Merkmalskarte (SOFM) ist eine ununterbrochene maschinelle Lerntechnik, mit der eine niederdimensionale (typisch zweidimensionale) Darstellung eines höherdimensionalen Datensatzes unter Erhaltung der topologischen Struktur der Daten erzeugt wird. Beispielsweise könnte ein Datensatz mit in n Beobachtungen gemessenen p-Variablen als Beobachtungscluster mit ähnlichen Werten für die Variablen dargestellt werden. Diese Cluster könnten dann als zweidimensionale Karte visualisiert werden, so dass Beobachtungen in proximalen Clustern ähnliche Werte aufweisen als Beobachtungen in distalen Clustern. Dadurch können hochdimensionale Daten einfacher visualisiert und analysiert werden. Ein SOM ist eine Art künstliches neuronales Netz, wird aber mit wettbewerbsfähigem Lernen trainiert, anstatt das von anderen künstlichen neuronalen Netzen verwendete fehlerkorrektions-Erlernen (z.B. Rückverbreitung mit Gradientenabstieg). Die SOM wurde von dem finnischen Professor Teuvo Kohonen in den 1980er Jahren eingeführt und wird daher manchmal als Kohonen-Karte oder Kohonen-Netzwerk bezeichnet. Die Kohonen-Karte oder das Netzwerk ist ein rechnerisch bequemes Abstraktionsgebäude an biologischen Modellen von neuronalen Systemen aus den 1970er-Jahren und Morphogenese-Modellen aus den 1950er-Jahren. Übersicht Selbstorganisierende Karten, wie die meisten künstlichen neuronalen Netzwerke, arbeiten in zwei Modi: Training und Kartierung. Zunächst verwendet das Training einen Eingabedatensatz (der "Eingaberaum"), um eine niederdimensionale Darstellung der Eingabedaten (der "Kartenraum") zu erzeugen. Zweitens klassifiziert das Mapping zusätzliche Eingabedaten mit der generierten Karte. In den meisten Fällen besteht das Ziel der Ausbildung darin, einen Eingaberaum mit p-Abmessungen als Kartenraum mit zwei Dimensionen darzustellen. Insbesondere soll ein Eingangsraum mit p-Variablen p-Abmessungen aufweisen. Ein Kartenraum besteht aus Komponenten namens Knoten oder Neuronen, die als sechseckiges oder rechteckiges Raster mit zwei Dimensionen angeordnet sind. Die Anzahl der Knoten und deren Anordnung wird vorher anhand der größeren Ziele der Analyse und der Erkundung der Daten festgelegt. Jeder Knoten im Kartenraum ist einem Gewichtsvektor zugeordnet, der die Position des Knotens im Eingaberaum ist. Während Knoten im Kartenraum fixiert bleiben, besteht das Training darin, Gewichtvektoren in Richtung der Eingabedaten zu bewegen (eine Entfernungsmetrie wie Euclide-Abstand zu reduzieren), ohne die aus dem Kartenraum induzierte Topologie zu verderben. Nach dem Training kann die Karte dazu verwendet werden, zusätzliche Beobachtungen für den Eingaberaum zu klassifizieren, indem der Knoten mit dem nächstgelegenen Gewichtsvektor (kleinste Distanz Metrik) zum Eingaberaumvektor gefunden wird. Lernalgorithmus Ziel des Lernens in der selbstorganisierenden Karte ist es, verschiedene Teile des Netzes zu veranlassen, ähnlich wie bestimmte Eingabemuster zu reagieren. Dies wird zum Teil dadurch motiviert, wie visuelle, auditive oder andere sensorische Informationen in separaten Teilen der zerebralen Kortex im menschlichen Gehirn behandelt werden. Die Gewichte der Neuronen werden entweder auf kleine Zufallswerte initialisiert oder gleichmäßig aus dem von den beiden größten Hauptkomponenten-Eigenvektoren aufgespannten Teilraum abgetastet. Mit der letztgenannten Alternative ist das Lernen viel schneller, da die Anfangsgewichte bereits eine gute Annäherung an SOM-Gewichte geben. Das Netzwerk muss eine Vielzahl von Beispielvektoren zugeführt werden, die möglichst nahe die bei der Kartierung erwarteten Vektortypen darstellen. Die Beispiele werden in der Regel mehrmals als Iterationen verabreicht. Die Ausbildung nutzt wettbewerbsfähiges Lernen. Wenn ein Trainingsbeispiel dem Netzwerk zugeführt wird, wird dessen Euclidesche Entfernung zu allen Gewichtsvektoren berechnet. Das Neuron, dessen Gewichtsvektor dem Eingang am ähnlichsten ist, wird als beste Anpassungseinheit (BMU) bezeichnet. Die Gewichte der BMU und der nahe daran liegenden Neuronen im SOM-Gitter werden in Richtung des Eingangsvektors eingestellt. Die Größe der Änderung nimmt mit der Zeit und mit dem Rasterabstand von der BMU ab. Die Aktualisierungsformel für ein Neuron v mit Gewichtsvektor Wv(s) ist W v (s + 1 ) = W v (s ) + θ (u , v , s ) α (s ) Θ (t ) Abhängig von den Implementierungen kann t den Trainingsdatensatz systematisch scannen (t ist 0, 1, 2...T-1, dann wiederholen, T ist die Größe der Trainingsprobe), zufällig aus dem Datensatz (Bootstrap-Sampling) gezogen werden oder eine andere Probenahmemethode (wie Jackknifing) implementieren. Die Nachbarschaftsfunktion Θ(u, v, s) (auch Funktion der lateralen Interaktion genannt) hängt vom Rasterabstand zwischen BMU (neuron u) und Neuron v ab. In der einfachsten Form ist es 1 für alle Neuronen in der Nähe von BMU und 0 für andere, aber die Gaussian und Mexican-hat-Funktionen sind gemeinsame Entscheidungen. Unabhängig von der Funktionsform schrumpft die Nachbarschaftsfunktion mit der Zeit. Zu Beginn, wenn die Nachbarschaft breit ist, findet die Selbstorganisation im globalen Maßstab statt. Wenn die Nachbarschaft zu nur ein paar Neuronen geschrumpft hat, konvergieren die Gewichte zu lokalen Schätzungen. In einigen Implementierungen nimmt der Lernkoeffizient α und die Nachbarschaftsfunktion Θ mit zunehmendem s stetig ab, in anderen (insbesondere in denen t den Trainingsdatensatz scannt) sinken sie schrittweise, sobald alle T-Schritte. Dieser Vorgang wird für jeden Eingangsvektor für eine (meist große) Anzahl von Zyklen λ wiederholt. Das Netzwerk verwindet Ausgangsknoten mit Gruppen oder Mustern im Eingangsdatensatz. Wenn diese Muster benannt werden können, können die Namen an den zugehörigen Knoten im trainierten Netz angebracht werden. Bei der Kartierung gibt es ein einziges gewinnendes Neuron: das Neuron, dessen Gewichtsvektor dem Eingangsvektor am nächsten liegt. Dies kann einfach durch Berechnung des Euclideschen Abstandes zwischen Eingangsvektor und Gewichtsvektor bestimmt werden. Während die Darstellung von Eingabedaten als Vektoren in diesem Artikel hervorgehoben wurde, kann jede Art von Objekt, die digital dargestellt werden kann, die ein entsprechendes Distanzmaß zugeordnet ist, und bei dem die notwendigen Operationen zum Training möglich sind, zur Erstellung einer selbstorganisierenden Karte verwendet werden. Dazu gehören Matrizen, kontinuierliche Funktionen oder sogar andere selbstorganisierende Karten. Variablen Dies sind die Variablen, die benötigt werden, mit Vektoren in fett, s {\displaystyle s} ist die aktuelle Iteration λ {\displaystyle \lambda } ist die Iterationsgrenze t {\displaystyle t} ist der Index des Zieleingangsdatenvektors im Eingabedatensatz D {\displaystyle \D} Algorithm Randomize the node weight vectors in a map Randomly picke einen Eingabevektor D (t ) {\displaystyle {D}(t) Traverse jeden Knoten in der Karte Verwenden Sie die Euclidean-Distanzformel, um die Ähnlichkeit zwischen dem Eingabevektor und dem Gewichtsvektor des Kartenknotens zu finden Verfolgen Sie den Knoten, der den kleinsten Abstand erzeugt (dieser Knoten ist die beste Anpassungseinheit, BMU) Aktualisieren Sie die Gewichtsvektoren der Knoten in der Nachbarschaft der BMU (einschließlich der BMU selbst), indem Sie sie näher an den Eingangsvektor W v (s + 1 ) = W v (s ) + θ (u , v , s ) α (s + 1 ) Alternative Algorithmus Randomize the map's nodes' weightvektors Traverse jeden Eingabevektor im Eingabedatensatz Traverse jeden Knoten in der Karte Verwenden Sie die Euclidean-Distanzformel, um die Ähnlichkeit zwischen dem Eingabevektor und dem Gewichtsvektor des Kartenknotens zu finden Verfolgen Sie den Knoten, der den kleinsten Abstand erzeugt (dieser Knoten ist die beste Anpassungseinheit, BMU) Aktualisieren der Knoten in der Nachbarschaft der BMU (einschließlich der BMU selbst), indem sie näher an den Eingangsvektor W v (s + 1 ) = W v (s ) + θ (u , v , s ) Kohonen hat ursprünglich eine zufällige Einleitung von Gewichten vorgeschlagen. ( Dieser Ansatz wird durch die oben beschriebenen Algorithmen reflektiert.) In jüngerer Zeit ist die Hauptkomponenten-Initialisierung, bei der erste Kartengewichte aus dem Raum der ersten Hauptkomponenten gewählt werden, aufgrund der exakten Reproduzierbarkeit der Ergebnisse populär geworden. Ein sorgfältiger Vergleich der zufälligen Initiierung zur Hauptkomponenten-Ionisation für eine eindimensionale Karte ergab jedoch, dass die Vorteile der Hauptkomponenten-Ionisation nicht universell sind. Die beste Initialisierungsmethode hängt von der Geometrie des spezifischen Datensatzes ab. Eine Hauptkomponenten- Initialisierung war bevorzugt (für eine eindimensionale Karte), wenn die den Datensatz annähernde Hauptkurve auf der ersten Hauptkomponente (Quasilinearsätze) ein- und linear projiziert werden konnte. Bei nichtlinearen Datensätzen wurde jedoch die statistische Initiierung besser durchgeführt. Auslegung Es gibt zwei Möglichkeiten, eine SOM zu interpretieren. Da in der Trainingsphase Gewichte des gesamten Viertels in der gleichen Richtung bewegt werden, neigen ähnliche Elemente dazu, benachbarte Neuronen anzuregen. Daher bildet SOM eine semantische Karte, in der ähnliche Proben in der Nähe zusammen abgebildet werden und voneinander abweichen. Dies kann durch einen U-Matrix (Euklideischen Abstand zwischen Gewichtsvektoren benachbarter Zellen) der SOM visualisiert werden. Der andere Weg ist, an neuronale Gewichte als Zeiger an den Eingangsraum zu denken. Sie bilden eine diskrete Annäherung der Verteilung von Ausbildungsproben. Mehr Neuronen zeigen Regionen mit hoher Trainingsprobenkonzentration und weniger, wo die Proben knapp sind. SOM kann als nichtlineare Verallgemeinerung der Hauptkomponentenanalyse (PCA) betrachtet werden. Es hat sich gezeigt, dass SOM mit künstlichen und realen geophysikalischen Daten gegenüber den herkömmlichen Merkmalsextraktionsverfahren wie Empirical Orthogonal Functions (EOF) oder PCA viele Vorteile hat. Ursprünglich wurde SOM nicht als Lösung für ein Optimierungsproblem formuliert. Dennoch gab es mehrere Versuche, die Definition von SOM zu ändern und ein Optimierungsproblem zu formulieren, das ähnliche Ergebnisse liefert. Zum Beispiel verwenden elastische Karten die mechanische Metapher der Elastizität, um Hauptkrümmer anzunähern: Die Analogie ist eine elastische Membran und Platte. Beispiele Fisher's Iris-Blumendaten Betrachten Sie eine n×m-Anordnung von Knoten, die jeweils einen Gewichtsvektor enthält und sich dessen Lage im Array bewusst ist. Jeder Gewichtsvektor ist dieselbe Dimension wie der Eingangsvektor des Knotens. Die Gewichte können zunächst auf Zufallswerte eingestellt werden. Jetzt brauchen wir Eingabe, um die Karte zu füttern. Farben können durch ihre roten, grünen und blauen Komponenten dargestellt werden. Infolgedessen werden wir Farben als Vektoren in der Einheit Würfel des freien Vektorraums über R darstellen, die von der Basis erzeugt werden: R = <255, 0, 0> G = <0, 255, 0> B = <0, 0, 255> Das gezeigte Diagramm vergleicht die Ergebnisse der Schulung auf den DatensätzendreiFarben = [255, 0, 0,] [0, 255, 0,] [0, 0, 255] achtFarben = [0, 0, 0, 0, 0,] [255, 0, 0,] [0, 255, 0] [0, 0, 255,] [255, 255] [5, 255] Beachten Sie die markante Ähnlichkeit zwischen den beiden. Ebenso kann die Karte nach dem Training eines 40×40 Rasters von Neuronen für 250 Iterationen mit einer Lernrate von 0,1 auf Fisher's Iris bereits die wichtigsten Unterschiede zwischen Spezies erkennen. Sonstige Projektpriorisierung und -auswahl Seismische Fazilitätenanalyse für Öl- und Gasexploration Ausfallmodus und -effektanalyse Erstellung von Kunstwerken Alternativen Die generative topographische Karte (GTM) ist eine mögliche Alternative zu SOMs. In dem Sinne, dass ein GTM explizit eine reibungslose und kontinuierliche Kartierung vom Eingaberaum zum Kartenraum erfordert, ist es die Topologie-Bewahrung. In praktischer Hinsicht fehlt jedoch diese Maßnahme der topologischen Erhaltung. Die Zeit adaptive selbstorganisierende Karte (TASOM) Netzwerk ist eine Erweiterung der grundlegenden SOM. Das TASOM beschäftigt adaptive Lernraten und Nachbarschaftsfunktionen. Es enthält auch einen Skalierungsparameter, um das Netzwerk invariant zu Skalierung, Übersetzung und Rotation des Eingangsraums zu machen. Das TASOM und seine Varianten wurden in mehreren Anwendungen eingesetzt, darunter adaptive Clustering, Multilevel-Schwellung, Eingangsraum-Annäherung und aktive Konturmodellierung. Darüber hinaus wurde ein Binary Tree TASOM oder BTASOM, ähnlich einem binären natürlichen Baum mit Knoten aus TASOM-Netzwerken vorgeschlagen, wo die Anzahl seiner Ebenen und die Anzahl seiner Knoten mit seiner Umgebung adaptiv sind. Die wachsende selbstorganisierende Karte (GSOM) ist eine wachsende Variante der selbstorganisierenden Karte. Die GSOM wurde entwickelt, um das Problem der Identifizierung einer geeigneten Kartengröße in der SOM anzugehen. Es beginnt mit einer minimalen Anzahl von Knoten (in der Regel vier) und wächst neue Knoten auf der Grenze basierend auf einer heuristischen. Durch Verwendung eines Wertes, der den Spreizfaktor genannt wird, hat der Datenanalyst die Fähigkeit, das Wachstum des GSOM zu kontrollieren. Die elastischen Karten nähern sich aus der Spline-Interpolation die Idee der Minimierung der elastischen Energie. Beim Lernen minimiert es die Summe der quadratischen Biege- und Streckenergie mit dem kleinsten Quadrat Näherungsfehler. Der konforme Ansatz, der eine konforme Kartierung verwendet, um jede Trainingsprobe zwischen Gitterknoten in einer kontinuierlichen Oberfläche zu interpolieren. Bei diesem Ansatz ist eine eins-zu-eins-glatte Kartierung möglich. Die orientierte und skalierbare Karte (OS-Map) verallgemeinert die Nachbarschaftsfunktion und die Gewinnerauswahl. Die homogene Gaussian Nachbarschaftsfunktion wird durch die Matrix exponential ersetzt. So kann die Orientierung entweder im Kartenraum oder im Datenraum vorgegeben werden. SOM hat eine feste Skala =(1,), so dass die Karten "optimal beschreiben die Domäne der Beobachtung". Aber was ist mit einer Karte, die die Domain zweimal oder in n-Falten abdeckt? Dies beinhaltet die Konzeption der Skalierung. Das OS-Map betrachtet die Skala als eine statistische Beschreibung, wie viele Best-matching-Knoten eine Eingabe auf der Karte hat. Siehe auch Neural Gas Lernen Vector Quantization Liquid state machine Hybrid Kohonen SOM Sparse Codierung Sparse verteilt SpeicherDeep Learning Neocognitron Topological data analysis Notes == Referenzen = Das Machine Intelligence Research Institute (MIRI), früher das Singularity Institute for Artificial Intelligence (SIAI), ist ein gemeinnütziges Forschungsinstitut, das seit 2005 auf die Identifizierung und Verwaltung potenzieller existentieller Risiken durch künstliche allgemeine Intelligenz ausgerichtet ist. Die Arbeit von MIRI konzentrierte sich auf einen freundlichen KI-Ansatz zur Systemgestaltung und auf die Vorhersage der Geschwindigkeit der Technologieentwicklung. Geschichte Im Jahr 2000 gründete Eliezer Yudkowsky das Singularity Institute for Artificial Intelligence mit Finanzierungen von Brian und Sabine Atkins, um die Entwicklung der künstlichen Intelligenz (KI) zu beschleunigen. Yudkowsky begann jedoch zu befürchten, dass künftig entwickelte KI-Systeme superintelligent werden könnten und Risiken für die Menschheit darstellen, und 2005 zog das Institut nach Silicon Valley und begann, sich auf Wege zu konzentrieren, diese Risiken zu identifizieren und zu verwalten, die damals von Wissenschaftlern auf dem Gebiet weitgehend ignoriert wurden. Ab 2006 organisierte das Institut den Singularity-Gipfel, um die Zukunft von KI zu diskutieren, einschließlich seiner Risiken, zunächst in Zusammenarbeit mit der Stanford University und mit Finanzierung von Peter Thiel. Die San Francisco Chronicle beschrieb die erste Konferenz als "Bay Area Come-out Party für die tech-inspirierte Philosophie namens Transhumanismus". 2011 waren seine Büros vier Wohnungen in der Innenstadt von Berkeley. Im Dezember 2012 verkaufte das Institut seinen Namen, Web-Domain und den Singularity-Gipfel an die Singularity-Universität und nahm im folgenden Monat den Namen "Machine Intelligence Research Institute". In den Jahren 2014 und 2015 wuchs das öffentliche und wissenschaftliche Interesse an den Risiken von KI und steigerte Spenden für die Forschung an MIRI und ähnlichen Organisationen. 2019 empfahl Open Philanthropy dem MIRI einen allgemeinen Förderzuschuss von rund 2,1 Millionen Dollar über zwei Jahre. Im April 2020 ergänzte Open Philanthropy dies mit einem Stipendium von 7,7 Millionen Dollar über zwei Jahre. Im Jahr 2021 spendete Vitalik Buterin mehrere Millionen Dollar von Ethereum zu MIRI. Der Ansatz von MIRI, die Risiken von KI zu identifizieren und zu verwalten, unter der Leitung von Yudkowsky, befasst sich in erster Linie mit der Gestaltung einer freundlichen KI, die sowohl die ursprüngliche Gestaltung von KI-Systemen als auch die Schaffung von Mechanismen umfasst, um sicherzustellen, dass sich entwickelnde KI-Systeme freundlich bleiben. MIRI-Forscher befürworten die frühe Sicherheitsarbeit als Vorsorgemaßnahme. MIRI-Forscher haben jedoch Skepsis über die Ansichten von Singularitätsbefürwortern wie Ray Kurzweil zum Ausdruck gebracht, dass Superintelligenz "nur um die Ecke" ist. MIRI hat prognostizierte Arbeiten durch eine Initiative namens AI Impacts finanziert, die historische Instanzen des diskontinuierlichen technologischen Wandels untersucht und neue Maßnahmen der relativen Rechenleistung von Mensch und Computerhardware entwickelt hat.MIRI richtet sich an die Prinzipien und Ziele der effektiven Altruismusbewegung. Werke von MIRI-Mitarbeiter Graves, Matthew (8. November 2017). " Warum sollten wir über künstliche Superintelligenz besorgt sein." Skeptic.The Skeptics Society.Retrieved 28 Juli 2018.LaVictoire, Patrick; Fallenstein, Benja; Yudkowsky, Eliezer; Bárász, Mihály; Christiano, Paul; Herreshoff, Marcello (2014)." Programm Equilibrium im Dilemma des Gefangenen über Löb's Theorem". Multiagente Interaktion ohne vorherige Koordination: Papiere aus dem AAAI-14 Workshop. AAAI Publikationen. Soares, Nate; Levinstein, Benjamin A. (2017). "Cheating Death in Damaskus" (PDF). Formal Epistemology Workshop (FEW). Retrieved 28. Juli 2018.Soares, Nate; Fallenstein, Benja; Yudkowsky, Eliezer; Armstrong, Stuart (2015)."Korrigabilität". AAAI-Workshops: Workshops auf der 55. AAAI-Konferenz über Künstliche Intelligenz, Austin, TX, 25. bis 26. Januar 2015.AAAI-Veröffentlichungen Soares, Nate; Fallenstein, Benja (2015). " Aligning Superintelligence with Human Interessen: A Technical Research Agenda" (PDF.) In Miller, James; Yampolskiy, Roman; Armstrong, Stuart; et al.(eds.). Die technologische Singularität: Verwaltung der Reise.Springer. Yudkowsky, Eliezer (2008)." Künstliche Intelligenz als positiver und negativer Faktor im globalen Risiko" (PDF.) In Bostrom, Nick; Ćirković, Mailand (Hrsg.). Globale Katastrophe Risiken. Oxford University Press.ISBN 978-0199606504.Taylor, Jessica (2016). " Quantilizer: Eine sicherere Alternative zu Maximizern für eine begrenzte Optimierung". Workshops auf der Thirtieth AAAI Konferenz über Künstliche Intelligenz. Yudkowsky, Eliezer (2011)." Komplexe Wertesysteme in Friendly AI" (PDF). Künstliche Allgemeine Intelligenz: 4. Internationale Konferenz, AGI 2011, Mountain View, CA, USA, 3.–6. August 2011.Berlin: Springer. Siehe auch Allen Institute for Artificial Intelligence Future of Humanity Institute Institute for Ethics and Emerging Technologies Referenzen Weiter lesen Russell, Stuart; Dewey, Daniel; Tegmark, Max (Winter 2015). "Forschungsprioritäten für eine robuste und beneidenswerte Künstliche Intelligenz".AI Magazine.36 (4): 6.arXiv:1602.03506.Bibcode:2016arXiv160203506R Offizielle Website