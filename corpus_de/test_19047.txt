Mangelhaftes Lernen (UL) ist eine Art von Algorithmen, die Muster aus ungenutzten Daten lernen. Man hofft, dass die Maschine durch die Mimicry gezwungen ist, eine einheitliche interne Darstellung ihrer Welt zu erstellen und anschließend kreative Inhalte zu erzeugen. Im Gegensatz zu überwachtem Lernen (SL), wo Daten von einem Menschen, z.B. als Auto oder Fisch usw., gekennzeichnet sind, zeigt die UL selbstorganisierung, die Muster als neuronale Vordilectationen oder Wahrscheinlichkeitsverweigerungen erfasst. Die anderen Ebenen des Überwachungsspektrums stärken das Lernen, wo die Maschine nur einen numerischen Leistungsanzeiger als ihre Leitlinie erhält, und das halbgesteuerte Lernen, bei dem ein kleiner Teil der Daten markiert ist. Zwei breite Methoden in der UL sind Neuralnetze und Probabilistische Methoden. Probabilistische Methoden Zwei der wichtigsten Methoden, die in unreguliertes Lernen verwendet werden, sind die Hauptkomponente und die Clusteranalyse. Cluster-Analysen werden in unkontrollierten Lernprozessen zur Gruppe oder zum Segment verwendet, Datensets mit gemeinsamen Eigenschaften, um die algerischen Beziehungen aufzuwerten. Clusteranalyse ist eine Branche des maschinenlesbaren Lernens, die die Daten zusammenfasst, die nicht gekennzeichnet, klassifiziert oder zusammengefasst wurden. Anstatt auf Feedback zu reagieren, zeigt die Clusteranalyse gemeinsame Merkmale in den Daten und reagiert auf das Vorhandensein oder Fehlen solcher Gemeinsamkeiten in jedem neuen Datenstück. Dieser Ansatz hilft, aomalartige Datenpunkte zu erkennen, die nicht in eine Gruppe passen. Eine zentrale Anwendung des unüberwachten Lernens liegt im Bereich der Erfassung der Dichte in Statistiken, obwohl unüberwachtes Lernen viele andere Bereiche umfasst, die die Zusammenfassung und Erläuterung der Datenmerkmale umfassen. Man kann sich mit dem beaufsichtigten Lernen widersetzen, indem er feststellt, dass das unter Aufsicht stehende Lernen eine Bedingungsverteilung p X ( x [ y ) HANAtextstyle p_{X}(x\,JJ\,y) auf dem Etikett y HANAtextstyle y} von Inputdaten in Anspruch nehmen möchte; unbegleitetes Lernen beabsichtigt, eine Voranmeldung für die Verteilung von P X ( x ) Memetext p_{X}(x) . Konzepte einiger der am häufigsten verwendeten Algorithmen für unreguliertes Lernen: (1) Clustering, (2) Anomaly Erkennung, (3) Neural Networks und (4) Konzepte für das Erlernen lateter variabler Modelle. Jeder Ansatz verwendet mehrere Methoden wie folgt: Clustermethoden umfassen: hierarchisches Clustering, k-means, Mischungsmodelle, DBSCAN und OPIK-Algorithmus Anomaly Erkennungsmethoden umfassen: Lokale Outlier-Faktoren und Isolierung Forest-Konzepte für das Erlernen von latenten variablen Modellen wie Erwartungs-maximisierungsgorithmus (EM), Methode für Momente und Blinde Signal-Entfernungstechniken (Principal-Komponentenanalyse, unabhängige Komponenteanalyse, nicht-negative Matrix-Faktorierung, Fingerabdruck-Decomposition) Neural Networks: Autoencoders, Deep Beru Nets, Hebbian Learning, Generative Adversarial-Netzwerke und Selbstorganisationskarte Eines der statistischen Ansätze für unüberwachtes Lernen ist die Methode der Momente. In der Art der Momente sind die unbekannten Parameter (von Interesse) im Modell mit den Augenblicken eines oder mehrerer Zufallsvariablen verknüpft, so dass diese unbekannten Parameter aufgrund der Momente geschätzt werden können. Die Momente werden in der Regel von Proben empirischen Ausmaßes geschätzt. Die grundlegenden Momente sind erste und zweite Mahnzeiten. Für einen Zufallsvektor ist der erste Bestelltermin der Mittelwert, und der zweite Bestelltermin ist die Kovarianzmatrix (bei Null). Längere Auftragszeiten werden in der Regel mit zehntausenden vertreten, die die Generalisierung von matrices zu höheren Aufträgen als multidimensionale Größen darstellen. Insbesondere zeigt sich, dass die Methode der Augenblicke wirksam ist, um die Parameter verspäteter variabler Modelle zu lernen. Verletzte variable Modelle sind statistische Modelle, wenn neben den beobachteten Variablen auch eine Reihe von latenten Variablen vorhanden sind, die nicht beobachtet werden. Ein sehr praktisches Beispiel für latente variable Modelle im Maschinenlernen ist das Thema, das ein statistisches Modell für die Erstellung der Wörter (observierte Variablen) in dem Dokument ist, das auf dem Thema (verschiedene Variablen) des Dokuments basiert. In dem Modell werden die Wörter in dem Dokument nach verschiedenen statistischen Parametern erzeugt, wenn das Thema des Dokuments geändert wird. Es wird gezeigt, dass die Methode der Momente (Stärkungstechniken) die Parameter einer großen Klasse latenten variabler Modelle unter einigen Annahmen konsequent wiedererlangt. Erwartet -maximisierungsgorithmus (EM) ist auch eines der praktischsten Methoden für das Erlernen latenten variabler Modelle. Jedoch kann es in der lokalen Optik bleiben, und es ist nicht garantiert, dass der Algorithmus mit den wirklich unbekannten Parametern des Modells übereinstimmen wird. Im Gegensatz dazu wird die globale Konvergenz unter bestimmten Bedingungen gewährleistet. Neuralnetze Grundzüge Erster, einiges Votum: Tasks UL-Methoden bereiten in der Regel ein Netzwerk für generative Aufgaben statt die Anerkennung vor, doch können Aufgaben als Beaufsichtigung oder nicht häzymiert werden. Zum Beispiel wurde in den 1980er-Jahren als SL eine handschriftliche Anerkennung eingeführt. Im Jahr 2007 wird die UL verwendet, um das Netz für SL danach zu leiten. Derzeit hat SL seine Position als bessere Methode zur Objektivierung wiedererlangt. Schulung während der Lernphase versucht ein unüberwachtes Netzwerk, die von ihm bereitgestellten Daten zu diktieren und den Fehler in seiner mimicked Output zu verwenden, um sich selbst zu korrigieren (z.B. Gewichte & Verzerrungen). Dies entspricht dem mimischen Verhalten von Kindern, da sie eine Sprache lernen. Manchmal wird der Fehler als geringe Wahrscheinlichkeit ausgedrückt, dass die fehlerhafte Produktion auftritt, oder es könnte sich als instabiler Energiezustand im Netz äußern. Energie Eine Energiefunktion ist eine makroökonomische Maßnahme des Zustands eines Netzes. Diese Analogie mit der Physik orientiert sich an der Analyse einer mikroskopischen Energie eines Gases von der mikroskopischen Wirkung der Partikelbewegung p ∝ WELLdisplaystyle \propto } eE/kT, wo k die Dauer von Poszmann und T Temperatur ist. Im RBM-Netz ist das Verhältnis p = e-E / Z, wo p & E sich über alle möglichen Aktivierungsmuster und Z =  over A l P a t e r s VILLEdisplaystyle \sum {_AllPatterns} e -E(pattern) genauer gesagt, p(a)=e-E(a/ Z, wo ein Aktivierungsmuster aller Neuronen (visible und versteckte) ist. So tragen frühzeitige Neuralnetze den Namen Schneiderzmann Maschinen. Paul Smolensky ruft -E der Harmonie. Ein Netzwerk sucht Energie, die hoch Harmonie ist. Netze Kaszmann und Helmholtz kamen vor Neuralnetz-Formeln, aber diese Netze wurden aus ihren Analysen aufgenommen, so dass diese Netze ihre Namen tragen. Hopfield trug jedoch direkt zur UL bei. Zwischen hier werden die Verteilungen p(x) und q(x) als P und q abgefasst. Geschichte mehr Vokular: Vergleich der spezifischen Netze Hier zeigen wir einige Merkmale jedes Netzes. Ferromagnetismus inspirierte Hopfield-Netze, Poszmann Maschinen und RBMs. Ein Neuron entspricht einer Eisen-Domäne mit binären magnetischen Augenblicken Up und Down, und Neuralverbindungen entsprechen dem Einfluss der Domain. Symmetrische Verbindungen ermöglichen eine globale Energieform. Jedes Land aktualisiert die Standardaktivierungsfunktion. Symmetrische Gewichte garantieren Konvergenz zu einem stabilen Aktivierungsmuster. Hopfield-Netze werden als CAMs genutzt und werden garantiert, sich auf ein bestimmtes Muster zu einigen. Ohne symmetrische Gewichte ist das Netz sehr schwer zu analysieren. Mit der richtigen Energiefunktion wird ein Netz konvergieren. Ziegenzmann Maschinen sind steinige Hopfield-Netze. Ihr staatlicher Wert wird wie folgt aus diesem PDF entnommen: ein binäres neurones Feuer mit der Wahrscheinlichkeit von Bernoulli(1)= 1/3 und Ruhe mit p(0) = 2/3. Ein Proben daraus, indem sie eine UNIFORMLY-Abfallnummer y einnehmen und sie in die inverteilte kumulierte Vertriebsfunktion einbinden, die in diesem Fall der Schritt auf zwei Ebenen ist. Inverse Funktion = { 0, wenn x == 2/3, 1 · Zwei Drittel } Helmholtz Maschinen sind frühzeitige Inspirationen für die unterschiedlichen Autovermieter. Mit den 2 Netzen, die in ein- bis ausländisches Gewicht kombiniert werden, ist die Anerkennung und die Abwärtsgewichtung der Fantasie. Es ist vielleicht das erste Netzwerk, um beides zu tun. Helmholtz funktionierte nicht im Maschinenlernen, sondern inspirierte die Ansicht "statistical inference Engine, deren Funktion darin besteht, wahrscheinliche Ursachen von sensorischen Inputs" (3). die katastrophale binäre Neuronen eine Wahrscheinlichkeit, dass ihr Staat 0 oder 1. In der Regel wird der Dateneintrag nicht als Schicht betrachtet, sondern im Helmholtz-Maschine-Generationsmodus erhält die Datenschicht Beiträge aus der mittleren Schicht getrennte Gewichte für diesen Zweck, so dass sie als Schicht gilt. Dieses Netzwerk verfügt daher über 3 Schichten. variabler Autoencoder (VAE) wird von den Helmholtz-Maschinen inspiriert und verbindet wahrscheinliches Netz mit Neuralnetzen. Ein Autoencoder ist ein 3-schichtiges CAM-Netzwerk, bei dem die mittlere Schicht eine interne Darstellung der Inputmuster sein soll. Die Gewichte sind phi & theta anstelle von W und V als in Helmholtz – ein kosmetischer Unterschied. Das kodierende Neuralnetz ist eine Wahrscheinlichkeitsverteilung qz(z|x) und das Decodernetz ist px(x195z). Diese 2 Netze können hier voll verbunden sein oder eine andere NN-Regelung verwenden. Hebbian Learning, ART, SOM Das klassische Beispiel des unüberwindten Lernens in der Studie über Neuralnetze ist Donald Hebbs Prinzip, das ist Neuronen, die zusammendrahten. In Hebbian Learning wird die Verbindung unabhängig von einem Fehler verstärkt, aber ist ausschließlich eine Funktion des Zufalls zwischen Aktionspotenzialen zwischen den beiden Neuronen. Eine ähnliche Version, die die synaptischen Gewichte moduliert, berücksichtigt die Zeit zwischen den Aktionspotenzialen (Schattung-unabhängige Kunststoffe oder STDP). Hebbian Learning ist hypothetisch, eine Reihe kognitiver Funktionen, wie die Mustererkennung und das erfahrungsgemäße Lernen zu unterbinden. Unter den neuralen Netzmodellen werden die Selbstorganisationskarte (SOM) und die Anpassungstheorie (ART) in unkontrollierten Lernalgorithmen verwendet. Die SOM ist eine topographische Organisation, in der nahe gelegene Standorte in der Karte mit ähnlichen Eigenschaften vertreten sind. Das ART-Modell ermöglicht es, die Anzahl der Cluster mit Problemgröße zu variieren und es dem Nutzer zu ermöglichen, den Grad der Ähnlichkeit zwischen den Mitgliedern derselben Cluster durch eine von Nutzern festgelegte ständige Warnparameter zu kontrollieren. ART-Netze werden für viele Aufgaben der Mustererkennung verwendet, wie die automatische Zielerkennung und die Nutzung von seismischen Signalen. Siehe auch automatisierte maschinelle Lernclusteranalyse Anomaly Erkennungserwartung –maximisierungs-Algorisierungsgorithmus Generative topographische Karte Meta-Learning (Computer Science)Multivariate Analyse Radial-Basis-Basis-Netz Weak-BeaufsichtigungsreferenzenFurther Bousquet, O; von Luxburg, U; Raetsch, G. (2004). Fortgeschrittene Lehrveranstaltungen zum Maschinenlernen.Springer-Verlag.ISBN UV3540231226.Duda, Richard O; Hart, Peter E; Stork, David G. (2001)." Lernen und Clustering. Musterklassifikation (2. Nick.ISBN 0-471-05669-3.Hastie, Trevor; Tibshirani, Robert (2009). Elemente des Statistischen Lernens: Data Mining, Inference und Prognose. New York: Springer.pp.485–586.doi:10.1007/978-0-387-858-7_14.ISBN UV0-387-84857-0.Hinton, Geoffrey; Sejnowski, Terrence J. eds. (1999) Lernen: Stiftungen von Neural Computation.MIT Presse.ISBN 0-262-58168-X. (Dieses Buch konzentriert sich auf unüberwachtes Lernen in Neuralnetzen)