In der Statistik ist die t-Statistik das Verhältnis der Abweichung des geschätzten Wertes eines Parameters von seinem hypothetischen Wert zu seinem Standardfehler. Es wird in Hypothesentests über den T-Test des Schülers verwendet. Die t-Statistik wird in einem t-Test verwendet, um festzustellen, ob die Nullhypothese unterstützt oder abgelehnt wird. Es ist dem Z-Score sehr ähnlich, aber mit dem Unterschied, dass t-statistisch verwendet wird, wenn die Stichprobengröße klein ist oder die Populationsstandardabweichung unbekannt ist. Beispielsweise wird die t-Statistik bei der Schätzung des Bevölkerungsmittels aus einer Stichprobenverteilung von Stichprobenmitteln verwendet, wenn die Populationsstandardabweichung unbekannt ist. Es wird zusammen mit p-Wert auch bei der Durchführung von Hypothesentests verwendet, wo der p-Wert uns sagt, was die Chancen von den Ergebnissen zu geschehen sind. Definition und Merkmale Lassen Sie β ^ {\displaystyle \scriptstyle {\widehat {\beta }}}} ein Schätzer des Parameters β in einem statistischen Modell sein. Eine t-Statistik für diesen Parameter ist dann jede Menge der Form t β ^ = β ^ - β 0 s . e . . {\displaystyle t_{\widehat {\beta {=}\frac {\widehat {\beta }-\beta _0}{\operatorname {s.e} {(\widehat {\beta}}}}}}}}}}}} wobei β0 eine nicht-zufällige, bekannte Konstante ist, die den tatsächlichen unbekannten Parameterwert β, und s . e . ‡ (β ^ ) {\displaystyle \operatorname {s.e} {(\widehat {\beta }}}}} den Standardfehler des Schätzers β ^ {displaystyle \script {style\widehat {\\\beta }}} für β ist. Standardmäßig melden die statistischen Pakete t-statistisch mit β0 = 0 (diese t-statistics werden verwendet, um die Bedeutung des entsprechenden Regressors zu testen). Wird zur Prüfung der Hypothese der Form H0: β = β0 jedoch t-statistisch benötigt, so kann ein nicht-Null β0 verwendet werden. Ist β ^ {\displaystyle \scriptstyle {\widehat {\beta }} ein gewöhnlicher, am wenigsten quadratischer Schätzer im klassischen linearen Regressionsmodell (d.h. mit normal verteilten und homoscedastic Fehlertermen) und ist der wahre Wert des Parameters β gleich β0, so ist die Abtastverteilung der t-Statistik die t-Distribution des Schülers mit (n - k) Freiheitsgraden, In den meisten Modellen ist der Schätzer β ^ {\displaystyle \scriptstyle {\widehat {\beta }}}} für β konsistent und wird asymmetrisch normal verteilt. Ist der tatsächliche Wert des Parameters β gleich β0 und die Menge s . e . ‡ (β ^ ) {\displaystyle \scriptstyle \operatorname {s.e} {(\widehat {\beta }}}})} die asymptotische Varianz dieses Schätzers richtig geschätzt, so wird die t-statistische asymptotisch die Standard-Normalverteilung aufweisen.In einigen Modellen unterscheidet sich die Verteilung der t-Statistik von der normalen Verteilung, auch asymptotisch. Wenn zum Beispiel eine Zeitreihe mit einer Einheitswurzel im erweiterten Dickey-Fuller-Test zurückgeht, wird der Test t-statistic asymptotisch eine der Dickey-Fuller-Verteilungen (abhängig von der Testeinstellung) aufweisen. Am häufigsten werden t-Statistiken in den t-Tests des Schülers, einer Form von statistischen Hypothesentests und in der Berechnung bestimmter Vertrauensintervalle verwendet. Die Haupteigenschaft der t-Statistik ist, dass sie eine entscheidende Größe ist – während sie in Bezug auf das Probenmittel definiert ist, hängt ihre Stichprobenverteilung nicht von den Bevölkerungsparametern ab und kann daher unabhängig davon verwendet werden, was diese sein können. Man kann auch einen Rest durch die Probennormalabweichung unterteilen: g ( x, X) = x - X ̄ s {\displaystyle g(x,X)={\frac x-{\overline X}}{s, um eine Schätzung für die Anzahl der Standardabweichungen zu berechnen, ist eine bestimmte Probe von dem Mittelwert, als Beispielversion eines z-Kerns, der z-Kern, der die Populationsparameter benötigt. Prädiktion Bei einer normalen Verteilung N (μ , σ 2 ) ist die t-Statistik einer zukünftigen Beobachtung X n + 1 , {\displaystyle X_{n+1}, nachdem man n Beobachtungen gemacht hat, eine ancilläre Statistik – eine zentrale Größe (nicht von den σ-Werten abhängig) Dies ermöglicht es, über die folgende t-Verteilung ein häufigistisches Prädiktionsintervall (ein vorhersagendes Vertrauensintervall) zu berechnen: X n + 1 - X ̄ n s n 1 + n - 1 T n - 1 {\displaystyle {\frac X_{n+1){\overline ! - Nein. T^{n-1} Lösungsmittel für X n + 1 {\displaystyle X_{n+1} liefert die Prädiktionsverteilung X ̄ n + s n 1 + n - 1 ⋅ T n - 1 {\displaystyle {\overline ) 1+n^^^ T^{n-1}, aus denen man Vorhersage-Konfidenzintervalle berechnen kann – bei einer Wahrscheinlichkeit p kann man Intervalle so berechnen, dass 100p% der Zeit, die nächste Beobachtung X n + 1 {\displaystyle X_{n+1} in diesem Intervall fallen wird. Geschichte Der Begriff t-Statistik ist von "hypothesis test statistic" abgekürzt. In der Statistik wurde die t-Distribution erstmals 1876 von Helmert und Lüroth als Posterior-Distribution abgeleitet. Die t-Distribution erschien auch in allgemeiner Form als Pearson Type IV Distribution in Karl Pearsons 1895 Papier.Der T-Distribution, auch als Student's T Distribution bekannt, erhält jedoch seinen Namen von William Sealy Gosset, der zuerst das Ergebnis in Englisch in seinem 1908-Dokument mit dem Titel "The Probable Error of a Mean" (in Biometrika) mit seinem Pseudonym Student veröffentlichte, weil sein Arbeitgeber bevorzugt ihre Mitarbeiter zu verwenden Stift Namen, wenn wissenschaftliche Papiere anstelle ihres echten Namens, so benutzte er den Namen Student seine Identität zu verstecken. Gosset arbeitete bei der Guinness Brewery in Dublin, Irland und interessierte sich für die Probleme kleiner Proben – zum Beispiel die chemischen Eigenschaften von Gerste, wo Probengrößen bis zu 3 sein könnten. Eine zweite Version der Ethymologie des Begriffs Student ist, dass Guinness nicht wollte, dass ihre Konkurrenten wissen, dass sie den t-Test verwendet haben, um die Qualität des Rohstoffs zu bestimmen. Obwohl es William Gosset war, nach dem der Begriff Student gestiftet wird, war es tatsächlich durch die Arbeit von Ronald Fisher, dass die Verteilung wurde bekannt als "Student's Distribution" und "Student's t-test" Sind die Populationsparameter bekannt, so kann man statt der Berechnung der t-Statistik den z-Score berechnen; analog, anstatt einen t-Test zu verwenden, verwendet man einen z-Test. Dies ist selten außerhalb der standardisierten Tests. Restbetrag: Bei der Regressionsanalyse variieren die Standardfehler der Schätzwerte an unterschiedlichen Datenpunkten (vergleiche die mittleren gegen Endpunkte einer einfachen linearen Regression), und so muss man die unterschiedlichen Reste durch unterschiedliche Schätzwerte für den Fehler unterteilen, was sogenannte studentisierte Restbestände ergibt. Siehe auch F-test t2-statistic Student's T-Distribution Student's t-test Hypothese testen Falt-t und Halb-t Distributionen Chi-Quadrat Verteilung Referenzen =Externe Links ==