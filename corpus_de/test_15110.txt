Data Mining ist ein Prozess der Extraktion und Entdeckung von Mustern in großen Datensätzen mit Methoden an der Schnittstelle von maschinellem Lernen, Statistiken und Datenbanksystemen. Data Mining ist ein interdisziplinäres Teilgebiet der Informatik und Statistik mit einem Gesamtziel, Informationen (mit intelligenten Methoden) aus einem Datensatz zu extrahieren und die Informationen in eine verständliche Struktur für den weiteren Einsatz zu transformieren. Data Mining ist der Analyseschritt der "Wissens-Erkennung in Datenbanken" oder KDD. Neben dem rohen Analyseschritt geht es auch um Datenbank- und Datenmanagementaspekte, Datenvorverarbeitung, Modell- und Inferenzbetrachtungen, Interessantkeitsmetriken, Komplexitätsbetrachtungen, Nachbearbeitung entdeckter Strukturen, Visualisierung und Online-Updates. Der Begriff "Datenabbau" ist ein Fehlrechner, denn das Ziel ist die Extraktion von Mustern und Kenntnissen aus großen Datenmengen, nicht die Gewinnung von Daten selbst. Es ist auch ein Schlagwort und wird häufig auf jede Form von groß angelegten Daten oder Informationsverarbeitung (Sammlung, Extraktion, Lagerung, Analyse und Statistiken) sowie jede Anwendung von Computer-Entscheidungsunterstützungssystem, einschließlich künstlicher Intelligenz (z.B. maschinelles Lernen) und Business Intelligence angewendet. Das Buch Data Mining: Praktische Werkzeuglernwerkzeuge und Techniken mit Java (die meist maschinelles Lernmaterial abdeckt) wurden ursprünglich nur als praktisches maschinelles Lernen bezeichnet, und der Begriff Data Mining wurde nur aus Marketinggründen hinzugefügt. Oft sind die allgemeineren Begriffe (große Skala) Datenanalyse und Analytik - oder, wenn es um konkrete Methoden, künstliche Intelligenz und maschinelles Lernen geht - angemessener. Die eigentliche Data Mining Aufgabe ist die semi-automatische oder automatische Analyse großer Datenmengen, um bisher unbekannte, interessante Muster wie Gruppen von Datensätzen (Clusteranalyse,) ungewöhnliche Aufzeichnungen (Anomalie-Erkennung) und Abhängigkeiten (association rule Mining, sequential pattern mine) zu extrahieren. Dabei werden in der Regel Datenbanktechniken wie Raumindizes verwendet. Diese Muster sind dann als eine Art Zusammenfassung der Eingangsdaten zu erkennen und können in weiterer Analyse oder beispielsweise in maschinellem Lernen und vorausschauender Analytik verwendet werden. Beispielsweise könnte der Data Mining-Schritt mehrere Gruppen in den Daten identifizieren, die dann verwendet werden können, um genauere Vorhersageergebnisse durch ein Entscheidungsunterstützungssystem zu erhalten. Weder die Datenerhebung, Datenaufbereitung, noch Ergebnisinterpretation und -berichterstattung sind Teil des Data Mining-Schrittes, sondern gehören als zusätzliche Schritte zum gesamten KDD-Prozess. Der Unterschied zwischen Datenanalyse und Datenabbau besteht darin, dass die Datenanalyse verwendet wird, um Modelle und Hypothesen auf dem Datensatz zu testen, z.B. die Wirksamkeit einer Marketingkampagne zu analysieren, unabhängig von der Datenmenge; im Gegensatz dazu verwendet der Datenabbau maschinelles Lernen und statistische Modelle, um geheime oder versteckte Muster in einem großen Datenvolumen aufzudecken. Die damit verbundenen Begriffe Datenüberflutung, Datenfischerei und Datenüberschreitung beziehen sich auf die Verwendung von Data Mining-Methoden zur Stichprobe von Teilen eines größeren Bevölkerungsdatensatzes, die zu klein sind oder sein können, um zuverlässige statistische Inferenzen über die Gültigkeit der gefundenen Muster zu treffen. Diese Methoden können jedoch bei der Erstellung neuer Hypothesen verwendet werden, um gegen die größeren Datenpopulationen zu testen. Etymologie In den 1960er Jahren nutzten Statistiker und Ökonomen Begriffe wie Datenfischen oder Daten, die erforschten, um sich auf die schlechten Praktiken der Analyse von Daten ohne eine a-priorie Hypothese zu beziehen. Der Begriff "Datenbergbau" wurde in ähnlich kritischer Weise von dem Wirtschaftswissenschaftler Michael Lovell in einem Artikel verwendet, der 1983 im Review of Economic Studies veröffentlicht wurde. Lovell deutet darauf hin, dass die Praxis "Massaraden unter einer Vielzahl von Alias, von Experimenten (positive) bis zum Fischen oder Schnauren (negativ). Der Begriff Data Mining erschien um 1990 in der Datenbank Gemeinschaft, in der Regel mit positiven Konnotationen. Für eine kurze Zeit in den 1980er Jahren wurde ein Begriff "Datenbankbergbau"TM verwendet, aber da es von HNC, einem San Diego-basierten Unternehmen, gekennzeichnet wurde, um ihre Datenbank Bergbau-Arbeitsplatz zu errichten; Forscher wandten sich folglich an den Datenbergbau. Andere Begriffe sind Datenarchäologie, Informationsernte, Informationsentdeckung, Wissensextraktion usw. Gregory Piatetsky-Shapiro prägte den Begriff "Wissen in Datenbanken" für den ersten Workshop zum gleichen Thema (KDD-1989) und dieser Begriff wurde in der KI- und maschinellen Lerngemeinschaft populärer.Der Begriff Data Mining wurde jedoch in den Geschäfts- und Pressegemeinden populärer. Derzeit werden die Begriffe Data Mining und Wissen Entdeckung austauschbar verwendet. In der akademischen Gemeinschaft begannen 1995 die großen Foren für die Forschung, als die erste Internationale Konferenz über Data Mining und Knowledge Discovery (KDD-95) unter der Schirmherrschaft von AAAI in Montreal begann. Es wurde von Usama Fayyyad und Ramasamy Uthurusamy mitgeführt. Ein Jahr später, 1996, startete Usama Fayyad die Zeitschrift von Kluwer namens Data Mining and Knowledge Discovery als Gründerredakteur. Später startete er den SIGKDD Newsletter SIGKDD Explorations. Die KDD Die internationale Konferenz wurde mit einer Abnahmerate von Forschungspapiereinträgen unter 18 % zur primär höchsten Qualitätskonferenz im Datenbergbau. Die Zeitschrift Data Mining and Knowledge Discovery ist die primäre Forschungszeitschrift des Bereichs. Hintergrund Die manuelle Extraktion von Mustern aus Daten ist seit Jahrhunderten aufgetreten. Frühe Methoden zur Identifizierung von Mustern in Daten umfassen Bayes' Theorem (1700s) und Regressionsanalyse (1800s). Die Verbreitung, Ubiquität und zunehmende Leistungsfähigkeit der Computertechnologie haben die Datenerfassung, Speicherung und Manipulationsfähigkeit drastisch erhöht. Da Datensätze in Größe und Komplexität gewachsen sind, wurde die direkte Datenanalyse zunehmend durch indirekte, automatisierte Datenverarbeitung verstärkt, unterstützt durch andere Entdeckungen in der Informatik, speziell im Bereich des maschinellen Lernens, wie neuronale Netzwerke, Clusteranalyse, genetische Algorithmen (1950s), Entscheidungsbäume und Entscheidungsregeln (1960s) und Unterstützungsvektormaschinen (1990s). Data Mining ist der Prozess der Anwendung dieser Methoden mit der Absicht, versteckte Muster aufzudecken. in großen Datensätzen. Es überbrückt die Lücke von angewandten Statistiken und künstlicher Intelligenz (die in der Regel den mathematischen Hintergrund zur Verfügung stellen) in der Datenbankverwaltung, indem die Art und Weise genutzt wird, wie Daten in Datenbanken gespeichert und indexiert werden, um die tatsächlichen Lern- und Entdeckungsalgorithmen effizienter auszuführen, so dass diese Methoden auf immer größere Datensätze angewendet werden können. Verfahren Die Wissensentdeckung in den Datenbanken (KDD) Prozess wird allgemein mit den Stadien definiert: Auswahlvorverarbeitung Transformation Data Mining Interpretation/Evaluation. Es gibt jedoch in vielen Variationen zu diesem Thema, wie z.B. das branchenübergreifende Standardverfahren für den Datenbergbau (CRISP-DM), das sechs Phasen definiert: Unternehmensverständigung Datenverstehen Datenvorbereitung Modellierung Evaluation Deploymentor ein vereinfachtes Verfahren wie (1) Vorverarbeitung, (2) Data Mining, und (3) Ergebnisse Validierung. Die in den Jahren 2002, 2004, 2007 und 2014 durchgeführten Polls zeigen, dass die CRISP-DM-Methodik die führende Methode ist, die von den Datenbergleuten verwendet wird. Der einzige andere in diesen Umfragen genannte Data Mining-Standard war SEMMA. Jedoch, 3-4 mal so viele Menschen mit CRISP-DM berichtet. Mehrere Forscherteams haben Berichte über Data Mining-Prozessmodelle veröffentlicht, und Azevedo und Santos führten 2008 einen Vergleich von CRISP-DM und SEMMA durch. Bevor Datenbergungsalgorithmen verwendet werden können, muss ein Zieldatensatz montiert werden. Da der Datenabbau nur tatsächlich vorhandene Muster in den Daten aufdecken kann, muss der Zieldatensatz groß genug sein, um diese Muster zu enthalten, während er innerhalb einer akzeptablen Frist knapp genug bleibt. Eine gemeinsame Datenquelle ist ein Daten- oder Datenlager. Die Vorverarbeitung ist unerlässlich, um die multivariaten Datensätze vor dem Datenabbau zu analysieren. Der Zielsatz wird dann gereinigt. Die Datenreinigung entfernt die Beobachtungen, die Lärm enthalten, und die mit fehlenden Daten. Data Mining Data Mining umfasst sechs gemeinsame Aufgabenklassen: Anomalie-Erkennung (Ausreiß-/Wechsel-/Abweichungserkennung)– Die Identifizierung ungewöhnlicher Datensätze, die interessant sein könnten oder Datenfehler, die eine weitere Untersuchung erfordern. Assoziationsregel Lernen (abhängige Modellierung) – Suchen nach Beziehungen zwischen Variablen. Zum Beispiel könnte ein Supermarkt Daten über Kundenkauf Gewohnheiten sammeln. Mit Hilfe von Assoziierungslehre kann der Supermarkt bestimmen, welche Produkte häufig zusammen gekauft werden und diese Informationen für Marketingzwecke verwenden. Dies wird manchmal als Marktkorbanalyse bezeichnet. Clustering – ist die Aufgabe, Gruppen und Strukturen in den Daten zu entdecken, die in gewisser Weise oder ähnlich sind, ohne bekannte Strukturen in den Daten zu verwenden. Klassifizierung – ist die Aufgabe, die bekannte Struktur für neue Daten zu verallgemeinern. Beispielsweise könnte ein E-Mail-Programm versuchen, eine E-Mail als legitim oder als Spam einzustufen.Regression – versucht, eine Funktion zu finden, die die Daten mit dem geringsten Fehler, das heißt, zur Schätzung der Beziehungen zwischen Daten oder Datensätzen, modelliert. Summarisierung – Bereitstellung einer kompakteren Darstellung des Datensatzes, einschließlich Visualisierung und Report-Generation. Ergebnisse Validierung Data Mining kann unbeabsichtigt missbräuchlich verwendet werden, und kann dann Ergebnisse hervorbringen, die signifikant zu sein scheinen; die aber nicht wirklich zukünftiges Verhalten vorhersagen und nicht auf einer neuen Datenprobe wiedergegeben werden können und wenig Gebrauch haben. Oft ergibt sich dies durch die Untersuchung zu vieler Hypothesen und nicht durch die Durchführung richtiger statistischer Hypothesentests. Eine einfache Version dieses Problems beim maschinellen Lernen ist als Überarbeitung bekannt, aber das gleiche Problem kann in unterschiedlichen Phasen des Prozesses auftreten und somit kann ein Zug/Test-Split - gegebenenfalls überhaupt - nicht ausreichen, um dies zu verhindern. Der letzte Schritt der Wissensentdeckung aus Daten besteht darin, zu überprüfen, ob die von den Data Mining Algorithmen erzeugten Muster im breiteren Datensatz auftreten. Nicht alle von Data Mining Algorithmen gefundenen Muster sind unbedingt gültig. Es ist üblich, dass Datenabbaualgorithmen Muster im Trainingsset finden, die nicht im allgemeinen Datensatz vorhanden sind. Das nennt man Overfitting. Um dies zu überwinden, verwendet die Auswertung einen Testsatz von Daten, auf die der Data Mining-Algorithmus nicht geschult wurde. Auf diesen Testsatz werden die erlernten Muster aufgetragen und der resultierende Ausgang mit der gewünschten Leistung verglichen. Beispielsweise würde ein Data Mining-Algorithmus, der versucht, Spam von legitimen E-Mails zu unterscheiden, auf einem Trainingssatz von Beispiel-E-Mails trainiert werden. Einmal trainiert, würden die gelernten Muster auf den Test-Set von E-Mails angewendet, auf denen es nicht geschult wurde. Die Genauigkeit der Muster kann dann gemessen werden, wie viele E-Mails sie korrekt klassifizieren. Zur Auswertung des Algorithmus können verschiedene statistische Methoden verwendet werden, wie z.B. ROC-Kurven. Wenn die gelernten Muster nicht den gewünschten Standards entsprechen, ist es dann notwendig, die Vorverarbeitungs- und Datenabbauschritte neu zu bewerten und zu ändern. Wenn die gelernten Muster den gewünschten Standards entsprechen, dann ist der letzte Schritt, die gelernten Muster zu interpretieren und zu Wissen zu machen. Forschung Die führende Berufsorganisation auf dem Gebiet ist die Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD). Seit 1989 veranstaltet diese ACM SIG eine jährliche internationale Konferenz und veröffentlichte ihr Verfahren, und seit 1999 hat sie eine zweijährige akademische Zeitschrift mit dem Titel "SIGKDD Explorations" veröffentlicht. Konferenzen zur Datengewinnung umfassen: CIKM Konferenz – ACM Konferenz zum Informations- und Wissensmanagement Europäische Konferenz zum maschinellen Lernen und Prinzipien und Praxis der Wissensentdeckung in Datenbanken KDD-Konferenz – ACM SIGKDD-Konferenz zur Wissensentdeckung und Data Mining Datenbergbauthemen sind auch auf vielen Datenmanagement/Datenbankkonferenzen wie der ICDE-Konferenz, der SIGMOD-Konferenz und der internationalen Konferenz über sehr große Datenbasisstandards Es gab einige Anstrengungen, Standards für den Data Mining-Prozess zu definieren, zum Beispiel den European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) von 1999 und den Java Data Mining Standard von 2004 (JDM 1.0). Die Entwicklung von Nachfolgern zu diesen Prozessen (CRISP-DM 2.0 und JDM 2.0) war im Jahr 2006 aktiv, hat aber seither gestoppt. JDM 2.0 wurde zurückgezogen, ohne einen endgültigen Entwurf zu erreichen. Für den Austausch der extrahierten Modelle – insbesondere für die Verwendung in der Prädiktivanalyse – ist der Schlüsselstandard die Predictive Model Markup Language (PMML), die eine von der Data Mining Group (DMG) entwickelte und von vielen Data Mining-Anwendungen als Austauschformat unterstützte XML-basierte Sprache ist. Wie der Name schon sagt, umfasst er nur Prädiktionsmodelle, eine bestimmte Data Mining-Aufgabe von hoher Bedeutung für Geschäftsanwendungen. Es wurden jedoch unabhängig von der DMG Erweiterungen zur Abdeckung (z.B.) Subraum-Clusterung vorgeschlagen. Bemerkenswerte Verwendungen Data Mining wird überall dort verwendet, wo heute digitale Daten verfügbar sind. Bemerkenswerte Beispiele für den Datenbergbau finden Sie in der gesamten Wirtschaft, Medizin, Wissenschaft und Überwachung. Datenschutz und Ethik Während der Begriff "Datenabbau" selbst keine ethischen Auswirkungen haben kann, ist er oft mit dem Abbau von Informationen in Bezug auf das Verhalten der Menschen verbunden (ethisch und anders). Die Art und Weise, wie der Datenbergbau genutzt werden kann, kann in einigen Fällen und Kontexten Fragen zu Datenschutz, Rechtmäßigkeit und Ethik stellen. Insbesondere Daten Bergbau-Regierungen oder kommerzielle Datensätze für nationale Sicherheits- oder Strafverfolgungszwecke, wie zum Beispiel im Total Information Awareness Program oder in ADVISE, haben die Datenschutzbelange angesprochen.Datenabbau erfordert Datenvorbereitung, die Informationen oder Muster aufdeckt, die Vertraulichkeit und Datenschutzverpflichtungen beeinträchtigen. Ein gemeinsamer Weg hierfür ist die Datenaggregation. Die Datenaggregation beinhaltet die Zusammenführung von Daten (möglicherweise aus verschiedenen Quellen) in einer Art und Weise, die die Analyse erleichtert (aber auch die Identifizierung von privaten, individuellen Daten, die deduzierbar oder anderweitig sichtbar sind). Dies ist nicht der Datenbergbau an sich, sondern das Ergebnis der Datenvorbereitung vor – und zu Zwecken – der Analyse. Die Bedrohung für die Privatsphäre einer Person kommt ins Spiel, wenn die Daten, einmal zusammengestellt, verursachen den Datenbergarbeiter oder jeder, der Zugriff auf den neu zusammengestellten Datensatz hat, in der Lage, bestimmte Personen zu identifizieren, insbesondere wenn die Daten ursprünglich anonym waren. Es wird empfohlen, sich vor der Erhebung der Daten Folgendes bewusst zu sein: Der Zweck der Datenerhebung und alle (bekannten) Data Mining-Projekte; Wie werden die Daten verwendet; Wer wird in der Lage sein, die Daten zu Bergbau und Nutzung der Daten und deren Derivate; Der Status der Sicherheit um den Zugriff auf die Daten; Wie gesammelte Daten können aktualisiert werden. Die Daten können auch so geändert werden, dass sie anonym werden, so dass Personen nicht leicht identifiziert werden können. Allerdings können sogar anonymisierte Datensätze möglicherweise genug Informationen enthalten, um die Identifizierung von Personen zu ermöglichen, wie es geschah, als Journalisten in der Lage waren, mehrere Personen auf der Grundlage einer Reihe von Suchhistorikern zu finden, die versehentlich von AOL veröffentlicht wurden. Die unbeabsichtigte Offenbarung persönlich identifizierbarer Informationen, die zum Anbieter führt, verletzt Fair Information Practices. Diese Indiskretion kann dem angegebenen Individuum finanzielle, emotionale oder körperliche Schäden verursachen. In einem Fall der Verletzung der Privatsphäre, die Patronen von Walgreens eingereicht eine Klage gegen das Unternehmen im Jahr 2011 für den Verkauf von verschreibungspflichtigen Informationen an Daten Bergbauunternehmen, die wiederum die Daten an Pharmaunternehmen. Die Situation in Europa hat recht starke Datenschutzgesetze, und die Bemühungen sind im Gange, die Rechte der Verbraucher weiter zu stärken. Der U.S.–E.U Safe Harbor Prinzipien, die zwischen 1998 und 2000 entwickelt wurden, zeigen derzeit die europäischen Nutzer effektiv der Nutzung der Privatsphäre durch US-Unternehmen. Infolge der globalen Offenlegung von Edward Snowdens Überwachung gab es eine verstärkte Diskussion, um dieses Abkommen zu widerrufen, da insbesondere die Daten der Nationalen Sicherheitsagentur vollständig ausgesetzt werden und Versuche, eine Einigung mit den Vereinigten Staaten zu erreichen, gescheitert sind. Im Vereinigten Königreich gab es vor allem Fälle von Unternehmen, die Datenbergbau nutzen, um bestimmte Gruppen von Kunden anzusprechen, die sie dazu zwingen, unfair hohe Preise zu zahlen. Diese Gruppen sind in der Regel Menschen mit niedrigeren sozioökonomischen Status, die nicht auf die Art und Weise, wie sie in digitalen Marktplätzen ausgenutzt werden können. Lage in den Vereinigten Staaten In den Vereinigten Staaten wurden Datenschutzbedenken vom US-Kongress über die Durchführung von Regulierungskontrollen wie dem Health Insurance Portability and Accountability Act (HIPAA) angesprochen. Die HIPAA verlangt von Einzelpersonen, ihre "informierte Zustimmung" in Bezug auf die von ihnen bereitgestellten Informationen und ihre beabsichtigte gegenwärtige und zukünftige Nutzung zu geben. Nach einem Artikel in der Biotech Business Week, "["i]n Praxis, HIPAA kann keinen größeren Schutz bieten als die langjährigen Vorschriften in der Forschungsarena", sagt der AAHC. Wichtiger ist, dass das Ziel der Regel des Schutzes durch eine fundierte Zustimmung auf ein Maß der Unverständlichkeit für durchschnittliche Personen nähert. " Dies unterstreicht die Notwendigkeit der Datenanonymität bei der Datenaggregation und Bergbaupraktiken. Die US-amerikanischen Datenschutzgesetze wie die HIPAA und das Family Educational Rights and Privacy Act (FERPA) gelten nur für die spezifischen Bereiche, die jedes Gesetz behandelt. Die Nutzung des Datenbergbaus durch die Mehrheit der Unternehmen in den USA wird durch keine Rechtsvorschriften kontrolliert. Urheberrecht Situation in Europa Unter europäischen Urheberrechts- und Datenbankgesetzen ist der Abbau von in-copyright-Werken (z.B. durch Web-Mining) ohne die Erlaubnis des Urheberrechtsinhabers nicht legal. Wenn eine Datenbank reine Daten in Europa ist, kann es sein, dass es kein Urheberrecht gibt, aber Datenbankrechte können existieren, so dass der Datenabbau den Rechte der geistigen Eigentumsinhaber unterliegt, die durch die Datenbankrichtlinie geschützt sind. Auf Empfehlung der Hargreaves-Bewertung führte dies zur Änderung des Urheberrechtsgesetzes im Jahr 2014, um den Content-Mining als Einschränkung und Ausnahme zu ermöglichen. Das Vereinigte Königreich war nach Japan das zweite Land der Welt, das 2009 eine Ausnahme für den Datenbergbau eingeführt hatte.Aufgrund der Einschränkung der Informationsgesellschaftsrichtlinie (2001) erlaubt die britische Ausnahme jedoch nur den Content Mining für nichtkommerzielle Zwecke. Das Urheberrecht des Vereinigten Königreichs erlaubt es auch nicht, diese Bestimmung durch vertragliche Bedingungen zu überschreiben. Die Europäische Kommission unterstützte 2013 die Stakeholder-Diskussion zum Text- und Datenbergbau unter dem Titel Lizenzen für Europa. Der Fokus auf die Lösung dieser rechtlichen Frage, wie Lizenzierung, anstatt Einschränkungen und Ausnahmen, führte zu Vertretern von Universitäten, Forschern, Bibliotheken, Zivilgesellschaftsgruppen und Open-Access-Verlegern, den Stakeholder-Dialog im Mai 2013 zu verlassen. Die Situation im US-amerikanischen Urheberrechtsgesetz, insbesondere seine Bestimmung zur fairen Nutzung, hält die Rechtmäßigkeit des Content-Minings in Amerika und andere faire Nutzungsländer wie Israel, Taiwan und Südkorea aufrecht. Da der Content-Mining transformativ ist, d.h. er supplant die ursprüngliche Arbeit nicht, wird er als rechtmäßig unter fairem Gebrauch angesehen. Zum Beispiel, im Rahmen der Google-Buch-Berechnung der Vorsitzende Richter auf dem Fall, dass Googles Digitalisierungsprojekt von in-copyright-Bücher war rechtmäßig, zum Teil wegen der transformativen Verwendungen, dass das Digitalisierungsprojekt angezeigt - ein Text- und Datenberging. Software Freie Open-Source-Daten Bergbau Software und Anwendungen Die folgenden Anwendungen sind unter Frei- und Open-Source-Lizenzen erhältlich. Öffentlicher Zugang zu Anwendungsquellencode ist ebenfalls verfügbar. Carrot2:Text und Suchergebnisse Clustering Framework. Chemicalize.org: Eine chemische Struktur Bergmann und Web-Suchmaschine. ELKI:Ein Universitätsforschungsprojekt mit fortschrittlicher Clusteranalyse und in Java-Sprache geschriebenen Outlier-Erkennungsmethoden. GATE: ein Werkzeug für die natürliche Sprachverarbeitung und Sprachentwicklung. KNIME: Der Konstanz Information Miner, ein benutzerfreundlicher und umfassender Datenanalyserahmen. Massive Online Analysis (MOA:) ein Echtzeit- Big Data Stream-Mining mit Konzept Drift-Tool in der Java Programmiersprache. MEPX: Cross-Platform-Tool für Regressions- und Klassifikationsprobleme basierend auf einer Genetischen Programmiervariante. ML-Flex: Ein Softwarepaket, mit dem Benutzer in beliebige Programmiersprachen geschriebene Automaten-Lernpakete von Drittanbietern integrieren können, Klassifikationsanalysen parallel über mehrere Rechenknoten ausführen und HTML-Berichte von Klassifikationsergebnissen erstellen können. mlpack: eine Sammlung von gebrauchsfertigen maschinellen Lernalgorithmen, die in der C+ Sprache geschrieben sind. NLTK (Natural Language Toolkit:) Eine Reihe von Bibliotheken und Programmen für die symbolische und statistische natürliche Sprachverarbeitung (NLP) für die Python-Sprache. OpenNN: Open neural network library. Orange: Eine in der Python-Sprache geschriebene komponentenbasierte Data Mining- und Machine Learning-Software-Suite. PSPP: Data Mining und Statistiken Software unter dem GNU Project ähnlich SPSS R: Eine Programmiersprache und Software-Umgebung für statistisches Computing, Data Mining und Grafik. Es ist Teil des GNU-Projekts. Scikit-learn: eine Open-Source-Maschinen-Lernbibliothek für die Programmiersprache Python Torch: Eine Open-Source-Tief-Learning-Bibliothek für die Lua-Programmiersprache und den wissenschaftlichen Computing-Framework mit breiter Unterstützung für Machine Learning Algorithmen. UIMA: Die UIMA (Unstructured Information Management Architecture) ist ein Komponentenrahmen für die Analyse unstrukturierter Inhalte wie Text, Audio und Video – ursprünglich von IBM entwickelt. Weka: Eine Reihe von maschinellen Lernsoftware-Anwendungen in der Java Programmiersprache geschrieben. proprietäre Datamining Software und Anwendungen Folgende Anwendungen sind unter proprietären Lizenzen erhältlich. Angoss KnowledgeSTUDIO: Data Mining Tool LIONsolver: eine integrierte Software-Anwendung für Data Mining, Business Intelligence und Modellierung, die den Ansatz Learning and Intelligent OptimizatioN (LION) implementiert. Megaputer Intelligence: Daten- und Textabbausoftware wird PolyAnalyst genannt. Microsoft Analysis Services: Data Mining Software von Microsoft zur Verfügung gestellt. NetOwl: Suite von mehrsprachigen Text- und Entity-Analyse-Produkten, die den Datenabbau ermöglichen. Daten von Oracle Bergbau: Data Mining Software von Oracle Corporation. PSeven: Plattform zur Automatisierung von Engineering-Simulation und -Analyse, multidisziplinäre Optimierung und Datenabbau von DATADVANCE. Qlucore Omics Explorer: Data Mining Software. RapidMiner: Eine Umgebung für maschinelles Lernen und Data Mining Experimente. SAS Enterprise Miner: Data Miner Software des SAS Institute. SPSS Modeler: Data Mining Software von IBM zur Verfügung gestellt.STATISTIK Data Miner: Data Miner Software zur Datengewinnung von StatSoft. Tanagra: Visualisierungsorientierte Data Mining Software, auch für die Lehre. Vertica: Data Mining Software von Hewlett-Packard zur Verfügung gestellt. Google Cloud Platform: automatisierte benutzerdefinierte ML-Modelle von Google verwaltet. Amazon SageMaker: Managed Service von Amazon zur Erstellung und Produktion von kundenspezifischen ML-Modellen. Siehe auch Methoden Anwendungsbereiche Anwendungsbeispiele Weitere Informationen über die Gewinnung von Informationen aus Daten (im Gegensatz zur Analyse von Daten) siehe: Andere RessourcenInternational Journal of Data Warehousing and Mining ReferencesWeitere Informationen Externe Links Knowledge Discovery Software bei Curlie Data Mining Tool Vendors bei Curlie