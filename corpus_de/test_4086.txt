In der Informatik ist die rechnerische Lerntheorie (oder nur Lerntheorie) ein Unterfeld der künstlichen Intelligenz, die dem Studium der Gestaltung und Analyse von maschinellen Lernalgorithmen gewidmet ist. Überblick Theoretische Ergebnisse im maschinellen Lernen beschäftigen sich hauptsächlich mit einer Art induktives Lernen, genannt betreutes Lernen. Im beaufsichtigten Lernen wird ein Algorithmus Proben gegeben, die auf sinnvolle Weise gekennzeichnet werden. Beispielsweise können die Proben Beschreibungen von Pilzen sein, und die Etiketten könnten sein, ob die Pilze essbar sind oder nicht. Der Algorithmus nimmt diese zuvor markierten Proben und verwendet sie, um einen Klassifikator zu induzieren. Dieser Klassifikator ist eine Funktion, die Etiketten Proben zuordnet, einschließlich Proben, die zuvor nicht vom Algorithmus gesehen wurden. Ziel des überwachten Lernalgorithmus ist es, einige Maß an Leistung zu optimieren, wie die Anzahl der Fehler, die an neuen Proben gemacht wurden, zu minimieren. Neben den Leistungsgrenzen untersucht die rechnerische Lerntheorie die Zeitkomplexität und Machbarkeit des Lernens. In der rechnerischen Lerntheorie wird eine Berechnung als machbar angesehen, wenn sie in der Polynomzeit durchgeführt werden kann. Es gibt zwei Arten von Zeit Komplexität Ergebnisse: Positive Ergebnisse – Zeigt, dass eine bestimmte Klasse von Funktionen in der Polynomzeit erlernbar ist. Negative Ergebnisse – Es zeigt, dass bestimmte Klassen nicht in polynomischer Zeit gelernt werden können. Negative Ergebnisse setzen sich oft auf allgemein geglaubte, aber dennoch unbewiesene Annahmen, wie: Computational Komplexität – P ≠ NP (das P versus NP Problem);Cryptographic – Einwegfunktionen existieren. Es gibt verschiedene Ansätze zur rechnerischen Lerntheorie, die auf unterschiedlichen Annahmen über die in Bezug auf die Verallgemeinerung von begrenzten Daten verwendeten Inferenzprinzipien beruhen. Dazu gehören unterschiedliche Definitionen der Wahrscheinlichkeit (siehe Häufigkeitswahrscheinlichkeit, Bayesische Wahrscheinlichkeit) und unterschiedliche Annahmen bei der Probenerzeugung. Die verschiedenen Ansätze umfassen: Genaues Lernen, vorgeschlagen von Dana Angluin; Wahrscheinlich ungefähr korrektes Lernen (PAC Lernen,) vorgeschlagen von Leslie Valiant; VC Theorie, vorgeschlagen von Vladimir Vapnik und Alexey Chervonenkis; Bayesische Inferenz; Algorithmische Lerntheorie, von der Arbeit von E. Mark Gold; Online-Maschinenlernen, von der Arbeit von Nick Littlestone. Während sein Hauptziel darin besteht, abstraktes Lernen zu verstehen, hat die rechnerische Lerntheorie zur Entwicklung praktischer Algorithmen geführt. Zum Beispiel, PAC-Theorie inspiriert Boosting, VC-Theorie führte zu Unterstützung Vektormaschinen, und Bayesian Inference führte zu Glaubensnetzwerken. Siehe auch Grammar Induction Information Theorie Stabilität (Lerntheorie) Fehlertoleranz (PAC Lernen) Referenzen Umfragen Angluin, D. 1992.Wettbewerbliche Lerntheorie: Umfrage und ausgewählte Bibliographie. In den Proceedings des Twenty-Fourth Annual ACM Symposium on Theory of Computing (Mai 1992), Seiten 351-369. http://portal.acm.org/cit.cfm?id=129712.129746D Haussler. Wahrscheinlich ungefähr korrektes Lernen. In AAAI-90 Proceedings of the Eight National Conference on Artificial Intelligence, Boston, MA, Seiten 1101–1108. American Association for Artificial Intelligence, 1990.http://citeseer.ist.psu.edu/haussler90probable.html VC-Dimension V. Vapnik und A. Chervonenkis. Auf der einheitlichen Konvergenz der relativen Frequenzen von Ereignissen zu ihren Wahrscheinlichkeiten. Theory of Probability and Its Applications, 16(2):264–280, 1971. Featureauswahl A. Dhagat und L. Hellerstein, "PAC learning with irrelevant attributes", in "Proceedings of the IEEE Symp.on Foundation of Computer Science", 1994.http://citeseer.ist.psu.edu/dhagat94pac.html Induktive Inferenz Gold, E. Mark (1967)."Language-Identifizierung in der Grenze" (PDF). Information und Control.10 (5:) 447–474. doi:10.1016/S0019-9958(67)91165-5 Optimal O notation learning Oded Goldreich, Dana Ron. Über allgemeine Lernalgorithmen.http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.2224 Negative Ergebnisse M. Kearns und Leslie Valiant. 1989.Kryptographische Einschränkungen des Lernens boolean Formeln und endlich automata. In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, Seiten 433–444, New York.ACM.http://citeseer.ist.psu.edu/kearns89cryptographic.html Boosting (Maschine Lernen)Robert E. Schapire. Die Stärke der schwachen Lernfähigkeit. Machine Learning, 5(2):197–227, 1990 http://citeseer.ist.psu.edu/schapire90strength.html Occam Learning Blumer, A.; Ehrenfeucht, A.; Haussler, D.; Warmuth, M. K. Occams Rasiermesser Inf.Proc.Lett.24, 377–380, 1987.Blumer, A.; Ehrenfeucht, A.; Haussler, D.; Warmuth, M. K. Lernfähigkeit und die Vapnik-Chervonenkis Dimension. Journal of the ACM, 36(4):929–865, 1989. Wahrscheinlich ungefähr korrektes Lernen L. Valiant. A Theorie des Lernenden. Kommunikation der ACM, 27(11):1134–1142, 1984. Fehlertoleranz Michael Kearns und Ming Li.Learning in Gegenwart von schädlichen Fehlern. SIAM Journal on Computing, 22(4):807–837, August 1993.http://citeseer.ist.psu.edu/kearns93learning.htmlKearns, M. (1993). Effiziente geräuschtolerantes Lernen aus statistischen Abfragen. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, Seiten 392–401.http://citeseer.ist.psu.edu/kearns93ffi.html Equivalenz D.Haussler, M.Kearns, N.Littlestone und M. Warmuth, Equivalenz von Modellen für polynome Erlernbarkeit, Proc.1stACM Workshop on Computational Learning Theory, (1988) 42-55.Pitt, L.; Warmuth, M. K.2290."Prediction-Preserving Reducibility".Journal of Computer and System Sciences Verteilung Lerntheorie Externe Links Grundlagen der Bayesischen Inferenz