Eine Filterblase ist ein von der Internet-Aktivistin Eli Pariser geprägter Begriff, der sich auf einen Zustand der intellektuellen Isolation bezieht, der sich aus personalisierten Suchvorgängen ergeben kann, wenn ein Website-Algorithmus selektiv errät, welche Informationen ein Nutzer anhand von Informationen über den Nutzer, wie Standort, vergangene Klick-Behavior und Suchhistorie sehen möchte. Infolgedessen werden die Nutzer von Informationen getrennt, die mit ihren Standpunkten nicht übereinstimmen, sie effektiv in ihren eigenen kulturellen oder ideologischen Blasen isolieren. Die Entscheidungen dieser Algorithmen sind nicht transparent. Hauptbeispiele sind Google Personalisierte Suchergebnisse und Facebook personalisierte Nachrichten-stream. Der Blaseneffekt kann negative Auswirkungen auf den bürgerlichen Diskurs haben, nach Pariser, aber kontrastierende Ansichten betrachten den Effekt als minimal und adressierbar. Die Ergebnisse der US-Präsidentschaftswahlen im Jahr 2016 wurden mit dem Einfluss von Social Media-Plattformen wie Twitter und Facebook verbunden, und als Ergebnis haben die Auswirkungen des "Filterblase"-Phänomens auf die Benutzerexposition mit gefälschten Nachrichten und Echokammern in Frage gestellt, mit vielen besorgt, dass das Phänomen kann die Demokratie und das Wohlergehen beeinträchtigen, indem die Auswirkungen von Fehlinformationen verschlimmert werden.(Technologie wie Social Media) Es ist super wichtig. Es stellte sich heraus, dass es mehr von einem Problem als ich, oder viele andere, hätte erwartet.“ Konzept Der Begriff wurde von der Internet-Aktivistin Eli Pariser um 2010 prämiert und in seinem gleichnamigen Buch 2011 diskutiert; nach Pariser werden die Nutzer weniger in Konflikt geraten und intellektuell in ihrer eigenen Informationsblase isoliert. Er verwandte ein Beispiel, in dem ein Benutzer Google nach BP suchte und bekam Investitionsnachrichten über British Petroleum, während ein anderer Sucher bekam Informationen über die Deepwater Horizon Ölspritze, und bemerkte, dass die beiden Suchergebnisse Seiten waren "strikingly different". Pariser definierte sein Konzept einer Filterblase formaler als "das persönliche Ökosystem von Informationen, die von diesen Algorithmen behandelt wurden". Die vergangene Browser- und Suchhistorie eines Internetnutzers wird im Laufe der Zeit aufgebaut, wenn sie das Interesse an Themen angeben, indem sie "Links anklicken, Freunde sehen, Filme in [ihre] Warteschlange setzen, Nachrichtengeschichten lesen", und so weiter. Eine Internet-Firma verwendet diese Informationen dann, um Werbung für den Benutzer anzusprechen, oder bestimmte Arten von Informationen erscheinen prominenter in Suchergebnisseseiten. Dieser Prozess ist nicht zufällig, da er unter einem dreistufigen Prozess arbeitet, pro Pariser, der sagt: "Erst, Sie herausfinden, wer die Menschen sind und was sie mögen. Dann bieten Sie ihnen Inhalte und Dienstleistungen, die am besten zu ihnen passen. Endlich, Sie stimmen, um die Passform genau richtig zu bekommen. Ihre Identität prägt Ihre Medien." Pariser berichtet auch: Laut einer Wall Street Journal Studie installieren die Top-Fünfzig Internet-Seiten, von CNN bis Yahoo bis MSN, im Durchschnitt 64 datenbeladene Cookies und persönliche Tracking-Beacons. Suchen Sie nach einem Wort wie Depression auf Dictionary.com, und die Website installiert bis zu 223 Tracking-Cookies und Beacons auf Ihrem Computer, so dass andere Websites können Sie mit Antidepressiva ansprechen. Teilen Sie einen Artikel über das Kochen auf ABC News, und Sie können im Internet durch Anzeigen für Teflon-beschichtete Töpfe gejagt werden. Offen – auch für einen Augenblick – eine Seite, auf der Zeichen aufgelistet werden, dass Ihr Ehepartner betrügen und bereiten kann, mit DNA Vaterschaft-Test-Anzeigen verfolgt werden. Durch den Zugriff auf die Daten von Link-Klicks, die durch Standort-Verkehrsmessungen angezeigt werden, wird festgestellt, dass Filterblasen kollektiv oder individuell sein können. Ab 2011 hatte ein Ingenieur Pariser gesagt, dass Google 57 verschiedene Datenstücke betrachtete, um die Suchergebnisse des Nutzers persönlich zu gestalten, einschließlich nicht-Cookie-Daten wie die Art des verwendeten Computers und die physische Lage des Nutzers. Andere Begriffe wurden verwendet, um dieses Phänomen zu beschreiben, darunter "ideologische Rahmen" und "die figurative Sphäre um Sie herum, wie Sie das Internet suchen". Ein verwandter Begriff, "Echokammer", wurde ursprünglich auf Nachrichtenmedien angewendet, wird nun aber auch auf soziale Medien angewendet. Parisers Idee der Filterblase wurde nach dem TED-Gespräch, den er im Mai 2011 gab, populär, in dem er Beispiele dafür gibt, wie Filterblasen funktionieren und wo sie gesehen werden können. In einem Test, der den Filterblaseneffekt demonstrieren wollte, bat Pariser mehrere Freunde, nach dem Wort Ägypten auf Google zu suchen und ihm die Ergebnisse zu schicken. Im Vergleich zu zwei der ersten Seiten der Freunde der Ergebnisse, während es Überschneidungen zwischen ihnen zu Themen wie Nachrichten und Reisen, ein Freund Ergebnisse prominent enthalten Links zu Informationen über die dann-ongoing ägyptische Revolution von 2011, während der andere Freund die erste Seite der Ergebnisse nicht enthielt solche Links. In The Filter Bubble warnt Pariser, dass ein potenzieller Nachteil der gefilterten Suche ist, dass es "verschließt uns zu neuen Ideen, Themen und wichtigen Informationen", und "erzeugt den Eindruck, dass unser enges Selbstinteresse ist alles, was existiert". Filterblasen sind seiner Ansicht nach sowohl für Individuen als auch für die Gesellschaft potenziell schädlich. Er kritisierte Google und Facebook, um Nutzer "zu viel Süßigkeiten, und nicht genug Karotten" anzubieten. Er warnte, dass "unsichtbare algorithmische Bearbeitung des Webs" unsere Belichtung auf neue Informationen beschränken und unsere Aussichten verengen kann. Laut Pariser schließen die schädlichen Auswirkungen von Filterblasen die allgemeine Gesellschaft in dem Sinne an, dass sie die Möglichkeit haben, "die Bürgerdiskurs zu untergraben" und die Menschen für "Propaganda und Manipulation" anfälliger zu machen. Er schrieb: Eine Welt, die aus der Vertrauten aufgebaut ist, ist eine Welt, in der es nichts zu lernen gibt... (da es da ist) unsichtbare Autopropaganda, die uns mit eigenen Ideen indoktriniert. Viele Menschen sind sich nicht bewusst, dass Filterblasen sogar existieren. Dies ist in einem Artikel über The Guardian zu sehen, der die Tatsache erwähnte, dass "mehr als 60% der Facebook-Nutzer sind völlig unwissen über jede Heilung auf Facebook überhaupt, glauben, statt dass jede einzelne Geschichte von ihren Freunden und gefolgten Seiten erschien in ihrem Nachrichten-Feed." Eine kurze Erklärung dafür, wie Facebook entscheidet, was auf dem News-Feed eines Nutzers geht, ist ein Algorithmus, der berücksichtigt, "wie Sie mit ähnlichen Posts in der Vergangenheit interagiert haben. " Eine Filterblase wurde als Verschärfung eines Phänomens beschrieben, das als Splitternet oder Cyberbalkanisierung bezeichnet wurde, das passiert, wenn das Internet in Untergruppen von Gleichgesinnten aufgeteilt wird, die in ihrer eigenen Online-Community isoliert werden und nicht in die Lage versetzt werden, mit verschiedenen Ansichten konfrontiert zu werden. Dieses Anliegen stammt aus den frühen Tagen des öffentlich zugänglichen Internets, wobei der Begriff Cyber-Balkanisierung 1996 geprägt ist. Ähnliche Konzepte In Nachrichtenmedien ist die Echokammer eine metaphorische Beschreibung einer Situation, in der Überzeugungen durch Kommunikation und Wiederholung innerhalb eines geschlossenen Systems verstärkt oder verstärkt werden. Durch den Besuch einer "Echokammer" können die Menschen Informationen suchen, die ihre vorhandenen Ansichten verstärken, möglicherweise als unbewusste Ausübung der Bestätigungsvoreingenommenheit. Dies kann politische und soziale Polarisation und Extremismus erhöhen. Der Begriff ist eine auf der akustischen Echokammer basierende Metapher, in der Geräusche in einem hohlen Gehäuse reverberieren."Echokammern" verstärken einen Individuen Glauben ohne sachliche Unterstützung. Sie sind von denen umgeben, die die gleichen Standpunkte erkennen und verfolgen. Barack Obamas Abschiedsadresse identifizierte ein ähnliches Konzept, um Blasen als "drei auf die Demokratie der Amerikaner" zu filtern, d.h. die "Retreat in unsere eigenen Blasen, vor allem unsere Social Media-Feeds, umgeben von Menschen, die wie uns aussehen und die gleichen politischen Ausblicke teilen und nie unsere Annahmen herausfordern... Und zunehmend werden wir in unseren Bläschen so sicher, dass wir anfangen, nur Informationen zu akzeptieren, ob es wahr ist oder nicht, das passt zu unseren Meinungen, anstatt unsere Meinungen über die Beweise, die da draußen sind." Reaktionen und Studien Medienreaktionen Es gibt widersprüchliche Berichte über das Ausmaß, in dem eine personalisierte Filterung stattfindet und ob diese Tätigkeit günstig oder schädlich ist. Analyst Jacob Weisberg, geschrieben im Juni 2011 für Slate, machte ein kleines nicht-wissenschaftliches Experiment, um Pariser Theorie zu testen, die fünf Mitarbeiter mit verschiedenen ideologischen Hintergründen, die eine Reihe von Suchvorgängen, "John Boehner", "Barney Frank", "Ryan Plan", und Obamacare, und senden Weisberg Screenshots ihrer Ergebnisse. Die Ergebnisse variierten nur in untergeordneter Hinsicht von Person zu Person, und keine Unterschiede schienen ideologiebezogen zu sein, führten Weisberg zu dem Schluss, dass eine Filterblase nicht in Kraft war, und zu schreiben, dass die Idee, dass die meisten Internetnutzer "Füttern an der Wanne eines Daily Me" war überblown. Weisberg bat Google um Stellungnahme, und ein Sprecher sagte, dass Algorithmen an Ort und Stelle waren, um bewusst "beschränken Personalisierung und fördern Vielfalt". Buchbesprecher Paul Boutin hat ein ähnliches Experiment mit Weisberg unter Menschen mit unterschiedlichen Suchhistorien gemacht, und wieder festgestellt, dass die verschiedenen Sucher fast identische Suchergebnisse erhielten. Die Interviews von Programmierern bei Google von der Plattenjournalist Per Grankvist fanden heraus, dass Nutzerdaten eine größere Rolle bei der Ermittlung von Suchergebnissen spielten, aber dass Google durch Tests feststellen konnte, dass die Suchanfrage bei weitem der beste Determinator für die angezeigten Ergebnisse ist. Es gibt Berichte, dass Google und andere Websites große Dossiers von Informationen über ihre Nutzer pflegen, die es ihnen ermöglichen, einzelne Internet-Erfahrungen weiter zu personalisieren, wenn sie dies beschlossen haben. Zum Beispiel besteht die Technologie für Google, um die Vergangenheit der Nutzer zu verfolgen, auch wenn sie keine persönlichen Google-Konto oder sind nicht in einem eingeloggt. Ein Bericht sagte, dass Google "10 Jahre Wert" von Informationen gesammelt hatte, die aus unterschiedlichen Quellen, wie Gmail, Google Maps und andere Dienstleistungen neben seiner Suchmaschine, obwohl ein Gegenteil-Bericht war, dass versuchen, das Internet für jeden Benutzer war technisch herausfordernd für eine Internet-Firma zu erreichen, trotz der riesigen Mengen an verfügbaren Daten. Analyst Doug Gross von CNN schlug vor, dass die gefilterte Suche für die Verbraucher hilfreicher als für die Bürger, und würde einen Verbraucher auf der Suche nach Pizza lokalen Lieferoptionen basierend auf einer personalisierten Suche und entsprechend filtern entfernte Pizza-Shops. Organisationen wie die Washington Post, The New York Times und andere haben mit der Schaffung neuer personalisierter Informationsdienste experimentiert, mit dem Ziel, die Suchergebnisse auf diejenigen zuzuschneiden, mit denen die Nutzer gerne oder einverstanden sind. Akadämiestudien und ReaktionenIn 'The Big Data Public and Its Problems' schlägt Tauel Harper vor, dass der Verlust der redaktionellen Subvention tatsächlich eine homogenere und normalisierte öffentliche Sphäre produziert als herkömmliche Printmedien. Der Prozess der Salience-Auswahl, das Gesetz der großen Zahlen und die Macht der vorbestehenden Netzwerke bedeutet, dass algorithmische Selektionen dazu neigen, Normen zu verfestigen und die Unterschiede in der digitalen Öffentlichkeit weiter zu marginalisieren. Eine wissenschaftliche Studie von Wharton, die personalisierte Empfehlungen analysierte, fand auch heraus, dass diese Filter tatsächlich Gemeinsamkeit schaffen können, nicht Fragmentierung, im Online-Musikgeschmack. Verbraucher nutzen die Filter, um ihren Geschmack zu erweitern, anstatt ihn zu begrenzen. Harvard-Rechtsprofessor Jonathan Zittrain bestritten, inwieweit die Personalisierung Filter Google-Suchergebnisse verzerren und sagen, dass "die Auswirkungen der Suche Personalisierung waren leicht". Darüber hinaus bietet Google die Möglichkeit für Nutzer, die Personalisierungsfunktionen abzuschalten, wenn sie wählen, indem Googles Datensatz ihrer Suchhistorie gelöscht und Google eingestellt wird, um nicht an ihre Suchbegriffe und besuchte Links in der Zukunft zu erinnern. Eine Studie aus der Internet Policy Review befasste sich mit dem Fehlen einer klaren und testbaren Definition für Filterblasen über Disziplinen hinweg; dies führt oft zu Forschern, die Filterblasen auf unterschiedliche Weise definieren und studieren. Anschließend erklärte die Studie einen Mangel an empirischen Daten für die Existenz von Filterblasen über Disziplinen und schlug vor, dass die ihnen zugeschriebenen Effekte mehr von vorbestehenden ideologischen Vorurteilen als von Algorithmen stammen können. Ähnliche Ansichten finden sich in anderen akademischen Projekten, die sich auch mit den Definitionen von Filterblasen und den Beziehungen zwischen ideologischen und technologischen Faktoren befassen. Eine kritische Überprüfung von Filterblasen schlug vor, dass "die Filterblase-Thesis oft eine spezielle Art von politischen Menschen, die Meinungen, die stark sind, aber gleichzeitig hoch mißbrauchbar" und dass es ein "paradox, dass Menschen eine aktive Agentur haben, wenn sie Inhalte auswählen, aber passive Empfänger sind, sobald sie den algorithmisch kuratierten Inhalten ausgesetzt sind, die ihnen empfohlen werden". Eine Studie von Forschern aus Oxford, Stanford und Microsoft untersuchte die Browser-Histolen von 1,2 Millionen US-Nutzern des Bing Toolbar-Add-on für Internet Explorer zwischen März und Mai 2013. Sie wählten 50.000 der Nutzer, die aktive Verbraucher von Nachrichten waren, und klassifizierten dann, ob die von ihnen besuchten News-Outlets links- oder rechts-Lernen waren, basierend darauf, ob die Mehrheit der Wähler in den mit Nutzer-IP-Adressen verbundenen Kreisen für Obama oder Romney bei der Präsidentschaftswahl 2012 gestimmt hat. Sie identifizierten dann, ob Nachrichtengeschichten nach dem direkten Zugriff auf die Website des Verlegers, über den Google News Aggregation Service, über Websuche oder über Social Media gelesen wurden. Die Forscher fanden heraus, dass, während Web-Suche und Social Media zur ideologischen Segregation beitragen, die überwiegende Mehrheit des Online-Nachrichtenkonsums aus Nutzern bestand direkt Besuch von links- oder rechts-leaning Mainstream-Nachrichtenseiten und folglich fast ausschließlich Ansichten von einer Seite des politischen Spektrums ausgesetzt. Einschränkungen der Studie beinhalteten Auswahlprobleme wie Internet Explorer-Nutzer skewing höher im Alter als die allgemeine Internet-Bevölkerung; Bing Toolbar-Nutzung und die freiwillige (oder unwissende) Teilung der Browser-Geschichte Auswahl für Benutzer, die weniger besorgt über die Privatsphäre sind; die Annahme, dass alle Geschichten in linken Publikationen sind links-Lernen, und die gleiche für die Recht-Lernen; und die Möglichkeit, dass Benutzer, die nicht aktive Nachrichten-Nutzer sind im Wesentlichen erhalten Sie ihre Erfahrungen Eine Studie von Forschern der Princeton University und der New York University, die die Auswirkungen von Filterblase und algorithmische Filterung auf Social Media Polarisation untersucht. Sie nutzten ein mathematisches Modell namens "stochastic block model" um ihre Hypothese auf die Umgebungen von Reddit und Twitter zu testen. Die Forscher untersuchten Polarisationsänderungen in regelmäßigen Social Media-Netzwerken und nicht-regulierten Netzwerken, um die prozentualen Polarisations- und Meinungsänderungen auf Reddit und Twitter zu messen. Sie stellten fest, dass die Polarisation bei 400% in nicht regulierten Netzen deutlich zugenommen hatte, während die Polarisation in regelmäßigen Netzen um 4% und die Meinungsverschiedenheit um 5 % zunahm. Plattformstudien Während Algorithmen die politische Vielfalt begrenzen, ist ein Teil der Filterblase das Ergebnis der Benutzerwahl. Eine Studie von Datenwissenschaftlern bei Facebook fand heraus, dass für alle vier Facebook-Freunde, die Ideologie teilen, Nutzer einen Freund mit kontrastierenden Ansichten haben. Egal, was Facebooks Algorithmus für seine Nachrichten Fütterung ist, Menschen sind einfach eher Freunde / Menschen folgen, die ähnliche Überzeugungen teilen. Die Art des Algorithmus ist, dass es Geschichten auf der Grundlage der Geschichte eines Benutzers rangiert, was zu einer Reduzierung der "politisch kreuzenden Inhalte um 5 Prozent für Konservative und 8 Prozent für Liberale" führt. Allerdings, auch wenn die Menschen die Möglichkeit erhalten, auf einen Link zu klicken, der kontrastierende Ansichten bietet, sie immer noch Standard für ihre am meisten angesehenen Quellen. ["U]ser Wahl verringert die Wahrscheinlichkeit eines Klickens auf einen Crosscutting Link um 17 Prozent für Konservative und 6 Prozent für Liberale. " Ein Cross-Cuting-Link ist eine, die einen anderen Blickwinkel als den vermuteten Blickwinkel des Nutzers einführt, oder was die Website als den Glauben des Nutzers gepinkelt hat. Eine jüngste Studie von Levi Boxell, Matthew Gentzkow und Jesse M. Shapiro schlägt vor, dass Online-Medien nicht die treibende Kraft für politische Polarisation sind. Das Papier argumentiert, dass Polarisierung von den demographischen Gruppen, die die wenigsten Zeit online verbringen getrieben wurde. Die größte ideologische Teilung ist unter den Amerikanern älter als 75, während nur 20% berichtet über Social Media seit 2012. Im Gegensatz dazu berichteten 80% der Amerikaner im Alter von 18–39 Jahren mit Social Media ab 2012. Die Daten deuten darauf hin, dass die jüngere Demografie 2012 nicht mehr polarisiert ist als es gewesen war, als Online-Medien im Jahr 1996 kaum existierten. Die Studie zeigt Unterschiede zwischen Altersgruppen und wie der Nachrichtenkonsum polarisiert bleibt, da die Menschen Informationen suchen, die ihre Vorurteile ansprechen. Ältere Amerikaner bleiben in der Regel in ihren politischen Ansichten stagnierend, da traditionelle Medienstellen weiterhin eine primäre Quelle von Nachrichten sind, während Online-Medien die führende Quelle für die jüngere demografische. Obwohl Algorithmen und Filterblasen die Inhaltsvielfalt schwächen, zeigt diese Studie, dass politische Polarisationstrends in erster Linie von vorbestehenden Ansichten und Nichterkennen fremder Quellen angetrieben werden. Eine 2020-Studie aus Deutschland nutzte das Modell Big Five Psychology, um die Auswirkungen einzelner Persönlichkeiten, Demografien und Ideologien auf den Konsum von Nutzernachrichten zu testen. Aufgrund ihrer Studie über die Vorstellung, dass die Anzahl der Nachrichtenquellen, die Nutzer konsumieren Auswirkungen ihre Wahrscheinlichkeit, in einer Filterblase gefangen zu werden - mit einer höheren Medienvielfalt, die die Chancen verringert - ihre Ergebnisse legen nahe, dass bestimmte Demografien (höheres Alter und männliche) zusammen mit bestimmten Persönlichkeitsmerkmalen (hohe Offenheit) positiv mit der Anzahl der von Einzelpersonen verbrauchten Nachrichtenquellen korrelieren. Die Studie fand auch einen negativen ideologischen Zusammenhang zwischen Medienvielfalt und dem Grad, in dem die Nutzer sich mit dem rechtsgerichteten Autoritarismus ausrichten. Darüber hinaus bietet diese Studie unterschiedliche individuelle Nutzerfaktoren, die die Rolle der Nutzerwahl beeinflussen können, auch Fragen und Assoziationen zwischen der Wahrscheinlichkeit, dass Nutzer in Filterbläschen gefangen werden und Benutzer Abstimmung Verhalten. Die Facebook-Studie fand, dass es unschlüssig war, ob der Algorithmus als eine große Rolle bei der Filterung von News Feeds gespielt hat, wie die Menschen vermuteten. Die Studie fand auch, dass "individuelle Wahl", oder Bestätigung Vorspannung, ebenfalls beeinflusst, was aus News Feeds gefiltert wird. Einige Sozialwissenschaftler kritisierten diese Schlussfolgerung, denn der Punkt des Protests der Filterblase ist, dass die Algorithmen und individuelle Wahl zusammenarbeiten, um News Feeds auszufiltern. Sie kritisierten auch die kleine Stichprobengröße von Facebook, die etwa "9% der tatsächlichen Facebook-Nutzer" ist, und die Tatsache, dass die Studienergebnisse "nicht reproduzierbar" sind, weil die Studie von "Facebook-Wissenschaftlern" durchgeführt wurde, die Zugriff auf Daten hatten, die Facebook nicht außerhalb von Forschern zur Verfügung stellt. Obwohl die Studie herausfand, dass nur etwa 15–20% der Facebook-Freunde des durchschnittlichen Nutzers die entgegengesetzte Seite des politischen Spektrums abonnieren, hat Julia Kaman von Vox theorisiert, dass dies potenziell positive Auswirkungen auf die Meinungsvielfalt haben könnte. Diese Freunde sind oft Bekannte, mit denen wir unsere Politik ohne das Internet nicht teilen würden. Facebook kann eine einzigartige Umgebung fördern, in der ein Benutzer sieht und möglicherweise mit Inhalten interagiert, die von diesen zweitrangigen Freunden veröffentlicht oder neu postiert werden. Die Studie fand heraus, dass "24 Prozent der Nachrichtenartikel Liberale sahen konservativ-leaning und 38 Prozent der Nachrichtenkonservativen sahen war liberal-leaning." "Liberale sind eher mit weniger Freunden verbunden, die Informationen von der anderen Seite teilen, verglichen mit ihren konservativen Gegenstücken." Dieses Zusammenspiel hat die Fähigkeit, verschiedene Informationen und Quellen zur Verfügung zu stellen, die die Ansichten der Nutzer moderieren könnten.In ähnlicher Weise kam eine Studie über die Filterblasen von Twitter von New York University zu dem Schluss, dass "Individuals nun Zugang zu einer breiteren Sicht auf Nachrichtenereignisse haben, und die meisten dieser Informationen kommen nicht durch die traditionellen Kanäle, sondern entweder direkt von politischen Akteuren oder durch ihre Freunde und Verwandten. Darüber hinaus schafft die interaktive Natur der sozialen Medien Möglichkeiten für Einzelpersonen, politische Ereignisse mit ihren Kollegen zu diskutieren, einschließlich derjenigen, mit denen sie schwache soziale Bindungen haben". Nach diesen Studien können soziale Medien vielfältige Informationen und Meinungen sein, mit denen die Nutzer in Kontakt kommen, obwohl es viel Spekulation um Filterblasen und ihre Fähigkeit, tiefere politische Polarisation zu schaffen. Ein Treiber und eine mögliche Lösung für das Problem ist die Rolle von Emotionen in Online-Inhalte. Eine Studie von 2018 zeigt, dass unterschiedliche Emotionen von Botschaften zu Polarisation oder Konvergenz führen können: Freude ist in emotionaler Polarisation verbreitet, während Traurigkeit und Angst wichtige Rollen in emotionaler Konvergenz spielen. Da es relativ einfach ist, den emotionalen Inhalt von Botschaften zu erkennen, können diese Erkenntnisse dazu beitragen, sozial verantwortlichere Algorithmen zu entwerfen, indem sie beginnen, sich auf den emotionalen Inhalt algorithmischer Empfehlungen zu konzentrieren. Soziale Bots wurden von verschiedenen Forschern genutzt, um Polarisation und damit verbundene Effekte zu testen, die auf Filterblasen und Echokammern zurückzuführen sind. Eine Studie von 2018 nutzte Social Bots auf Twitter, um bewusste Nutzerexposition mit Partisan Viewpoints zu testen. Die Studie behauptete, dass es Unterschiede zwischen der Exposition gegenüber unterschiedlichen Ansichten zeigte, obwohl sie warnte, dass die Ergebnisse auf Partei registrierte amerikanische Twitter-Nutzer beschränkt werden sollten. Eines der Hauptergebnisse war, dass selbstregistrierte Republikaner nach der Exposition gegenüber unterschiedlichen Ansichten (vorgesehen von den Bots) konservativer wurden, während selbstregistrierte Liberale weniger ideologische Veränderungen zeigten, wenn überhaupt keine. Eine andere Studie aus der Volksrepublik China nutzte soziale Bots auf Weibo – die größte Social Media-Plattform in China –, um die Struktur von Filterblasen in Bezug auf ihre Auswirkungen auf die Polarisation zu untersuchen. Die Studie unterscheidet zwischen zwei Begriffen der Polarisation. Ein Wesen, wo Menschen mit ähnlichen Ansichten Gruppen bilden, ähnliche Meinungen teilen und sich von unterschiedlichen Standpunkten (Opinionpolarisation) blockieren und das andere Wesen, wo Menschen nicht auf verschiedene Inhalte und Informationsquellen zugreifen (Informationspolarisation). Durch die Nutzung von Social Bots anstelle von menschlichen Freiwilligen und die Fokussierung mehr auf Informationspolarisation und nicht auf Meinungsbasis, kamen die Forscher zu dem Schluss, dass es zwei wesentliche Elemente einer Filterblase gibt: eine große Konzentration von Nutzern um ein einzelnes Thema und eine unidirektionale, sternartige Struktur, die Schlüsselinformationen beeinflusst. Im Juni 2018 führte die Plattform DuckDuckGo eine Studie über die Google Web Browser Platform durch. Für diese Studie haben 87 Erwachsene an verschiedenen Standorten rund um die kontinentalen Vereinigten Staaten gleichzeitig drei Schlüsselwörter angegriffen: Einwanderung, Waffenkontrolle und Impfungen. Auch wenn im privaten Browser-Modus, die meisten Menschen sah Ergebnisse einzigartig für sie. Google enthielt einige Links für einige, die es nicht für andere Teilnehmer, und die Nachrichten und Videos Infoboxen zeigten erhebliche Variation. Google bestritten öffentlich diese Ergebnisse sagen, dass Search Engine Results Page (SERP) Personalisierung ist meist ein Mythos. Google Search Liaison, Danny Sullivan, sagte, dass “In den Jahren hat ein Mythos entwickelt, dass Google Search so viel personalisiert, dass für die gleiche Abfrage, verschiedene Menschen könnten deutlich verschiedene Ergebnisse von einander erhalten. Das ist nicht der Fall. Die Ergebnisse können sich jedoch in der Regel aus nicht personalisierten Gründen unterscheiden.“ Wenn Filterblasen vorhanden sind, können sie bestimmte Momente erzeugen, die Wissenschaftler Whoa Moments nennen. A Whoa moment is when a article, ad, post, etc. erscheint auf Ihrem Computer, die in Bezug auf eine aktuelle Aktion oder aktuelle Nutzung eines Objekts. Wissenschaftler entdeckten diesen Begriff, nachdem eine junge Frau ihre tägliche Routine, die Trinken Kaffee, als sie öffnete ihren Computer und bemerkte eine Werbung für die gleiche Marke von Kaffee, die sie trinken. "Sat down und eröffnet Facebook heute Morgen, während ich meinen Kaffee, und dort waren sie zwei Anzeigen für Nespresso. Eine Art Hure Moment, wenn das Produkt, das Sie trinken, auf dem Bildschirm vor Ihnen auftaucht." "Wer Momente auftreten, wenn Menschen gefunden werden." Das bedeutet, dass Werbealgorithmen bestimmte Nutzer auf der Grundlage ihres "Klickverhaltens" ansprechen, um ihre Verkaufserlöse zu erhöhen. Mehrere Designer haben Werkzeuge entwickelt, um den Auswirkungen von Filterblasen entgegenzuwirken (siehe § Gegenmaßnahmen). Die Schweizer Radiostation SRF hat das Wort Filterblase (die deutsche Übersetzung der Filterblase) des Jahres 2016 gewählt. Gegenmaßnahmen In The Filter Bubble: Was das Internet von Ihnen verschiebt, betont der Internet-Aktivist Eli Pariser, wie das zunehmende Auftreten von Filterblasen den Wert des überbrückenden Sozialkapitals weiter unterstreicht, wie Robert Putman definiert. Zwar entspricht das Bindungskapital einerseits der Errichtung einer starken Verbindung zwischen Gleichgesinnten, wodurch ein gewisses Gefühl der sozialen Homogenität gestärkt wird, andererseits stellt die Überbrückung des Sozialkapitals die Schaffung schwacher Verbindungen zwischen Menschen mit potenziell unterschiedlichen Interessen und Standpunkten dar, wodurch eine deutlich höhere Heterogenität eingeführt wird. In diesem Sinne ist hohes Überbrückungskapital viel wahrscheinlicher, um die soziale Eingliederung zu fördern, indem wir unsere Exposition gegenüber einem Raum erhöhen, in dem wir die Probleme ansprechen, die unsere Nischen und enge Selbstinteressen durchsetzen. Das Überbrückungskapital, z.B. durch die Verbindung mit mehr Menschen in informeller Umgebung, kann daher eine effektive Möglichkeit sein, den Einfluss des Filterblasenphänomens zu reduzieren. Nutzer können in der Tat viele Aktionen ergreifen, um durch ihre Filterblasen zu platzen, zum Beispiel indem sie bewusste Anstrengungen unternehmen, um zu bewerten, welche Informationen sie selbst aussetzen, und indem sie kritisch darüber nachdenken, ob sie mit einer breiten Palette von Inhalten in Verbindung stehen. Diese Ansicht argumentiert, dass die Nutzer die Psychologie ändern sollten, wie sie Medien angehen, anstatt sich auf Technologie zu verlassen, um ihren Voreingenommenheiten entgegenzuwirken. Nutzer können bewusst Nachrichtenquellen vermeiden, die nicht oder schwach sind. Chris Glushko, der VP Marketing bei IAB, befürwortet die Verwendung von Fact-Checking-Seiten, um gefälschte Nachrichten zu identifizieren. Technologie kann auch eine wertvolle Rolle bei der Bekämpfung von Filterblasen spielen. Einige zusätzliche Plug-Ins, wie Media Bias Fact Check, zielten darauf ab, den Menschen zu helfen, aus ihren Filterblasen herauszutreten und sie ihrer persönlichen Perspektive bewusst zu machen; so zeigen diese Medien Inhalte, die mit ihren Überzeugungen und Meinungen widersprechen. Zum Beispiel, Escape Your Bubble fordert Benutzer auf, eine bestimmte politische Partei, über die sie informiert werden wollen. Das Plug-in wird dann Artikel aus etablierten Quellen vorschlagen, um über diese politische Partei zu lesen und die Nutzer zu ermutigen, über die andere Partei gebildet zu werden. Neben Plug-Ins gibt es Apps, die mit der Aufgabe erstellt werden, die Benutzer zu ermutigen, ihre Echokammern zu öffnen. UnFound.news bietet eine KI (Künstliche Intelligenz) kuratierte Nachrichten-App an Leser, die ihnen Nachrichten aus unterschiedlichen und unterschiedlichen Perspektiven präsentieren und ihnen helfen, rationale und informierte Meinung zu bilden, anstatt ihre eigenen Vorurteile zu belasten. Es drängt auch die Leser, verschiedene Perspektiven zu lesen, wenn ihr Lesemuster auf eine Seite/Ideologie hin vorgespannt ist. Read Across the Aisle ist eine Nachrichten-App, die zeigt, ob Benutzer aus verschiedenen neuen Quellen lesen, die mehrere Perspektiven beinhalten. Jede Quelle ist farblich koordiniert, was die politische Anlehnung jedes Artikels darstellt. Wenn Nutzer nur aus einer Perspektive Nachrichten lesen, kommuniziert die App dem Benutzer und ermutigt Leser, andere Quellen mit entgegengesetzten Blickpunkten zu erkunden. Obwohl Apps und Plug-ins Werkzeuge sind, die Menschen verwenden können, Eli Pariser sagte "sicher, es gibt einige individuelle Verantwortung hier, um wirklich neue Quellen und Menschen, die nicht wie Sie suchen. " Da webbasierte Werbung die Wirkung der Filterblasen weiter verbessern kann, indem Nutzer mehr von denselben Inhalten ausgesetzt werden, können Nutzer durch Löschen ihrer Suchhistorie viel Werbung blockieren, gezielte Anzeigen ausschalten und Browsererweiterungen herunterladen. Erweiterungen wie Escape your Bubble for Google Chrome Ziel, Inhalte zu heilen und zu verhindern, dass Benutzer nur voreingenommene Informationen ausgesetzt werden, während Mozilla Firefox Erweiterungen wie Lightbeam und Self-Destructing Cookies ermöglichen Benutzern zu visualisieren, wie ihre Daten verfolgt werden, und lässt sie einige der Tracking-Cookies entfernen. Einige verwenden anonyme oder nicht personalisierte Suchmaschinen wie YaCy, DuckDuckGo, Qwant, Startpage.com, Disconnect und Searx, um zu verhindern, dass Unternehmen ihre Web-Suchdaten sammeln. Schweizer Tageszeitung Die Neue Zürcher Zeitung testet eine personalisierte News-Engine-App, die maschinelles Lernen nutzt, um zu erraten, an welchen Inhalt ein Nutzer interessiert ist, während "immer auch ein Element der Überraschung"; die Idee ist, in Geschichten zu mischen, die ein Benutzer in der Vergangenheit unwahrscheinlich verfolgt hat. Die Europäische Union trifft Maßnahmen, um die Wirkung der Filterblase zu verringern. Das Europäische Parlament sponsert Untersuchungen darüber, wie Filterblasen die Möglichkeit des Zugriffs auf verschiedene Nachrichten beeinflussen. Darüber hinaus hat sie ein Programm eingeführt, um die Bürger über soziale Medien zu informieren. In den USA schlägt das CSCW-Panel die Verwendung von Nachrichten-Aggregator-Apps vor, um die Nachrichtenaufnahme von Medienkonsumenten zu erweitern. Nachrichten-Aggregator-Apps scannen alle aktuellen Nachrichtenartikel und leiten Sie Sie zu verschiedenen Ansichten zu einem bestimmten Thema. Benutzer können auch einen vielfältigen Nachrichten Balancer verwenden, der den Medienkonsumenten visuell zeigt, wenn sie sich links oder rechts lehnen, wenn es darum geht, die Nachrichten zu lesen, Anzeige rechts-Lernen mit einer größeren roten Bar oder links-Lernen mit einer größeren blauen Bar. Eine Studie, die diesen News Balancer bewertete, fand "eine kleine, aber spürbare Veränderung des Leseverhaltens, hin zu einer ausgewogeneren Exposition, bei Nutzern, die das Feedback sehen, im Vergleich zu einer Kontrollgruppe." Von Medienunternehmen Angesichts der jüngsten Bedenken bezüglich der Informationsfilterung auf sozialen Medien erkannte Facebook das Vorhandensein von Filterblasen an und versuchte, sie zu entfernen. Im Januar 2017, Facebook entfernte die Personalisierung von seiner Trending Topics-Liste in Reaktion auf Probleme mit einigen Benutzern nicht sehen hoch gesprochene Ereignisse dort. Die Strategie von Facebook besteht darin, die Funktion Zugehörige Artikel, die sie 2013 implementiert hatte, umzukehren, was verwandte Nachrichtengeschichten posten würde, nachdem der Benutzer einen gemeinsamen Artikel gelesen hat. Die überarbeitete Strategie würde diesen Prozess umsetzen und Artikel aus verschiedenen Perspektiven auf das gleiche Thema veröffentlichen. Facebook versucht auch, durch einen Vetting-Prozess zu gehen, wobei nur Artikel aus seriösen Quellen angezeigt werden. Zusammen mit dem Gründer von Craigslist und einigen anderen hat Facebook 14 Millionen Dollar in die Bemühungen investiert, "das Vertrauen in den Journalismus auf der ganzen Welt zu erhöhen und das öffentliche Gespräch besser zu informieren". Die Idee ist, dass selbst wenn Menschen nur Beiträge von ihren Freunden lesen, zumindest diese Beiträge glaubwürdig sein werden. Ebenso hat Google zum 30. Januar 2018 die Existenz von Filterblasenschwierigkeiten innerhalb seiner Plattform anerkannt. Weil der Strom Google-Suche ziehen algorithmisch rangierte Ergebnisse basierend auf Berechtigung und Relevanz, die zeigen und verbergen bestimmte Suchergebnisse, Google versucht, diese zu bekämpfen. Durch die Ausbildung seiner Suchmaschine, um die Absicht einer Suchanfrage zu erkennen, anstatt die wörtliche Syntax der Frage, Google versucht, die Größe der Filterblasen zu begrenzen. Ab sofort wird die erste Phase dieser Ausbildung im zweiten Quartal 2018 eingeführt. Fragen, die Vorurteile und/oder kontroverse Meinungen beinhalten, werden erst zu einem späteren Zeitpunkt behandelt, indem ein größeres Problem ausgelöst wird, das noch besteht: ob die Suchmaschine entweder als Arbiter der Wahrheit oder als kenntnisreicher Leitfaden fungiert, durch den Entscheidungen getroffen werden sollen. Im April 2017 gaben Nachrichten heraus, dass Facebook, Mozilla und Craigslist zur Mehrheit einer Spende von $14M an CUNYs "News Integrity Initiative" beitrugen, um gefälschte Nachrichten zu beseitigen und ehrlichere Nachrichtenmedien zu schaffen. Später, im August, Mozilla, Hersteller des Firefox Web-Browsers, kündigte die Bildung der Mozilla Information Trust Initiative (MITI). Die +MITI würde als kollektives Bemühen dienen, Produkte, Forschung und gemeinschaftsbasierte Lösungen zu entwickeln, um die Auswirkungen von Filterblasen und die Verbreitung gefälschter Nachrichten zu bekämpfen. Das Open Innovation Team von Mozilla leitet die Initiative, die versucht, Fehlinformationen zu bekämpfen, mit einem spezifischen Fokus auf das Produkt in Bezug auf Alphabetisierung, Forschung und kreative Interventionen. Ethische Auswirkungen Da die Popularität der Cloud-Dienste zunimmt, werden personalisierte Algorithmen, die zur Konstruktion von Filterblasen verwendet werden, voraussichtlich breiter werden. Gelehrte haben begonnen, die Wirkung von Filterblasen auf die Nutzer von sozialen Medien aus ethischer Sicht zu betrachten, insbesondere in Bezug auf die Bereiche persönliche Freiheit, Sicherheit und Informationsvorspannung. Filterblasen in beliebten sozialen Medien und personalisierten Suchseiten können die einzelnen Inhalte, die von Nutzern gesehen werden, oft ohne ihre direkte Zustimmung oder Anerkennung, aufgrund der Algorithmen, die verwendet werden, um diese Inhalte zu heilen. Selbst erstellte Inhalte, die sich aus Verhaltensmustern manifestieren, können zu Teilinformationen Blindheit führen. Kritik an der Verwendung von Filterblasen spekulieren, dass Personen über ihre eigene soziale Medienerfahrung Autonomie verlieren und ihre Identitäten durch die Pervasivität von Filterblasen sozial konstruiert haben. Techniker, Social Media-Ingenieure und Informatiker haben auch die Prävalenz von Filterblasen untersucht. Mark Zuckerberg, Gründer von Facebook, und Eli Pariser, Autor von The Filter Bubble, haben Bedenken hinsichtlich der Risiken von Privatsphäre und Informationspolarisation geäußert. Die Informationen der Nutzer von personalisierten Suchmaschinen und Social-Media-Plattformen sind nicht privat, obwohl einige glauben, dass es sein sollte. Die Sorge um die Privatsphäre hat zu einer Debatte geführt, ob es moralisch ist, dass die Informatiker die Online-Aktivitäten der Nutzer übernehmen und die zukünftige Exposition gegenüber verwandten Informationen manipulieren. Einige Wissenschaftler haben Bedenken hinsichtlich der Auswirkungen von Filterblasen auf individuelles und soziales Wohlbefinden geäußert, d.h. die Verbreitung von Gesundheitsinformationen an die breite Öffentlichkeit und die potenziellen Auswirkungen von Internet-Suchmaschinen auf das gesundheitliche Verhalten. Ein 2019 multidisziplinäres Buch berichtete Forschung und Perspektiven auf die Rollen Filterblasen spielen in Bezug auf Gesundheit Fehlinformationen. Ausgehend von verschiedenen Bereichen wie Journalismus, Recht, Medizin und Gesundheitspsychologie befasst sich das Buch mit unterschiedlichen kontroversen gesundheitlichen Überzeugungen (z.B. alternative Medizin und Pseudowissenschaft) sowie potenziellen Abhilfemaßnahmen gegen die negativen Auswirkungen von Filterblasen und Echokammern auf verschiedene Themen im Gesundheitsdiskurs. Eine 2016 Studie über die potenziellen Auswirkungen von Filterblasen auf Suchmaschinen-Ergebnisse im Zusammenhang mit Selbstmord fand heraus, dass Algorithmen eine wichtige Rolle in spielen, ob Helplines und ähnliche Suchergebnisse für Benutzer angezeigt werden und die Auswirkungen ihrer Forschung auf die Gesundheitspolitik haben kann. Eine weitere 2016 Studie aus der kroatischen Medizinischen Zeitschrift schlug einige Strategien zur Minderung der potenziell schädlichen Auswirkungen von Filterblasen auf Gesundheitsinformationen vor, wie: die Öffentlichkeit mehr über Filterblasen und ihre damit verbundenen Auswirkungen informieren, Benutzer wählen, um alternative [zu Google] Suchmaschinen zu versuchen, und mehr Erklärung der Prozesse, die Suchmaschinen verwenden, um ihre angezeigten Ergebnisse zu bestimmen. Da die Inhalte einzelner Social Media-Nutzer durch Algorithmen beeinflusst werden, die Filterblasen erzeugen, sind Nutzer von Social Media-Plattformen eher anfällig für Bestätigungsversagen und können voreingenommenen irreführenden Informationen ausgesetzt werden. Soziale Sortierung und andere unbeabsichtigte diskriminierende Praktiken werden auch durch personalisierte Filterung erwartet. Angesichts der 2016 US-Präsidentschaftswahlwissenschaftler haben ebenfalls Bedenken über die Wirkung von Filterblasen auf Demokratie und demokratische Prozesse sowie den Aufstieg "ideologischer Medien" geäußert. Diese Gelehrten befürchten, dass die Nutzer nicht in der Lage sein werden, über [ihre] enges Selbstinteresse hinaus zu denken, da Filterblasen personalisierte soziale Futtermittel schaffen, sie von verschiedenen Gesichtspunkten und ihren umliegenden Gemeinschaften isolieren. Aus diesem Grund wird zunehmend die Möglichkeit diskutiert, soziale Medien mit mehr Gelassenheit zu gestalten, d.h. Inhalte, die außerhalb der Filterblase liegen, proaktiv zu empfehlen, einschließlich herausfordernder politischer Informationen und schließlich, um den Nutzern Filter und Werkzeuge zu ermöglichen. Ein damit verbundenes Anliegen ist in der Tat, wie Filterblasen zur Verbreitung von "Fake-News" beitragen und wie dies Einfluss auf die politische Magerung haben kann, einschließlich der Abstimmung der Nutzer. Enthüllungen im März 2018 der Ernte und Nutzung von Nutzerdaten von Cambridge Analytica für mindestens 87 Millionen Facebook-Profile während der Präsidentschaftswahl 2016 unterstreichen die ethischen Auswirkungen von Filterblasen. Mitbegründer und Whistleblower von Cambridge Analytica Christopher Wylie, wie das Unternehmen die Fähigkeit hatte, psychografische Profile dieser Nutzer zu entwickeln und die Informationen zu nutzen, um ihr Abstimmungsverhalten zu gestalten. Der Zugriff auf Nutzerdaten durch Dritte wie Cambridge Analytica kann bestehende Filterblasen-Nutzer exasperieren und verstärken, die bestehende Vorspannungen künstlich erhöhen und die Gesellschaft weiter teilen. Dangers Filter Blasen haben sich von einem Anstieg der Medien Personalisierung, die Benutzer fangen kann. Die Verwendung von KI zur Personalisierung von Angeboten kann dazu führen, dass Nutzer nur Inhalte anzeigen, die ihre eigenen Standpunkte verstärken, ohne sie herauszufordern. Social-Media-Websites wie Facebook können auch Inhalte in einer Weise präsentieren, die es schwierig macht, die Quelle der Inhalte zu bestimmen, was sie dazu führt, für sich selbst zu entscheiden, ob die Quelle zuverlässig oder gefälscht ist. Das kann dazu führen, dass Menschen daran gewöhnt werden, zu hören, was sie hören wollen, was dazu führen kann, dass sie radikaler reagieren, wenn sie einen entgegengesetzten Standpunkt sehen. Die Filterblase kann dazu führen, dass die Person gegensätzliche Standpunkte als falsch sieht und so die Medien Ansichten auf die Verbraucher zwingen können. Forschungen erklären, dass die Filterblase verstärkt, was man schon denkt. Deshalb ist es äußerst wichtig, Ressourcen zu nutzen, die verschiedene Gesichtspunkte bieten. Erweiterungen des Konzepts Das Konzept einer Filterblase wurde in andere Bereiche erweitert, um Gesellschaften zu beschreiben, die sich nach politischen Ansichten, aber auch wirtschaftlichen, sozialen und kulturellen Situationen selbst scheiden. Dass das Bubble zu einem Verlust der breiteren Gemeinschaft führt und den Sinn schafft, dass zum Beispiel Kinder nicht an sozialen Ereignissen gehören, es sei denn, diese Ereignisse waren besonders geplant, für Kinder zu appellieren und für Erwachsene ohne Kinder nicht zu erscheinen. Siehe auch Hinweise ReferenzenWeiter lesen Pariser, Eli.The Filter Bubble: Was das Internet ist Verleih von Ihnen, Pinguin Press (New York, 2011) ISBN 978-1-59420-300-8 Green, Holly(August 29, 2011)."Breaking Out of Your Internet Filter Bubble". Forbes.Retrieved Dezember 4, 2011.Friedman, Ann (2014). "Going Viral".Columbia Journalism Review.52 (6): 33–34.Bozdag, Engin; van den Hoven, Jeroen (Dezember 2015). " Durchbrechen der Filterblase: Demokratie und Design". Ethik und Informationstechnologie.17 (4): 249–265.doi:10.1007/s10676-015-9380-y boyd, danah m.; Ellison, Nicole B. (Oktober 2007). "Social Network Sites: Definition, Geschichte und Stipendium".Journal of Computer-Mediated Communication.13 (1): 210–230.doi:10.11/j.1083-6101.2007.00393.x S2CID 52810295. Nguyen, Tien T.; Hui, Pik-Mai; Harper, F. Maxwell; Terveen, Loren; Konstan, Joseph A. (2014)." Ausloten der Filterblase: die Wirkung der Verwendung von Empfehlungssystemen auf Inhaltsvielfalt". Proceedings of the 23rd International Conference on World Wide Web – WWW '14: 677–686.doi:10.1145/2566486.2568012.S2CID 16747810.Resnick, Paul; Garrett, R. Kelly; Kriplean, Travis; Munson, Sean A.; Stroud, Natalie Jomini (2013). " Verbrennen Sie Ihre (Filter) Blase: Strategien zur Förderung der vielfältigen Belichtung". Proceedings of the 2013 Conference on Computer Supported Cooperative Work Companion – CSCW '13: 95.doi:10.1145/2441955.2441981.S2CID 20865375.Liao, Q. Vera; Fu, Wai-Tat (2013). " Jenseits der Filterblase: interaktive Effekte von wahrgenommener Bedrohung und Themenbeteiligung bei selektiver Informationsexposition". Proceedings of the SIGCHI Conference on Human Factors in Computing Systems – CHI '13: 2359.doi:10.1145/2470654.2481326.S2CID 8504434.Holone, Harald (2016). "Die Filterblase und ihre Wirkung auf online persönliche Gesundheitsinformationen". Kroatisches medizinisches Journal.57 (3): 298–301. doi:10.3325/cmj.2016.57.298.PMC 4937233.PMID 27374832. Externe Links Beware Online Filter Bubbles.TED Talks, März 2011