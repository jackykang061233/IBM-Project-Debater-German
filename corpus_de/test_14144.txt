Big Data ist ein Feld, das Wege behandelt, Informationen systematisch aus oder anderweitig mit Datensätzen zu analysieren, die zu groß oder komplex sind, um von der herkömmlichen Software zur Datenverarbeitung behandelt werden. Daten mit vielen Feldern (Spalten) bieten mehr statistische Leistung, während Daten mit höherer Komplexität (mehr Attribute oder Spalten) zu einer höheren falschen Entdeckungsrate führen können. Zu den großen Herausforderungen bei der Datenanalyse zählen die Erfassung von Daten, Datenspeicherung, Datenanalyse, Suche, Weitergabe, Visualisierung, Abfrage, Aktualisierung, Datenschutz und Datenquelle. Große Daten wurden ursprünglich mit drei Schlüsselkonzepten verbunden: Volumen, Vielfalt und Geschwindigkeit. Die Analyse großer Daten stellt Herausforderungen bei der Probenahme dar und erlaubt somit bisher nur Beobachtungen und Probenahmen. Daher enthält Big Data oft Daten mit Größen, die die Kapazität der traditionellen Software zu verarbeiten innerhalb einer akzeptablen Zeit und Wert. Die aktuelle Nutzung des Begriffs Big Data bezieht sich tendenziell auf die Verwendung von prädiktiven Analytik, Benutzerverhaltensanalysen oder bestimmte andere fortschrittliche Datenanalysemethoden, die Wert aus Big Data extrahieren, und selten auf eine bestimmte Größe von Datensatz. " Es besteht wenig Zweifel daran, dass die jetzt verfügbaren Datenmengen zwar groß sind, aber das ist nicht das relevanteste Merkmal dieses neuen Daten-Ökosystems." Die Analyse von Datensätzen kann neue Korrelationen zu "Geschäftstrends vorfinden, Krankheiten, Kampfkriminalität usw. verhindern". Wissenschaftler, Geschäftsführende, medizinische Praktiker, Werbung und Regierungen treffen regelmäßig Schwierigkeiten mit großen Datenmengen in Bereichen wie Internet-Suche, Fintech, Gesundheitsanalyse, geographische Informationssysteme, Stadtinformatik und Wirtschaftsinformatik. Wissenschaftler treten Einschränkungen in der e-Science-Arbeit auf, darunter Meteorologie, Genomik, Connectomik, komplexe Physiksimulationen, Biologie und Umweltforschung. Die Größe und Anzahl der verfügbaren Datensätze ist schnell gewachsen, da Daten von Geräten wie Mobilgeräten, billigen und zahlreichen Informations-sensierenden Internet von Dingen Geräten, Antennen- (Remote-Sensing,) Software-Logs, Kameras, Mikrofone, Hochfrequenz-Identifikation (RFID)-Lesern und drahtlosen Sensornetzwerken erfasst werden. Die technologische Per-Cita-Kapazität der Welt, Informationen zu speichern, hat sich seit den 1980er Jahren etwa alle 40 Monate verdoppelt; ab 2012 werden täglich 2,5 Exabyte (2,5 x 260 Bytes) von Daten erzeugt. Basierend auf einer IDC-Berichtsprognose wurde prognostiziert, dass das globale Datenvolumen zwischen 2013 und 2020 von 4,4 zettabytes bis 44 zettabytes exponentiell wachsen würde. Bis 2025 prognostiziert IDC, dass es 163 Zettabytes von Daten geben wird. Eine Frage für große Unternehmen ist, wer Big-Data-Initiativen besitzen sollte, die die gesamte Organisation beeinflussen. Relationale Datenbank-Management-Systeme und Desktop-Statistik-Software-Pakete, die zur Visualisierung von Daten verwendet werden, haben oft Schwierigkeiten bei der Verarbeitung und Analyse großer Daten. Die Verarbeitung und Analyse großer Daten kann "massiv parallele Software auf zehn, Hunderten oder sogar Tausenden von Servern laufen" erfordern. Was als "große Daten" qualifiziert ist, variiert je nach den Fähigkeiten derer, die sie und ihre Werkzeuge analysieren. Darüber hinaus machen die Erweiterungsfähigkeiten große Daten zu einem bewegten Ziel. " Für einige Organisationen, die Hunderte von Gigabyte von Daten zum ersten Mal konfrontiert können, kann eine Notwendigkeit, die Datenverwaltung Optionen zu überdenken. Für andere kann es Zehn oder Hunderte von Terabytes nehmen, bevor die Datengröße zu einer erheblichen Berücksichtigung wird." Begriff Der Begriff Big Data wurde seit den 1990er Jahren verwendet, mit einigen geben Kredit an John Mashey für die Popularisierung des Begriffs. Big Data enthält in der Regel Datensätze mit Größen über die Fähigkeit der häufig verwendeten Software-Tools, Daten zu erfassen, zu heilen, zu verwalten und zu verarbeiten innerhalb einer tolerierbaren verstrichenen Zeit. Die große Datenphilosophie umfasst unstrukturierte, halbstrukturierte und strukturierte Daten, der Schwerpunkt liegt jedoch auf unstrukturierten Daten. Die große Datengröße ist ein ständig bewegendes Ziel; ab 2012 reicht von einigen Dutzend Terabytes bis zu vielen Zettabytes von Daten. Große Daten erfordern eine Reihe von Techniken und Technologien mit neuen Formen der Integration, um Erkenntnisse aus Datenmengen, die vielfältig, komplex und massiv sind, zu zeigen. "Variety, Veracity und verschiedene andere Vs werden von einigen Organisationen hinzugefügt, um es zu beschreiben, eine Revision von einigen Industriebehörden herausgefordert. Die Vs der großen Daten wurden oft als die "drei Vs", "vier Vs" und "fünf Vs" bezeichnet. Sie repräsentierten die Qualitäten der großen Daten in Volumen, Vielfalt, Geschwindigkeit, Ungenauigkeit und Wert. Variabilität ist oft als zusätzliche Qualität von Big Data enthalten. In einer Definition von 2018 heißt es: "Große Daten sind dort, wo parallele Rechenwerkzeuge benötigt werden, um Daten zu verarbeiten", und Anmerkungen: "Dies stellt eine deutliche und eindeutig definierte Änderung der verwendeten Informatik über parallele Programmiertheorien und Verluste einiger Garantien und Fähigkeiten von Coddds relationalem Modell dar. " In einer vergleichenden Studie von Big Datasets fanden Kitchin und McArdle heraus, dass keine der allgemein betrachteten Eigenschaften von Big Data in allen analysierten Fällen konsequent erscheinen. Aus diesem Grund identifizierten andere Studien die Neudefinition der Leistungsdynamik in der Wissensentdeckung als die definierende Eigenschaft. Anstatt sich auf die intrinsischen Eigenschaften von Big Data zu konzentrieren, wird durch diese alternative Perspektive ein relationales Verständnis des Objekts vorangetrieben, in dem es darauf ankommt, wie Daten gesammelt, gespeichert, bereitgestellt und analysiert werden. Big Data vs. Business Intelligence Die zunehmende Reife des Konzepts lenkt den Unterschied zwischen "großen Daten" und "Business Intelligence" deutlicher ab: Business Intelligence verwendet angewandte Mathematik-Tools und beschreibende Statistiken mit Daten mit hoher Informationsdichte, um Dinge zu messen, Trends zu erkennen, etc. Big Data verwendet mathematische Analyse, Optimierung, induktive Statistiken und Konzepte von nichtlinearer Systemidentifikation, um Gesetze (Regressionen, nichtlineare Beziehungen und ursächliche Effekte) von großen Datensätzen mit geringer Informationsdichte zu offenbaren Beziehungen und Abhängigkeiten, oder Vorhersagen von Ergebnissen und Verhaltensweisen. Merkmale Große Daten können durch folgende Merkmale beschrieben werden: Volumen Die Menge der generierten und gespeicherten Daten. Die Größe der Daten bestimmt den Wert und die potenzielle Einsicht, ob sie als Big Data betrachtet werden können oder nicht. Die Größe der Big Data ist in der Regel größer als Terabytes und Petabytes. Sorte Art und Art der Daten. Die früheren Technologien wie RDBMS konnten strukturierte Daten effizient und effektiv verarbeiten. Die Veränderungen von Art und Art von strukturierten bis halbstrukturierten oder unstrukturierten Herausforderungen stellten jedoch die bestehenden Werkzeuge und Technologien in Frage. Die großen Datentechnologien entwickelten sich mit der Hauptabsicht, die halbstrukturierten und unstrukturierten (Variety-)Daten mit hoher Geschwindigkeit (Vulity) und großer Größe (Volume) zu erfassen, zu speichern und zu verarbeiten. Später wurden diese Werkzeuge und Technologien untersucht und für die Verarbeitung strukturierter Daten verwendet, jedoch für die Speicherung bevorzugt. Schließlich wurde die Verarbeitung strukturierter Daten noch als optional gehalten, entweder mit Big Data oder traditionellen RDBMSs. Dies hilft bei der Analyse von Daten zur effektiven Nutzung der verborgenen Erkenntnisse, die aus den über Social Media, Log-Dateien, Sensoren usw. gesammelten Daten bestehen. Große Daten stammen aus Text, Bildern, Audio, Video; plus es vervollständigt fehlende Stücke durch Datenfusion. Geschwindigkeit Die Geschwindigkeit, mit der die Daten generiert und verarbeitet werden, um den Anforderungen und Herausforderungen gerecht zu werden, die im Pfad des Wachstums und der Entwicklung liegen. Große Daten sind oft in Echtzeit verfügbar. Im Vergleich zu kleinen Daten werden immer mehr Big Data produziert. Zwei Arten von Geschwindigkeiten im Zusammenhang mit Big Data sind die Häufigkeit der Erzeugung und die Häufigkeit der Handhabung, Aufzeichnung und Verlagswesen. Genauigkeit Die Wahrhaftigkeit oder Zuverlässigkeit der Daten, die sich auf die Datenqualität und den Datenwert bezieht. Große Daten dürfen nicht nur groß sein, sondern müssen auch zuverlässig sein, um Wert bei der Analyse zu erreichen. Die Datenqualität der erfassten Daten kann stark variieren, was eine genaue Analyse betrifft. Wert Der Wert der Informationen, die durch die Verarbeitung und Analyse großer Datensätze erreicht werden können. Der Wert kann auch durch eine Bewertung der anderen Qualitäten von Big Data gemessen werden. Der Wert kann auch die Rentabilität der Informationen darstellen, die aus der Analyse der Big Data gewonnen werden. Variabilität Die Charakteristik der sich ändernden Formate, Struktur oder Quellen von Big Data. Big Data kann strukturierte, unstrukturierte oder Kombinationen strukturierter und unstrukturierter Daten umfassen. Big Data Analyse kann Rohdaten aus mehreren Quellen integrieren. Bei der Verarbeitung von Rohdaten kann es sich auch um Transformationen unstrukturierter Daten zu strukturierten Daten handeln. Andere mögliche Eigenschaften von Big Data sind: Ob das gesamte System (d.h. n \{textstyle n} =all) erfasst oder aufgezeichnet wird oder nicht. Big Data kann oder darf nicht alle verfügbaren Daten aus Quellen enthalten. Feinkörnige und einzigartige Lexika Jeweils der Anteil der spezifischen Daten jedes einzelnen Elements pro erfasstem Element und wenn das Element und seine Eigenschaften richtig indiziert oder identifiziert werden. Verhältnis Wenn die erhobenen Daten gemeinsame Felder enthalten, die eine Zusammenführung oder Metaanalyse verschiedener Datensätze ermöglichen würden. Erweiterung Wenn neue Felder in jedem Element der erhobenen Daten einfach hinzugefügt oder geändert werden können. Skalierbarkeit Wenn sich die Größe des großen Datenspeichersystems schnell erweitern kann. Architektur Big Data Repositories haben in vielen Formen existiert, oft von Unternehmen mit einem besonderen Bedarf gebaut. Kommerzielle Anbieter boten historisch parallele Datenbankverwaltungssysteme für Big Data ab den 1990er Jahren an. Seit vielen Jahren veröffentlicht WinterCorp den größten Datenbankbericht. Die Teradata Corporation vermarktete 1984 das Parallelverarbeitungssystem DBC 1012. Teradata-Systeme waren die ersten, die 1992 1 Terabyte Daten speichern und analysieren. Festplattenlaufwerke waren 1991 2,5 GB, so dass sich die Definition von Big Data kontinuierlich nach Kryders Gesetz entwickelt. Teradata installierte 2007 das erste RDBMS-basierte System der petabyte Klasse. Ab 2017 gibt es ein paar Dutzend petabyte Klasse Teradata relationale Datenbanken installiert, von denen die größten über 50 PB. Systeme bis 2008 waren 100% strukturierte relationale Daten. Seitdem hat Teradata unstrukturierte Datentypen wie XML, JSON und Avro hinzugefügt.Im Jahr 2000 entwickelte Seisint Inc. (jetzt LexisNexis Risk Solutions) eine C++-basierte verteilte Plattform für die Datenverarbeitung und Abfrage, die als HPCC Systems Plattform bekannt ist. Dieses System verteilt, vertreibt, speichert und liefert strukturierte, halbstrukturierte und unstrukturierte Daten über mehrere Warenserver. Benutzer können Datenverarbeitungspipeline und Abfragen in einer deklarativen Datenfluss-Programmiersprache ECL schreiben. Datenanalysten, die in ECL arbeiten, sind nicht erforderlich, um Datenschemas vorn zu definieren und können sich vielmehr auf das jeweilige Problem konzentrieren, Daten in bestmöglicher Weise umgestalten, da sie die Lösung entwickeln. Im Jahr 2004 erwarb LexisNexis die Seisint Inc. und ihre High-Speed-parallele Verarbeitungsplattform und nutzte diese Plattform erfolgreich, um die Datensysteme von Choicepoint Inc. zu integrieren, wenn sie dieses Unternehmen im Jahr 2008 erworben haben. 2011 wurde die HPCC-Systemplattform unter der Apache v2.0-Lizenz Open Source. CERN und andere Physik-Experimente haben seit vielen Jahrzehnten große Datensätze gesammelt, die meist über Hochdurchsatz-Computing analysiert werden, anstatt die von der aktuellen "großen Daten"-Bewegung gemeint sind. Im Jahr 2004 veröffentlichte Google ein Papier über einen Prozess namens MapReduce, der eine ähnliche Architektur verwendet. Das MapReduce-Konzept bietet ein paralleles Verarbeitungsmodell und eine damit verbundene Implementierung wurde freigegeben, um riesige Datenmengen zu verarbeiten. Mit MapReduce werden Abfragen auf parallele Knoten aufgeteilt und parallel bearbeitet (dem Kartenschritt). Die Ergebnisse werden dann gesammelt und geliefert (der Reduktionsschritt). Der Rahmen war sehr erfolgreich, so dass andere den Algorithmus replizieren wollten. Daher wurde eine Umsetzung des MapReduce-Rahmens durch ein Apache Open-Source-Projekt namens Hadoop angenommen." Apache Spark wurde im Jahr 2012 als Reaktion auf Einschränkungen im MapReduce Paradigm entwickelt, da es die Möglichkeit, viele Operationen einzurichten (nicht nur Karte, gefolgt von Reduktion). MIKE2.0 ist ein offener Ansatz für das Informationsmanagement, der die Notwendigkeit von Überarbeitungen aufgrund großer Datenbedeutungen erkennt, die in einem Artikel mit dem Titel "Big Data Solution Angebot" identifiziert werden. Die Methodik befasst sich mit der Verarbeitung großer Daten in Bezug auf nützliche Permutationen von Datenquellen, Komplexität in Zusammenhängen und Schwierigkeit, einzelne Datensätze zu löschen (oder zu modifizieren). Studien im Jahr 2012 zeigten, dass eine mehrschichtige Architektur eine Option war, um die Probleme, die Big Data präsentiert. Eine verteilte parallele Architektur verteilt Daten über mehrere Server; diese parallelen Ausführungsumgebungen können die Datenverarbeitungsgeschwindigkeiten drastisch verbessern. Diese Art der Architektur fügt Daten in ein paralleles DBMS ein, das die Nutzung von MapReduce und Hadoop-Frameworks implementiert. Diese Art von Rahmen sieht vor, die Verarbeitungsleistung für den Endbenutzer durch Verwendung eines Frontend-Anwendungsservers transparent zu machen. Der Datensee ermöglicht es einer Organisation, ihren Fokus von zentraler Kontrolle auf ein gemeinsames Modell zu verschieben, um auf die sich ändernde Dynamik des Informationsmanagements zu reagieren. Dies ermöglicht eine schnelle Trennung von Daten in den Datensee, wodurch die Überkopfzeit reduziert wird. Technologies A 2011McKinsey Global Institute Report charakterisiert die wichtigsten Komponenten und das Ökosystem von Big Data wie folgt: Techniken zur Analyse von Daten, wie A/B-Tests, maschinelles Lernen und natürliche Sprachverarbeitung Big Data Technologien, wie Business Intelligence, Cloud Computing und Datenbanken Visualisierung, wie Charts, Graphen und andere Anzeigen der DatenMultidimensionale Big Data können auch als OLAP-Datenwürfel oder mathematische Tensors dargestellt werden. Array-Datenbanksysteme haben darauf hingerichtet, Speicher und hochrangige Abfrageunterstützung auf diesem Datentyp bereitzustellen. Zu den weiteren Technologien, die auf Big Data angewendet werden, gehören effiziente Tensor-basierte Berechnungen, wie multilineares Subraum-Erlernen, massiv parallelverarbeitende (MPP) Datenbanken, Such-basierte Anwendungen, Data Mining, verteilte Dateisysteme, verteilte Cache (z.B. Burst Puffer und Memcached), verteilte Datenbanken, Cloud- und HPC-basierte Infrastruktur (Anwendungen, Speicher- und Rechenressourcen) und Internet. Obwohl viele Ansätze und Technologien entwickelt wurden, bleibt es immer noch schwierig, maschinelles Lernen mit großen Daten durchzuführen. Einige MPP relationale Datenbanken haben die Möglichkeit, Petabytes von Daten zu speichern und zu verwalten. Implizit ist die Fähigkeit, die Verwendung der großen Datentabellen im RDBMS zu laden, zu überwachen, zu sichern und zu optimieren. Das Topologische Datenanalyse-Programm von DARPA sucht die grundlegende Struktur massiver Datensätze und im Jahr 2008 ging die Technologie mit dem Start eines Unternehmens namens Ayasdi bekannt. Die Praktizierenden von Big Data Analytics-Prozessen sind in der Regel feindlich, um die gemeinsame Speicherung zu verlangsamen, wobei die direkte Speicherung (DAS) in ihren verschiedenen Formen von Solid State Drive (SSD) bis High Capacity SATA-Disk bevorzugt wird, die innerhalb von parallelen Verarbeitungsknoten vergraben ist. Die Wahrnehmung gemeinsamer Speicherarchitekturen – Speicherfeldnetz (SAN) und netzgebundener Speicher (NAS) – ist, dass sie relativ langsam, komplex und teuer sind. Diese Qualitäten sind nicht mit großen Datenanalysesystemen, die auf die Systemleistung, die Waareninfrastruktur und die geringen Kosten stoßen, vereinbar. Die Echtzeit- oder Nah-Real-Time-Informations-Lieferung ist eine der charakteristischen Merkmale der Big Data Analytics. Die Latency wird daher immer und wo immer möglich vermieden. Daten in direkt angebundenem Speicher oder Datenträger sind gut - Daten auf Speicher oder Festplatte am anderen Ende einer FC SAN-Verbindung sind nicht. Die Kosten eines SAN in der für Analyseanwendungen benötigten Größenordnung sind wesentlich höher als andere Speichertechniken. Anwendungen Big Data hat den Bedarf an Informationsmanagement-Spezialisten so stark erhöht, dass Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP und Dell mehr als 15 Milliarden US-Dollar für Softwareunternehmen ausgegeben haben, die sich auf das Datenmanagement und die Analytik spezialisiert haben. Im Jahr 2010 war diese Branche mehr als 100 Milliarden Dollar wert und wächst mit fast 10 Prozent im Jahr: etwa doppelt so schnell wie das Softwaregeschäft insgesamt. Entwickelte Volkswirtschaften nutzen zunehmend datenintensive Technologien. Weltweit gibt es 4,6 Milliarden Mobilfunk-Abonnements, und zwischen 1 Milliarde und 2 Milliarden Menschen Zugang zum Internet. Zwischen 1990 und 2005 gingen weltweit mehr als 1 Milliarde Menschen in die Mittelschicht ein, was bedeutet, dass mehr Menschen mehr Literaten wurden, was wiederum zu Informationswachstum führte. Die effektive Fähigkeit der Welt, Informationen über Telekommunikationsnetze auszutauschen, betrug 1986 281 Petabytes, 1993 471 Petabytes, 2000 2,2 Exabytes, 2007 65 Exabytes und Prognosen, die den Internetverkehr bis 2014 jährlich auf 667 Exabytes verlagern. Nach einer Schätzung ist ein Drittel der global gespeicherten Informationen in Form von alphanumerischen Texten und Bilddaten, die für die meisten Big Data-Anwendungen am nützlichsten sind. Dies zeigt auch das Potential noch nicht genutzter Daten (d.h. in Form von Video- und Audioinhalten). Während viele Anbieter Off-the-Shelf-Lösungen für Big Data anbieten, empfehlen Experten die Entwicklung von In-house-Lösungen, die speziell für die Lösung des Problems des Unternehmens zur Verfügung stehen, wenn das Unternehmen über ausreichende technische Fähigkeiten verfügt. Regierung Die Verwendung und die Übernahme großer Daten innerhalb staatlicher Prozesse ermöglicht Effizienzen in Bezug auf Kosten, Produktivität und Innovation, kommt aber nicht ohne ihre Fehler. Die Datenanalyse erfordert oft mehrere Teile der Regierung (zentrale und lokale) in Zusammenarbeit zu arbeiten und neue und innovative Prozesse zu schaffen, um das gewünschte Ergebnis zu liefern. Eine gemeinsame Regierungsorganisation, die Big Data nutzt, ist die National Security Administration (NSA), die die Aktivitäten des Internets ständig auf der Suche nach potenziellen Mustern von verdächtigen oder illegalen Aktivitäten überwacht, die ihr System abholen kann. Zivile Registrierung und lebenswichtige Statistiken (CRVS) sammeln alle Zertifikate Status von Geburt bis Tod. CRVS ist eine Quelle großer Daten für Regierungen. Internationale Entwicklung Die Forschung über die effektive Nutzung von Informations- und Kommunikationstechnologien für die Entwicklung (auch bekannt als ICT4D) schlägt vor, dass Big Data-Technologie wichtige Beiträge leisten kann, aber auch einzigartige Herausforderungen für die internationale Entwicklung darstellen kann. Fortschritte in der Big Data-Analyse bieten kostengünstige Möglichkeiten, um die Entscheidungsfindung in kritischen Entwicklungsbereichen wie Gesundheit, Beschäftigung, wirtschaftliche Produktivität, Kriminalität, Sicherheit und Naturkatastrophe und Ressourcenmanagement zu verbessern. Darüber hinaus bietet die nutzergenerierte Daten neue Möglichkeiten, dem Ungehörten eine Stimme zu geben. Die Herausforderungen für die Entwicklung von Regionen wie unzureichender technologischer Infrastruktur und wirtschaftlicher und menschlicher Ressourcenknappheit verschärften jedoch bestehende Bedenken mit großen Daten wie Datenschutz, unvollkommener Methodik und Interoperabilitätsfragen. Die Herausforderung von "großen Daten für die Entwicklung" entwickelt sich zur Anwendung dieser Daten durch maschinelles Lernen, bekannt als "künstliche Intelligenz für die Entwicklung (AI4D). Leistungen Eine große praktische Anwendung großer Daten für die Entwicklung war "die Armut mit Daten zu bekämpfen". Im Jahr 2015 schätzten Blumenstock und Kollegen Armut und Reichtum von Mobiltelefon-Metadaten voraus und 2016 kombinierten Jean und Kollegen Satellitenbild und maschinelles Lernen, um Armut vorherzusagen. Mit digitalen Spurendaten zum Studium des Arbeitsmarktes und der digitalen Wirtschaft in Lateinamerika argumentieren Hilbert und Kollegen, dass digitale Spurendaten mehrere Vorteile wie: Thematische Abdeckung: einschließlich Bereiche, die bisher schwierig oder unmöglich waren, Geografische Abdeckung zu messen: unsere internationalen Quellen lieferten für fast alle Länder sibilisierbare und vergleichbare Daten, darunter viele kleine Länder, die in der Regel nicht in internationalen Erfindern enthalten sind. Detailtiefe: Bereitstellung feinkörniger Daten mit vielen miteinander verbundenen Variablen und neuen Aspekten, wie Netzwerkverbindungen Zeit- und Zeitreihen: Grafiken können innerhalb von Tagen nach der Erhebung erstellt werden Herausforderungen Gleichzeitig werden die Arbeiten mit digitalen Spurendaten anstelle von traditionellen Umfragedaten nicht die traditionellen Herausforderungen beseitigt, die bei der Arbeit im Bereich der internationalen quantitativen Analyse mit sich bringen.Prioritäten ändern sich, aber die grundlegenden Diskussionen bleiben gleich. Zu den wichtigsten Herausforderungen gehören: Repräsentativität. Während sich die traditionellen Entwicklungsstatistiken hauptsächlich auf die Repräsentativität von Stichproben der Zufallsstichproben beziehen, sind digitale Spurendaten nie eine Stichprobe. Allgemeines. Während Beobachtungsdaten diese Quelle immer sehr gut darstellen, stellt sie nur das dar, was sie darstellt, und nichts mehr. Während es versucht, von bestimmten Beobachtungen einer Plattform zu breiteren Einstellungen zu verallgemeinern, ist dies oft sehr täuschend. Harmonisierung. Digitale Daten für die Daten für die Spurensicherung erfordern weiterhin eine internationale Harmonisierung der Indikatoren. Sie fügt die Herausforderung der sogenannten Datenfusion, der Harmonisierung verschiedener Quellen hinzu. Datenüberlastung. Analysten und Institutionen werden nicht verwendet, um effektiv mit einer Vielzahl von Variablen umzugehen, die effizient mit interaktiven Dashboards durchgeführt wird. Die Praktizierenden haben noch immer keinen Standard-Workflow, der Forschern, Nutzern und Politikern ermöglicht, effizient und effektiv zu arbeiten. Healthcare Big Data Analytics hat die Verbesserung der Gesundheitsversorgung durch die Bereitstellung personalisierter Medizin und präskriptiver Analytik, klinischer Risikoeingriff und prognostizierter Analytik, Abfall- und Pflegevariabilitätsreduktion, automatisierte externe und interne Berichterstattung von Patientendaten, standardisierte medizinische Bedingungen und Patientenregistrierungen und fragmentierte Punktlösungen unterstützt. Einige Verbesserungsbereiche sind eher aspirational als tatsächlich umgesetzt. Das Niveau der Daten, die innerhalb der Gesundheitssysteme generiert werden, ist nicht trivial. Mit der zusätzlichen Annahme von mHealth, eHealth und Wearable Technologien wird das Datenvolumen weiter zunehmen. Dazu gehören elektronische Gesundheitsdaten, Abbildungsdaten, geduldig generierte Daten, Sensordaten und andere Formen schwer zu verarbeitender Daten. Es besteht nun ein noch größeres Bedürfnis, dass solche Umgebungen stärker auf die Daten- und Informationsqualität achten. " Große Daten bedeuten oft "Dirty Data" und der Anteil an Datenungenauigkeiten steigt mit Datenvolumenwachstum." Die menschliche Kontrolle auf der großen Datenskala ist unmöglich und es besteht ein dringender Bedarf an Gesundheitsdiensten für intelligente Werkzeuge zur Genauigkeit und Glaubwürdigkeitskontrolle und zum Umgang mit Informationen. Während umfangreiche Informationen im Gesundheitswesen jetzt elektronisch sind, passt es unter den großen Datenschirm, da die meisten unstrukturiert und schwer zu bedienen sind. Die Nutzung großer Daten im Gesundheitswesen hat erhebliche ethische Herausforderungen angestoßen, die von Risiken für individuelle Rechte, Privatsphäre und Autonomie bis hin zu Transparenz und Vertrauen reichen. Die großen Daten in der Gesundheitsforschung sind besonders vielversprechend in der explorativen biomedizinischen Forschung, da die datengesteuerte Analyse schneller voranschreiten kann als die hypothesisgetriebene Forschung. Dann können Trends in der Datenanalyse in der traditionellen, Hypothese-getriebenen folgen biologischen Forschung und schließlich klinische Forschung getestet werden. Ein damit zusammenhängender Anwendungsteilbereich, der sich stark auf Big Data stützt, ist im Gesundheitsbereich die computergestützte Diagnose in der Medizin. So ist es beispielsweise üblich, zur Epilepsieüberwachung täglich 5 bis 10 GB Daten zu erstellen. Ebenso durchschnittlich ein einziges unkomprimiertes Bild der Brust-Tomosynthese 450 MB Daten. Dies sind nur wenige der vielen Beispiele, bei denen die computergestützte Diagnose große Daten verwendet. Aus diesem Grund wurden große Daten als eine der sieben wichtigsten Herausforderungen erkannt, die computergestützte Diagnosesysteme überwinden müssen, um das nächste Leistungsniveau zu erreichen. Bildung Eine McKinsey Global Institute Studie fand einen Mangel von 1,5 Millionen hochqualifizierten Daten-Profis und Manager und eine Reihe von Universitäten einschließlich Universität von Tennessee und UC Berkeley, haben Master-Programme für diese Nachfrage erstellt. Private Boot Camps haben auch Programme entwickelt, um diese Nachfrage zu erfüllen, einschließlich kostenlose Programme wie The Data Incubator oder kostenpflichtige Programme wie General Assembly. Im speziellen Bereich des Marketings ist eines der von Wedel und Kannan hervorgehobenen Probleme, dass das Marketing mehrere Sub-Domains (z.B. Werbung, Werbeaktionen, Produktentwicklung, Branding) hat, die alle verschiedene Arten von Daten verwenden. Da ein-Größe-fits-alle analytischen Lösungen nicht wünschenswert sind, sollten Business Schools Marketing Manager vorbereiten, um breites Wissen über alle verschiedenen Techniken, die in diesen Subdomains verwendet werden, um ein großes Bild zu erhalten und effektiv mit Analysten zu arbeiten. Medien Um zu verstehen, wie die Medien Big Data verwenden, ist es zunächst notwendig, einen gewissen Kontext in den Mechanismus für den Medienprozess bereitzustellen. Es wurde vorgeschlagen, dass die Praktizierenden in den Medien und in der Werbung große Daten als viele Aktionspunkte von Informationen über Millionen von Individuen ansprechen. Die Industrie scheint sich von der traditionellen Methode der Verwendung von speziellen Medienumgebungen wie Zeitungen, Zeitschriften oder Fernsehsendungen wegzubewegen und greift stattdessen in Verbraucher mit Technologien ein, die gezielte Menschen zu optimalen Zeiten an optimalen Orten erreichen. Das ultimative Ziel ist es, eine Botschaft oder einen Inhalt zu dienen oder zu vermitteln, die (statistisch gesprochen) im Einklang mit der Denkweise des Verbrauchers ist. Zum Beispiel werden die Publikationsumgebungen zunehmend zugeschnittene Botschaften (Werbungen) und Inhalte (Artikel) an Verbraucher appellieren, die ausschließlich durch verschiedene Data-Mining-Aktivitäten gelehrt wurden. Ziel der Verbraucher (für Werbung durch Vermarkter)Datenerfassung Journalismus: Verleger und Journalisten nutzen Big Data Tools, um einzigartige und innovative Einblicke und Infografiken zu bieten. Kanal 4, der britische öffentlich-rechtliche Fernsehsender, ist ein führender Anbieter im Bereich der Big Data- und Datenanalyse. Versicherung Krankenversicherungsanbieter sammeln Daten über soziale "Bestimmungen der Gesundheit" wie Nahrung und TV-Verbrauch, Familienstand, Kleidungsgröße und Kaufgewohnheiten, von denen sie Vorhersagen über die Gesundheitskosten machen, um gesundheitliche Probleme in ihren Kunden zu erkennen. Es ist umstritten, ob diese Vorhersagen derzeit für die Preisgestaltung verwendet werden. Internet der Dinge (IoT)Big-Daten und die IoT-Arbeit in Verbindung. Daten, die aus IoT-Geräten gewonnen werden, ermöglichen eine Abbildung der Geräte-Interconnectivity. Solche Kartierungen wurden von der Medienindustrie, von Unternehmen und Regierungen genutzt, um ihr Publikum genauer anzusprechen und die Medieneffizienz zu steigern. Das IoT wird auch zunehmend als Mittel zur Erfassung sensorischer Daten angenommen, und diese sensorischen Daten wurden in medizinischen, Fertigungs- und Transportkontexten verwendet. Kevin Ashton, der digitale Innovationsexperte, der mit der Prägung des Begriffs ausgezeichnet wird, definiert das Internet der Dinge in diesem Zitat: "Wenn wir Computer hatten, die alles wussten, was über Dinge zu wissen war – mit Daten, die sie ohne Hilfe von uns gesammelt haben – würden wir alles verfolgen und zählen können und Abfälle, Verlust und Kosten stark reduzieren. Wir würden wissen, wann die Dinge ersetzt, repariert oder daran erinnert werden müssen und ob sie frisch oder am besten vorbei waren." Informationstechnologie Gerade seit 2015 sind große Daten in den Geschäftsbetrieben als Instrument zur effizienteren Arbeit und zur Optimierung der Sammlung und Verbreitung von Informationstechnologie (IT) aufgetreten. Die Verwendung großer Daten zur Lösung von IT- und Datenerfassungsproblemen innerhalb eines Unternehmens wird als IT-Betriebsanalyse (ITOA) bezeichnet. Durch die Anwendung großer Datenprinzipien in die Konzepte der Maschinenintelligenz und der Tiefenrechnung können IT-Abteilungen potenzielle Probleme vorhersagen und Lösungen bereitstellen, bevor die Probleme sogar passieren. In dieser Zeit begannen die ITOA-Unternehmen auch eine wichtige Rolle im Systemmanagement zu spielen, indem sie Plattformen anbieten, die einzelne Datensilos zusammenbrachen und Erkenntnisse aus dem gesamten System generierten, anstatt aus isolierten Datentaschen. Fallstudien Regierung China Die integrierte gemeinsame Operationsplattform (IJOP, ↓ERTICHERHEITSSCHUTZ DER VERÖFFENTLICHUNGEN) wird von der Regierung genutzt, um die Bevölkerung zu überwachen, insbesondere die Uyghurs. Biometrie, einschließlich DNA-Proben, werden durch ein Programm der freien Körper gesammelt. Bis 2020 plant China, allen seinen Bürgern eine persönliche "Sozialkredit"-Score basierend auf dem Verhalten zu geben. Das Social Credit System, das jetzt in einer Reihe von chinesischen Städten pilotiert wird, wird als eine Form der Massenüberwachung betrachtet, die Big Data Analyse-Technologie verwendet. Indien Große Datenanalyse wurde für die BJP versucht, die indische Generalwahl 2014 zu gewinnen. Die indische Regierung nutzt zahlreiche Techniken, um festzustellen, wie die indische Wähler auf Regierungsaktionen reagieren, sowie Ideen für politische Augmentation. Israel Personalisierte diabetische Behandlungen können durch die große Datenlösung von GlucoMe erstellt werden. Vereinigtes Königreich Beispiele für die Verwendung großer Daten in öffentlichen Diensten: Daten zu verschreibungspflichtigen Medikamenten: Durch die Verbindung von Herkunft, Ort und Zeit jedes Rezepts konnte eine Forschungseinheit die beträchtliche Verzögerung zwischen der Freisetzung eines bestimmten Medikaments und einer englischweiten Anpassung des National Institute for Health and Care Excellence-Richtlinien zum Ausdruck bringen. Dies deutet darauf hin, dass neue oder aktuellste Medikamente einige Zeit brauchen, um sich an den allgemeinen Patienten zu filtern. Daten einbinden: Eine lokale Behörde hat Daten über Dienstleistungen wie Straßenbauflös, mit Dienstleistungen für gefährdete Menschen, wie etwa Mahlzeiten auf Rädern, kombiniert. Die Verbindung der Daten erlaubte der lokalen Behörde, jegliche Wettervorhersage zu vermeiden. USA 2012 kündigte die Obama-Regierung die Big Data Research and Development Initiative an, um herauszufinden, wie Big Data verwendet werden könnte, um wichtige Probleme der Regierung zu lösen. Die Initiative besteht aus 84 verschiedenen großen Datenprogrammen, die sich über sechs Abteilungen verteilen. Große Datenanalyse spielte in Barack Obamas erfolgreicher Wiederwahlkampagne 2012 eine große Rolle. Die US-Bundesregierung besitzt fünf der zehn mächtigsten Supercomputer der Welt. Das Utah Data Center wurde von der National Security Agency der Vereinigten Staaten errichtet. Nach Fertigstellung kann die Anlage eine große Menge an Informationen verarbeiten, die von der NSA über das Internet gesammelt werden. Die genaue Menge an Speicherplatz ist unbekannt, aber neuere Quellen behaupten, es wird auf der Reihenfolge von einigen Exabytes sein. Dies hat Sicherheitsbedenken bezüglich der Anonymität der erhobenen Daten vorgelegt. Retail Walmart behandelt mehr als 1 Million Kundentransaktionen jede Stunde, die in Datenbanken importiert werden geschätzt mehr als 2,5 Petabytes (2560 Terabytes) Daten enthalten - das entspricht 167 mal der Informationen in allen Büchern in der US-Kongressbibliothek. Windermere Real Estate nutzt Standortinformationen von fast 100 Millionen Fahrern, um neue Heimkäufer zu helfen, ihre typischen Antriebszeiten zu und von der Arbeit über verschiedene Tageszeiten zu bestimmen. FICO Card Detection System schützt Konten weltweit. Wissenschaft Die großen Hadron Collider-Experimente stellen etwa 150 Millionen Sensoren dar, die Daten 40 Millionen Mal pro Sekunde liefern. Es gibt fast 600 Millionen Kollisionen pro Sekunde. Nach dem Filtern und Unterlassen von der Aufnahme von mehr als 99,99995% dieser Ströme gibt es 1.000 interessierende Kollisionen pro Sekunde. Der Datenfluss aus allen vier LHC-Experimenten stellt somit nur mit weniger als 0,001 % der Sensorstromdaten vor der Replikation 25 petabytes Jahresrate dar (Stand 2012). Dies wird nach der Replikation fast 200 Petabytes. Würden alle Sensordaten in LHC aufgezeichnet, wäre der Datenfluss äußerst schwer zu bearbeiten. Der Datenfluss würde vor der Replikation 150 Millionen Petabytes jährlich oder fast 500 Exabyte pro Tag übersteigen. Um die Zahl in Perspektive zu setzen, entspricht dies 500 Quitillion (5 x 1020) Bytes pro Tag, fast 200 mal mehr als alle anderen Quellen in der Welt kombiniert. Der Quadratkilometer Array ist ein Funkteleskop, das aus Tausenden von Antennen gebaut wird. Es wird voraussichtlich bis 2024 einsatzbereit sein. Diese Antennen sollen zusammenfassend 14 Exabyte sammeln und ein Petabyte pro Tag speichern. Es gilt als eines der ehrgeizigsten wissenschaftlichen Projekte, die je durchgeführt wurden. Als die Sloan Digital Sky Survey (SDSS) im Jahr 2000 astronomische Daten sammelte, wuchs sie in den ersten Wochen mehr als alle Daten, die in der Geschichte der Astronomie gesammelt wurden. SDSS mit einer Rate von etwa 200 GB pro Nacht hat mehr als 140 Terabyte von Informationen angesammelt. Wenn das große Synoptische Umfrage-Teleskop, Nachfolger von SDSS, online kommt 2020, seine Designer erwarten, dass es diese Menge an Daten alle fünf Tage zu erwerben. Das menschliche Genom zu dekodieren dauerte ursprünglich 10 Jahre, um zu verarbeiten; jetzt kann es in weniger als einem Tag erreicht werden. Die DNA-Sequencer haben die Sequenzierungskosten in den letzten zehn Jahren um 10.000 geteilt, was 100 mal billiger ist als die durch Moore's Gesetz vorhergesagte Kostensenkung. Das NASA Center for Climate Simulation (NCCS) speichert 32 Petabytes von Klimabeobachtungen und Simulationen auf dem Supercomputing Cluster Discover. Die DNAStack von Google kompiliert und organisiert DNA-Proben von genetischen Daten aus der ganzen Welt, um Krankheiten und andere medizinische Defekte zu identifizieren. Diese schnellen und exakten Berechnungen eliminieren jegliche "Friktionspunkte" oder menschliche Fehler, die von einem der zahlreichen Wissenschafts- und Biologieexperten mit der DNA gemacht werden könnten. DNAStack, ein Teil von Google Genomics, ermöglicht es Wissenschaftlern, die riesige Probe der Ressourcen von Googles Suchserver zu skalieren soziale Experimente, die in der Regel Jahre dauern würde, sofort. Die DNA-Datenbank von 23andme enthält genetische Informationen von über 1.000.000 Menschen weltweit. Das Unternehmen erforscht den Verkauf der "anonymen aggregierten genetischen Daten" an andere Forscher und Pharmaunternehmen zu Forschungszwecken, wenn Patienten ihre Zustimmung geben. Ahmad Hariri, Professor für Psychologie und Neurowissenschaften an der Duke University, der seit 2009 23andMe in seiner Forschung verwendet, erklärt, dass der wichtigste Aspekt des neuen Dienstes des Unternehmens ist, dass es genetische Forschung zugänglich und relativ billig für Wissenschaftler macht. Eine Studie, die 15 Genom-Standorte im Zusammenhang mit Depression in 23andMe Datenbank identifiziert, führt zu einem Anstieg der Anforderungen an das Projektarchiv mit 23andMe-Feldern fast 20 Anträge auf Zugriff auf die Depressionsdaten in den zwei Wochen nach Veröffentlichung des Papiers. Die rechnerische Fluiddynamik (CFD) und die hydrodynamische Turbulenzforschung erzeugen massive Datensätze. Die Johns Hopkins Turbulence Databases (JHTDB) enthält über 350 Terabyte von Spatiotemporalfeldern aus direkten numerischen Simulationen verschiedener turbulenter Strömungen. Solche Daten waren mit herkömmlichen Methoden wie dem Herunterladen von Flat-Simulationsausgabedateien schwer zu teilen. Die Daten innerhalb von JHTDB können über "virtuelle Sensoren" mit verschiedenen Zugriffsmodi aufgerufen werden: von direkten Web-Browser-Abfragen, Zugriff über Matlab, Python, Fortran und C-Programme, die auf Client-Plattformen ausgeführt werden, um Dienste zum Herunterladen von Rohdaten auszuschneiden. Die Daten wurden in über 150 wissenschaftlichen Publikationen verwendet. Sport Große Daten können verwendet werden, um Trainings- und Verständnis-Konkurrenten mit Sportsensoren zu verbessern. Es ist auch möglich, Sieger in einem Spiel mit Big Data Analytics vorherzusagen. Auch die zukünftige Performance von Spielern könnte vorhergesagt werden. So wird der Wert und das Gehalt der Spieler durch die während der Saison erhobenen Daten bestimmt. In der Formel Ein Rennen, Rennwagen mit Hunderten von Sensoren erzeugen Terabytes von Daten. Diese Sensoren erfassen Datenpunkte vom Reifendruck bis zur Kraftstoffverbrennungseffizienz. Anhand der Daten entscheiden Ingenieure und Datenanalysten, ob Anpassungen vorgenommen werden sollen, um ein Rennen zu gewinnen. Darüber hinaus versuchen Rennteams mit Big Data, die Zeit vorherzusagen, die sie das Rennen vorher beenden werden, basierend auf Simulationen, die über die Saison erhobene Daten verwenden. Technology eBay.com verwendet zwei Datenlager bei 7.5 petabytes und 40PB sowie einen 40PB Hadoop Cluster für Suche, Verbraucherempfehlungen und Merchandising. Amazon.com behandelt jeden Tag Millionen Back-End-Operationen sowie Anfragen von mehr als einer halben Million Drittanbieter-Händler. Die Kerntechnologie, die Amazon laufen lässt, ist Linux-basiert und seit 2005 hatten sie die drei größten Linux-Datenbanken der Welt, mit Kapazitäten von 7,8 TB, 18.5 TB und 24.7 TB. Facebook behandelt 50 Milliarden Fotos von seiner Nutzerbasis. Ab Juni 2017 erreichte Facebook 2 Milliarden monatlich aktive Nutzer. Google hat ab August 2012 rund 100 Milliarden Recherchen pro Monat durchgeführt. COVID-19 Während der COVID-19 Pandemie wurden große Daten erhoben, um die Auswirkungen der Krankheit zu minimieren. Wichtige Anwendungen von Big Data beinhalteten die Minimierung der Ausbreitung des Virus, der Fallerkennung und der Entwicklung der medizinischen Behandlung. Regierungen nutzten große Daten, um infizierte Menschen zu verfolgen, um die Verbreitung zu minimieren. Zu den frühen Adoptern gehörten China, Taiwan, Südkorea und Israel. Forschungsaktivitäten Verschlüsselte Suche und Clusterbildung in großen Daten wurden im März 2014 bei der American Society of Engineering Education gezeigt. Gautam Siwach, der sich bei der Bewältigung der Herausforderungen von Big Data durch das MIT Computer Science and Artificial Intelligence Laboratory und Amir Esmailpour der UNH Research Group beschäftigte, untersuchte die wichtigsten Merkmale großer Daten als Clusterbildung und deren Vernetzung. Sie konzentrierten sich auf die Sicherheit von Big Data und die Orientierung des Begriffs auf das Vorhandensein von verschiedenen Arten von Daten in verschlüsselter Form an der Cloud-Schnittstelle, indem sie die Rohdefinitionen und Echtzeitbeispiele in der Technologie bereitstellen. Darüber hinaus schlugen sie einen Ansatz zur Identifizierung der Kodierungstechnik vor, um eine beschleunigte Suche über verschlüsselten Text voranzutreiben, der zu den Sicherheitsverbesserungen in großen Daten führt. Im März 2012 kündigte das Weiße Haus eine nationale "Big Data Initiative" an, die aus sechs Bundesabteilungen und Agenturen bestand, die mehr als 200 Millionen US-Dollar für Big Data Research-Projekte begehen. Die Initiative umfasste eine National Science Foundation "Expeditionen in Computing" Zuschüsse von 10 Millionen US-Dollar über fünf Jahre an das AMPLab an der University of California, Berkeley. Das AMPLab erhielt auch Gelder von DARPA, und über ein Dutzend Industriesponsoren und nutzt große Daten, um eine breite Palette von Problemen zu attackieren, von der Vorhersage der Verkehrsverstopfung bis zur Bekämpfung von Krebs. Das Weiße Haus Big Data Die Initiative umfasste auch ein Engagement der Abteilung Energie, 25 Mio. US-Dollar für die Finanzierung von über fünf Jahren zur Errichtung des Skalierbaren Data Management, Analysis and Visualization (SDAV) Instituts, geleitet vom Lawrence Berkeley National Laboratory der Energieabteilung bereitzustellen. Der SDAV Das Institut will die Expertise von sechs nationalen Labors und sieben Universitäten zusammenbringen, um neue Werkzeuge zu entwickeln, um Wissenschaftlern bei der Verwaltung und Visualisierung von Daten über die Supercomputer der Abteilung zu helfen. Der US-Bundesstaat Massachusetts kündigte im Mai 2012 die Massachusetts Big Data Initiative an, die von der Staatsregierung und Privatunternehmen an eine Vielzahl von Forschungseinrichtungen finanziert. Das Massachusetts Institute of Technology beherbergt das Intel Science and Technology Center for Big Data im MIT Computer Science and Artificial Intelligence Laboratory, das Regierungs-, Unternehmens- und institutionelle Förder- und Forschungsanstrengungen kombiniert. Die Europäische Kommission fördert das zweijährige Big Data Public Private Forum durch ihr Siebtes Rahmenprogramm, um Unternehmen, Akademiker und andere Interessenvertreter bei der Diskussion großer Datenfragen zu engagieren. Das Projekt zielt darauf ab, eine Strategie in Bezug auf Forschung und Innovation zu definieren, um Unterstützungsmaßnahmen der Europäischen Kommission bei der erfolgreichen Umsetzung der Big Data Economy zu unterstützen. Ergebnisse dieses Projekts werden als Input für Horizon 2020, ihr nächstes Rahmenprogramm, verwendet. Die britische Regierung gab im März 2014 die Gründung des Alan Turing Institute bekannt, benannt nach dem Computer-Pionier und Code-Brecher, der sich auf neue Wege zur Sammlung und Analyse großer Datensätze konzentrieren wird. An der University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day zeigten die Teilnehmer, wie die Datenvisualisierung das Verständnis und die Attraktivität großer Datensätze erhöhen und ihre Geschichte der Welt vermitteln kann. Computational social sciences – Jeder kann die von großen Dateninhabern wie Google und Twitter bereitgestellten Applikations-Programmierschnittstellen (APIs) nutzen, um in den Sozial- und Verhaltenswissenschaften zu forschen. Oft werden diese APIs kostenlos zur Verfügung gestellt.Tobias Preis et al.used Google Trendsdaten zeigen, dass Internetnutzer aus Ländern mit einem höheren Bruttoinlandsprodukt pro Kopf (BIPs) eher nach Informationen über die Zukunft suchen als Informationen über die Vergangenheit. Die Ergebnisse legen nahe, dass es eine Verbindung zwischen Online-Verhalten und realen Wirtschaftsindikatoren geben kann. Die Autoren der Studie untersuchten Google-Abfragen-Logs, die nach dem Verhältnis des Suchvolumens für das kommende Jahr (2011) zum Suchvolumen für das vorherige Jahr (2009) erstellt wurden, das sie den "zukünftigen Orientierungsindex" nennen. Sie verglichen den zukünftigen Orientierungsindex mit dem Pro-Kopf-BIP jedes Landes und fanden eine starke Tendenz für Länder, in denen Google-Nutzer mehr über die Zukunft nachfragen, um ein höheres BIP zu haben. Tobias Preis und seine Kollegen Helen Susannah Moat und H. Eugene Stanley stellten eine Methode zur Identifizierung von Online-Vorläufern für Aktienmarktbewegungen vor, wobei Handelsstrategien auf Basis von Suchvolumendaten von Google Trends verwendet werden. Ihre Analyse von Google-Suchvolumen für 98 Begriffe unterschiedlicher finanzieller Relevanz, veröffentlicht in wissenschaftlichen Berichten, deutet darauf hin, dass die Zunahmen der Suchvolumen für finanziell relevante Suchbegriffe tendenziell großen Verlusten in den Finanzmärkten vorangehen. Große Datensätze kommen mit algorithmischen Herausforderungen, die bisher nicht existierten. Es ist daher zu erkennen, dass manche die Verarbeitungsweisen grundlegend verändern müssen. Die Workshops zu Algorithmen für moderne Massive Data Sets (MMDS) bringen Informatiker, Statistiker, Mathematiker und Datenanalyse-Praktizierende zusammen, um algorithmische Herausforderungen von Big Data zu diskutieren. In Bezug auf Big Data sind solche Größenbegriffe relativ. Wie gesagt wird: "Wenn die Vergangenheit von irgendwelchen Anleitungen ist, werden heute die großen Daten wahrscheinlich in naher Zukunft nicht als solche betrachtet." Eine Forschungsfrage, die über große Datensätze gestellt wird, ist, ob man die vollständigen Daten ansieht, um bestimmte Schlussfolgerungen über die Eigenschaften der Daten zu ziehen, oder ob eine Probe ausreichend ist. Der Name Big Data selbst enthält einen Begriff, der mit der Größe zusammenhängt, und das ist ein wichtiges Merkmal der Big Data. Die Stichprobe ermöglicht jedoch die Auswahl der richtigen Datenpunkte innerhalb des größeren Datensatzes, um die Merkmale der gesamten Bevölkerung zu schätzen. Bei der Herstellung verschiedener Arten von sensorischen Daten wie Akustik, Vibration, Druck, Strom, Spannung und Controller-Daten sind in kurzen Zeitabständen verfügbar. Um Ausfallzeiten vorherzusagen, ist es möglicherweise nicht notwendig, alle Daten zu betrachten, aber eine Probe kann ausreichend sein. Große Daten können durch verschiedene Datenpunktkategorien wie demographische, psychografische, Verhaltens- und Transaktionsdaten aufgeschlüsselt werden. Mit großen Datensätzen können Vermarkter für mehr strategisches Ziel maßgeschneiderte Verbrauchersegmente erstellen und nutzen. Es gab einige Arbeiten in Probenahmealgorithmen für große Daten. Es wurde eine theoretische Formulierung zur Stichprobenerhebung von Twitter-Daten entwickelt. Critique Critiques of the big data paradigm kommen in zwei Geschmacksrichtungen: diejenigen, die die Auswirkungen des Ansatzes selbst in Frage stellen, und diejenigen, die die Art und Weise, wie es derzeit getan wird. Ein Ansatz zu dieser Kritik ist der Bereich kritischer Datenstudien. Kritiken des Big Data Paradigmas "Ein entscheidendes Problem ist, dass wir nicht viel über die zugrunde liegenden empirischen Mikroprozesse wissen, die zur Entstehung der typischen Netzwerkeigenschaften von Big Data führen. " In ihrer Kritik weisen Snijders, Matzat und Reips darauf hin, dass oft sehr starke Annahmen über mathematische Eigenschaften gemacht werden, die überhaupt nicht reflektieren, was wirklich auf der Ebene der Mikroprozesse geschieht. Mark Graham hat auf Chris Andersons Behauptung, dass große Daten das Ende der Theorie buchstabieren werden, große Kritiken geebnet: Insbesondere die Fokussierung, dass große Daten immer in ihren sozialen, wirtschaftlichen und politischen Kontexten kontextualisiert werden müssen. Auch wenn Unternehmen acht- und neunstellige Summen investieren, um Einblicke aus Informationen zu gewinnen, die von Lieferanten und Kunden fließen, haben weniger als 40 % der Mitarbeiter ausreichend reife Prozesse und Fähigkeiten dazu. Um dieses Einsichtsdefizit zu überwinden, müssen große Daten, egal wie umfassend oder gut analysiert, durch "großes Urteil" ergänzt werden, nach einem Artikel in der Harvard Business Review. In der gleichen Linie wurde darauf hingewiesen, dass die Entscheidungen, die auf der Analyse von Big Data beruhen, zwangsläufig "von der Welt, wie sie in der Vergangenheit war, oder, wie sie derzeit ist" "informiert werden. Fed von einer Vielzahl von Daten über vergangene Erfahrungen, Algorithmen können zukünftige Entwicklung vorhersagen, wenn die Zukunft der Vergangenheit ähnlich ist. Wenn die Dynamik des zukünftigen Wandels des Systems (wenn es kein stationärer Prozess ist), kann die Vergangenheit wenig über die Zukunft sagen. Um Vorhersagen in sich verändernden Umgebungen zu machen, wäre es notwendig, ein gründliches Verständnis der Systemdynamik zu haben, was die Theorie erfordert. Als Antwort auf diese Kritik legen Alemany Oliver und Vayre vor, "ableitende Argumentation als erster Schritt im Forschungsprozess zu verwenden, um Kontexte auf die digitalen Spuren der Verbraucher zu bringen und neue Theorien hervorzubringen". Darüber hinaus wurde vorgeschlagen, große Datenansätze mit Computersimulationen, wie z.B. agentenbasierte Modelle und komplexe Systeme, zu kombinieren. Agentenbasierte Modelle werden zunehmend besser, das Ergebnis von sozialen Komplexitäten von sogar unbekannten zukünftigen Szenarien durch Computersimulationen vorherzusagen, die auf einer Sammlung von voneinander abhängigen Algorithmen basieren. Schließlich haben sich die Verwendung von multivariaten Methoden bewährt, die für die latente Struktur der Daten, wie z.B. Faktoranalyse und Clusteranalyse, als analytische Ansätze, die weit über die bei kleineren Datensätzen typischerweise eingesetzten bivariaten Ansätze (cross-tabs) hinausgehen, bewährt. In der Gesundheits- und Biologie basieren herkömmliche wissenschaftliche Ansätze auf Experimenten. Für diese Ansätze ist der Grenzwert die relevanten Daten, die die anfängliche Hypothese bestätigen oder widerlegen können. Ein neues Postulat wird jetzt in Biowissenschaften akzeptiert: Die Informationen, die die Daten in riesigen Volumina (Omik) ohne vorherige Hypothese zur Verfügung stellen, sind komplementär und manchmal notwendig für konventionelle Ansätze auf der Grundlage von Experimenten. In den massiven Ansätzen ist es die Formulierung einer relevanten Hypothese, um die Daten zu erklären, die der begrenzende Faktor ist. Die Suchlogik wird umgekehrt und die Grenzen der Induktion ("Glory of Science and Philosophy Skandal", C. D. Broad, 1926) sind zu berücksichtigen. Datenschutzbeauftragte sind besorgt über die Bedrohung für die Privatsphäre durch die zunehmende Speicherung und Integration von personenbezogenen Daten; Expertenpanels haben verschiedene Richtlinienempfehlungen veröffentlicht, um die Praxis den Erwartungen der Privatsphäre anzupassen. Der Missbrauch großer Daten in mehreren Fällen durch Medien, Unternehmen und sogar die Regierung hat es ermöglicht, Vertrauen in fast jede grundlegende Institution, die die Gesellschaft hält, zu beseitigen. Nayef Al-Rodhan argumentiert, dass eine neue Art von Sozialvertrag erforderlich sein wird, um einzelne Freiheiten im Kontext von Big Data und riesigen Unternehmen zu schützen, die riesige Mengen an Informationen besitzen, und dass die Verwendung von Big Data auf nationaler und internationaler Ebene überwacht und besser reguliert werden sollte. Barocas und Nissenbaum argumentieren, dass eine Möglichkeit zum Schutz einzelner Nutzer darin besteht, über die Art der gesammelten Informationen, mit denen sie geteilt werden, unter welchen Zwängen und zu welchen Zwecken zu informieren. Kriterien des V-Modells Das V-Modell der großen Daten ist, da es sich um die Rechenskalierbarkeit und den Mangel an Verlust um die Wahrnehmung und Verständlichkeit von Informationen dreht. Dies führte zum Rahmen kognitiver Big Data, die große Datenanwendungen nach: Datenvollständigkeit: Verständnis der nicht-obvious aus Daten Korrelation, Kausierung und Vorhersagbarkeit charakterisiert: Kausalität als nicht wesentliche Voraussetzung, um Vorhersagefähigkeit zu erreichen Erklärbarkeit und Interpretationsfähigkeit: Menschen wollen verstehen und akzeptieren, was sie verstehen, wo Algorithmen wurden nicht mit dieser Ebene der automatisierten Entscheidungsfindung kopiert: Algorithmen, die automatisierte Entscheidungsfindung unterstützen In den letzten Jahrzehnten haben Wissenschaftsexperimente wie CERN Daten in ähnlichen Größenordnungen zu aktuellen kommerziellen "großen Daten" erstellt. Wissenschaftsexperimente haben jedoch tendenziell ihre Daten mit spezialisierten, maßgeschneiderten Hochleistungs-Computing (Super-Computing)-Clustern und -Gittern analysiert, anstatt Wolken von billigen Rohstoff-Computern wie in der aktuellen kommerziellen Welle, was einen Unterschied in Kultur und Technologie-Stack bedeutet. Kritiken der Big Data-Execution Ulf-Dietrich Reips und Uwe Matzat schrieben im Jahr 2014, dass große Daten in der wissenschaftlichen Forschung verblüfft worden seien. Der Forscher danah boyd hat Bedenken hinsichtlich der Verwendung von Big Data in der Wissenschaft, die Prinzipien wie die Auswahl einer repräsentativen Probe durch zu besorgt über die Handhabung der riesigen Mengen von Daten erhoben. Dieser Ansatz kann zu Ergebnissen führen, die in einer oder einer anderen Weise Vorspannung haben. Integration über heterogene Datenressourcen – einige, die als Big Data und andere nicht betrachtet werden könnten – stellt sowohl formidable logistische als auch analytische Herausforderungen, aber viele Forscher argumentieren, dass solche Integrationen die vielversprechendsten neuen Grenzen der Wissenschaft darstellen. Im provokativen Artikel "Kritische Fragen für Big Data" nennen die Autoren große Daten einen Teil der Mythologie: "große Datensätze bieten eine höhere Form von Intelligenz und Wissen [...], mit der Aura der Wahrheit, Objektivität und Genauigkeit". Nutzer großer Daten sind oft "verloren in der schiere Anzahl", und "die Arbeit mit Big Data ist immer noch subjektiv, und was sie quantifiziert, hat nicht notwendigerweise einen näheren Anspruch auf objektive Wahrheit". Neuere Entwicklungen in der BI-Domain, wie z.B. proaktive Berichterstattung, insbesondere Zielverbesserungen bei der Nutzbarkeit von Big Data, durch automatisierte Filterung nicht nutzbarer Daten und Korrelationen. Große Strukturen sind voll von spurigen Korrelationen entweder wegen nicht-kausalen Koinzidenz (Gesetz von wirklich großen Zahlen,) allein die Natur der großen Zufallskraft (Ramsey-Theorie,) oder das Vorhandensein von nicht eingeschlossenen Faktoren, so die Hoffnung, der frühen Experimenter, große Datenbanken von Zahlen "Sprich für sich selbst" zu machen und die wissenschaftliche Methode zu revolutionieren, wird in Frage gestellt. Eine große Datenanalyse ist im Vergleich zur Analyse kleinerer Datensätze oft flach. In vielen großen Datenprojekten gibt es keine große Datenanalyse, aber die Herausforderung ist der Extrakt, die Transformation, der Lastteil der Datenvorverarbeitung. Big Data ist ein Schlagwort und ein "vague term", aber gleichzeitig eine Obsession mit Unternehmern, Beratern, Wissenschaftlern und Medien. Große Datenschaufenster wie Google Flu Trends konnten in den letzten Jahren keine guten Prognosen liefern, was die Ausbrüche der Grippe um einen Faktor von zwei übertraf. Ähnlich waren die Oscar-Preise und Wahlvorhersagen, die ausschließlich auf Twitter basieren, häufiger als auf dem Ziel. Große Daten stellen oft die gleichen Herausforderungen wie kleine Daten; das Hinzufügen von mehr Daten löst keine Probleme der Vorspannung, sondern kann andere Probleme betonen. Insbesondere Datenquellen wie Twitter sind nicht repräsentativ für die Gesamtbevölkerung, und Ergebnisse aus solchen Quellen können dann zu falschen Schlussfolgerungen führen. Google Translate – die auf der statistischen Analyse von Texten von Big Data basiert – macht einen guten Job bei der Übersetzung von Webseiten. Die Ergebnisse von spezialisierten Domains können jedoch dramatisch gesät werden. Auf der anderen Seite können auch große Daten neue Probleme einführen, wie zum Beispiel das Problem der Mehrfachvergleiche: gleichzeitig testen eine große Menge von Hypothesen wird wahrscheinlich viele falsche Ergebnisse produzieren, die irrtümlich signifikant erscheinen. Ioannidis argumentierte, dass "die meisten veröffentlichten Forschungsergebnisse falsch" seien, da im Wesentlichen der gleiche Effekt: wenn viele wissenschaftliche Teams und Forscher jeweils viele Experimente durchführen (d.h. eine große Anzahl von wissenschaftlichen Daten verarbeiten; wenn auch nicht mit Big Data-Technologie,), wächst die Wahrscheinlichkeit eines signifikanten Ergebnisses schnell – noch mehr, wenn nur positive Ergebnisse veröffentlicht werden. Darüber hinaus sind Big Data Analytics Ergebnisse nur so gut wie das Modell, auf dem sie vorgebracht werden. In einem Beispiel nahmen große Daten an dem Versuch teil, die Ergebnisse der US-Präsidentschaftswahl 2016 mit unterschiedlichen Erfolgsstufen vorherzusagen. Kritiken der Big Data-Pllikation und -Überwachung Big Data wurde bei der Polizei und Überwachung von Institutionen wie Strafverfolgung und Unternehmen verwendet. Aufgrund der weniger sichtbaren Art der datenbasierten Überwachung im Vergleich zu der herkömmlichen Methode des Polierens, sind Einwände gegen Big Data Policing weniger wahrscheinlich. Laut Sarah Braynes Big Data Überwachung: Der Fall der Polik, Big Data policing kann bestehende gesellschaftliche Ungleichheiten auf drei Arten reproduzieren: Verdächtige Verbrecher unter verstärkter Überwachung unter Verwendung der Rechtfertigung eines mathematischen und daher unvoreingenommenen Algorithmus Erhöhung des Umfanges und der Anzahl der Menschen, die der Verfolgung von Strafverfolgungsbehörden unterliegen und die bestehende rassische Überrepräsentation im Strafrechtssystem verstärken. Werden diese potenziellen Probleme nicht korrigiert oder geregelt, können die Auswirkungen der Big Data-Pllicing auch weiterhin gesellschaftliche Hierarchien prägen. Die bewusste Verwendung von Big Data-Pllicing könnte verhindern, dass einzelne Level-Bias institutionellen Bias, Brayne auch bemerkt. In der Volkskultur Bücher Moneyball ist ein Non-Fiction-Buch, das erforscht, wie die Oakland Athletics statistische Analyse verwendet, um Teams mit größeren Budgets zu übertreffen. 2011 wurde eine Filmanpassung mit Brad Pitt veröffentlicht. FilmIn Captain America: The Winter Soldier, H.Y.D.R.A (verkleidet als S.H.I.E.L.D) entwickelt Helicarriers, die Daten verwenden, um Bedrohungen auf der ganzen Welt zu bestimmen und zu beseitigen. In The Dark Knight verwendet Batman ein Sonargerät, das auf allen Gotham City spionieren kann. Die Daten werden von den Mobiltelefonen von Menschen in der Stadt gesammelt. Siehe auch Referenzen Weiter lesen Peter Kinnaird; Inbal Talgam-Cohen, eds. (2012). "Big Data". XRDS:Crossroads, The ACM Magazine for Students.Vol.19 no. 1. Verband für Computing Machinery.ISSN 1528-4980.OCLC 779657714. Jure Leskovec; Anand Rajaraman; Jeffrey D. Ullman (2014). Bergbau von massiven Datensätzen. Cambridge University Press.ISBN 9781107077232.OCLC 888463433.Viktor Mayer-Schönberger; Kenneth Cukier (2013). Big Data: Eine Revolution, die transformiert, wie wir leben, arbeiten und denken. Houghton Mifflin Harcourt.ISBN 9781299903029.OCLC 828620988.Press, Gil (9. Mai 2013)."Eine sehr kurze Geschichte von Big Data".forbes.com. Jersey City, NJ. Erschienen am 17. September 2016."Big Data: The Management Revolution". Harvard Business Review.Oktober 2012.O'Neil, Cathy (2017). Waffen der Mathe-Destruktion: Wie Big Data die Ungleichheit erhöht und die Demokratie bedroht. Broadway Books. ISBN 978-0553418835. Externe Links Medien im Zusammenhang mit Big Data bei Wikimedia Commons Die Wörterbuchdefinition der Big Data bei WiktionaryKreischa ist eine Gemeinde im Sächsischen Schweiz-Osterzgebirge, Sachsen, Deutschland. Sie grenzt direkt an die sächsische Hauptstadt Dresden und besteht aus 14 Bezirken. Kreischa wurde im Jahre 1282 im Namen Heinricus de Kryschowe erwähnt. Der Name könnte von einem alten slawischen Wort abgeleitet werden, das heißt verkrüppelt oder lahm. Schwesterstädte Kreischa ist gepaart mit: Loffenau Bezirk Rastatt, Baden-Württemberg, Deutschland seit 1990 Gemeindeuntergebiete Babisnau Bärenklause Brösgen Gombsen Kautzsch Kleba Kleincarsdorf Lungkwitz Quohren Saida Sobrigau Theisewitz Kreischa-Wittgensdorf Zscheckwitz == Referenzen ==