TensorFlow ist eine kostenlose und Open-Source-Software-Bibliothek für maschinelles Lernen. Es kann über eine Reihe von Aufgaben verwendet werden, hat aber einen besonderen Schwerpunkt auf Ausbildung und Inferenz von tiefen neuronalen Netzwerken. Tensorflow ist eine symbolische Mathe-Bibliothek basierend auf Datenfluss und differenzierbare Programmierung. Es wird für Forschung und Produktion bei Google verwendet. TensorFlow wurde vom Google Brain Team für den internen Google-Einsatz entwickelt. Es wurde 2015 unter der Apache License 2.0 veröffentlicht. Geschichte DistBeliefStarting im Jahr 2011, Google Brain baute DistBelief als proprietäres maschinelles Lernsystem auf der Basis von tief lernenden neuronalen Netzwerken. Sein Einsatz wuchs in verschiedenen Alphabet-Unternehmen sowohl in der Forschung als auch in kommerziellen Anwendungen rapide. Google beauftragte mehrere Informatiker, darunter Jeff Dean, die Codebasis von DistBelief zu einer schnelleren, robusteren Applikations-Bibliothek zu vereinfachen und zu refaktorieren, die TensorFlow wurde. 2009 hatte das Team unter der Leitung von Geoffrey Hinton eine verallgemeinerte Rückverbreitung und andere Verbesserungen umgesetzt, die die Erzeugung von neuronalen Netzen mit wesentlich höherer Genauigkeit ermöglichten, etwa eine 25%ige Reduzierung von Fehlern bei der Spracherkennung. Tensor Fluss Tensor Flow ist das System der zweiten Generation von Google Brain. Version 1.0.0 wurde am 11. Februar 2017 veröffentlicht. Während die Referenz-Implementierung auf Einzelgeräten abläuft, kann TensorFlow auf mehreren CPUs und GPUs (mit optionalen CUDA- und SYCL-Erweiterungen für das Universal-Computing auf grafischen Verarbeitungseinheiten) laufen. TensorFlow ist auf 64-Bit Linux, macOS, Windows und mobilen Computer-Plattformen einschließlich Android und iOS verfügbar. Seine flexible Architektur ermöglicht den einfachen Einsatz von Berechnungen auf verschiedenen Plattformen (CPUs, GPUs, TPUs) und von Desktops bis hin zu Clustern von Servern bis zu mobilen und Edge-Geräten. TensorFlow Berechnungen werden als Stateful Dataflow Graphs ausgedrückt. Der Name TensorFlow leitet sich von den Operationen ab, dass solche neuronalen Netzwerke auf mehrdimensionalen Datenfeldern, die als Tensors bezeichnet werden, ausführen. Während der Google I/O Konferenz im Juni 2016, Jeff Dean sagte, dass 1.500 Repositories auf GitHub erwähnt TensorFlow, von denen nur 5 von Google waren. Im Dezember 2017 haben Entwickler von Google, Cisco, RedHat, CoreOS und CaiCloud Kubeflow auf einer Konferenz vorgestellt. Kubeflow ermöglicht Betrieb und Einsatz von TensorFlow auf Kubernetes. Im März 2018 kündigte Google TensorFlow.js Version 1.0 für maschinelles Lernen in JavaScript an. Im Januar 2019 kündigte Google TensorFlow 2.0 an. Es wurde offiziell inSep 2019 verfügbar. Im Mai 2019, Google kündigte TensorFlow Graphics für tiefes Lernen in Computergrafiken. Tensor-Verarbeitungseinheit (TPU) Im Mai 2016 kündigte Google seine Tensor-Verarbeitungseinheit (TPU) an, eine anwendungsspezifische integrierte Schaltung (ASIC, ein Hardware-Chip), die speziell für das maschinelle Lernen gebaut und auf TensorFlow zugeschnitten ist. Ein TPU ist ein programmierbarer KI-Beschleuniger, der darauf ausgelegt ist, einen hohen Durchsatz von arithmetisch niedriger Präzision (z.B. 8-Bit) zu liefern und anstatt sie zu trainieren, auf Gebrauchs- oder Laufmodelle zu orientieren. Google gab bekannt, dass sie seit mehr als einem Jahr TPUs in ihren Rechenzentren betrieben hatten und sie gefunden hatten, eine Größenordnung besser optimierte Leistung pro Watt für maschinelles Lernen zu liefern. Im Mai 2017 kündigte Google die zweite Generation sowie die Verfügbarkeit der TPUs in Google Compute Engine an. Die TPUs der zweiten Generation liefern bis zu 180 Teraflops der Leistung, und wenn sie in Cluster von 64 TPUs organisiert werden, bieten bis zu 11,5 Petaflops. Im Mai 2018 kündigte Google die TPUs der dritten Generation an, die bis zu 420 Teraflops der Leistung und 128 GB High-Band-Speicher (HBM) liefern. Cloud TPU v3 Pods bieten 100+ petaflops der Leistung und 32 TB HBM. Im Februar 2018 kündigte Google an, dass sie TPUs in Beta auf der Google Cloud Platform zur Verfügung stellen. Edge TPUIm Juli 2018 wurde die Edge TPU angekündigt. Edge TPU ist der zielgerichtete ASIC-Chip von Google, der TensorFlow Lite Machine Learning (ML)-Modelle auf kleinen Client-Computing-Geräten wie Smartphones, die als Edge Computing bekannt sind, ausführen soll. Tensor Flow LiteIm Mai 2017 kündigte Google einen Software-Stapel speziell für die mobile Entwicklung an, TensorFlow Lite. Im Januar 2019 veröffentlichte TensorFlow Team eine Entwicklervorschau der mobilen GPU-Inferenzmaschine mit OpenGL ES 3.1 Compute Shaders auf Android-Geräten und Metal Compute Shaders auf iOS-Geräten. Im Mai 2019 kündigte Google an, dass ihr TensorFlow Lite Micro (auch bekannt als TensorFlow Lite für Microcontroller) und ARMs uTensor verschmelzen würden. TensorFlow Lite verwendet FlatBuffers als Data Serialisierungsformat für Netzwerkmodelle, wobei das von Standard TensorFlow-Modellen verwendete Protokoll-Buffers-Format zugrunde liegt. Pixel Visual Core (PVC) Im Oktober 2017 veröffentlichte Google das Google Pixel 2 mit seinem Pixel Visual Core (PVC) einen voll programmierbaren Bild-, Vision- und KI-Prozessor für mobile Geräte. Das PVC unterstützt TensorFlow für maschinelles Lernen (und Halide für die Bildverarbeitung.) Anwendungen Google offiziell veröffentlicht RankBrain am 26. Oktober 2015, unterstützt von TensorFlow. Google auch veröffentlicht Colaboratory, das ist eine TensorFlow Jupyter Notebook-Umgebung, die keine Setup zu verwenden erfordert. Machine Learning Crash Course (MLCC)Am 1. März 2018 veröffentlichte Google seinen Machine Learning Crash Course (MLCC). Ursprünglich entworfen, um Google-Mitarbeiter mit praktischen künstlichen Intelligenz und maschinellen Lerngrundsätzen auszustatten, rollte Google seine kostenlosen TensorFlow-Workshops in mehreren Städten auf der ganzen Welt aus, bevor er schließlich den Kurs an die Öffentlichkeit freigab. TensorFlow 2.0A Tensor Der Marktanteil von Flow unter Forschungspapieren verringerte sich auf den Vorteil von PyTorch, das TensorFlow Team kündigte im September 2019 eine neue große Version der Bibliothek an. TensorFlow 2.0 führte viele Änderungen ein, das bedeutendste ist TensorFlow Eager, die das automatische Differenzierungsschema aus dem statischen Rechendiagramm geändert, auf das Define-by-Run-System ursprünglich von Chainer und später PyTorch populär gemacht. Weitere wichtige Änderungen waren die Entfernung von alten Bibliotheken, die Kreuzkompatibilität zwischen geschulten Modellen auf verschiedenen Versionen von TensorFlow und wesentliche Verbesserungen der Leistung auf GPU. Eigenschaften TensorFlow bietet stabile Python (für Version 3.7 auf allen Plattformen) und C-APIs; und ohne API-Rückwärts-Kompatibilitätsgarantie: C,+ Go, Java, JavaScript und Swift (archiviert und Entwicklung ist beendet). Pakete von Drittanbietern stehen für C,# Haskell, Julia, MATLAB, R, Scala, Rust, OCaml und Crystal zur Verfügung. "Neue Sprachunterstützung sollte auf der C API gebaut werden. Allerdings ist [.] noch nicht alle Funktionalität in C verfügbar." Einige weitere Funktionalitäten werden von der Python API bereitgestellt. Anwendungen Unter den Anwendungen, für die TensorFlow die Grundlage ist, sind automatisierte Bildverarbeitungssoftware, wie DeepDream. Siehe auch Vergleich der Deep Learning Software Differenzierbare Programmierung Referenzen Bibliographie Externe Links Offizielle Website