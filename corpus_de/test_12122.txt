Datenanalyse ist ein Prozess der Inspektion, Reinigung, Umwandlung und Modellierung von Daten mit dem Ziel, nützliche Informationen zu entdecken, Schlussfolgerungen zu unterrichten und Entscheidungsfindung zu unterstützen. Datenanalyse hat mehrere Facetten und Ansätze, die unterschiedliche Techniken unter einer Vielzahl von Namen umfassen, und wird in verschiedenen Bereichen der Wirtschaft, Wissenschaft und Gesellschaft verwendet. Datenanalyse spielt in der heutigen Geschäftswelt eine Rolle bei Entscheidungen, die wissenschaftlicher werden und den Unternehmen helfen, effektiver zu arbeiten. Data Mining ist eine spezielle Datenanalysetechnik, die sich auf statistische Modellierung und Wissenssuche für prädiktive und nicht rein deskriptive Zwecke konzentriert, während die Unternehmenskenntnis Datenanalyse, die stark auf Aggregation beruht, vor allem auf Unternehmensinformationen. In statistischen Anwendungen kann die Datenanalyse in beschreibende Statistiken, explorative Datenanalyse (EDA) und Bestätigung der Datenanalyse (CDA) unterteilt werden. EDA konzentriert sich auf die Entdeckung neuer Merkmale in den Daten, während CDA sich auf die Bestätigung oder Verfälschung bestehender Hypothesen konzentriert. Kritische Analyse konzentriert sich auf die Anwendung statistischer Modelle für die Vorhersage oder Klassifizierung, während die Textanalyse statistische, sprachliche und strukturelle Techniken zur Gewinnung und Klassifizierung von Informationen aus Textquellen, einer Art unstrukturierter Daten, anwendet. All dies sind Sorten der Datenanalyse. Datenintegration ist ein Grundstoff für die Datenanalyse, und Datenanalyse ist eng mit der Datenauswertung und der Datenverbreitung verknüpft. Der Prozess der Datenanalyse bezieht sich auf die Aufteilung eines gesamten Teils in seine verschiedenen Komponenten für die individuelle Prüfung. Datenanalyse, ist ein Verfahren zur Erlangung von Rohdaten und anschließende Umwandlung in Informationen, die für die Entscheidungsfindung der Nutzer nützlich sind. Daten, werden erhoben und analysiert, um Fragen, Testhypothesen oder unprove Theorien zu beantworten. Statistician John Tukey, definierte Datenanalyse im Jahr 1961, wie: „Verfahren für die Analyse von Daten, Techniken zur Interpretation der Ergebnisse solcher Verfahren, Wege der Planung der Datenerhebung, um ihre Analyse einfacher, genauer oder genauer zu gestalten und alle Maschinen und Ergebnisse (mathematische) Statistiken, die für die Analyse von Daten gelten. " Es gibt mehrere Phasen, die nachstehend beschrieben werden können. Die Phasen sind iterativ, da das Feedback aus späteren Phasen zu einer zusätzlichen Arbeit in früheren Phasen führen kann. Der CRISP-Rahmen, der im Data Mining verwendet wird, hat ähnliche Schritte. Datenanforderungen Die Daten sind als Input für die Analyse erforderlich, die auf den Anforderungen dieser Analyse (oder Kunden, die das fertige Produkt der Analyse verwenden) basiert. Die allgemeine Art der Einheit, bei der die Daten erhoben werden, wird als Versuchseinheit (z.B. eine Person oder Bevölkerung von Menschen) bezeichnet. Konkrete Variablen für eine Bevölkerung (z.B. Alter und Einkommen) können spezifiziert und erhalten werden. Daten können numerische oder kategorisch sein (d. h. ein Textetikett für Zahlen). Datensammlungsdaten werden aus verschiedenen Quellen erhoben. Die Anforderungen können von Analysten an Verwahrer der Daten, wie etwa das Personal der Informationstechnologie innerhalb einer Organisation, mitgeteilt werden. Daten können auch von Sensoren in der Umwelt erhoben werden, darunter Verkehrskameras, Satelliten, Aufzeichnungsgeräte usw. Sie kann auch durch Interviews, Downloaden aus Online-Quellen oder Lesen erhalten. Datenverarbeitungsdaten, wenn sie ursprünglich erhoben werden, müssen zur Analyse verarbeitet oder organisiert werden. Dies kann beispielsweise dazu führen, dass Daten in Folge und Spalten in einem Tischformat (bekannt als strukturierte Daten) für weitere Analysen, oft durch den Einsatz von Verbreitungsblättern oder statistischer Software, bereitgestellt werden. Datenreinigung Nach Verarbeitung und Organisation können die Daten unvollständig sein, Doppelungen enthalten oder Fehler enthalten. Die Notwendigkeit der Datenreinigung wird sich aus Problemen ergeben, wie das Datum eingegeben und gespeichert wird. Datenreinigung ist der Prozess der Vermeidung und Korrektur dieser Fehler. Gemeinsame Aufgaben umfassen die Erfassung der Daten, die Gesamtqualität bestehender Daten, die Dedusion und die Segmentierung der Spalten. Solche Datenprobleme können auch durch eine Vielzahl von Analysemethoden ermittelt werden. Beispielsweise können mit finanziellen Informationen die Gesamtsummen für bestimmte Variablen gegen gesondert veröffentlichte Zahlen verglichen werden, die als zuverlässig erachtet werden. above Beträge, die über oder unterhalb festgelegter Schwellenwerte liegen, können ebenfalls überprüft werden. Es gibt mehrere Arten von Datenreinigungen, die von der Art der Daten im Set abhängig sind; dies könnte Telefonnummern, E-Mail-Adressen, Arbeitgeber oder andere Werte sein. Quantitative Datenmethoden für den Aufspüren können genutzt werden, um Daten zu beseitigen, die eine höhere Wahrscheinlichkeit aufweisen, falsch zu sein. Textuale Datenstichprobenprüfer können verwendet werden, um die Menge falscher Wörter zu verringern. Es ist jedoch schwieriger zu sagen, ob die Worte selbst korrekt sind. Analyse von Daten Nach der Reinigung der Daten können sie dann analysiert werden. Analysten können eine Vielzahl von Techniken anwenden, die als Sondierungs-Datenanalyse bezeichnet werden, um das Verständnis der in den Daten enthaltenen Botschaften zu beginnen. Der Prozess der Datenxploration kann zu zusätzlichen Datenreinigungen oder zusätzlichen Datenanfragen führen; so kann die Paraphierung der in diesem Abschnitt genannten Iterativen Phasen erfolgen. Deskriptive Statistiken, wie z.B. der Durchschnitt oder die Medien, können für Beihilfen im Verständnis der Daten generiert werden. Datenauswertung ist auch eine Technik, bei der der Analyst die Daten in einem grafischen Format prüfen kann, um zusätzliche Einblicke in die Nachrichten innerhalb der Daten zu erhalten. Modellierungs- und mathematische Formeln oder Modelle (bekannt als Algorithmen) können auf die Daten angewendet werden, um die Beziehungen zwischen den Variablen zu ermitteln, z.B. mit Korrelation oder Kausalation. Allgemein können Modelle entwickelt werden, um eine bestimmte Variablen zu bewerten, die auf anderen Variablen basieren, die in der Datenset enthalten sind, mit einigen Restfehlern je nach Genauigkeit des eingeführten Modells (z.B. Daten = Modell + Fehler). Inferenzstatistiken enthalten Techniken, die die Beziehungen zwischen bestimmten Variablen messen. Beispiel: Regressionsanalyse kann zum Beispiel verwendet werden, um festzustellen, ob eine Änderung der Werbung (unabhängige variable X) eine Erklärung für die Schwankungen des Verkaufs (unabhängige variable Variante Y) enthält. Kurzum: Y (Verkauf) ist eine Funktion von X (Werbung). Man kann als (Y = aX + b + Fehler) bezeichnet werden, wenn das Modell so konzipiert ist, dass (a) und b) den Fehler minimieren, wenn das Modell Y für eine Reihe von Werten von X. Analysten plant, können auch versuchen, Modelle zu bauen, die beschreibend für die Daten sind, um Analyse- und Kommunikationsergebnisse zu vereinfachen. Datenprodukt Ein Datenprodukt ist eine Computeranwendung, die Dateneingaben und Outputs herstellt, die sie wieder in die Umwelt einfließt. Es kann auf einem Modell oder auf einem Algorithmus basieren. z.B. ein Antrag, der Daten über die Kaufgeschichte von Kunden analysiert und die Ergebnisse nutzt, um anderen Käufern zu empfehlen, kann davon ausgegangen werden. Kommunikation Nach einer Analyse der Daten kann es den Nutzern der Analyse in vielen Formaten gemeldet werden, um ihre Anforderungen zu erfüllen. Die Nutzer können Feedback haben, was zu einer zusätzlichen Analyse führt. Dies ist ein großer Teil des Analysezyklus. Bei der Feststellung, wie die Ergebnisse vermittelt werden können, kann der Analyst prüfen, ob eine Vielzahl von Datenauswertungstechniken umgesetzt werden, um die Botschaft klarer und effizienter an das Publikum weiterzugeben. Datenauswertung verwendet Informationsbildschirme (Daten, Tabellen und Karten), um Schlüsselbotschaften in den Daten zu kommunizieren. Tabellen sind ein wertvolles Werkzeug, das die Fähigkeit eines Benutzers ermöglicht, auf bestimmte Zahlen zu antworten und sich auf bestimmte Zahlen zu konzentrieren; während Karten (z.B. Barkarten oder Linienkarten) die in den Daten enthaltenen quantitativen Botschaften erklären können. Quantitative Botschaften Weniger Stephen nannte acht Arten von quantitativen Botschaften, die die Nutzer versuchen könnten, von einer Reihe von Daten zu verstehen oder zu kommunizieren, und die zugehörigen Graphen, die zur Kommunikation der Botschaft verwendet werden. Kunden, die die Anforderungen und Analysten angeben, die die Datenanalyse durchführen, können diese Botschaften während des Prozesses berücksichtigen. Zeitreihen: Über einen Zeitraum von zehn Jahren wird eine einzige Variablen erfasst, wie die Arbeitslosenquote. Eine Liniestabelle kann verwendet werden, um den Trend zu demonstrieren. Ranking: Categorische Unterteilungen werden in aufsteigender oder absteigender Reihenfolge aufgeführt, wie etwa ein Ranking der Verkaufsleistung (die Maßnahme) von Vertriebsunternehmen (die Kategorie, mit jeder Verkaufsperson eine Kategorisierung) während eines einzigen Zeitraums. Ein Barplan kann verwendet werden, um den Vergleich zwischen den Verkäufern zu zeigen. Teil-to-whole: Categorische Untergebiete werden als Verhältnis zum Gesamt (d. h. einem Prozentsatz von 100)% gemessen. Ein Pauschal- oder Barplan kann den Vergleich von Verhältnisen, wie z.B. der von Wettbewerbern auf einem Markt vertretene Marktanteil, zeigen. Abweichungen: Categorische Untergebiete werden mit einem Verweis verglichen, wie etwa einem Vergleich der tatsächlichen Ausgaben gegenüber dem Budget für mehrere Dienststellen eines Unternehmens für einen bestimmten Zeitraum. Eine Barkarte kann den Vergleich des tatsächlichen versus Referenzbetrag zeigen. Frequenzverteilung: Kennzeichnen Sie die Anzahl der Beobachtungen einer bestimmten Variablen für einen bestimmten Zeitraum, wie die Anzahl der Jahre, in denen die Rendite des Aktienmarktes zwischen Intervallen wie 0–10,% 11–20,% usw. liegt. Ein histogramm, eine Art von Bar, kann für diese Analyse verwendet werden. Korelation: Vergleich zwischen Beobachtungen, die durch zwei Variablen (X,Y) vertreten sind, um festzustellen, ob sie sich in derselben oder entgegengesetzten Richtungen bewegen. z.B. die Bemessung der Arbeitslosigkeit (X) und die Inflation (Y) für eine Stichprobe von Monaten. In der Regel wird für diese Botschaft ein uneinheitliches Grundstück verwendet. Nominaler Vergleich: Vergleich der kategorischen Untergebiete in keiner bestimmten Reihenfolge, wie z.B. das Verkaufsvolumen nach Produktcode. Ein Barplan kann für diesen Vergleich verwendet werden. geographische oder geospatial: Vergleich einer variablen Karte oder Ausstattung, wie die Arbeitslosenquote nach Staat oder die Anzahl der Personen an den verschiedenen Gebäuden. Ein Kartogramm ist eine typische Grafik. Methoden zur Analyse quantitativer Datenautor Jonathan Koomey hat eine Reihe bewährter Verfahren empfohlen, um quantitative Daten zu verstehen. Hierzu gehören: Überprüfung der Rohdaten für Anomalien vor der Durchführung einer Analyse; Überprüfung wichtiger Berechnungen, wie Überprüfung der Spalten von Daten, die entwickelt werden; Bestätigung der wichtigsten Gesamtsummen sind die Summe der Teilsummen; Überprüfung der Beziehungen zwischen den Zahlen, die auf vorhersehbare Weise in Zusammenhang stehen sollten; Normalisierungsnummern, um Vergleiche zu erleichtern, wie etwa Analysebeträge pro Person oder im Verhältnis zum BIP oder als Indexwert im Vergleich zu einem Basisjahr; Probleme in Komponenten durch Analyse von Analysefaktoren, die zu den Ergebnissen führen, wie z.B. Für die untersuchten Variablen erhalten Analysten in der Regel beschreibende Statistiken für sie, wie den Mittelwert (durchschnittlich), Medien und Standardabweichung. Sie können auch die Verteilung der wichtigsten Variablen analysieren, um zu sehen, wie die einzelnen Wertecluster um den Mittelwert herum. Die Berater in McKinsey und Company nannten eine Methode, um ein quantitatives Problem in ihre Komponenten zu verwandeln, die als Grundsatz der MECE bezeichnet wurden. Jede Schicht kann in ihre Komponenten zerbrochen werden; jede der Teilkomponenten muss sich gegenseitig ausschließen und gemeinsam auf die obengenannte Schicht aufzustocken. Die Beziehung wird als "Mutually Exklusiv und Kollektivexhaustive" oder MECE bezeichnet. Zum Beispiel kann der Gewinn nach der Definition in die Gesamteinnahmen und die Gesamtkosten unterteilt werden. wiederum können die Gesamteinnahmen von ihren Komponenten analysiert werden, wie die Einnahmen der Spaltungen A, B und C (die sich gegenseitig ausschließen) und die Gesamteinnahmen (kollektiv erschöpfend). Analysten können robuste statistische Messungen verwenden, um bestimmte Analyseprobleme zu lösen. Hypothesis-Tests werden verwendet, wenn eine besondere Hypothese über den wahren Zustand der Dinge vom Analysten und Daten erhoben wird, um festzustellen, ob dieser Zustand tatsächlich oder falsch ist. So könnte die Hypothese beispielsweise sein, dass "Arbeitslosigkeit keine Auswirkungen auf die Inflation hat", die sich auf ein wirtschaftliches Konzept bezieht, das die Phillips-Kurve bezeichnet. Hypothesis-Tests berücksichtigen die Wahrscheinlichkeit von Typ I und Typ II- Fehlern, die sich auf die Annahme oder Ablehnung der Hypothese beziehen. Regressionsanalysen können verwendet werden, wenn der Analyst versucht, festzustellen, inwieweit unabhängige variable X die abhängige variable Y(z.B. „Inwieweit Änderungen der Arbeitslosenquote (X) die Inflationsrate (Y)? Dies ist ein Versuch, eine Gleichungslinie oder eine Kurve zu den Daten zu entwerfen, so dass Y eine Funktion der X.Necessary Bedingunganalyse (NCA) ist, wenn der Analyst versucht, festzustellen, in welchem Ausmaß unabhängige variable X variable Y (z.B. „zu welchem Umfang eine bestimmte Arbeitslosenquote (X) für eine bestimmte Inflationsrate (Y)? Da (multiple) Regressionsanalysen die Notwendigkeitslogik verwenden, wenn jeder X-variable das Ergebnis erzeugen kann und die X-Venture für jeden anderen (ausreichend, aber nicht notwendig) kompensieren kann, nutzt die notwendige Bedingungsanalyse (NCA), wenn eine oder mehrere X-variablen das Ergebnis ermöglichen, aber nicht produzieren können (sie sind notwendig, aber nicht ausreichend). Jede einzelne notwendige Voraussetzung muss vorhanden sein und Entschädigung ist nicht möglich. Analysetätigkeiten von Datenbenutzern können im Gegensatz zu den oben genannten allgemeinen Nachrichten besondere Datenpunkte von Interesse im Rahmen einer Datenreihe haben. In der folgenden Tabelle werden solche Aktivitäten auf niedrigem Niveau präsentiert. Die Steueronomy kann auch von drei Tätigkeitsschwerpunkten organisiert werden: Rückführung von Werten, Feststellung von Datenpunkten und Authentifizierung von Datenpunkten. Hindernisse für eine wirksame Analyse können zwischen den Analysten bestehen, die die Datenanalyse oder das Publikum durchführen. kognitive Verzerrungen und Rechnen sind alle Herausforderungen für eine solide Datenanalyse. Verweigerung der Tatsachen und MeinungEffective Analyse erfordert die Erlangung relevanter Fakten, um Fragen zu beantworten, einen Abschluss oder eine förmliche Stellungnahme zu unterstützen oder Hypothesen zu testen. Fakten nach der Definition sind unwiderruflich, was bedeutet, dass jede Person, die an der Analyse beteiligt ist, sich auf sie einigen kann. Im August 2010 schätzte das Congressional Budget Office (CBO) die Verlängerung der Steuersenkungen für die Jahre 2001 und 2003 für den Zeitraum 2011–2020 um etwa 3,3 Billionen $ auf die nationale Schulden. Jeder sollte sich darüber einig sein, dass dies tatsächlich das ist, was CBO berichtet hat; sie können alle den Bericht prüfen. Dies macht das. Ob Personen mit der CBO einverstanden oder nicht einverstanden sind, ist ihre eigene Meinung. In einem anderen Beispiel muss der Prüfer eines öffentlichen Unternehmens eine förmliche Stellungnahme abgeben, ob die Finanzausweise öffentlich gehandelter Unternehmen "fairerweise in allen materiellen Belangen angegeben sind". Dies erfordert eine umfassende Analyse der tatsächlichen Daten und Beweise, um ihre Meinung zu unterstützen. Wenn man den Sprung von Fakten zu Meinungen macht, besteht immer die Möglichkeit, dass die Stellungnahme fehlerhaft ist. Kognitive Verzerrungen Es gibt eine Vielzahl von kognitiven Verzerrungen, die die Analyse beeinträchtigen können. Bestätigungsverzerrung ist beispielsweise die Tendenz, Informationen in einer Weise zu suchen oder zu interpretieren, die die Vorurteile bestätigt. Außerdem können Einzelpersonen Informationen, die ihre Ansichten nicht unterstützen, nicht abschaffen. Analysten können speziell ausgebildet werden, um sich dieser Verzerrungen bewusst zu sein und wie sie überwunden werden können. In seinem Buch Psychologie der Intelligenzanalyse im Ruhestand CIA Richards Keys schrieb, dass die Analysten ihre Annahmen und Ketten von Gleichgültigkeit klar abgrenzen und den Grad und die Quelle der in den Schlussfolgerungen genannten Unsicherheit angeben sollten. Er hob Verfahren hervor, um alternative Standpunkte zu erörtern. Leistungsfähige Analysten sind in der Regel mit einer Vielzahl numerischer Techniken behaftet. Publikumsschichten können jedoch nicht über eine solche Lese- oder Rechenleistung verfügen; sie sind der Meinung, dass sie in Zahlen sind. Personen, die die Daten vermitteln, können auch versuchen, irreführend oder falsch zu sein, absichtlich mit schlechten numerischen Techniken. Zum Beispiel, ob eine Zahl steigt oder fällt, kann nicht der Schlüsselfaktor sein. Mehr Bedeutung kann die Zahl im Vergleich zu einer anderen Zahl sein, wie etwa die Größe der öffentlichen Einnahmen oder Ausgaben im Verhältnis zur Größe der Wirtschaft (BIP) oder die Höhe der Kosten im Verhältnis zu den Einnahmen in Unternehmensabschlüssen. Diese numerische Technik wird als Normalisierung oder gemeinsame Nutzung bezeichnet. Es gibt viele solche Techniken, die von Analysten eingesetzt werden, ob sie sich auf die Inflation (d. h. den Vergleich realer oder nominaler Daten) oder die Anhebung der Bevölkerung, demographische Aspekte usw. einstellen. Analysten wenden eine Vielzahl von Techniken an, um die verschiedenen quantitativen Aussagen des Abschnitts zu behandeln. Analysten können auch Daten unter verschiedenen Annahmen oder Szenarios analysieren. Sobald die Analysten eine Finanzanalyse durchführen, werden sie oft die unter verschiedenen Annahmen enthaltenen Finanzinformationen umgestalten, um zu einer Schätzung des künftigen Cashflows zu gelangen, die dann auf der Grundlage einer gewissen Zinsquote abweicht, um die Bewertung des Unternehmens oder seines Bestands zu bestimmen. In ähnlicher Weise analysiert die CBO die Auswirkungen verschiedener politischer Optionen auf die Einnahmen, Ausgaben und Defizite der Regierung und schafft alternative Zukunftsszenarien für wichtige Maßnahmen. Andere Themen Intelligente Gebäude Ein Datenanalysekonzept können genutzt werden, um den Energieverbrauch in Gebäuden vorherzusagen. Die verschiedenen Stufen des Datenanalyseverfahrens werden durchgeführt, um intelligente Gebäude zu realisieren, wo das Gebäudemanagement und die Kontrolloperationen einschließlich Heizung, Belüftung, Klimaanlage, Beleuchtung und Sicherheit automatisch realisiert werden, indem die Bedürfnisse der Baunutzer und die Optimierung der Ressourcen wie Energie und Zeit angegeben werden. Log and Business Intelligence Analytics ist die „verlängerte Nutzung von Daten, statistischer und quantitativer Analyse, erläuternden und prädikativen Modellen und der faktischen Verwaltung zur Durchführung von Entscheidungen und Maßnahmen“. Es ist ein Teil der Geschäftskenntnis, das eine Reihe von Technologien und Prozessen ist, die Daten verwenden, um die Geschäftsleistung zu verstehen und zu analysieren, um die Entscheidungsfindung voranzutreiben. Bildung Bildungseinrichtungen haben Zugang zu einem Datensystem zum Zwecke der Analyse von Studiendaten. Diese Datensysteme stellen Daten vor, die den Erziehern in einem Over-the-counter-Datenformat (Grundetiketten, Zusatzdokumentation und Hilfesystem sowie wichtige Paket-/Ausnahme- und Inhaltsentscheidungen) zur Verbesserung der Genauigkeit der Datenanalysen von Educators dienen. Ärzte Dieser Abschnitt enthält eher technische Erklärungen, die Praktikern helfen können, aber über den typischen Anwendungsbereich eines Papier-Artikels hinausgehen. Initiale Datenanalyse Die wichtigste Unterscheidung zwischen der anfänglichen Datenanalysephase und der Hauptanalysephase ist, dass bei der ersten Datenanalyse eine Analyse, die auf die Beantwortung der ursprünglichen Forschungsfrage gerichtet ist, nicht berücksichtigt wird. Die erste Datenanalysephase wird von folgenden vier Fragen geleitet: Qualität der Daten Die Qualität der Daten sollte so früh wie möglich überprüft werden. Datenqualität kann auf verschiedene Weise bewertet werden, wobei verschiedene Arten von Analysen verwendet werden: Häufigkeit, beschreibende Statistiken (mean, Standardabweichung, Medien), Normalität (Schweche, Kurtose, Häufigkeitsprofilen), Normalisierung ist erforderlich. Analyse extremer Beobachtungen: Ausgeschlossene Beobachtungen in den Daten werden analysiert, um zu sehen, ob sie den Vertrieb stören scheinen. Vergleich und Korrektur von Unterschieden bei den Programmierprogrammen: Variablen werden mit den Programmierprogrammen von Variablen, die außerhalb der Datenregelung liegen, verglichen und möglicherweise berichtigt, wenn Programmierprogramme nicht vergleichbar sind. Test für die gemeinsame Varianz. Die Auswahl der Analysen zur Bewertung der Datenqualität in der ersten Datenanalysephase hängt von den Analysen ab, die in der Hauptanalysephase durchgeführt werden.Qualität der Messungen Die Qualität der Messinstrumente sollte nur in der ersten Datenanalysephase überprüft werden, wenn dies nicht der Schwerpunkt oder die Forschungsfrage der Studie ist. Man sollte prüfen, ob die Struktur der Messinstrumente der in der Literatur gemeldeten Struktur entspricht. Es gibt zwei Möglichkeiten zur Bewertung der Messqualität: Bewertung der Homogenität (interne Konsistenz), die eine Angabe der Zuverlässigkeit eines Messinstruments enthält. In dieser Analyse untersucht man die Schwankungen der Gegenstände und der Größenordnungen, die s des Cronbachs und die Änderung des Cronbach-Algorithmus, wenn ein Punkt aus einer Größenumwandlung gelöscht würde Nach der Bewertung der Qualität der Daten und der Messungen kann man sich entscheiden, fehlende Daten einzurechnen oder erste Umwandlungen einer oder mehrerer Variablen vorzunehmen, obwohl dies auch während der Hauptanalysephase möglich ist. Mögliche Veränderungen der Variablen sind: Quadratwurzelumwandlung (wenn sich der Vertrieb von normal unterscheidet) Log-Transformation (wenn die Verteilung erheblich von der normalen)Inverse Transformation (wenn sich die Verteilung stark von der Normal unterscheidet) Kategorisch (ordinal / Dichotomous) (wenn sich die Verteilung stark von normal unterscheidet und keine Transformationshilfe) Hat die Durchführung der Studie die Absichten des Forschungsdesigns erfüllt? Eines sollte den Erfolg des Randomisierungsverfahrens prüfen, beispielsweise indem überprüft wird, ob Hintergrund- und materiellrechtliche Variablen gleichermaßen innerhalb und in allen Gruppen verteilt werden. Wenn die Studie kein Zufallsverfahren brauchte oder nutzte, sollte man den Erfolg der nicht-randomischen Probenahme prüfen, indem sie beispielsweise prüfen, ob alle Interessengruppen in der Stichprobe vertreten sind. Andere mögliche Datenverzerrungen, die überprüft werden sollten: Rückgang (dies sollte während der ersten Datenanalysephase ermittelt werden) Nichtverantwortliche (ob dies zufällig ist oder sollte während der ersten Datenanalysephase nicht geprüft werden)Treatmentqualität (Nutzung von Manipulationskontrollen). Merkmale der Datenstichprobe In jedem Bericht oder Artikel muss die Struktur der Probe genau beschrieben werden. Insbesondere ist es wichtig, die Struktur der Probe (insbesondere die Größe der Untergruppen) genau zu bestimmen, wenn Untergruppenanalysen während der Hauptanalysephase durchgeführt werden. Die Merkmale der Datenstichprobe können untersucht werden: Die Grundstatistiken wichtiger Variablen, die sich in der Endphase der ersten Datenanalyse befinden, und die Ergebnisse der ersten Datenanalyse werden dokumentiert und notwendig, vorzugsweise, und mögliche Korrekturmaßnahmen werden getroffen. Ebenfalls kann und sollte der ursprüngliche Plan für die wichtigsten Datenanalysen im Einzelnen oder in einer Neuschrift angegeben werden. Um dies zu tun, können mehrere Entscheidungen über die wichtigsten Datenanalysen getroffen werden: bei Nichtnormalen: sollte eine Umwandlungsvariable sein; Variablen kategorisch (ordinal/dichotomous); Anpassung der Analysemethode? im Falle fehlender Daten: sollten die fehlenden Daten vernachlässigt oder verwertet werden; die Anrechnungstechnik sollte verwendet werden? bei Auslierern: sollte man robuste Analysemethoden verwenden? Kommt es nicht zum Maßstab: sollte eine Anpassung des Messinstruments durch die Freigabe von Gegenständen erfolgen oder eine Vergleichbarkeit mit anderen (Verwendungen) Messinstrument(en) gewährleisten? Bei (too) kleinen Untergruppen: sollte die Hypothesis über Gruppenunterschiede verkleinert oder kleine Stichprobentechniken wie genaue Tests oder Schwimmen verwendet werden? Kann das Randomisierungsverfahren defekt sein: kann und sollte eine Berechnung der Propensity-Ergebnisse vornehmen und diese als Kovariate in den wichtigsten Analysen enthalten? Analyse verschiedener Analysen können während der ersten Datenanalysephase verwendet werden: Univariate Statistiken (einheitliche variable) Bivariate Verbände (Korrelations) Graphikologien (Schatten) Es ist wichtig, die Messwerte der Variablen für die Analysen zu berücksichtigen, da für jede Ebene besondere statistische Techniken zur Verfügung stehen: Nominale und ordinale Variablen Häufigkeit (Nummern und Prozentsätze)Associations Umstandambulations (Crosstabulations) Hierarchien (begrenzt auf maximal 8 Variablen) Loglineare Analyse (zur Ermittlung relevanter/importanter Variablen und möglicher Gründungen) Genaue Tests oder Schwimmen (bei Untergruppen sind kleine) Fertigstellung neuer Variablen permanente Variablen Verteilungsstatistiken (M, SD, Varianz, Skewness, kurtosis) Fotos von Stem-and-leaf Kartons gibt es nichtlineare Analyse nichtlinearer Analysen, wenn die Daten von einem nichtlinearen System erfasst werden. Nichtlineare Systeme können komplexe dynamische Effekte zeigen, darunter Bifuritionen, Chaos, Harmonien und Subharmonen, die nicht mit einfachen linearen Methoden analysiert werden können. Nichtlineare Datenanalyse ist eng mit der Identifizierung nichtlinearer Systeme verbunden. Hauptdatenanalyse In der Hauptanalysephase werden Analysen durchgeführt, die darauf abzielen, die Forschungsfrage zu beantworten, sowie alle anderen relevanten Analysen, die erforderlich sind, um den ersten Entwurf des Forschungsberichts zu schreiben. Sondierungs- und Bestätigungsansätze In der Hauptanalysephase kann entweder ein Sondierungs- oder Bestätigungsansatz verabschiedet werden. In der Regel wird der Ansatz beschlossen, bevor Daten gesammelt werden. In einer Sondierungsanalyse wird keine klare Hypothese vor der Analyse der Daten angegeben, und die Daten werden für Modelle, die die Daten gut beschreiben, gesucht. In einer Bestätigungsanalyse werden klare Hypothesen über die Daten getestet. Sondierung der Datenanalyse sollte sorgfältig interpretiert werden. Bei der Prüfung mehrerer Modelle gibt es eine große Chance, mindestens eine von ihnen zu nennen, aber dies kann auf einen Fehlertyp 1 zurückzuführen sein. Es ist wichtig, die Bedeutung bei der Prüfung mehrerer Modelle mit einer Bon Ferroni-Korrektur stets anzupassen. Man sollte auch nicht eine Sondierungsanalyse mit einer Bestätigungsanalyse im selben Datensatz verfolgen. Eine Sondierungsanalyse wird verwendet, um Ideen für eine Theorie zu finden, aber nicht, diese Theorie auch zu testen. Wenn ein Modell in einem Datenset exploriert wird, könnte die Analyse mit einer Bestätigungsanalyse im selben Datenset einfach bedeuten, dass die Ergebnisse der Bestätigungsanalyse auf den gleichen Typ 1-fehler zurückzuführen sind, der zum ersten Mal zum Sondierungsmodell führte. Die Bestätigungsanalyse wird daher nicht mehr informativer sein als die ursprüngliche Sondierungsanalyse. Stabilität der Ergebnisse Es ist wichtig, einige Hinweise darüber zu erhalten, wie die Ergebnisse allgemein sichtbar sind. Obwohl dies oft schwierig ist, kann man die Stabilität der Ergebnisse prüfen. Sind die Ergebnisse zuverlässig und unerträglich? Es gibt zwei Möglichkeiten, dies zu tun. Cross-validierung. Indem wir die Daten in mehrere Teile aufgeteilt haben, können wir prüfen, ob eine Analyse (wie ein Modell) auf der Grundlage eines Teils der Daten, der sich auf einen anderen Teil der Daten erstreckt, durchgeführt wird. Cross-validierung ist in der Regel unangemessen, wenn es Korrelationen innerhalb der Daten gibt, z.B. mit Paneldaten. Andere Validierungsmethoden müssen daher manchmal genutzt werden. Mehr zu diesem Thema siehe statistische Modellierung. Sensitivitätsanalyse. Ein Verfahren, um das Verhalten eines Systems oder Modells zu untersuchen, wenn globale Parameter (systematisch) unterschiedlich sind. Ein Weg, um das zu tun. kostenlose Software zur Datenanalyse Keine kostenlose Software für Datenanalyse: DevInfo – Ein Datenbanksystem, das von der Entwicklungsgruppe der Vereinten Nationen zur Überwachung und Analyse der menschlichen Entwicklung gebilligt wurde. ELKI – Data Mining Framework in Java mit forschungsorientierten Visualisierungsfunktionen. KNIME – The Rostock Information Miner, ein benutzerfreundlicher und umfassender Datenanalyserahmen. Orange – ein visuelles Programmierungsinstrument mit interaktiven Datenauswertungs- und -methoden für statistische Datenanalyse, Data Mining und maschinellem Lernen. Pandas – Bibliothek für Datenanalyse. PAW – FORTRAN/C-Datenanalyse-Rahmen für CERN.R – eine Programmierungssprache und Softwareumgebung für statistisches Daten- und Grafikverfahren. ROOT – C+-Datenanalyse-Rahmen für CERN. SciPy – Python Bibliothek für Datenanalyse. Julia - Eine Programmierungssprache ist für numerische Analysen und Rechenwissenschaft gut geeignet. Internationale Datenanalyse Wettbewerbe unterschiedliche Unternehmen oder Organisationen führen Datenanalysen durch, um Forscher zu ermutigen, ihre Daten zu nutzen oder eine bestimmte Frage mittels Datenanalyse zu lösen. Manche Beispiele für bekannte internationale Datenanalyse-Wettbewerbe sind: Kaggle Wettbewerb von Kaggle LTPP-Datenanalysewettbewerb, der von FHWA und ASCE durchgeführt wurde. Lesen Sie auch die Literatur Adèr, Herman J. (2008a). „Kapitel 14: Phasen und erste Schritte in der Datenanalyse“. In Adèr, Herman J. Mellenbergh, Hil J. Hand, David J (eds). Beratung über Forschungsmethoden: ein Berater. Huizen, Niederlande: Johannes van Kessel Pub.pp.333-356 ISBN 9789079418015.OCLC 905799857.Adèr, Herman J. (2008b). " 15 15: Die wichtigste Analysephase". In Adèr, Herman J. Mellenbergh, Hil J. Hand, David J (eds). Beratung über Forschungsmethoden: ein Berater. Huizen, Niederlande: Johannes van Kessel Pub.pp.357-386.ISBN 9789079418015.OCLC 905799857. Tabachnick, B.G & Fidell, L.S (2007) Kapitel 4: Reinigung Ihres Akts. Überprüfung der Daten vor der Analyse. B.G Tabachnick & L.S Fidell (Eds), Multivariate Statistiken, Fünfh Edition (pp. 60–116). Boston: Pearson Education, Inc. / Allyn und Bacon. Weitere Lesung Adèr, H.J & Mellenbergh, G.J (mit Beiträgen von D.J Hand) (2008). Forschungsberater Methoden: Ein Berater. Huizen, Niederlande: Johannes van Kessel Publishing. New York, John M; Cleveland, William S; Kleiner, Beat; Tukey, Paul A. 1983. Grafikverfahren für Datenanalyse, Wadsworth/Duxbury Press. ISBN 0-534-98052-X Fandango, Armando (2017). Skype Data Analysis, 2nd Edition. Packt Publishers.ISBN gegen 1787127487 Juran, Joseph M; Godfrey, A. Blanton (1999). Juran's Quality Handbuch, 5. New York: McGraw Hill.ISBN 0-07-034003-X Lewis-Beck, Michael S. (1995). Datenanalyse: Einführung, Sage Veröffentlichungen Inc, ISBN 0-8039-5772-6 NIST/SEMATECH (2008) Handbuch für statistische Methoden, Pyzdek, T, (2003). Quality Engineering Handbuch, ISBN 0-8247-4614-7 Richard sehrard (1984). Prag Datenanalyse. Oxford: Blackwell wissenschaftliche Veröffentlichungen.ISBN 0-632-01311-7 Tabachnick, B.G; Fidell, L.S (2007) Multivariate Statistiken, 5. Boston: Pearson Education, Inc. / Allyn und Bacon, [www.0-205-45938-4A diplomatischer Beutel, auch als diplomatischer Beutel bekannt, ist ein Container mit bestimmten rechtlichen Schutzmaßnahmen, die für die Durchführung offizieller Schriftstücke oder sonstiger Gegenstände zwischen einer diplomatischen Mission und ihrer Heimatregierung oder einer anderen diplomatischen, konsularischen oder sonstigen Einrichtung verwendet werden. Das physikalische Konzept eines "Diploms" ist flexibel und kann daher viele Formen annehmen (z.B. eine Kartonbox, kurzen Fall, duffelbeutel, großer Koffer, crate oder sogar einen Schiffscontainer). Hinzu kommt, dass ein diplomatischer Beutel in der Regel eine Form von Schleusen und/oder Tamper-evident-Siegel enthält, die ihm beigefügt ist, um Störungen durch unbefugte Dritte abzuschrecken oder aufzudecken. Der wichtigste Punkt ist, dass der Beutel, solange er außen gekennzeichnet ist, seinen Status zu zeigen, diplomatische Immunität von der Suche oder Beschlagnahme hat, wie in Artikel 27 des Wiener Übereinkommens von 1961 über diplomatische Beziehungen kodifiziert. Es kann nur Artikel enthalten, die für den amtlichen Gebrauch bestimmt sind. Es wird oft von einem diplomatischen Kurier begleitet, der ebenfalls von Festnahme und Haft bedroht ist. Winston Churchill berichtete während des Zweiten Weltkriegs über Lieferungen von kubanischen Zigarren nach dieser Methode. Triplex war ein britisches Espionage-Betrieb im Zweiten Weltkrieg, das den Inhalt der diplomatischen Säcke neutraler Länder geheim überführte. 1964 wurde ein marokkanischer israelischer Doppelagent mit dem Namen Mordechai Louk Drogen, gebunden und in einem diplomatischen Brief crate in der ägyptischen Botschaft in Rom untergebracht, aber von italienischen Behörden gerettet. Die Box, die er in "haef fast sicher vor dem Transport von Humangütern verwendet wurde", einschließlich möglicherweise für einen ägyptischen Militärbeamten, der sich vor einigen Jahren in Italien versäumt hatte, aber dann ohne Rückverfolgbarkeit vor dem Wiederauftreten des ägyptischen Sorgerechts und vor Gerichtsverfahrens verschwand. Die kanadische Regierung schickte kanadische Pässe und andere Materialien über einen diplomatischen Beutel nach Teheran, um bei der Erhitzung von sechs amerikanischen Diplomaten zu helfen, die während der Beschlagnahme der Botschaft der Vereinigten Staaten geimpft hatten. Während des Falklandskriegs von 1982 nutzte die argentinische Regierung einen diplomatischen Beutel, um mehrere Minen an ihre Botschaft in Spanien zu schmuggeln, die in der abdeckten Operation Algeciras verwendet werden sollen, in der argentinische Agenten einen britischen Kriegsverbrecher im Royal Marine Dockyard in Gibraltar verdrängen. Das Grundstück wurde von der spanischen Polizei entdeckt und gestoppt, bevor die Sprengstoffe eingesetzt werden könnten. Im Jahr 1984 wurde Dikko Affair, ein ehemaliger niger niger Regierungsminister, Umaru Dikko, in einem Schiff entführt und platziert, um ihn aus dem Vereinigten Königreich wieder nach Nigeria zu transportieren. Man war jedoch nicht als diplomatischer Beutel gekennzeichnet, der britische Zoll erlaubte, ihn zu öffnen. 1984 wurde der Sterling-Untermaschinenfeuer, der zur Abtötung von WPC Yvonne Fletcher aus der libyschen Botschaft in London verwendet wurde, in einem von 21 diplomatischen Taschen aus dem Vereinigten Königreich geschmuggelt. Simbabwe war im März 2000 Gegenstand eines internationalen politischen Interesses, als es eine britische diplomatische Sendung öffnete. Im Mai 2008 wurde eine Ersatzpumpe für die Toiletten an der Internationalen Weltraumstation in einem diplomatischen Beutel aus Russland an die Vereinigten Staaten geschickt, bevor sie die nächste Shuttle-Mission abheben. 2012 wurde in New York eine 16 Kilogramm Sendung von Kokain an die Vereinten Nationen in einem als diplomatischer Beutel verschleierten Beutel geschickt. Italien hat im Januar 2012 40 Kilogramm Kokain entdeckt, die in einem diplomatischen Beutel aus Ecuador geschmuggelt wurden und fünf feststellten. Ecuador hat die Sendung für Drogen im Aussenministerium geprüft, bevor sie nach Mailand versandt wurde. November 2013 behauptete die britische Regierung, dass die Guardia Civil an der Grenze Gibraltar-spanisch einen offiziellen diplomatischen Protest öffnete. Die spanische Regierung reagierte darauf, dass der Beutel, der vom Gouverneur Gibraltars von einem Kurierunternehmen innerhalb eines Mailbags mit anderen Paketen transportiert wird, nicht den Kriterien entspricht, die im Transit zwischen einer diplomatischen Mission und einer Heimatregierung stehen. Im Juli 2020 entdeckte die indische Zollabteilung 30 Kilogramm Goldschmuggel in verschleierten diplomatischen Sendungen aus den Vereinigten Arabischen Emiraten, die am internationalen Flughafen Thiruvananthapuram in Kerala durch das Zollministerium beschlagnahmt wurden. Indiens Nationale Untersuchungsagentur hatte gezeigt, dass ehemalige konsularische Mitarbeiter von VAE an Goldschmugling beteiligt waren. Siehe auch Militärpost diplomatische Kabelreferenzen Externe Links eDiplomat.com: Glossar der diplomatischen Bedingungen „Is gibt es eine solche Sache als diplomatischer Beutel?" Schneller Dope.Dezember 20, 2005Retrieved September 23, 2012.