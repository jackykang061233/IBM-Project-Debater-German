Bei der Berechnung ist ein Compiler ein Computerprogramm, das den in einer Programmiersprache (der Quellsprache) geschriebenen Computercode in eine andere Sprache (die Zielsprache) übersetzt. Der Name Compiler wird in erster Linie für Programme verwendet, die Quellcode von einer hochrangigen Programmiersprache in eine untere Ebene Sprache übersetzen (z.B. Montagesprache, Objektcode oder Maschinencode), um ein ausführbares Programm zu erstellen. Es gibt viele verschiedene Arten von Compilern, die in verschiedenen nützlichen Formen Ausgabe produzieren. Ein Cross-Compiler erzeugt Code für ein anderes CPU- oder Betriebssystem als derjenige, auf dem der Cross-Compiler selbst läuft. Ein Bootstrap-Compiler wird in der Sprache geschrieben, die er kompilieren will. Ein Programm, das von einer niederen Sprache auf eine höhere Ebene übersetzt, ist ein Dekompiler. Ein Programm, das zwischen hochrangigen Sprachen übersetzt, wird in der Regel als Source-to-Source-Compiler oder Transpiler bezeichnet. Ein Sprachrewriter ist in der Regel ein Programm, das die Form der Ausdrücke ohne Sprachänderung übersetzt. Ein Compiler-Compiler ist ein Compiler, der einen Compiler (oder Teil eines) produziert. Ein Compiler wird wahrscheinlich einige oder alle folgenden Operationen ausführen, oft Phasen genannt: Vorverarbeitung, lexical analysis, parsing, semantic analysis (syntax-directed translation,) Umwandlung von Eingabeprogrammen in eine Zwischendarstellung, Codeoptimierung und Codegenerierung. Die Compiler implementieren diese Phasen in der Regel als modulare Komponenten und fördern die effiziente Gestaltung und Korrektheit von Transformationen von Source-Eingang zu Target-Ausgang. Programmfehler, die durch falsches Compiler-Verhalten verursacht werden, können sehr schwierig sein, aufzuspüren und zu arbeiten; daher investieren Compiler-Inhaber erhebliche Anstrengungen, um Compiler Korrektheit zu gewährleisten. Compiler sind nicht der einzige Sprachprozessor, der verwendet wird, um Quellprogramme zu transformieren. Ein Dolmetscher ist eine Computersoftware, die die angegebenen Operationen transformiert und ausführt. Der Übersetzungsprozess beeinflusst die Gestaltung von Computersprachen, was zu einer Vorliebe von Compilation oder Interpretation führt. Eine Programmiersprache kann theoretisch sowohl einen Compiler als auch einen Dolmetscher haben. In der Praxis sind Programmiersprachen eher mit nur einem (Kompilator oder Dolmetscher) verbunden. Die von Wissenschaftlern, Mathematikern und Ingenieuren entwickelten theoretischen Rechenkonzepte bildeten die Grundlage der digitalen modernen Computerentwicklung während des Zweiten Weltkriegs. Primitive binäre Sprachen entwickelt, weil digitale Geräte nur diejenigen und Nullen verstehen und die Schaltmuster in der zugrundeliegenden Maschinenarchitektur. In den späten 1940er Jahren wurden Montagesprachen erstellt, um eine arbeitbarere Abstraktion der Computerarchitekturen zu bieten. Eine begrenzte Speicherkapazität von frühen Computern führte zu erheblichen technischen Herausforderungen, als die ersten Compiler konzipiert wurden. Daher musste der Compilationsprozess in mehrere kleine Programme unterteilt werden. Die vorderen Endprogramme produzieren die Analyseprodukte, die von den hinteren Endprogrammen zur Erzeugung von Zielcode verwendet werden. Da die Computertechnologie mehr Ressourcen zur Verfügung stellte, könnten Compiler-Designs besser mit dem Compilationsprozess ausrichten. Es ist in der Regel produktiver für einen Programmierer, eine hochrangige Sprache zu verwenden, so dass die Entwicklung von hochrangigen Sprachen natürlich von den Fähigkeiten von digitalen Computern gefolgt. Hochrangige Sprachen sind formale Sprachen, die streng durch ihre Syntax und Semantik definiert sind, die die hochrangige Spracharchitektur bilden. Elemente dieser formalen Sprachen umfassen: Alphabet, jede endliche Reihe von Symbolen; String, eine endliche Folge von Symbolen; Sprache, jede Reihe von Zeichenfolgen auf einem Alphabet. Die Sätze in einer Sprache können durch eine Reihe von Regeln definiert werden, die eine Grammatik genannt werden. Backus–Naur Form (BNF) beschreibt die Syntax von Sätzen einer Sprache und wurde für die Syntax von Algol 60 von John Backus verwendet. Die Ideen stammen aus den kontextfreien Grammatikkonzepten von Noam Chomsky, einem Linguisten. " BNF und seine Erweiterungen sind zu Standard-Tools zur Beschreibung der Syntax von Programmiernotationen geworden, und in vielen Fällen werden Teile von Compilern automatisch aus einer BNF-Beschreibung generiert. " In den 1940er Jahren entwarf Konrad Zuse eine algorithmische Programmiersprache namens Plankalkül ("Plan Calculus"). Während bis in die 1970er-Jahre keine tatsächliche Umsetzung stattgefunden hat, präsentierte sie in den späten 1950er-Jahren von Ken Iverson entworfenen APL Konzepte. APL ist eine Sprache für mathematische Berechnungen. Das hochrangige Sprachdesign in den formativen Jahren des Digital Computing lieferte nützliche Programmierwerkzeuge für eine Vielzahl von Anwendungen: FORTRAN (Formula Translation) für Ingenieur- und Wissenschaftsanwendungen gilt als die erste hochrangige Sprache. COBOL (Common Business-Oriented Language) entwickelte sich von A-0 und FLOW-MATIC zur dominanten High-Level-Sprache für Geschäftsanwendungen. LISP (List Processor) zur symbolischen Berechnung. Die Compiler-Technologie entwickelte sich aus der Notwendigkeit einer streng definierten Transformation des High-Level-Source-Programms in ein Low-Level-Zielprogramm für den digitalen Computer. Der Compiler konnte als vorderes Ende angesehen werden, um mit der Analyse des Quellcodes und einem hinteren Ende die Analyse in den Zielcode zu synthetisieren. Eine Optimierung zwischen dem vorderen Ende und dem hinteren Ende könnte einen effizienteren Zielcode erzeugen. Einige frühe Meilensteine in der Entwicklung der Compiler-Technologie: 1952: Ein Autocode-Compiler, der von Alick Glennie für den Manchester Mark I Computer an der University of Manchester entwickelt wurde, wird von einigen als die erste kompilierte Programmiersprache angesehen. 1952: Grace Hoppers Team von Remington Rand schrieb den Compiler für die A-0 Programmiersprache (und prägte den Begriff Compiler, um es zu beschreiben), obwohl der A-0-Compiler mehr als ein Loader oder Linker als die moderne Vorstellung eines Vollkompilators funktionierte.1954–1957: Ein Team unter der Leitung von John Backus bei IBM entwickelte FORTRAN, die in der Regel als erste hochrangige Sprache gilt. 1957 fertigten sie einen FORTRAN-Compiler, der in der Regel als den ersten eindeutig kompletten Compiler eingeführt wird. 1959:Die Konferenz über Data Systems Language (CODASYL) initiierte die Entwicklung von COBOL. Das COBOL-Design zeichnete A-0 und FLOW-MATIC aus. Anfang der 1960er Jahre wurde COBOL auf mehreren Architekturen zusammengestellt.1958–1962:John McCarthy am MIT hat LISP entworfen. Die Fähigkeiten der Symbolverarbeitung lieferten nützliche Funktionen für die künstliche Intelligenzforschung. Im Jahr 1962, LISP 1.5 Veröffentlichung bemerkt einige Werkzeuge: ein Dolmetscher geschrieben von Stephen Russell und Daniel J. Edwards, ein Compiler und Assembler geschrieben von Tim Hart und Mike Levin. Frühe Betriebssysteme und Software wurden in der Montagesprache geschrieben. In den 1960er und frühen 1970er Jahren war der Einsatz von hochrangigen Sprachen für die Systemprogrammierung aufgrund von Ressourcenbeschränkungen noch umstritten. Allerdings begannen mehrere Forschungs- und Industrieanstrengungen den Übergang zu hochrangigen Programmiersprachen, z.B. BCPL, BLISS, B und C. BCPL (Basic Combined Programming Language), die 1966 von Martin Richards an der University of Cambridge entworfen wurden, wurde ursprünglich als Compiler-Schreibwerkzeug entwickelt. Mehrere Compiler wurden implementiert, Richards' Buch gibt Einblicke in die Sprache und ihren Compiler. BCPL war nicht nur eine einflussreiche Programmiersprache, die noch in der Forschung verwendet wird, sondern auch eine Grundlage für die Gestaltung von B- und C-Sprachen. BLISS (Basic Language for Implementation of System Software) wurde für eine Digital Equipment Corporation (DEC) PDP-10 Computer von W.A Wulf's Carnegie Mellon University (CMU) Forschungsteam entwickelt. Das CMU-Team hat ein Jahr später 1970 den BLISS-11-Compiler entwickelt. Multics (Multiplexed Information and Computing Service), ein zeitübergreifendes Betriebssystemprojekt, beteiligt MIT, Bell Labs, General Electric (später Honeywell) und wurde von Fernando Corbató vom MIT geleitet. Multics wurde in der von IBM und IBM User Group entwickelten PL/I-Sprache geschrieben. Ziel von IBM war es, die Anforderungen an die Unternehmens-, wissenschaftliche und Systemplanung zu erfüllen. Es gab andere Sprachen, die berücksichtigt werden konnten, aber PL/I bot die vollständigste Lösung, obwohl sie nicht umgesetzt worden war. In den ersten Jahren des Multics-Projekts konnte eine Untergruppe der Sprache mit dem Early PL/I (EPL)-Compiler von Doug McIlory und Bob Morris von Bell Labs zur Montagesprache kompiliert werden. EPL unterstützte das Projekt, bis ein Boot-Strapping-Compiler für die komplette PL/I entwickelt werden konnte. Bell Labs verließ 1969 das Multics-Projekt: "Die Hoffnung wurde immer durch Frustration ersetzt, da die Gruppenanstrengung zunächst ein wirtschaftlich nützliches System nicht schaffte. "Die kontinuierliche Beteiligung würde die Kosten für die Projektunterstützung erhöhen. So wandten sich Forscher zu anderen Entwicklungsbemühungen. Eine System-Programmiersprache B basierend auf BCPL-Konzepten wurde von Dennis Ritchie und Ken Thompson geschrieben. Ritchie erstellte einen Boot-Strapping-Compiler für B und schrieb Unics (Uniplexed Information and Computing Service) Betriebssystem für ein PDP-7 in B. Unics wurde schließlich buchstabiert Unix. Bell Labs begann die Entwicklung und Erweiterung von C basierend auf B und BCPL. Der BCPL-Compiler wurde von Bell Labs nach Multics transportiert und BCPL war eine bevorzugte Sprache bei Bell Labs. Anfangs ein Programm an Bell Labs' B-Compiler wurde verwendet, während ein C-Compiler entwickelt wurde. 1971 lieferte ein neuer PDP-11 die Ressource, um Erweiterungen zu B zu definieren und den Compiler neu zu schreiben. Bis 1973 war das Design der C-Sprache im Wesentlichen vollständig und der Unix-Kernel für einen PDP-11 wurde in C. Steve Johnson begann die Entwicklung von Portable C Compiler (PCC) zu unterstützen Retargeting von C-Compilern zu neuen Maschinen. Objektorientierte Programmierung (OOP) bot einige interessante Möglichkeiten zur Anwendungsentwicklung und Wartung. OOP-Konzepte gehen weiter zurück, waren aber Teil der Sprachwissenschaft LISP und Simula. Bei Bell Labs wurde die Entwicklung von C+ an OOP.C+ erstmals 1980 für die Systemprogrammierung eingesetzt. Die erste Konstruktion verhebelt C-Sprachsysteme Programmierfähigkeiten mit Simula-Konzepten. 1983 wurden objektorientierte Einrichtungen hinzugefügt. Das Cfront-Programm implementierte ein C+-Frontend für C84-Sprachkompilator. In den folgenden Jahren wurden mehrere C+-Compiler entwickelt, als C+ Popularität wuchs. In vielen Anwendungsbereichen, die Idee, eine übergeordnete Sprache schnell aufgegriffen. Aufgrund der wachsenden Funktionalität, die von neueren Programmiersprachen unterstützt wird, und der zunehmenden Komplexität von Computerarchitekturen, wurden Compiler komplexer. DARPA (Defense Advanced Research Projects Agency) förderte 1970 ein Compiler-Projekt mit Wulfs CMU-Forschungsteam. Die Produktionsqualität Compiler-Compiler PQCC Design würde einen Produktionsqualität Compiler (PQC) aus formalen Definitionen der Quellsprache und dem Ziel produzieren. PQCC versuchte, den Begriff Compiler-Compiler über die traditionelle Bedeutung als Parser-Generator (z.B. Yacc) ohne viel Erfolg zu erweitern. PQCC könnte besser als Compiler-Generator bezeichnet werden. Die PQCC-Forschung zur Codegenerierung versuchte, ein wirklich automatisches Compiler-Schreibsystem aufzubauen. Der Aufwand hat die Phasenstruktur des PQC entdeckt und entworfen. Der BLISS-11-Compiler lieferte die erste Struktur. Die Phasen enthielten Analysen (Vorderseite,) Zwischenübersetzung zu virtueller Maschine (Mitte Ende) und Übersetzung zum Ziel (Rückende). TCOL wurde für die PQCC-Forschung entwickelt, um sprachspezifische Konstrukte in der Zwischendarstellung zu handhaben. Variationen von TCOL unterstützten verschiedene Sprachen. Das PQCC-Projekt untersuchte Techniken des automatisierten Compilerbaus. Die Designkonzepte haben sich bei der Optimierung von Compilern und Compilern für die objektorientierte Programmiersprache Ada bewährt. Das Ada Stoneman Dokument formalisierte die Programmunterstützungsumgebung (APSE) zusammen mit dem Kernel (KAPSE) und minimal (MAPSE). Ein Ada-Interpreter NYU/ED unterstützte Entwicklungs- und Standardisierungsbemühungen mit dem American National Standards Institute (ANSI) und der International Standards Organization (ISO). Die erste Ada-Compiler-Entwicklung durch die US-Militärdienste umfasste die Compiler in einer komplett integrierten Designumgebung entlang der Linien des Stoneman-Dokuments. Army und Navy arbeiteten an dem Ada Language System (ALS) Projekt, das auf die DEC/VAX-Architektur ausgerichtet war, während die Air Force auf der Ada Integrated Environment (AIE) mit dem Ziel der IBM 370-Serie begann. Während die Projekte nicht die gewünschten Ergebnisse lieferten, trugen sie zum Gesamtaufwand für die Entwicklung von Ada bei. Andere Ada-Compiler-Bemühungen wurden in Großbritannien an der Universität York und in Deutschland an der Universität Karlsruhe durchgeführt. In den USA lieferte Verdix (später erworben von Rational) das Verdix Ada Development System (VADS) an die Armee. VADS lieferte eine Reihe von Entwicklungstools einschließlich eines Compilers. Unix/VADS konnte auf einer Vielzahl von Unix-Plattformen wie DEC Ultrix und der Sun 3/60 Solaris auf Motorola 68020 in einer Army CECOM-Bewertung gehostet werden. Es gab bald viele Ada-Compiler, die die Ada Validation Tests bestanden. Das Projekt Free Software Foundation GNU hat die GNU Compiler Collection (GCC) entwickelt, die eine Kernfunktion zur Unterstützung mehrerer Sprachen und Ziele bietet. Die Ada-Version GNAT ist einer der am weitesten verbreiteten Ada-Compiler. GNAT ist frei, aber es gibt auch kommerzielle Unterstützung, zum Beispiel AdaCore, wurde 1994 gegründet, um kommerzielle Softwarelösungen für Ada zu bieten. GNAT Pro umfasst die GNU GCC-basierte GNAT mit einer Tool-Suite, um eine integrierte Entwicklungsumgebung bereitzustellen. Hochrangige Sprachen fuhren weiter zur Compilerforschung und -entwicklung. Schwerpunktbereiche waren Optimierung und automatische Code-Generierung. Trends in Programmiersprachen und Entwicklungsumgebungen beeinflussten die Compiler-Technologie. Weitere Compiler wurden in Sprachverteilungen (PERL, Java Development Kit) und als Bestandteil einer IDE (VADS, Eclipse, Ada Pro) aufgenommen. Die Vernetzung und die Interdependenz von Technologien wuchsen. Das Aufkommen von Webservices förderte das Wachstum von Websprachen und Skriptsprachen. Scripts verweisen auf die frühen Tage der Kommandozeilenschnittstellen (CLI), wo der Benutzer Befehle eingeben könnte, die vom System ausgeführt werden sollen. Benutzer Shell Konzepte entwickelt mit Sprachen zu schreiben Shell-Programme. Frühe Windows-Designs bot eine einfache Batch-Programmierfähigkeit. Die konventionelle Transformation dieser Sprache benutzte einen Dolmetscher. Während nicht weit verbreitet, Bash und Batch-Compiler wurden geschrieben. Neuere ausgeklügelte Sprachen wurden Teil des Entwickler-Tool-Kit. Moderne Skripting-Sprachen umfassen PHP, Python, Ruby und Lua.(Lua ist in der Spielentwicklung weit verbreitet.) Alle diese haben Dolmetscher- und Compilerunterstützung. " Als Ende der 50er Jahre das Feld der Zusammenstellung begann, war sein Fokus auf die Übersetzung hochrangiger Sprachprogramme in den Maschinencode beschränkt. Das Compilerfeld wird zunehmend mit anderen Disziplinen wie Computerarchitektur, Programmiersprachen, formale Methoden, Softwaretechnik und Computersicherheit vernetzt." Der Artikel "Compiler Research: The Next 50 Years" stellte die Bedeutung von objektorientierten Sprachen und Java fest. Sicherheit und Parallel Computing wurden unter den künftigen Forschungszielen genannt. Compiler Bau Ein Compiler implementiert eine formale Transformation von einem hochrangigen Quellprogramm zu einem Low-Level-Zielprogramm. Das Compiler-Design kann eine End-to-End-Lösung definieren oder eine definierte Teilmenge angehen, die mit anderen Compilationstools wie z.B. Preprozessoren, Assembler, Linker verknüpft ist. Die Konstruktionsanforderungen umfassen streng definierte Schnittstellen sowohl intern zwischen Compilerkomponenten als auch extern zwischen Stützwerkzeugen. In den frühen Tagen wurde der Ansatz zur Erstellung des Entwurfs direkt von der Komplexität der zu bearbeitenden Computersprache, der Erfahrung der Person(en) und den verfügbaren Ressourcen beeinflusst. Ressourcenbeschränkungen führten dazu, dass der Quellcode mehr als einmal passieren muss. Ein Compiler für eine relativ einfache Sprache, die von einer Person geschrieben wird, könnte ein einziges, monolithisches Stück Software sein. Da jedoch die Quellsprache in der Komplexität wächst, kann das Design in eine Reihe von voneinander abhängigen Phasen aufgeteilt werden. Separate Phasen bieten Designverbesserungen, die die Entwicklung auf die Funktionen im Compilationsprozess konzentrieren. Einpass gegen Multipass-Compiler Die Klassifizierung von Compilern nach Anzahl der Pässe hat seinen Hintergrund in der Hardware-Ressource Einschränkungen von Computern. Compiling beinhaltet die Durchführung viel Arbeit und frühe Computer haben nicht genug Speicher, um ein Programm, das alle diese Arbeit. So wurden Compiler in kleinere Programme aufgespalten, die jeweils einen Pass über die Quelle (oder einige Darstellung davon) gemacht haben, die einige der benötigten Analyse und Übersetzungen durchführten. Die Fähigkeit, in einem einzigen Pass zu kompilieren, wurde klassisch als Vorteil gesehen, weil es die Aufgabe vereinfacht, einen Compiler zu schreiben und Einpass-Compiler führen in der Regel Kompilationen schneller als Multipass-Compiler. So wurden zum Teil durch die Ressourcenbegrenzungen früherer Systeme viele frühe Sprachen gezielt so konzipiert, dass sie in einem einzigen Pass (z.B. Pascal) kompiliert werden konnten. In einigen Fällen kann die Gestaltung einer Sprachfunktion einen Compiler benötigen, um mehr als einen Pass über die Quelle durchzuführen. Betrachten Sie beispielsweise eine in Zeile 20 der Quelle erscheinende Erklärung, die die Übersetzung einer in Zeile 10 erscheinenden Erklärung beeinflusst. In diesem Fall muss der erste Pass Informationen über Erklärungen sammeln, die nach Aussagen, die sie betreffen, erscheinen, wobei die tatsächliche Übersetzung während eines späteren Passes erfolgt. Nachteilig bei der Zusammenstellung in einem einzigen Durchgang ist, dass es nicht möglich ist, viele der ausgefeilten Optimierungen durchzuführen, die zur Erzeugung von hochwertigem Code erforderlich sind. Es kann schwierig sein, genau zu zählen, wie viele Passiert ein optimierender Compiler macht. Beispielsweise können verschiedene Phasen der Optimierung einen Ausdruck oft analysieren, aber nur einmal einen anderen Ausdruck analysieren. Die Aufteilung eines Compilers in kleine Programme ist eine Technik, die von Forschern verwendet wird, die an der Herstellung von nachweislich korrekten Compilern interessiert sind. Die Bestätigung der Korrektheit eines Satzes kleiner Programme erfordert oft weniger Anstrengung, als die Richtigkeit eines größeren, einzigen, gleichwertigen Programms zu beweisen. Dreistufige CompilerstrukturSollte die genaue Anzahl der Phasen im Compiler-Design, die Phasen können einer von drei Stufen zugeordnet werden. Die Stufen umfassen ein vorderes Ende, ein mittleres Ende und ein hinteres Ende. Das vordere Ende scannt die Eingabe und überprüft Syntax und Semantik nach einer bestimmten Quellsprache. Für statisch eingegebene Sprachen führt sie eine Typkontrolle durch Sammeln von Typinformationen durch. Wenn das Eingabeprogramm syntaktisch falsch ist oder einen Typfehler aufweist, erzeugt es Fehler- und/oder Warnmeldungen, die in der Regel den Ort im Quellcode identifizieren, an dem das Problem erkannt wurde; in einigen Fällen kann der tatsächliche Fehler (much) früher im Programm sein. Zu den Aspekten des vorderen Endes gehören lexische Analyse, Syntaxanalyse und semantische Analyse. Das vordere Ende verwandelt das Eingabeprogramm in eine Zwischendarstellung (IR) zur Weiterverarbeitung durch das mittlere Ende. Dieser IR ist in der Regel eine untere Darstellung des Programms bezüglich des Quellcodes. Das mittlere Ende führt Optimierungen auf dem IR durch, die unabhängig von der angestrebten CPU-Architektur sind. Diese Quellcode/Maschinencodeunabhängigkeit soll es ermöglichen, generische Optimierungen zwischen Versionen des Compilers zu teilen, die verschiedene Sprachen und Zielprozessoren unterstützen. Beispiele für mittlere Endoptimierungen sind die Entfernung nutzloser (Dead Code Elimination) oder unlösbarer Code (Reachability Analysis,) Entdeckung und Ausbreitung von konstanten Werten (Constant Propagation,) Umlagerung der Berechnung auf einen weniger häufig ausgeführten Ort (z.B. aus einer Schleife) oder die Spezialisierung der Berechnung basierend auf dem Kontext. Schließlich wird der optimierte IR erzeugt, der vom hinteren Ende verwendet wird. Das hintere Ende nimmt den optimierten IR vom mittleren Ende. Es kann mehr Analysen, Transformationen und Optimierungen durchführen, die für die Ziel-CPU-Architektur spezifisch sind. Das hintere Ende erzeugt den zielabhängigen Montagecode und führt dabei eine Registerzuordnung durch.Das hintere Ende führt die Befehlsplanung durch, die die Befehle neu anordnet, parallele Ausführungseinheiten durch Abfüllverzögerungsschlitze zu halten. Obwohl die meisten Optimierungsprobleme NP-hart sind, sind heuristische Techniken zur Lösung gut entwickelt und derzeit in produktionsnahen Compilern implementiert. Typischerweise ist der Ausgang eines Backends ein für einen bestimmten Prozessor und Betriebssystem spezialisierter Maschinencode. Dieser Front-/Mitte/Backend-Ansatz ermöglicht es, Frontenden für unterschiedliche Sprachen mit Rückenenden für unterschiedliche CPUs zu kombinieren und gleichzeitig die Optimierungen des mittleren Endes zu teilen. Praktische Beispiele für diesen Ansatz sind die GNU Compiler Collection, Clang (LLVM-basierter C/C+ Compiler) und das Amsterdam Compiler Kit, das mehrere Frontends, gemeinsame Optimierungen und mehrere Backends aufweist. Das vordere Ende analysiert den Quellcode, um eine interne Darstellung des Programms zu erstellen, die als Zwischendarstellung (IR) bezeichnet wird. Es verwaltet auch die Symboltabelle, eine Datenstruktur, die jedes Symbol im Quellcode auf zugehörige Informationen wie Ort, Typ und Umfang abbildet. Während das Frontend eine einzelne monolithische Funktion oder ein Programm sein kann, wie in einem Scannerless-Parser, wurde es traditionell als mehrere Phasen implementiert und analysiert, die sequentiell oder gleichzeitig ausgeführt werden können. Diese Methode wird durch ihre Modularität und Trennung von Anliegen begünstigt. Am häufigsten wird heute das Frontend in drei Phasen zerbrochen: lexische Analyse (auch als Lexing oder Scannen bekannt), Syntaxanalyse (auch als Scannen oder Parsing bekannt) und semantische Analyse. Lexing und Parsing umfassen die syntaktische Analyse (Worts-Syntax bzw. Phrasen-Syntax) und in einfachen Fällen können diese Module (Lexer und Parser) automatisch aus einer Grammatik für die Sprache generiert werden, obwohl sie in komplexeren Fällen manuelle Modifikation erfordern. Die lexische Grammatik und Phrasen-Grammatik sind in der Regel kontextfreie Grammatiken, die die Analyse deutlich vereinfacht, wobei Kontext-Sensitivität an der semantischen Analysephase behandelt wird. Die semantische Analysephase ist in der Regel komplexer und von Hand geschrieben, kann aber mit Hilfe von Attribut grammars teilweise oder vollständig automatisiert werden. Diese Phasen selbst können weiter aufgeschlüsselt werden: Lexing als Scannen und Auswerten und Parsing als Aufbau eines konkreten Syntaxbaums (CST, Parse tree) und dann transformieren es in einen abstrakten Syntaxbaum (AST, Syntaxbaum). In einigen Fällen werden zusätzliche Phasen verwendet, insbesondere Linienrekonstruktion und Vorverarbeitung, aber diese sind selten. Die Hauptphasen des vorderen Endes umfassen: Linienrekonstruktion wandelt die Eingabezeichenfolge in eine für den Parser bereite kanonische Form um. Sprachen, die ihre Keywords stropieren oder willkürliche Leerzeichen innerhalb von Kennungen zulassen, benötigen diese Phase. Die in den 1960er Jahren verwendeten top-down-, rekursiv-descenten, tischgetriebenen Parser lesen typischerweise den Quelltext zu einer Zeit und erforderten keine separate Tokenisierungsphase. Atlas Autocode und Imp (und einige Implementierungen von ALGOL und Coral 66) sind Beispiele für zerstreute Sprachen, deren Compiler eine Line Reconstruction Phase haben würden. Vorverarbeitung unterstützt Makrosubstitution und bedingte Kompilation. Typischerweise erfolgt die Vorverarbeitungsphase vor der syntaktischen oder semantischen Analyse; z.B. bei C manipuliert der Vorprozessor lexische Token anstatt syntaktische Formen. Einige Sprachen wie Scheme unterstützen jedoch Makrosubstitutionen basierend auf syntaktischen Formen. Lexische Analyse (auch als Lexing oder Tokenization bekannt) bricht den Quellcodetext in eine Folge kleiner Stücke namens Lexical Tokens. Diese Phase kann in zwei Stufen unterteilt werden: das Scannen, das den Eingabetext in syntaktische Einheiten mit dem Namen lexemes segmentiert und ihnen eine Kategorie zuordnet; und die Auswertung, die lexemes in einen verarbeiteten Wert umwandelt. Ein Token ist ein Paar, bestehend aus einem Tokennamen und einem optionalen Tokenwert. Gemeinsame Tokenkategorien können Identifikatoren, Keywords, Separatoren, Operatoren, Literatur und Kommentare umfassen, obwohl die Anzahl der Tokenkategorien in verschiedenen Programmiersprachen variiert. Die Lexeme-Syntax ist typischerweise eine reguläre Sprache, so dass ein endlicher Zustandsautomat, der aus einem regelmäßigen Ausdruck aufgebaut ist, verwendet werden kann, um sie zu erkennen. Die lexische Analyse der Software wird als lexical Analysator bezeichnet. Dies kann nicht ein separater Schritt sein - er kann mit dem Parsing-Schritt in Scannerless-Parsing kombiniert werden, wobei die Parsierung auf der Zeichenebene, nicht auf der Zeichenebene erfolgt. Syntax-Analyse (auch Parsing genannt) beinhaltet die Parsierung der Tokensequenz, um die syntaktische Struktur des Programms zu identifizieren. Diese Phase baut typischerweise einen Parsebaum, der die lineare Folge von Token durch eine Baumstruktur ersetzt, die nach den Regeln einer formalen Grammatik gebaut wird, die die Syntax der Sprache definiert. Der Parsebaum wird oft analysiert, erweitert und durch spätere Phasen im Compiler transformiert. Semantische Analyse fügt semantische Informationen zum Parsebaum hinzu und baut die Symboltabelle. Diese Phase führt semantische Überprüfungen, wie z.B. die Typprüfung (Prüfung von Typfehlern) oder die Objektbindung (Verknüpfung von Variablen und Funktionsreferenzen mit deren Definitionen), oder eine bestimmte Zuordnung (Anforderung aller vor der Verwendung zu initialisierenden lokalen Variablen), die falsche Programme ablehnt oder Warnungen ausgibt. Semantische Analyse erfordert in der Regel einen kompletten Parsebaum, d.h., dass diese Phase logisch der Parsingphase folgt und der Codegenerierungsphase logisch vorausgeht, obwohl es oft möglich ist, mehrere Phasen in einen Pass über den Code in einer Compiler-Implementierung zu falten. Mittleres Ende Das mittlere, auch als Optimierer bezeichnete Ende führt Optimierungen in der Zwischendarstellung durch, um die Leistung und Qualität des erzeugten Maschinencodes zu verbessern. Das mittlere Ende enthält diejenigen Optimierungen, die unabhängig von der angestrebten CPU-Architektur sind. Die Hauptphasen des mittleren Endes umfassen: Analyse: Dies ist das Sammeln von Programminformationen aus der von der Eingabe abgeleiteten Zwischendarstellung; Datenflussanalyse wird verwendet, um gebrauchsdefinierte Ketten zusammen mit Abhängigkeitsanalyse, Aliasanalyse, Pointeranalyse, Fluchtanalyse etc. aufzubauen. Eine genaue Analyse ist die Grundlage für jede Compiler-Optimierung. Üblicherweise werden während der Analysephase auch die Steuer-Flow-Diagramme jeder kompilierten Funktion und der Ruf-Diagramm des Programms erstellt. Optimierung: Die Zwischensprachendarstellung wird in funktionell gleichwertige, aber schnellere (oder kleinere) Formen umgewandelt. Beliebte Optimierungen sind Inline-Erweiterung, Totcode Eliminierung, konstante Ausbreitung, Schleifentransformation und sogar automatische Parallelisierung. Die Compiler-Analyse ist die Voraussetzung für jede Compiler-Optimierung und sie arbeiten eng zusammen. Beispielsweise ist die Abhängigkeitsanalyse entscheidend für die Schleifentransformation. Der Umfang der Compiler-Analysen und -Optimierungen variiert stark; ihr Umfang kann vom Betrieb innerhalb eines Basisblocks, bis zu ganzen Prozeduren oder sogar des gesamten Programms reichen. Es gibt einen Kompromiss zwischen der Granularität der Optimierungen und den Kosten der Zusammenstellung. Beispielsweise sind Peephole-Optimierungen schnell während der Compilation durchzuführen, beeinflussen jedoch nur ein kleines lokales Fragment des Codes und können unabhängig von dem Kontext durchgeführt werden, in dem das Codefragment erscheint. Dagegen erfordert die interprozedurale Optimierung mehr Compilationszeit und Speicherplatz, sondern ermöglicht Optimierungen, die nur durch die Betrachtung des Verhaltens mehrerer Funktionen gleichzeitig möglich sind. Interprocedural Analyse und Optimierungen sind in modernen kommerziellen Compilern von HP, IBM, SGI, Intel, Microsoft und Sun Microsystems üblich. Die kostenlose Software GCC wurde für eine lange Zeit wegen mangelnder leistungsfähiger interproceduraler Optimierungen kritisiert, aber es ändert sich in dieser Hinsicht. Ein weiterer Open Source-Compiler mit vollständiger Analyse- und Optimierungsinfrastruktur ist Open64, der von vielen Organisationen für Forschung und kommerzielle Zwecke verwendet wird. Aufgrund der zusätzlichen Zeit und des Platzes, die für Compiler-Analysen und Optimierungen benötigt werden, überspringen einige Compiler sie standardmäßig. Benutzer müssen Compilation-Optionen verwenden, um dem Compiler explizit zu sagen, welche Optimierungen aktiviert werden sollen. Back endDas hintere Ende ist für die CPU-Architektur spezifische Optimierungen und für die Codegenerierung verantwortlich. Die Hauptphasen des hinteren Endes umfassen: Maschinenabhängige Optimierungen: Optimierungen, die von den Details der CPU-Architektur abhängen, die der Compiler zielt. Ein prominentes Beispiel sind Peephole-Optimierungen, die kurze Sequenzen von Montageanleitungen in effizientere Anweisungen neu schreiben. Code-Generation: Die transformierte Zwischensprache wird in die Ausgabesprache übersetzt, in der Regel die Muttersprache des Systems. Dazu gehören Ressourcen- und Speicherentscheidungen, wie z.B. die Entscheidung, welche Variablen in Register und Speicher passen und die Auswahl und Terminierung entsprechender Maschinenanweisungen zusammen mit ihren zugehörigen Adressierungsmodi (siehe auch Sethi-Ullman-Algorithmus). Debug-Daten müssen auch generiert werden, um das Debugging zu erleichtern. Compiler Korrektheit Compiler Korrektheit ist der Zweig der Software-Engineering, der versucht zu zeigen, dass sich ein Compiler nach seiner Sprachspezifikation verhalten. Die Techniken umfassen die Entwicklung des Compilers mit formalen Methoden und die Verwendung strenger Tests (oft Compiler Validierung genannt) auf einem vorhandenen Compiler. Compiled versus interpretierte Sprachen Hochrangige Programmiersprachen erscheinen in der Regel mit einer Art Übersetzung im Sinne: entweder als kompilierte Sprache oder als interpretierte Sprache konzipiert. In der Praxis gibt es jedoch selten etwas über eine Sprache, die es erfordert, ausschließlich kompiliert oder ausschließlich interpretiert zu werden, obwohl es möglich ist, Sprachen zu entwerfen, die sich auf Neuinterpretation zu Laufzeit verlassen. Die Kategorisierung spiegelt in der Regel die beliebtesten oder weit verbreiteten Implementierungen einer Sprache wider – zum Beispiel wird BASIC manchmal als interpretierte Sprache bezeichnet, und C als kompilierte, trotz der Existenz von BASIC-Compilern und C-Interpreten. Die Interpretation ersetzt die Zusammenstellung nicht vollständig. Es versteckt es nur vom Benutzer und macht es allmählich. Auch wenn ein Dolmetscher selbst interpretiert werden kann, wird ein direkt ausgeführtes Programm irgendwo am Boden des Stapels benötigt (siehe Maschinensprache). Weiterhin können Compiler Dolmetscher aus Optimierungsgründen enthalten. Wenn beispielsweise ein Ausdruck während der Zusammenstellung und die in das Ausgabeprogramm eingefügten Ergebnisse ausgeführt werden kann, so verhindert es, dass er jedes Mal neu berechnet werden muss, wenn das Programm läuft, was das Endprogramm stark beschleunigen kann. Moderne Trends zur just-in-time-Compilation und Bytecode-Interpretation verschwimmen manchmal die traditionellen Kategorisierungen von Compilern und Dolmetschern noch weiter. Einige Sprachspezifikationen deuten darauf hin, dass Implementierungen eine Compilationsanlage umfassen müssen; zum Beispiel Common Lisp. Es gibt jedoch nichts, das der Definition von Common Lisp innewohnt, die es davon abhält, interpretiert zu werden. Andere Sprachen haben Funktionen, die sehr einfach in einem Dolmetscher implementiert werden können, aber einen Compiler viel härter schreiben; zum Beispiel APL, SNOBOL4 und viele Skriptsprachen erlauben Programme, beliebigen Quellcode zu Laufzeit mit regelmäßigen String-Operationen zu konstruieren und diesen Code dann auszuführen, indem es zu einer speziellen Auswertungsfunktion weitergeleitet wird. Um diese Funktionen in einer kompilierten Sprache umzusetzen, müssen Programme in der Regel mit einer Laufzeitbibliothek versendet werden, die eine Version des Compilers selbst enthält. Arten Eine Klassifizierung von Compilern erfolgt durch die Plattform, auf der ihr generierter Code ausgeführt wird. Dies ist als Zielplattform bekannt. Ein nativer oder gehosteter Compiler ist ein, dessen Ausgang direkt auf der gleichen Art von Computer und Betriebssystem laufen soll, dass der Compiler selbst läuft. Der Ausgang eines Kreuzkompilators ist auf einer anderen Plattform ausgeführt. Cross-Compiler werden häufig bei der Entwicklung von Software für Embedded-Systeme verwendet, die keine Software-Entwicklungsumgebung unterstützen sollen. Die Ausgabe eines Compilers, der Code für eine virtuelle Maschine (VM) erzeugt, kann oder darf nicht auf derselben Plattform ausgeführt werden wie der Compiler, der sie produziert hat. Aus diesem Grund werden solche Compiler in der Regel nicht als native oder Cross-Compiler eingestuft. Die untere Ebene Sprache, die das Ziel eines Compilers ist, kann selbst eine hochrangige Programmiersprache sein. C, von einigen als eine Art tragbare Montagesprache betrachtet, ist häufig die Zielsprache solcher Compiler. Zum Beispiel verwendet Cfront, der Original-Compiler für C,+ C als Zielsprache. Der C-Code, der von einem solchen Compiler erzeugt wird, ist in der Regel nicht beabsichtigt, von Menschen lesbar und aufrechterhalten zu werden, so indent Stil und Erstellen von hübschen C-Zwischencode werden ignoriert. Einige der Merkmale von C, die es zu einer guten Zielsprache machen, sind die #line-Richtlinie, die vom Compiler generiert werden kann, um das Debugging der Originalquelle zu unterstützen, und die breite Plattform-Unterstützung, die mit C-Compilern zur Verfügung steht. Während ein gemeinsamer Compiler-Typ den Maschinencode ausgibt, gibt es viele andere Arten: Source-to-Source-Compiler sind eine Art Compiler, der eine hochrangige Sprache als Eingang nimmt und eine hochrangige Sprache ausgibt. Beispielsweise wird ein automatischer Parallelisierungskompilator häufig ein hochrangiges Sprachprogramm als Eingabe einnehmen und dann den Code transformieren und mit parallelen Code-Annotationen (z.B. OpenMP) oder Sprachkonstrukten (z.B. Fortrans DOALL-Anweisungen) annotieren. Bytecode-Compiler, die zur Montagesprache einer theoretischen Maschine kompilieren, wie einige Prolog-Implementierungen Diese Prolog-Maschine ist auch als Warren Abstract Machine (oder WAM) bekannt. Bytecode-Compiler für Java, Python sind auch Beispiele dieser Kategorie. Just-in-time-Compiler (JIT-Compiler) deferieren die Compilation bis zur Laufzeit. JIT-Compiler existieren für viele moderne Sprachen wie Python, JavaScript, Smalltalk, Java, Microsoft . NET's Common Intermediate Language (CIL) und andere. Ein JIT-Compiler läuft in der Regel in einem Dolmetscher. Wenn der Dolmetscher erkennt, dass ein Codepfad heiß ist, d.h. er häufig ausgeführt wird, wird der JIT-Compiler aufgerufen und der Hot-Code für eine erhöhte Leistung kompiliert. Für einige Sprachen, wie z.B. Java, werden zunächst Anwendungen mit einem Bytecode-Compiler zusammengestellt und in einer maschinenunabhängigen Zwischendarstellung geliefert. Ein Bytecode-Interpreter führt den Bytecode aus, aber der JIT-Compiler übersetzt den Bytecode zum Maschinencode, wenn eine erhöhte Leistung erforderlich ist. Hardware-Compiler (auch als Synthese-Tools bekannt) sind Compiler, deren Ausgang eine Beschreibung der Hardware-Konfiguration anstelle einer Folge von Anweisungen ist. Der Ausgang dieser Compiler Target Computer Hardware auf sehr niedrigem Niveau, beispielsweise ein feldprogrammierbares Gate-Array (FPGA) oder eine strukturierte anwendungsspezifische integrierte Schaltung (ASIC). Solche Compiler sollen Hardware-Compiler sein, weil der Quellcode, den sie kompilieren, die endgültige Konfiguration der Hardware effektiv steuert und wie sie arbeitet. Der Ausgang der Compilation ist nur eine Verbindung von Transistoren oder Lookup-Tabellen. Ein Beispiel für den Hardware-Compiler ist XST, das Xilinx-Synthese-Tool zur Konfiguration von FPGAs. Ähnliche Tools sind von Altera, Synplicity, Synopsys und anderen Hardware-Anbietern erhältlich. Ein Assembler ist ein Programm, das menschliche lesbare Montagesprache auf Maschinencode, die tatsächlichen Anweisungen, die von Hardware ausgeführt. Das inverse Programm, das den Maschinencode in die Montagesprache übersetzt, wird als Disassembler bezeichnet. Ein Programm, das von einer niederen Sprache auf eine höhere Ebene übersetzt, ist ein Dekompiler. Ein Programm, das zwischen hochrangigen Sprachen übersetzt wird, wird in der Regel als Sprachübersetzer, Quelltext-Compiler, Sprachumsetzer oder Sprachumschreiber bezeichnet. Der letzte Begriff wird in der Regel auf Übersetzungen angewendet, die keinen Sprachwechsel beinhalten. Ein Programm, das in ein Objektcode-Format übersetzt, das nicht auf der Compilation-Maschine unterstützt wird, wird als Cross-Compiler bezeichnet und wird häufig verwendet, um Code für eingebettete Anwendungen vorzubereiten. Ein Programm, das Objektcode unter Anwendung von Optimierungen und Transformationen wieder in die gleiche Art von Objektcode umschreibt, ist ein binärer Recompiler. Weitere Informationen Weitere Informationen Externe Links Compiler bei Curlie Incremental Approach to Compiler Construction – ein PDF-Tutorial Compile-Howto Basics of Compiler Design auf der Wayback Machine (archiviert am 15. Mai 2018) Kurzanimation auf YouTube, die den entscheidenden konzeptionellen Unterschied zwischen Compilern und Interpreten erklärt Syntax Analysis & LL1 Parsing auf YouTube Wir bauen einen Compiler, von Jack Crenshaw Forum über Compilerentwicklung auf der Wayback Machine (archiviert 10. Oktober 2014)