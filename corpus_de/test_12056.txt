lineare Regression ist ein linearer Ansatz zur Modellierung der Beziehung zwischen einer scalar-Reaktion und einem oder mehreren erläuternden Variablen (auch als abhängige und unabhängige Variablen bekannt). Bei einer erläuternden Variablen wird eine einfache lineare Regression genannt; für mehr als ein wird der Prozess mehrfach lineare Regression genannt. Dieser Begriff unterscheidet sich von einer multivarialen linearen Regression, bei der mehrere korrelierte Abhängigkeitsvariablen vorhergesagt werden, statt einer einzigen scalar variablen Variablen. lineare Regression werden die Beziehungen unter Verwendung linearer Vorhersehbarkeitsfunktionen entwickelt, deren unbekannte Modellparameter auf den Daten geschätzt werden. Solche Modelle werden als lineare Modelle bezeichnet. Häufig wird davon ausgegangen, dass die bedingten Mittel der Reaktion aufgrund der Werte der erläuternden Variablen (oder Vorhersehbare) eine affine Funktion dieser Werte darstellen; weniger häufig werden die gebundenen Medien oder einige andere Quantile verwendet. Wie alle Formen der Regressionsanalyse konzentriert sich lineare Regression auf die bedingte Wahrscheinlichkeitsverteilung der Reaktion aufgrund der Werte der Berechtigten und nicht auf die gemeinsame Wahrscheinlichkeitsverteilung aller dieser Variablen, d. h. der Bereich der multivarialen Analyse. Lineare Regression war die erste Art von Regressionsanalysen, die streng untersucht und in praktischen Anwendungen umfassend genutzt werden soll. Dies ist, weil Modelle, die linear auf ihre unbekannten Parameter angewiesen sind, leichter zu passen sind als Modelle, die nicht linear mit ihren Parametern verknüpft sind und weil die statistischen Eigenschaften der daraus resultierenden Ester leichter zu bestimmen sind. Lineare Regression hat viele praktische Anwendungen. Die meisten Anwendungen fallen in eine der folgenden beiden Kategorien: Liegt das Ziel darin, Vorhersagen, Vorhersagen oder Fehlerverringerung vorzunehmen, kann lineare Regression verwendet werden, um ein prädikatives Modell für die beobachteten Werte der Reaktions- und erläuternden Variablen anzupassen. Nach der Entwicklung eines solchen Modells, wenn zusätzliche Werte der erläuternden Variablen ohne begleitenden Antwortwert gesammelt werden, kann das installierte Modell verwendet werden, um eine Vorhersage der Reaktion vorzunehmen. Wenn das Ziel darin besteht, Unterschiede in der Reaktionsvariable zu erklären, die auf die Unterschiede in den erläuternden Variablen zurückzuführen sind, kann eine lineare Regressionsanalyse angewendet werden, um die Stärke der Beziehung zwischen der Antwort und den erläuternden Variablen zu quantifizieren und insbesondere zu ermitteln, ob einige erläuternde Variablen möglicherweise keine lineare Beziehung mit der Antwort haben oder zu ermitteln, welche Untersätze von erläuternden Variablen überflüssige Informationen über die Reaktion enthalten. Lineare Regressionsmodelle werden oft mit dem am wenigsten Quadrate-Ansatz ausgerüstet, aber sie können auch auf andere Weise installiert werden, wie etwa durch Minimierung der "Einsparung" in einigen anderen Normen (wie mit mindestens absoluten Abweichungen), oder durch Minimierung einer verhängungsfreien Version der am wenigsten Quadraten Kostenfunktion wie bei der Aufladung (L2-Norm) und der Laso (L1-Norm). Umgekehrt kann der am wenigsten Quadrate-Ansatz verwendet werden, um Modelle zu passen, die nicht lineare Modelle sind. Obwohl die Begriffe "least Quadrate" und "lineares Modell" eng miteinander verbunden sind, sind sie nicht gleich. Formulierung in Form eines Datensatzes { y i , x i 1 , ... , x i p } i = 1 n {\displaystyle y_{i},\,x_{i1},\ldots ,x_{ip__{i=1}^{n statistischer Einheiten, ein lineares Regressionsmodell geht davon aus, dass die Beziehung zwischen der abhängigen variablen y und dem p-vector of regressors x linear ist. Diese Beziehung wird durch eine Störungsfrist oder Fehlervariable ε – eine unobservierte Zufallsvariable, die die lineare Beziehung zwischen den abhängigen variablen und Regressoren verstärkt. So wird das Modell in Form y i = β 0 + β 1 x 1 +  + + β p x i p + ε i = x i T β +  i i , i = 1 , ... n , n , y_{i} {_0}+\beta 1}x_{i1}+\cdots +\beta p}x_{ip}+\varepsilon {_i}=\ Mathematik {x} _i}^{\ Mathematiksf T.print Symbol Memebeta +\}varepsilon {_i},\qquad i=1,\ldots n}, wo T die Umsetzung erfolgt, so dass xiTβ das Innenprodukt zwischen Vektors xi und β ist. Häufig werden diese n Gleichungen zusammengestapelt und in der Matrixnotation als y = X β + ed , {\displaystyle \ Mathematik {y} =X olibeta {+)sh Symbol {\,\,}, wo y = ( y 1 y 2 ) y n ) , Memedisplaystyle \ Mathematik {y}=beginp Matrix}y_{1}\\y_{2vvdots) y_{n}\end{pmatrix}},\quad } X = ( x 1 T x 2 T  T x n T ) = ( 1 x 11 ⋯ x 1 x 21  2 x 2 p ⋮  1  1 1 x  1 1 x n 1 ⋯ x n 1 ) x n p ) n p ) ) ) )  X  X _2s Mathematiksf {Tsvdots \dl {x} _ns Mathematiksf T11end{p Matrix}}=Getbegin{p Matrix}1 &x_{11} x_{1p11 &x_{21} &\cdots &x_{2pvvdots &\vdots &\ddots &\vdots 1 &x_{n1} &\cdots &x_{np}\p.end{p.end{p}\end{p Matrix, β = ( β 0 β 1 β 2 ⋮ β  p p  p )  2  2  2  2  2  2  2  2  2  2  2  2  2  2 ) ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . KINGstyle {p Matrix}\beta {_0}\\\beta {_11beta {_2vvdots \beta _p{end{pmatrix}},\quad Memeset Symbol Memevare =Begin{p Matrix}\varepsilon {_1varvarepsilon {_2vvdots \varepsilon _n}\end{pmatrix. Notation und Terminologie y Memedisplaystyle \ Mathematik {y} } ist ein Vektor der beobachteten Werte y i (i = 1 , ... n ) faserstyle y_{i= (i=1,\ldots ,n)} der variablen Bezeichnung Regressand, endogene, Reaktionsvariable, variables, variables Kriterium oder variabler Variablen. Diese Variablen sind manchmal auch als vorhergesagte Variablen bekannt, aber dies sollte nicht mit vorhergesagten Werten verwechselt werden, die geleugnet sind Yendisplaystyle Memehat {y}. Die Entscheidung, auf der sich die variablen Daten als abhängige Variablen ausbilden und die als unabhängige Variablen modelliert werden, kann auf einer Vermutung beruhen, dass der Wert einer der Variablen durch die anderen Variablen verursacht oder direkt beeinflusst wird. Alternativ kann ein operativer Grund sein, eine der Variablen in Bezug auf die anderen zu modellieren, in denen es keine Vermutung der Kausalität gibt. X {\displaystyle X} kann als Matrix von Folgereaktoren x i {\displaystyle \ Mathematik {x} {_i} oder von n-dimensionalen Spalten-Säulen X j {\displaystyle X_{j} angesehen werden, die als Regressoren, exogene Variablen, erläuternde Variablen, Kovariate, Inputvariablen, Vorhersehbarkeitsvariablen oder unabhängige Variablen (nicht mit dem Konzept unabhängiger Variablen) bekannt sind. Die Matrix X Memestyle X} wird manchmal als Designmatrix bezeichnet. In der Regel ist ein konstanter Bestandteil der Regressoren. Insbesondere x i 0 = 1 Memedisplaydisplaystyle \chino {x} {_i0}=1 für i = 1 , ... , n {\displaystyle i=1,\ldots ,n} .Das entsprechende Element der β wird als Überwachung bezeichnet. Viele statistische Anrechnungsverfahren für lineare Modelle erfordern ein Abfangen, so dass es oft auch bei theoretischen Erwägungen enthalten ist, dass sein Wert Null sein sollte. Manchmal kann eine der Regressoren eine nicht-lineare Funktion eines anderen Regressor oder der Daten sein, wie bei Polynomialer Regression und segmentierter Regression. Das Modell bleibt linear, solange es linear im Parametervektor β ist. Die Werte xij können entweder als beobachtete Werte von Zufallsvariablen angesehen werden Xj oder als feste Werte, die vor der Beaufsichtigung der abhängigen Variablen gewählt wurden. Beide Auslegungen können in unterschiedlichen Fällen angemessen sein, und sie führen in der Regel zu denselben Bewertungsverfahren; in diesen beiden Situationen werden jedoch unterschiedliche Ansätze zur asymptotischen Analyse verwendet. β {\displaystyle faserzeichen Memebeta ) ist ein ( p + 1 ) Memestyle (p+1)} -dimensionale Parametervektoren, bei denen β 0 {\displaystyle \beta {_0} der Überwachungsbegriff ist (wenn ein in das Modell aufgenommen wird) - anderewise β {\displaystyle faser Symbol Memebeta ). ist p-dimensional. Ihre Elemente sind als Wirkungs- oder Regressionskoeffizienten bekannt (obwohl dieser Begriff manchmal für die geschätzten Auswirkungen reserviert ist). In einfacher linearer Regression, p=1 und der Koeffizient ist als Regression bekannt. Statistische Schätzung und Gleichgültigkeit im linearen Regression konzentriert sich auf β. Die Elemente dieses Parametervektors werden als Teilderivate der abhängigen variablen Variablen in Bezug auf die verschiedenen unabhängigen Variablen interpretiert.   HANAdisplaystyle faserzeichen Memevarepsilon . ist ein Vektor von Werten  i i RARstyle \varepsilon {_i}. Dieser Teil des Modells wird als Fehlerzeit, Störungsfrist oder manchmal Lärm (im Gegensatz zu dem vom übrigen Modell bereitgestellten Signal) bezeichnet. Diese Variablen erfassen alle anderen Faktoren, die die abhängige variable y andere beeinflussen als die Regressoren x. Die Beziehung zwischen dem Fehlertermin und den Regressoren, beispielsweise deren Korrelation, ist eine entscheidende Berücksichtigung bei der Formulierung eines linearen Regressionsmodells, da sie die geeignete Schätzungsmethode bestimmen wird. Ein lineares Modell zu einem bestimmten Datensatz muss in der Regel die Regressionskoeffizienten β Memedisplaystyle faserzeichen Memebeta s so auswerten, dass die Fehlerfrist  = = y − X β {\displaystyle faserzeichen Memevarepsilon =\} Mathematikpo {y} -X Steuerzeichen Memebeta . wird minimiert. Zum Beispiel ist es üblich, die Summe der Quadratkilometern zu verwenden. Beispiel für eine Situation, in der ein kleiner Ball in der Luft angelaufen ist, und dann messen wir die Höhe des Aufstiegs in verschiedenen Augenblicken in der Zeitschwelle. Physik sagt uns, dass die Beziehung als h i = β 1 t i + β 2 t i 2 +  i i , Memestyle h_{i}=\beta 1}t_{i}+\beta 2}t_{i}^{2}+\varepsilon {_i,}, wo β1 die anfängliche Geschwindigkeit des Balls bestimmt, ist β2 proportional zur Standardlast, und εi ist auf Messfehler zurückzuführen. Lineare Regression kann verwendet werden, um die Werte von β1 und β2 aus den gemessenen Daten zu bewerten. Dieses Modell ist nicht linear in der Zeitvariable, aber es ist linear in den Parametern β1 und β2; wenn wir Regressoren xi= (xi1, xi2) (=ti, t2) nehmen, dauert das Modell auf dem Standardformular h i = x i T β +  i i . KINGstyle h_{i}=\ Mathematik {x} _i}^{\ Mathematik T print-Logo Tel.: {_i}. Konvermutungen Standard lineare Regressionsmodelle mit Standardschätzungstechniken machen eine Reihe von Annahmen über die vorhergesagten Variablen, die Reaktionsvariablen und ihre Beziehung. Viele Erweiterungen wurden entwickelt, die jede dieser Annahmen gelockert (d. h. auf eine schwächere Form reduziert) und in einigen Fällen vollständig beseitigt werden können.Insgesamt machen diese Erweiterungen das Bewertungsverfahren komplexer und zeitaufwendiger und erfordern möglicherweise auch mehr Daten, um ein ebenso präzises Modell zu erstellen. Folgende sind die wichtigsten Annahmen von Standard- linearen Regressionsmodellen mit Standardschätzungstechniken (z.B. gewöhnliche Höhen): Wirak exogenität. Dies bedeutet im Wesentlichen, dass die vorhergesagten Variablen x als feste Werte behandelt werden können, anstatt Zufallsvariablen. Dies bedeutet zum Beispiel, dass die vorhergesagten Variablen fehlerfrei sind – d. h. nicht mit Messfehlern kontaminiert sind. Obwohl diese Annahme in vielen Einstellungen nicht realistisch ist, führt sie zu deutlich schwierigeren Fehler-in-variablen-Modellen. Linearität Dies bedeutet, dass der Mittelwert der Reaktionsvariable eine lineare Kombination der Parameter (Regressionskoeffizienten) und der vorhergesagten Variablen ist. Hinweis darauf, dass diese Annahme sehr viel weniger restriktiv ist als sie zu Beginn erscheint. Da die vorhergesagten Variablen als feste Werte behandelt werden (siehe oben), ist die Linearität wirklich nur eine Beschränkung der Parameter. Die vorhergesagten Variablen selbst können willkürlich umgestaltet werden, und in der Tat können mehrere Kopien derselben zugrunde liegenden Vorhersehbarkeitsvariable hinzugefügt werden, je nachdem, welches verändert wird. Diese Technik wird beispielsweise in Polynomialregression verwendet, die lineare Regression nutzt, um die Reaktionsvariable als willkürliche Polynomialfunktion (bis zu einem bestimmten Rang) einer vorhergesagten Variablen anzupassen. Mit dieser viel Flexibilität haben Modelle wie Polynomiale Regression oft „zu viel Macht“, so dass sie die Daten in der Regel überlasten. Infolgedessen müssen einige Arten der Regularisierung in der Regel genutzt werden, um unverhältnismäßige Lösungen zu vermeiden, die aus dem Bewertungsprozess kommen. Häufige Beispiele sind die Regression und die Regression von lasso. Laesische lineare Regression kann auch verwendet werden, was durch seine Natur mehr oder weniger immun für das Problem der Überrüstung ist. (In der Tat können die Rationalisierung und die Laso-Regression sowohl als Sonderfälle der linearen Regression von Bayesian angesehen werden, wobei besondere Arten von Vorvertrieben auf den Regressionskoeffizienten aufgeführt sind).) Kontamination (a.k.a Homoscedasticity). Dies bedeutet, dass die Differenzierung der Fehler nicht von den Werten der vorhergesagten Variablen abhängt. So ist die Variabilität der Antworten auf bestimmte feste Werte der Berechtigten gleich, unabhängig davon, wie groß oder klein die Reaktionen sind. Dies ist oft nicht der Fall, da ein variabler, der bedeutend ist, in der Regel eine größere Varianz aufweisen wird als ein, der klein ist. Zum Beispiel kann eine Person, deren Einkommen auf 100.000 $ berechenbar ist, leicht ein tatsächliches Einkommen von 80.000 $ oder 120.000 $ - d. h. eine Standardabweichung von rund 20.000 $ haben, während eine andere Person mit einem vorhergesagten Einkommen von 10.000 $ unwahrscheinlich ist, dass sie die gleiche Standardabweichung von 20.000 $ haben, da sie ihr tatsächliches Einkommen überall zwischen ‐10.000 und 30.000 $ variieren könnte. (In vielen Fällen, in denen die Annahme von normalerweise verteilten Fehlern missachtet wird – die Varianz oder die Standardabweichung sollten in einem angemessenen Verhältnis zum Mittel und nicht zu konstanten Fällen vorhergesagt werden). Das Fehlen von Homoscedasticity wird als Heterogenität bezeichnet. Um diese Annahme zu überprüfen, kann ein Grundstück der Reste gegenüber vorhergesagten Werten (oder den Werten der einzelnen Vorhersehbaren) für einen "Ferneffekt" geprüft werden (d. h. die zunehmende oder abnehmende vertikale Verbreitung als ein Schritt auf dem Grundstück). Ein Grundstück der absoluten oder kalkulierten Reste gegenüber den vorhergesagten Werten (oder jedem Berechtigten) kann auch für einen Trend oder eine Eindämmung untersucht werden. Formaltests können auch verwendet werden; siehe Hetersssensitivität. Das Vorhandensein von Heterogenität führt zu einer durchschnittlichen Schätzung der Varianz statt eines, die der wahren Varianzstruktur Rechnung trägt. Dies führt zu weniger präzisen (aber im Falle gewöhnlicher mindestens Quadrate, nicht einseitige) Parameterschätzungen und unparteiischen Standardfehlern, was irreführende Tests und Intervallschätzungen zur Folge hat. Der durchschnittliche Fehler für das Modell wird auch falsch sein. Verschiedene Abschätzungstechniken, einschließlich gewichteter mindestens Quadrate und die Verwendung von heteroscedasticity-konsistenten Standardfehlern können die Heterogenität in einer ganz allgemeinen Weise handhaben. Laesische lineare Regressionstechniken können auch verwendet werden, wenn die Varianz als Funktion des Mittels gilt. In einigen Fällen ist es auch möglich, das Problem durch die Anwendung einer Umwandlung auf die Reaktionsvariable (z.B. die Anpassung des Logarithm der Reaktionsvariable anhand eines linearen Regressionsmodells, das bedeutet, dass die Reaktionsvariable selbst eine Log-normale Verteilung anstelle einer normalen Verteilung hat). Unabhängigkeit der Fehler. Man geht davon aus, dass die Fehler der Reaktionsvariablen nicht miteinander verknüpft sind. (Rechtliche statistische Unabhängigkeit ist eine stärkere Voraussetzung als nur fehlende Korrelation und wird oft nicht benötigt, obwohl sie genutzt werden kann, wenn sie bekannt ist.) Manche Methoden wie allgemeinisierte, am wenigsten Quadrate sind in der Lage, korrelative Fehler zu bearbeiten, obwohl sie in der Regel deutlich mehr Daten erfordern, es sei denn, eine Art der Regularisierung wird verwendet, um das Modell auf unkorrekte Fehler zu reagieren. Laesische lineare Regression ist eine allgemeine Art der Behandlung dieses Problems. Mangel an perfekter Multicollinearität in den Berechtigten. Standard-minimierungsmethoden von mindestens Quadratmetern muss die Designmatrix X eine vollständige Spaltenklasse haben; ansonsten ist eine perfekte Multicollinearität in den vorhergesagten Variablen vorhanden, was eine lineare Beziehung zwischen zwei oder mehr vorhergesagten Variablen gibt. Dies kann durch zufällige Entbindung einer variablen Datenmenge verursacht werden, wobei eine lineare Umwandlung einer Variablen zusammen mit dem Original (z.B. die gleichen Temperaturmessungen in Belastungheit und Celsius) oder eine lineare Kombination mehrerer Variablen im Modell, wie z.B. deren Mittelwert, verwendet wird. Man kann auch passieren, wenn es zu wenig Daten gibt, die im Vergleich zur Anzahl der zu geschätzten Parameter (z.B. weniger Datenpunkte als Regressionskoeffizienten) zu erwarten sind. Knappe Verstöße gegen diese Annahme, bei denen die Vorhersehbaren hoch, aber nicht vollkommen korreliert sind, können die Genauigkeit der Parameterschätzungen verringern (siehe Varianzfaktor). Im Falle einer perfekt multicollinearität wird der Parametervektor β nichtidentifizierbar sein – es hat keine einzigartige Lösung. In einem solchen Fall können nur einige der Parameter ermittelt werden (d. h. ihre Werte können nur in einem linearen Teilraum des gesamten Parameterraums Rp geschätzt werden). Lesen Sie teilweise mindestens Quadratmeter Regression. Methoden für die Installation linearer Modelle mit Multicollinearität wurden entwickelt, von denen einige zusätzliche Annahmen wie "Wirksamkeit" erfordern – die einen großen Teil der Effekte sind genau Null. Hinweis darauf, dass die mehr rechnerisch teuren iterierten Algorithmen für die Parameterschätzung, wie die in allgemeinisierten linearen Modellen verwendeten, nicht unter diesem Problem leiden. Mehr als diese Annahmen beeinflussen mehrere andere statistische Eigenschaften der Daten stark die Leistung verschiedener Schätzungsmethoden: Die statistische Beziehung zwischen den Fehlerbedingungen und den Regressoren spielt eine wichtige Rolle bei der Bestimmung, ob ein Bewertungsverfahren wünschenswerte Stichprobeneigenschaften wie unvorhergesehen und konsequent ist. Die Vereinbarung oder die wahrscheinliche Verteilung der vorhergesagten Variablen x hat einen wichtigen Einfluss auf die Richtigkeit der Schätzungen von β.Sampling und Design von Experimenten sind hoch entwickelte Teilgebiete von Statistiken, die Leitlinien für die Erhebung von Daten in einer Weise vorsehen, um eine genaue Schätzung von β zu erzielen. Dolmetschen Ein montiertes lineares Regressionsmodell kann verwendet werden, um die Beziehung zwischen einem einzigen vorhergesagten variablen xj und der Reaktionsvariable zu ermitteln, wenn alle anderen vorhergesagten Variablen im Modell „befestigt“ sind. Konkret ist die Auslegung von βj die erwartete Änderung in y für eine einheitliche Änderung in xj, wenn die anderen Kovariaten festgehalten werden – d. h. der erwartete Wert des Teilderivats von y gegenüber xj. manchmal wird die einmalige Wirkung von xj auf y genannt. Jedoch kann der marginale Effekt von xj auf y anhand eines Korrelationskoeffizienten oder eines einfachen linearen Regressionsmodells bewertet werden, das nur xj bis y betrifft; dies ist der Gesamtderivat von y gegenüber xj. Vorsicht ist bei der Auslegung von Regressionsergebnissen zu beachten, da einige der Regressoren möglicherweise keine geringfügigen Veränderungen (wie Scheinvariablen oder Überwachungsfristen) zulassen können, während andere nicht festgehalten werden können (siehe Beispiel der Einführung: Es wäre unmöglich, "Ti" festzuhalten und gleichzeitig den Wert von t2) zu ändern. Es ist möglich, dass die einzigartige Wirkung nahezu Null sein kann, selbst wenn die Margin groß ist. Dies kann bedeuten, dass einige andere Kovariate alle Informationen in xj erfassen, so dass, sobald diese variable Wirkung im Modell liegt, kein Beitrag von xj zur Variationen von y gibt. Umgekehrt kann der einzigartige Effekt von xj groß sein, während seine marginale Wirkung nahezu null ist. Dies würde passieren, wenn die anderen Kovariaten einen großen Teil der Yen-Dimension erklären, aber sie erklären vor allem, dass die Unterschiede in einer Weise, die sich auf das, was von xj erfasst wird, ergänzen. In diesem Fall, einschließlich der anderen Variablen des Modells, verringert den Teil der Variabilität von y, der nicht mit xj zusammenhängt, und stärkt damit die offensichtliche Beziehung mit xj. Die Bedeutung des Ausdrucks "bestimmt" kann davon abhängen, wie sich die Werte der vorhergesagten Variablen ergeben. Wenn der Versucher die Werte der vorhergesagten Variablen nach einem Studiendesign direkt festlegt, können die Interessenvergleiche den Vergleiche zwischen Einheiten entsprechen, deren vorhergesagte Variablen vom Experimenter bestätigt wurden. Alternativ kann der Ausdruck "bestimmt" auf eine Auswahl verweisen, die im Rahmen der Datenanalyse stattfindet. In diesem Fall halten wir "ein variables Fest", indem wir unsere Aufmerksamkeit auf die Teilsätze der Daten beschränken, die einen gemeinsamen Wert für die angegebene Berechenbarkeit haben. Dies ist die einzige Auslegung von "behaltenen Fest", die in einer Beobachtungsstudie verwendet werden kann. Der Begriff eines "unique-Effekt" ist bei der Prüfung eines komplexen Systems interessant, bei dem mehrere miteinander verbundene Komponenten die Reaktionsvariable beeinflussen. Man kann in einigen Fällen als ursächliche Wirkung eines Eingriffs interpretiert werden, der mit dem Wert einer vorhergesagten Variablen zusammenhängt. Jedoch wurde argumentiert, dass in vielen Fällen mehrere Regressionsanalysen die Beziehungen zwischen den vorhergesagten Variablen und der Reaktionsvariable nicht klarstellen, wenn die Berechtigten miteinander in Verbindung stehen und nicht nach einem Studiendesign zugewiesen werden. Komplementaritätsanalyse kann hilfreich sein, um die gemeinsamen und einzigartigen Auswirkungen von korrelierten unabhängigen Variablen zu entbinden. Es wurden zahlreiche Verlängerungen der linearen Regression entwickelt, die eine gewisse oder alle Annahmen ermöglichen, die dem Grundmodell zugrunde liegen. Einfache und mehrfach lineare Regression Der sehr einfache Fall eines einzigen scalar vorhersehbaren variablen x und einer einzigen scalar-Reaktionsvariable ist bekannt als einfache lineare Regression. Die Verlängerung auf mehrere und/oder Vector-Wertd vorhersehbare Variablen (nicht mit einem Kapital X) ist bekannt als mehrfach lineare Regression, auch bekannt als multivariable lineare Regression (nicht mit multivariate linearer Regression). Multiple lineare Regression ist eine allgemeine Einführung einer einfachen linearen Regression im Falle von mehr als einer unabhängigen Variablen und einem besonderen Fall allgemeiner linearer Modelle, die auf eine abhängige Variablen beschränkt sind. Grundmodell für mehrfach lineare Regression ist Y i = β 0 + β 1 X i 1 + β 2 X i 2 + ... + β p X i p + ε i KINGstyle Y_{i}=\beta {_0}+\beta 1}X_{i1}+\beta 2}X_{i2}+\ldots +\beta p}X_{ip}+\epsilon {_i} für jede Beobachtung i = 1, ... n.In der oben genannten Formel betrachten wir die Beobachtungen einer abhängigen variablen und einer unabhängigen Variablen. also, Yi ist die Erhbeobachtung der abhängigen variablen, Xij ist die Beobachtung der jrd unabhängigen variablen, j = 1, 2, ,p. Die Werte βj stellen Parameter dar, die geschätzt werden müssen, und εi ist der unabhängige, identisch verteilte normale Fehler. In allgemeiner multivariativer linearer Regression gibt es eine Formel für jede von m° 1 abhängige Variablen, die denselben Satz von erläuternden Variablen teilen und daher gleichzeitig mit einander geschätzt werden: Y i j = β 0 j + β 1 j X i 1 + β 2 j X i 2 +... + β p j X i p +   i j KINGstyle Y_{ij} {_0j}+\beta 1j}X_{i1}+\beta 2j}X_{i2}+\ldots +\beta pj}X_{ip}+\epsilon {_ij} für alle als i = 1, ... , n und für alle abhängigen Variablen, die als j = 1, ... , m indexiert sind. Knapp alle realen Regressionsmodelle umfassen mehrere Vorhersehbarer, und die grundlegenden Beschreibungen der linearen Regression werden häufig im Hinblick auf das Multiple Regressionsmodell dargestellt. Hinweis: In diesen Fällen ist die Reaktionsvariable y immer noch ein scalar. Ein weiterer Begriff, multivariate lineare Regression, bezieht sich auf Fälle, in denen y ein Vektor ist, d. h. die gleiche allgemeine lineare Regression. Allgemeine lineare Modelle Das allgemeine lineare Modell betrachtet die Situation, wenn die Reaktionsvariable kein scalar (für jede Beobachtung) ist, sondern ein Vektor, yi.Conditional linearity of E ( y  i x i) = x i T B {\displaystyle E(\klon) {y} \mid \mid \mid {x} {_i} _i. Mathematiksf {T}}B wird immer noch angenommen, mit einer Matrix B, die den Vektor β des klassischen linearen Regressionsmodells ersetzt. Multivariate Analoge von normalen mindestens Quadraten (OLS) und allgemeinisierte, mindestens Quadrate (GLS) wurden entwickelt." Allgemeine lineare Modelle werden ebenfalls als "multivariative lineare Modelle" bezeichnet. Diese sind nicht dieselben wie multivariable lineare Modelle (auch „multiple lineare Modelle“). Heterssstische Modelle Verschiedene Modelle wurden geschaffen, die Heterogenität ermöglichen, d. h. die Fehler für verschiedene Reaktionsvariablen können unterschiedliche Unterschiede aufweisen. gewichtete mindestens Quadrate sind beispielsweise eine Methode zur Schätzung linearer Regressionsmodelle, wenn die Reaktionsvariablen unterschiedliche Fehler aufweisen können, möglicherweise mit korrelativen Fehlern. (Siehe auch lineare mindestens Quadrate, und allgemeineisierte mindestens Quadrate).) Heterssssenkische Standardfehler sind eine verbesserte Methode für die Verwendung mit unkorrekten, aber möglicherweise heterosen Fehlern. Allgemeine lineare Modelle Allgemeine lineare Modelle (GLMs) sind ein Rahmen für Modellierungsvariablen, die gebunden oder getrennt sind. Dies wird z.B. verwendet, wenn man positive Mengen (z.B. Preise oder Bevölkerungsgruppen) modelliert, die sich über ein großes Maß hinweg unterscheiden – die besser beschrieben werden, wie die Log-normale Verteilung oder Poisson-Vertrieb (obwohl GLMs nicht für Log-normale Daten verwendet werden, anstatt die Reaktionsvariable einfach mit der Logarithm-Funktion zu verändern), wenn man kategorisierende kategorische Daten, wie die Wahl eines bestimmten Kandidaten in einer Wahl (die besser beschrieben wird, wenn man eine Bernlili-/bin-Vertrieb für eine sinnvolle Auswahl oder eine bestimmte Menge, aber nicht sinnvolle Auswahl, sondern als „g“ bezeichnet. Allgemeine lineare Modelle ermöglichen eine willkürliche Verbindungsfunktion, g, die das Verhältnis zwischen den Reaktionsvariablen (s) und den Berechenten betrifft: E (J ) = g  - 1 ( X B ) KINGstyle E(Y)=g-1-1}(XB ) Die Verbindungsfunktion ist oft mit der Verteilung der Antwort verbunden, und insbesondere hat sie in der Regel die Wirkung, zwischen der (∞ ,   ) Memestyle (-\infty ,\infty )} des linearen Vorhersehbaren und der Bandbreite der Reaktionsvariable umzuwandeln. Manche gemeinsame Beispiele von GLMs sind: Poisson Regression für die Erfassung von Daten. Logistische Regression und Probit-Regression für binäre Daten. Multinomial logistische Regression und Multinomial probit Regression für kategorische Daten. bestellte Logit und bestellte Probit-Regression für ordinale Daten. Einheitliche Indexmodelle ermöglichen ein gewisses Maß an Nichtlinearität in der Beziehung zwischen x und y und erhalten gleichzeitig die zentrale Rolle des linearen Prognoseers .x wie im klassischen linearen Regressionsmodell. Unter bestimmten Voraussetzungen wird die bloße Anwendung von OLS auf Daten aus einem einzigen Index-Modell eine konsistente Schätzung von β bis zu einer Proportionalitätskontinuität ermöglichen. Hierarchische lineare Modelle Hierarchische lineare Modelle (oder Multi-Level-Regression) organisieren die Daten in eine Hierarchie der Regressionen, z.B. wenn A auf B regressiert wird und B auf C regressiert wird. Es wird oft verwendet, wenn die Variablen von Interesse eine natürliche hierarchische Struktur aufweisen, wie z.B. in Bildungsstatistiken, in denen Studierende in Klassenzimmern, Klassenzimmern in Schulen leben, und Schulen werden in einigen administrativen Gruppen wie einem Schulbezirk untergebracht. Die Reaktionsvariable könnte ein Maß an Schülerleistung sein, wie ein Testergebnis, und verschiedene Kovariate würden auf dem Klassenzimmer, in der Schule und im Schulbezirk gesammelt. Fehler-in-variablen Fehler-in-variablen Modelle (oder "Messungsfehlermodelle") erweitern das traditionelle lineare Regressionsmodell, um die vorhergesagten Variablen X mit Fehlern zu beobachten. Dieser Fehler führt dazu, dass Standardester von β einseitig werden. Insgesamt ist die Form von Verzerrungen eine Anhäufung, was bedeutet, dass die Auswirkungen auf Null sind. Andere In Dempster–Shafer Theorie oder eine lineare Überzeugungsfunktion können insbesondere ein lineares Regressionsmodell als teilweise gestreckte Matrix vertreten sein, die mit ähnlichen matrices kombiniert werden kann, die Beobachtungen und andere angenommene normale Verteilungen und staatliche Gleichungen repräsentieren. Die Kombination von Wirbelstürmen oder unmatrices bietet eine alternative Methode zur Schätzung linearer Regressionsmodelle. Methoden zur Schätzung In linearer Regression wurden zahlreiche Verfahren für die Schätzung und Gleichgültigkeit entwickelt.Diese Methoden unterscheiden sich in der rechnerischen Einfachheit von Algorithmen, der Vorhandensein einer geschlossenen Lösung, der Robustheit in Bezug auf die stark abgestimmten Vertriebenen und theoretische Annahmen, die zur Validierung wünschenswerter statistischer Eigenschaften wie Konsistenz und asymptotische Effizienz erforderlich sind. Manche der häufigeren Schätzungstechniken für lineare Regression werden unten zusammengefasst. Least-Grad-Schätzung und verwandte Techniken gehen davon aus, dass die unabhängige Variablen x i → = [ x 1 i , x 2 i , ... , x m i ] {\displayvec x_{i==\left[x_{1}^{i},x_{2}^{i},\ldots ,x_{mii}\right] und die Parameter des Modells sind β → [ β 0 , β 1 , ... , β m ] KINGstyle Memevec Memebeta =}[\beta {_0},\beta {_1},\ldots ,\beta {_m}\right] , dann wäre die Vorhersage des Modells y i ≈ β 0 +  j j = 1 m β j × x j i displaystyle y_{i}\ca \beta {_0}+\sum _j=1}^{m}\beta {_jtimetimes x_{j}^{i .If x i → KINGstyle Memevec x_{i} wird auf x i → [ 1 , x 1 i , x 2 i , ... , x m i ] faserstyle faservec x_{i==\left[1,x_{1}^{i},x_{2}^{i},\ldots ,x_{m}^{i]right] dann Yendisplaystyle y_{i} wäre ein Punktprodukt des Parameters und der unabhängigen Variablen, d. h. y i ≈  j j = 0 m β j × x j i = β →   x i → KINGstyle y_{i}\ca \sum _j=0}^{m}\beta {_jstimes x_{j}^{i} 7.8beta \}cdot Memevec x_{i} .In den am wenigsten geeigneten Parameter ist der optimale Parameter so definiert, dass die Summe des durchschnittlichen Verlusts minimiert wird: β ^ → einrg min → L ( D , β → ) = einrg min β →  i i = 1 n ( β  i x i → − y i) 2 {\vec Memebeta )unterset Memevec 7.8beta \}mbox{arg min}}}\,L\left(D, cuvec 574beta }right)= livvec 7.8beta {\mbox{arg min _sum _i=1}^{n1n(n{\(n{\n{\(Sec 574beta \}cdot 7.8vec x_{i}}}-y_{i)right)22 Jetzt können die unabhängigen und abhängigen Variablen in matrices X {\displaystyle X} bzw. Y WELLdisplaystyle Y}, die Verlustfunktion kann wie folgt neu geschrieben werden: L ( D , β → ) =  X X → Y  be 2 = ( X β → Y ) T ( X · · Y ) T ( X · → Y }) = Y T Y T β X → X → Y T → Y T → X  + X  X X  X X → X L X L X L X → X L L L L  X  X  X  X  X X L L  X  X  X  X  X  X  X  X  X  X  X  X  X  X Y\|^{2= &=\left(Xggiovec ggiobeta) -}Y\right))textsf T}}\left(XILLAvec ggiobeta) -}Y\right)& &=Y^{\textsf TYY-Ystextef T}} XILLAvec Memebeta {-vevec 7.8beta stextsf TXXstextef TYY+SSOvec 7.8beta stextef TXXstextef T}} Xggiovec HANAbeta \end{aligned Da der Verlust konvex ist, liegt die beste Lösung bei einem Entwicklungsstand Null. Kennzeichnend für die Verlustfunktion ist die Denominator-Konstellation:  L L (D , β → ) ) ) →  Y ( Y T Y T X β → T X T Y + β → T Y + β → T X T X T X T X T β → → ) ) ) = 2 X T Y + 2 X T Y + 2 X T β → 7.8style beginnt {aledffracial L\(D, SIvec 574) TYY-Ystextef T}} XILLAvec Memebeta {-vevec 7.8beta stextsf TXXstextef TYY+SSOvec 7.8beta stextef TXXstextef TXXHANAvec HANAbeta rechts)}{\partial 7.8vec ggiobeta &-=}2Xstextef TYY+2Xstextef T}} Xggiovec HANAbeta \end{aligned Kennzeichnend für Null ist der optimale Parameter: − 2 X T Y + 2 X T X T X β → 0 WERK X T Y = X T X T X β → ß β ^ → ( X T X X X ) − 1 X T Y 574displaystyle beginnen {align}-2X^{\textsf TYY+2Xstextef T}} XILLAvec HANAbeta &=}0 XRightarrow Xstextsf TYY &=X^{\textef T}} XILLAvec Memebeta )Rightarrow HANAvec WELLhat HANAbeta &=} (X^{\textsf TXX\right)-1-1}X)textsf T}} Y\end{align Hinweis: Um zu beweisen, dass die β ^ WELLdisplaydisplaystyle Memehat ggiobeta . tatsächlich das lokale Minimum ist, muss man die Hessische Matrix einmal mehr unterscheiden und zeigen, dass sie positiv ist. Dies wird von den Gausss – Markov theorem bereitgestellt. Lineare Methoden von mindestens Quadratmetern umfassen hauptsächlich: Ordinary mindestens Quadrate gewichtete mindestens Quadrate Generalisierte mindestens Quadrate der höchstwahrscheinlichen Schätzung und damit verbundene Techniken höchstwahrscheinliche Schätzung können durchgeführt werden, wenn die Verteilung der Fehlerbedingungen bekannt ist, einer bestimmten parametrischen Familienzugehörigkeit der Wahrscheinlichkeitsverteilungen angehören.F. ist ein normaler Vertrieb mit Null und Varianz ., die entsprechende Schätzung ist identisch mit der OLS Schätzung. GLS-Schätzungen sind höchstwahrscheinlich Schätzungen, wenn ε eine multivariate normale Verteilung mit einer bekannten Kovarianzmatrix folgt. Um die Variabilität der Schätzung zu verringern, führt die Landrückführung und andere Formen der benachteiligten Schätzung, wie z.B. die Regression von Lasso, absichtlich zu einer Abschätzung der β ein. Die daraus resultierenden Schätzungen haben in der Regel einen geringeren durchschnittlichen Fehler als die OLS-Schätzungen, insbesondere wenn die Multicollinearität vorhanden ist oder die Überrüstung ein Problem darstellt. Sie werden in der Regel verwendet, wenn das Ziel darin besteht, den Wert der Reaktionsvariable y für Werte der vorhergesagten x vorherzusagen, die noch nicht beobachtet wurden. Diese Methoden werden nicht so häufig verwendet, wenn das Ziel in der Gleichgültigkeit liegt, da es für die Verzerrung schwierig ist. Least absolute Abweichung (LAD) ist eine robuste Schätzungstechnik, da sie weniger empfindlich auf das Vorhandensein von Auslierern als OLS ist (aber weniger effizient als OLS, wenn keine Auslierer vorhanden sind). Es entspricht einer maximalen Wahrscheinlichkeitsschätzung nach einem Laplace-Vertriebsmodell für ..Adaptive Schätzung. Wenn wir davon ausgehen, dass die Fehlerbedingungen unabhängig von den Regressoren sind,  i i WELLdisplaystyle \varepsilon {_ipperp \ Mathematik {x} {_i} {_i}, dann ist der optimale Ester der 2-Schritt-MLE, wo der erste Schritt zur nicht-metrischen Schätzung der Verteilung des Fehlerbegriffs verwendet wird. Andere Abschätzungstechniken gelten für die lineare Regression der Bayesischen Statistiken für lineare Regression. (Siehe auch multivariate lineare Regression). Insbesondere werden die Regressionskoeffizienten β als Zufallsvariablen mit einer bestimmten Vorverteilung angenommen. Die vorherige Verteilung kann die Lösungen für die Regressionskoeffizienten auf eine Weise einschalten, die ähnlich ist (aber mehr als) die Regression oder die Regression von Laso. Darüber hinaus erstellt der Bayesian-Erschätzungsprozess keine einzige Schätzung für die besten Werte der Regressionskoeffizienten, aber eine ganze Posterior-Vertrieb, die Unsicherheit im Zusammenhang mit der Menge vollständig beschreibt. Dies kann verwendet werden, um die besten Koeffizienten mit dem Mittelwert, dem Modus, den Medien, jedem Quantile (siehe Quantile Regression) oder einer anderen Funktion des Posterior-Vertriebs abzuschätzen. Quantile Regression konzentriert sich auf die bedingten Mengen von y gegebenen X anstelle des bedingten Mittelwerts von y gegeben X. lineare quantitative Regressionsmodelle eine bestimmte abhängige Quantile, zum Beispiel die bedingten Medien, als lineare Funktion βTx der Berechtigten. Mischmodelle werden häufig verwendet, um lineare Regressionsverhältnisse mit abhängigen Daten zu analysieren, wenn die Abhängigkeiten eine bekannte Struktur aufweisen. Gemeinsame Anwendungen gemischter Modelle umfassen die Analyse von Daten, die wiederholte Messungen, wie z.B. die Längsschnittdaten oder die Daten, die bei der Gruppenprobenerhebung gesammelt wurden. Sie sind in der Regel als parametrische Modelle geeignet, wobei die maximale Wahrscheinlichkeit oder die Biesische Schätzung verwendet werden. In Fällen, in denen die Fehler als normale zufällige Variablen modelliert werden, besteht eine enge Verbindung zwischen gemischten Modellen und allgemeinisierten mindestens Quadraten. Schätzung fester Effekte ist ein alternativer Ansatz, um diese Art von Daten zu analysieren. Hauptkomponenten-Regression (PCR) wird verwendet, wenn die Zahl der vorhergesagten Variablen groß ist oder wenn starke Korrelationen zwischen den vorhergesagten Variablen bestehen. In diesem zweistufigen Verfahren werden zunächst die vorhergesagten Variablen mit der Hauptkomponentenanalyse verringert, dann werden die reduzierten Variablen in einem OLS-Regressions-Anpassungs-Anpassungs passen. Obwohl es oft gut in der Praxis funktioniert, gibt es keinen allgemeinen theoretischen Grund, dass die informative lineare Funktion der vorhergesagten Variablen zu den wichtigsten Komponenten der multivarialen Verteilung der vorhergesagten Variablen liegen sollte. Die partielle Rückführung von mindestens Quadratmetern ist die Verlängerung der PCR-Methode, die nicht unter dem genannten Mangel leidet. Least-winkel-Regression ist ein Bewertungsverfahren für lineare Regressionsmodelle, die entwickelt wurden, um hochdimensionale Kovariatvektoren zu bewältigen, die möglicherweise mit mehr Kovariaten als Beobachtungen kombiniert werden. Theil–Sen estimator ist eine einfache robuste Schätzungstechnik, die die Steigung der Anschlusslinie wählt, um die medianen Abfahrten durch Paar von Stichpunkten zu sein. Es hat ähnliche statistische Effizienzeigenschaften wie einfache lineare Regression, ist aber viel weniger empfindlich gegenüber Auslierern. Andere robuste Schätzungstechniken, einschließlich der α-trimmierten Mittelansatz, und L,- M,- S,- und R-Schätzer wurden eingeführt. Anwendungen lineare Regression wird in biologischen, Verhaltens- und Sozialwissenschaften weit verbreitet, um mögliche Beziehungen zwischen Variablen zu beschreiben. Sie ist eines der wichtigsten Instrumente, die in diesen Disziplinen verwendet werden. Trendlinie Eine Trendlinie stellt einen Trend dar, die langfristige Bewegung in Zeitreihendaten nach anderen Komponenten wurde berücksichtigt. Er weist darauf hin, dass ein bestimmtes Datensatz (Test-BIP, Ölpreise oder Lagerpreise) im Laufe der Zeit gestiegen oder zurückgegangen ist. Eine Trendlinie könnte einfach durch eine Reihe von Datenpunkten ausgelesen werden, doch wird ihr Standpunkt und ihre Neigung durch statistische Techniken wie lineare Regression besser berechnet. Trendlinien sind in der Regel gerade Linien, obwohl einige Abweichungen höhere Polynomialen verwenden, je nachdem, was in der Linie gewünscht wird. Trendlinien werden manchmal in der Unternehmensanalyse verwendet, um Änderungen der Daten im Laufe der Zeit zu zeigen. Dies hat den Vorteil, einfach zu sein. Trendlinien werden oft verwendet, um zu argumentieren, dass eine bestimmte Maßnahme oder Veranstaltung (wie Ausbildung oder Werbekampagne) zu einem Zeitpunkt zu beobachteten Veränderungen geführt hat. Dies ist eine einfache Technik und erfordert keine Kontrollgruppe, experimentelles Design oder eine anspruchsvolle Analysetechnik. Mangelnde wissenschaftliche Gültigkeit in Fällen, in denen andere mögliche Änderungen die Daten beeinflussen können. Epidemiologie Frühzeitige Nachweise über Tabakrauch in Mortalität und Morbidität stammen aus Beobachtungsstudien mit Regressionsanalysen. Um die spärlichen Korrelationen bei der Analyse von Beobachtungsdaten zu verringern, umfassen Forscher in der Regel mehrere Variablen in ihren Regressionsmodellen neben der variablen Primärinteresses. In einem Regressionsmodell, in dem das Zigarettenrauchen die unabhängige Variablen von primärem Interesse ist und die abhängige variable Lebensdauer in Jahren gemessen wird, könnten Forscher Bildung und Einkommen als zusätzliche unabhängige Variablen umfassen, um sicherzustellen, dass die beobachteten Auswirkungen des Rauchens auf die Lebensdauer nicht auf die anderen sozioökonomischen Faktoren zurückzuführen sind. Jedoch ist es nie möglich, alle möglichen Konfoundierungsvariablen in eine empirische Analyse einzubeziehen. Beispielsweise könnte ein hypothetisches Gen die Sterblichkeit erhöhen und auch Menschen dazu bringen, mehr zu rauchen. Aus diesem Grund sind randomisierte kontrollierte Prüfungen oft in der Lage, zwingendere Nachweise von Kausalzusammenhangen zu generieren, als durch Regressionsanalysen von Beobachtungsdaten gewonnen werden kann. Wenn kontrollierte Experimente nicht möglich sind, können Varianten der Regressionsanalyse wie maßgebliche Variablen Regression verwendet werden, um Kausalzusammenziehungen aus Beobachtungsdaten abzuschätzen. Finanzen Das Kapitalpreismodell verwendet lineare Regression sowie das Konzept der Beta zur Analyse und Quantifizierung des systematischen Investitionsrisikos. Dies geschieht direkt aus dem Beta-Koeffizienten des linearen Regressionsmodells, das die Rendite der Investition auf die Rückkehr auf alle risikoreichen Vermögenswerte betrifft. Wirtschaftliche lineare Regression ist das vorherrschende empirische Instrument in den Wirtschaften. Zum Beispiel wird es verwendet, um Konsumausgaben, feste Investitionsausgaben, Inventarinvestitionen, den Kauf von Exporten eines Landes, den Export von Importen, die Nachfrage nach liquiden Vermögenswerten, Arbeitsnachfrage und Arbeitsangebot vorherzusagen. Umweltwissenschaftliche lineare Regression findet Anwendung in einer Vielzahl von Anwendungen der Umweltwissenschaft. In Kanada verwendet das Umweltverträglichkeitsprüfungsprogramm statistische Analysen zu Fischen und benthischen Umfragen, um die Auswirkungen von Zellstahl oder Metallminen auf das aquatische Ökosystem zu messen. Lineare Regression von Maschinen spielt eine wichtige Rolle im Bereich der künstlichen Intelligenz, bekannt als Maschinenbau. Der lineare Regressions-Algorithmus ist aufgrund seiner relativen Einfachheit und der bekannten Eigenschaften eines der fundamental überwachten maschinenlesbaren Algorithmen. Geschichte Least Quadrats lineare Regression als Mittel, um eine gute lineare Anpassung an eine Reihe von Punkten zu finden, wurde von Legendre (1805) und Gausss (1809) für die Vorhersage der Planetenbewegung durchgeführt. Quetelet war dafür verantwortlich, das Verfahren bekannt zu machen und in den Sozialwissenschaften umfassend zu nutzen. Siehe auch Citations Sources Weitere Lesung Pedhazur, Elazar J (1982). Multiple Regression in Verhaltensforschung: Explanation und Vorhersage (2. ed). New York: Holt, Rinehart und Winston. ISBN:0-03-041760-3.Mathieu Rouaud, 2013: Probability, Statistik und Estimation Kapitel 2: lineare Regression, lineare Regression mit Fehlern und Nichtlineare Regression. National Physical Laboratory (1961). " 1: 1: Lineare Equations- und Matrices: Direktmethoden". moderne Rechenmethoden. Hinweise auf angewandte Wissenschaft.16 (2. ed). Ihr Majesty's 1988. Außenbeziehungen Least-Squares Regression, PhET Interactive Simulationen, University of North at Rip Linear Fit Oceania, wie andere Regionen, sind in ihren Gesetzen über Homosexualität sehr unterschiedlich. Dies reicht von bedeutenden Rechten, die der LGBT-Gemeinschaft in Neuseeland, Australien, Kuba, Jamaika, den nördlichen Marianen, Wallis und Futuna, Neuseeland, Französisch-Polynesien und den Pitcairn-Inseln gewährt werden, bis sie Strafen für homosexuelle Aktivitäten in 6 Ländern und einem Gebiet verhängen. Obwohl die Akzeptanz im gesamten Pazifik wächst, bleiben Gewalt und soziale Stigma für LGBTI-Gemeinschaften weiterhin problematisch. Dies führt auch zu Problemen mit der Gesundheitsversorgung, einschließlich des Zugangs zu HIV-Behandlung in Ländern wie Papua-Neuguinea und den Salomonen, in denen Homosexualität krimineller Art ist. Das Vereinigte Königreich hat im gesamten britischen Reich konservative soziale Einstellungen und Anti-LGBT-Gesetze eingeführt, einschließlich seiner Kolonien im gesamten Pazifik. In Anti-LGBT-Gesetzen, die in den meisten Ländern in der späteren Gemeinschaft der Vereinten Nationen gefunden wurden, besteht dieses Erbe. Kritiker von LGBT-Rechten in Ozeanien haben ihren Standpunkt durch die Argumentation gerechtfertigt, dass sie durch Tradition unterstützt wird und dass Homosexualität ein „westlicher Vize“ ist, obwohl Anti-LGBT-Gesetze selbst ein koloniales britisches Erbe sind. Mehrere pazifische Länder verfügen über alte Traditionen, die eine einzigartige lokale Perspektive der sexuellen Vielfalt und des Geschlechts widerspiegeln, wie die Fa'afine in Samoa und die Fakaleiti in Tonga. Rechtsvorschriften für Land oder Gebiet Australasia Melanesia Micronesia Polynesia Siehe auch die Anerkennung von gleichgeschlechtlichen Gewerkschaften in Ozeanien australische Eherecht Postal Survey LGBT-Rechten durch Länder oder Gebiet LGBT-Rechte in Europa LGBT-Rechte in Amerika LGBT-Recht in Asien LGBT-Rechte in Afrika (14 Referenzen)