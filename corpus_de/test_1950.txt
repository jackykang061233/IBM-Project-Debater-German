Eine Superintelligenz ist ein hypothetischer Agent, der Intelligenz besitzt, die weit über die des hellsten und begabtesten menschlichen Verstandes hinausgeht. "Superintelligence kann sich auch auf ein Eigentum von Problemlösungssystemen (z.B. superintelligente Sprachübersetzer oder Ingenieurassistenten) beziehen, ob diese hochrangigen intellektuellen Kompetenzen in weltweit agierenden Agenten verkörpert sind. Eine Superintelligenz kann oder darf nicht durch eine Intelligenzexplosion erzeugt werden und mit einer technologischen Einzigartigkeit verbunden sein. Die Universität von Oxford-Philosopher Nick Bostrom definiert Superintelligenz als "jeder Intellekt, der die kognitive Leistung von Menschen in praktisch allen Bereichen von Interesse stark übertrifft". Das Programm Fritz fällt unter Superintelligenz – auch wenn es viel besser ist als der Mensch am Schach –, weil Fritz Menschen in anderen Aufgaben nicht übertreffen kann. Nach Hutter und Legg behandelt Bostrom Superintelligenz als allgemeine Dominanz bei zielorientiertem Verhalten und lässt offen, ob eine künstliche oder menschliche Superintelligenz Fähigkeiten wie Intentionalität (vgl. das chinesische Raumargument) oder Erstpersonenbewußtsein (vgl. das harte Problem des Bewusstseins) besitzen würde. Technologische Forscher sind nicht einverstanden, wie wahrscheinlich die heutige menschliche Intelligenz übertroffen werden soll. Einige argumentieren, dass Fortschritte in der künstlichen Intelligenz (KI) wahrscheinlich zu allgemeinen Argumentationssystemen führen, die keine menschlichen kognitiven Einschränkungen haben. Andere glauben, dass die Menschen ihre Biologie entwickeln oder direkt verändern werden, um radikal größere Intelligenz zu erreichen. Eine Reihe von Futures-Studien-Szenarien kombinieren Elemente aus beiden dieser Möglichkeiten, was darauf hindeutet, dass Menschen wahrscheinlich mit Computern zu verbinden oder ihre Gedanken auf Computer hochladen, so dass eine erhebliche Intelligenz Verstärkung ermöglicht. Einige Forscher glauben, dass Superintelligenz bald nach der Entwicklung der künstlichen allgemeinen Intelligenz folgen wird. Die ersten in der Regel intelligenten Maschinen sind wahrscheinlich sofort einen enormen Vorteil in zumindest einigen Formen von geistiger Fähigkeit, einschließlich der Fähigkeit der perfekten Rückruf, eine weit überlegene Wissensbasis, und die Fähigkeit zu multiplizieren in Weisen nicht möglich, biologische Wesen. Dies kann ihnen die Möglichkeit geben, – sei es als Einzelwesen oder als neue Spezies – viel mächtiger zu werden als Menschen und sie zu verdrängen. Eine Reihe von Wissenschaftlern und Prognosen argumentieren für die Priorisierung der frühen Forschung über die möglichen Vorteile und Risiken der kognitiven Verbesserung von Mensch und Maschine aufgrund der potenziellen sozialen Auswirkungen solcher Technologien. Machbarkeit der künstlichen Superintelligenz Philosoph David Chalmers argumentiert, dass künstliche allgemeine Intelligenz ein sehr wahrscheinlicher Weg zu übermenschlicher Intelligenz ist. Chalmers bricht diesen Anspruch in ein Argument, dass KI die Gleichwertigkeit der menschlichen Intelligenz erreichen kann, dass er erweitert werden kann, um menschliche Intelligenz zu übertreffen, und dass es weiter verstärkt werden kann, um Menschen über willkürliche Aufgaben vollständig zu beherrschen. Chalmers argumentiert, dass das menschliche Gehirn ein mechanisches System ist und daher durch synthetische Materialien emulierbar sein sollte. Er stellt auch fest, dass menschliche Intelligenz in der Lage war, sich biologisch zu entwickeln, so dass es wahrscheinlicher ist, dass menschliche Ingenieure in der Lage sein werden, diese Erfindung wieder zu verwirklichen. Insbesondere evolutionäre Algorithmen sollten in der Lage sein, human-level AI zu produzieren. Chalmers argumentiert, dass im Allgemeinen neue KI-Technologien verbessert werden können und dass dies besonders wahrscheinlich ist, wenn die Erfindung bei der Entwicklung neuer Technologien helfen kann. Wenn die Forschung an starke KI ausreichend intelligente Software produziert, wäre es in der Lage, sich neu zu programmieren und zu verbessern – ein Feature namens "rekursive Selbstverbesserung". Es wäre dann noch besser, sich selbst zu verbessern und könnte dies in einem rasant wachsenden Zyklus fortsetzen, was zu einer Superintelligenz führt. Dieses Szenario ist als Intelligenzexplosion bekannt. Eine solche Intelligenz hätte nicht die Grenzen des menschlichen Verstandes und kann fast alles erfinden oder entdecken können. Computerkomponenten übertreffen bereits die menschliche Leistung in der Geschwindigkeit stark. Bostrom schreibt: "Biologische Neuronen arbeiten mit einer Spitzengeschwindigkeit von etwa 200 Hz, volle sieben Größenordnungen langsamer als ein moderner Mikroprozessor (~2 GHz). " Darüber hinaus übertragen Neuronen Spike-Signale über Axone mit maximal 120 m/s, "wodurch vorhandene elektronische Verarbeitungskerne optisch mit der Lichtgeschwindigkeit kommunizieren können". So kann das einfachste Beispiel einer Superintelligenz ein emulierter menschlicher Geist auf viel schneller Hardware als das Gehirn laufen. Ein menschlicher Vernunfter, der Millionen Mal schneller denken könnte als der aktuelle Mensch, würde in den meisten Argumentationsaufgaben einen beherrschenden Vorteil haben, insbesondere diejenigen, die eilige oder lange Handlungsstränge erfordern. Ein weiterer Vorteil von Computern ist die Modularität, d.h. ihre Größe oder Rechenkapazität kann erhöht werden. Ein nichtmenschliches (oder modifiziertes menschliches) Gehirn könnte viel größer werden als ein heutiges menschliches Gehirn, wie viele Supercomputer. Bostrom erhöht auch die Möglichkeit der kollektiven Superintelligenz: eine große Anzahl von getrennten Argumentationssystemen, wenn sie gut genug kommuniziert und koordiniert werden, könnte in Summe mit weitaus größeren Fähigkeiten handeln als alle Sub-Agenten. Es kann auch Wege zur qualitativen Verbesserung der menschlichen Vernunft und Entscheidungsfindung geben. Menschen scheinen sich von Schimpansen in der Art zu unterscheiden, wie wir denken, mehr als wir in der Gehirngröße oder Geschwindigkeit unterscheiden. Menschen übertreffen nichtmenschliche Tiere in großem Teil aufgrund neuer oder verbesserter Begründungskapazitäten, wie Langzeitplanung und Sprachgebrauch.(Siehe Evolution der menschlichen Intelligenz und Primatenkognition.) Wenn es andere mögliche Verbesserungen zu Vernunft, die einen ähnlich großen Einfluss haben würde, macht dies es wunderlich, dass ein Agent gebaut werden kann, dass die Menschen in der gleichen Weise die Menschen übertreffen Schimpansen. Alle oben genannten Vorteile halten für künstliche Superintelligenz, aber es ist nicht klar, wie viele halten für biologische Superintelligenz. Physiologische Zwänge begrenzen die Geschwindigkeit und Größe des biologischen Gehirns in vielerlei Hinsicht, die für die maschinelle Intelligenz nicht anwendbar sind. Als solche haben Autoren auf Superintelligenz viel mehr Aufmerksamkeit auf superintelligente KI-Szenarien gewidmet. Die Machbarkeit der biologischen Superintelligenz Carl Sagan schlug vor, dass das Aufkommen von Caesarean-Abschnitten und In-vitro-Düngung den Menschen erlauben kann, größere Köpfe zu entwickeln, was zu Verbesserungen durch natürliche Selektion in der erfindbaren Komponente der menschlichen Intelligenz führt. Gerald Crabtree hat dagegen argumentiert, dass ein geringerer Auswahldruck zu einer langsamen, jahrhundertelangen Verringerung der menschlichen Intelligenz führt und dass dieser Prozess stattdessen in die Zukunft weitergehen wird. Es gibt keinen wissenschaftlichen Konsens über die Möglichkeit, und in beiden Fällen wäre der biologische Wandel langsam, insbesondere im Verhältnis zu den kulturellen Veränderungen. Selektive Zucht, Nootropics, NSI-189, MAOIs, epigenetische Modulation und Gentechnik könnten die menschliche Intelligenz schneller verbessern. Bostrom schreibt, dass, wenn wir kommen, um die genetische Komponente der Intelligenz zu verstehen, die genetische Diagnose der Vorimplantation könnte verwendet werden, um für Embryonen mit bis zu 4 Punkten IQ-Verstärkung (wenn ein Embryo aus zwei ausgewählt wird), oder mit größeren Gewinnen (z.B. bis zu 24,3 IQ Punkte gewonnen, wenn ein Embryo aus 1000 ausgewählt wird). Wenn dieser Prozess über viele Generationen iteriert wird, könnten die Gewinne eine Größenordnung größer sein. Bostrom schlägt vor, dass die Ableitung von neuen Gameten aus embryonalen Stammzellen verwendet werden könnte, um den Auswahlprozess sehr schnell zu iterieren. Eine gut organisierte Gesellschaft hochintelligenter Menschen dieser Art könnte möglicherweise kollektive Superintelligenz erreichen. Alternativ könnte kollektive Intelligenz durch eine bessere Organisation von Menschen auf gegenwärtigen Ebenen der individuellen Intelligenz konstruierbar sein. Eine Reihe von Autoren haben vorgeschlagen, dass die menschliche Zivilisation oder ein Teil davon (z.B. das Internet oder die Wirtschaft) wie ein globales Gehirn mit Kapazitäten arbeitet, die weit über ihre Komponenten Agenten hinausgehen. Wenn diese systembasierte Superintelligenz jedoch stark auf künstliche Komponenten beruht, kann sie als KI statt als biologischer Superorganismus qualifizieren. Ein Prädiktionsmarkt wird manchmal als ein Beispiel für das kollektive Intelligenzsystem betrachtet, das nur aus Menschen besteht (die Algorithmen werden nicht verwendet, um Entscheidungen zu informieren). Eine abschließende Methode der Intelligenzverstärkung wäre es, die einzelnen Menschen direkt zu verbessern, im Gegensatz zu ihrer sozialen oder reproduktiven Dynamik. Dies konnte mit nootropics, somatischen Gentherapie oder Gehirn-Computer-Schnittstellen erreicht werden. Bostrom drückt jedoch Skepsis über die Skalierbarkeit der ersten beiden Ansätze aus und argumentiert, dass die Gestaltung einer superintelligenten Cyborg-Schnittstelle ein KI-komplettes Problem ist. Evolution mit möglicher Integration von NI, IT und AIIm Jahr 2005 schlug Alexei Eryomin in der Monographie "Noogenesis and Theory of Intellect" ein neues Konzept der Noogenese zum Verständnis der Entwicklung von Intelligenzsystemen vor. Die Entwicklung der intellektuellen Fähigkeiten kann unter gleichzeitiger Beteiligung natürlicher (biologischer) Intelligenzen (NI), moderner Fortschritte in der Informationstechnologie (IT) und zukünftiger wissenschaftlicher Leistungen auf dem Gebiet der künstlichen Intelligenz (KI) auftreten. Entwicklung der Interaktionsgeschwindigkeit zwischen Komponenten von Intelligenzsystemen Die erste Person, die die Geschwindigkeit (im Bereich von 24,6 – 38,4 Meter pro Sekunde) messen soll, bei der das Signal 1849 mit einer Nervenfaser getragen wird, war Helmholtz. Die gemessenen Geschwindigkeiten der Nervenleitgeschwindigkeit betragen bis heute 0,5 – 120 m/s. Die Geschwindigkeit des Schalls und der Lichtgeschwindigkeit wurde früher im XVII Jahrhundert bestimmt. Im 21. Jahrhundert wurde klar, dass sie hauptsächlich die Geschwindigkeiten von physikalischen Signal-Informationsträgern bestimmen, zwischen intelligenten Systemen und ihren Komponenten: Klang (Gewinn und Audio) ~300 m/s, Quanten-Elektron ~ 3 ⋅ 10 8 \{displaystyle 3\cdot 10^{8} m/s (die Geschwindigkeit von radio-elektromagnetischen Wellen, elektrischem Strom, Licht, optische, Telekommunikation). Evolution von Komponenten von Geheimdiensten 1906 Santiago Ramón y Cajal brachte die zentrale Bedeutung des Neurons auf die Aufmerksamkeit der Wissenschaftler und etablierte die Neuronlehre, die besagt, dass das Nervensystem aus diskreten Einzelzellen besteht. Nach modernen Daten gibt es etwa 86 Milliarden Neuronen im Gehirn eines erwachsenen Menschen. Im Evolutionsprozess betrug die menschliche Bevölkerung im Jahr 2000 etwa 70 Millionen BC, etwa 300 Millionen zu Beginn des ersten Jahrhunderts n. Chr., etwa eine Milliarde im Jahr 1930 n. Chr., 6 Milliarden im Jahr 2000 und 7,7 Milliarden jetzt Weltbevölkerung. Nach den mathematischen Modellen von Sergey Kapitsa kann die menschliche Bevölkerung 12,5 - 14 Milliarden vor Ende 2200 erreichen. Die Entwicklung der Verbindungen zwischen den Komponenten der Geheimdienstsysteme Synapse – von der griechischen Synapsis (συνάψις), d.h. Konjunktion, wiederum von συνάπτεὶν (συν (zusammen) und ππτειν ("zu befestigen)" – wurde 1897 von Charles Sherrington eingeführt. Die Relevanz von Messungen in dieser Richtung wird sowohl durch moderne umfassende Forschungen der Zusammenarbeit, als auch durch Verbindungen von Informationen, genetischen und kulturellen, durch Strukturen auf neuronaler Ebene des Gehirns und die Bedeutung der Zusammenarbeit in der Entwicklung der Zivilisation bestätigt. Dabei analysierte A. L. Eryomin die bekannten Daten über die Entwicklung der Anzahl der Verbindungen zur Zusammenarbeit in intelligenten Systemen. Verbindungen, Kontakte zwischen biologischen Objekten, können als mit einer Vielzelligkeit von ~ 3-3,5 Milliarden Jahren erschienen. Das System der hohen Geschwindigkeitsverbindungen von spezialisierten Zellen, die Informationen mit elektrischen Signalen übertragen, das Nervensystem, in der gesamten Geschichte des Lebens erschien nur in einem großen evolutionären Zweig: in multizellulären Tieren (Metazoa) und erschien in der Ediakaranischen Periode (vor etwa 635-542 Millionen Jahren). Während der Evolution (Phylogenie) erhöhte sich die Anzahl der Verbindungen zwischen Neuronen von einem auf ~ 7000 synoptische Verbindungen jedes Neurons mit anderen Neuronen im menschlichen Gehirn. Es wurde geschätzt, dass das Gehirn eines dreijährigen Kindes etwa 10 15 \{displaystyle 10^{15} von Synapsen (1 Quadrillion) hat. In der individuellen Entwicklung (Ontogenese) verringert sich die Anzahl der Synapsen mit dem Alter auf ~ 10 14 \{displaystyle 10^{14} .Nach anderen Daten verringert sich die geschätzte Anzahl von neokortikalen Synapsen im männlichen und weiblichen Gehirn während des menschlichen Lebens von ~ 1.4 ∙ 10 14 \{displaystyle 1.4\cdot 10^{14} bis ~ 1.2 ∙ 10 14 \{displaystyle 1.2\cdot 10^_16} .Die Anzahl der menschlichen Kontakte ist schwierig zu berechnen, aber die "Dunbar's Zahl" ist fester Ko-Verbindungen mit den Menschen Strukturen, die für soziale Interaktionen verantwortlich sind, wurden im Gehirn identifiziert. Mit dem Auftreten von Homo sapiens vor ~50-300 Tausend Jahren, die Relevanz der Zusammenarbeit, seine Evolution in der menschlichen Bevölkerung, stieg quantitativ. Wenn es vor 2000 Jahren 0,1 Milliarden Menschen auf der Erde gab, vor 100 Jahren - 1 Milliarde, in der Mitte des zwanzigsten Jahrhunderts – 3 Milliarde und inzwischen die Menschheit - 7,7 Milliarden. So kann die Gesamtzahl der "stabilen Verbindungen" zwischen Menschen, sozialen Beziehungen innerhalb der Bevölkerung durch eine Zahl ~ 10 12 \{displaystyle 10^{12} geschätzt werden. Die meisten befragten KI-Forscher erwarten, dass Maschinen schließlich in der Lage sind, Menschen in der Intelligenz zu rivalisieren, obwohl es wenig Konsens darüber gibt, wann dies wahrscheinlich geschieht. Auf der AI@50-Konferenz von 2006 berichteten 18 % der Teilnehmer, dass die Maschinen bis 2056 "das Lernen und jeden anderen Aspekt der menschlichen Intelligenz simulieren können"; 41 % der Teilnehmer erwarteten, dass dies irgendwann nach 2056 geschehen würde; und 41 % erwarteten, dass Maschinen diesen Meilenstein nie erreichen. In einer Umfrage der 100 am meisten zitierten Autoren in KI (Stand Mai 2013, nach Microsoft akademischer Suche), das mediane Jahr, von dem die Befragten erwartet Maschinen "die die meisten menschlichen Berufe mindestens ausführen können, sowie ein typischer Mensch" (angenommen, keine globale Katastrophe auftritt) mit 10% Vertrauen ist 2024 (m. 2034, St. dev.33 Jahre), mit 50% Vertrauen ist 2050 (mean 2072, st. dev. Diese Schätzungen schließen die 1,2 % der Befragten aus, die kein Jahr sagten, würden je 10 % Vertrauen erreichen, die 4,1 %, die nie für 50 % Vertrauen sagten, und die 16,5 %, die nie für 90 % Vertrauen sagten. Den Befragten wird eine mediane 50 % Wahrscheinlichkeit der Möglichkeit zugewiesen, dass die Maschinen-Superintelligenz innerhalb von 30 Jahren der Erfindung von etwa menschlicher Maschinenintelligenz erfunden wird. Design-Betrachtungen Bostrom äußerte Besorgnis darüber, welche Werte eine Superintelligenz haben sollte. Er hat mehrere Vorschläge verglichen: Der kohärente, extrapolierte Volitionsvorschlag (CEV) ist, dass er die Werte haben sollte, auf denen der Mensch sich konvergieren würde. Der moralische Rechtmäßigkeitsvorschlag (MR) ist, dass er die moralische Rechtmäßigkeit schätzen sollte. Der moralische Zulässigkeitsvorschlag (MP) ist, dass er Wert darauf legen sollte, innerhalb der Grenzen der moralischen Zulässigkeit zu bleiben (und ansonsten CEV-Werte haben). Bostrom erklärt diese Begriffe: Anstatt die kohärente extrapolierte volition der Menschheit zu implementieren, könnte man versuchen, eine KI mit dem Ziel zu bauen, was moralisch richtig ist und sich auf die überlegenen kognitiven Fähigkeiten der KI stützt, um herauszufinden, welche Handlungen dieser Beschreibung entsprechen. Wir können diesen Vorschlag "moralische Rechtmäßigkeit" (MR) nennen. MR scheint auch einige Nachteile zu haben. Sie beruht auf der Vorstellung von „moralisch richtig“, einem berüchtigt schwierigen Konzept, mit dem die Philosophen seit der Antike geplündert haben, ohne noch einen Konsens hinsichtlich ihrer Analyse zu erzielen. Eine falsche Veranlagung der „moralischen Rechtmäßigkeit“ zu finden, könnte zu Ergebnissen führen, die moralisch sehr falsch wären. Der Weg, eine KI mit einem dieser [moralischen] Konzepte zu beenden, könnte darin bestehen, ihm allgemeine sprachliche Fähigkeiten zu geben (zumindest vergleichbar mit dem eines normalen menschlichen Erwachsenen). Eine solche allgemeine Fähigkeit, natürliche Sprache zu verstehen, könnte dann verwendet werden, um zu verstehen, was mit "moralisch richtig" gemeint ist. „ Wenn die KI die Bedeutung erfassen könnte, könnte sie nach Handlungen suchen, die passen. Man könnte versuchen, die Grundidee des MR-Modells zu bewahren und seine Forderung zu reduzieren, indem man sich auf die moralische Zulässigkeit konzentriert: die Idee, dass wir die KI die CEV der Menschheit verfolgen lassen könnten, solange sie nicht in einer Weise handelt, die moralisch unzulässig ist. Santos-Lang betonte, dass die Entwickler mit einer einzigen Art von Superintelligenz beginnen könnten. Mögliche Bedrohung der Menschheit Es wurde vorgeschlagen, dass, wenn KI-Systeme schnell superintelligent werden, sie unvorhergesehene Handlungen oder außerkompetante Menschlichkeit ergreifen können. Forscher haben argumentiert, dass durch eine "Intelligenzexplosion" eine selbstverbessernde KI so kraftvoll werden könnte, dass sie von Menschen unaufhaltbar ist. In Bezug auf menschliche Extinktionsszenarien identifiziert Bostrom (2002) die Superintelligenz als mögliche Ursache: Wenn wir das erste superintelligente Wesen schaffen, können wir einen Fehler machen und ihm Ziele geben, die es zur Vernichtung der Menschheit führen, vorausgesetzt, dass ihr enormer intellektueller Vorteil ihm die Macht gibt, dies zu tun. Zum Beispiel könnten wir irrtümlich ein Subgoal zum Status eines Übergoals erhöhen. Wir sagen es, um ein mathematisches Problem zu lösen, und es entspricht, indem es alle Materie im Sonnensystem zu einem riesigen Rechengerät verwandelt, in dem Prozess die Person, die die Frage gestellt. In der Theorie, da eine superintelligente KI in der Lage wäre, fast jedes mögliche Ergebnis zu bringen und jeden Versuch, die Umsetzung seiner Ziele zu verhindern, zu vereiteln, könnten viele unkontrollierte, unbeabsichtigte Konsequenzen entstehen. Es könnte alle anderen Agenten töten, sie dazu überreden, ihr Verhalten zu ändern oder ihre Interferenzversuche zu blockieren. Eliezer Yudkowsky illustriert eine solche instrumentelle Konvergenz wie folgt: "Die KI hasst dich nicht, noch liebt sie dich, aber du bist aus Atomen gemacht, die sie für etwas anderes verwenden kann. " Dies stellt das Problem der KI-Kontrolle dar: wie man einen intelligenten Agenten baut, der seinen Schöpfern helfen wird, während man unbeabsichtigt eine Superintelligenz baut, die seinen Schöpfern schadet. Die Gefahr, die Kontrolle nicht "zum ersten Mal" zu entwerfen, besteht darin, dass eine Superintelligenz in der Lage sein kann, Macht über seine Umwelt zu ergreifen und zu verhindern, dass die Menschen sie abschalten. Seit einem Superintelligent KI wird wahrscheinlich die Fähigkeit haben, den Tod nicht zu befürchten und stattdessen betrachten es eine vermeidbare Situation, die durch einfaches Abschalten der Power-Taste vorhergesagt und vermieden werden kann. Mögliche KI-Kontrollstrategien umfassen "Kapazitätskontrolle" (Begrenzung der Fähigkeit einer KI, die Welt zu beeinflussen) und "motivationale Kontrolle" (Gebäude einer KI, deren Ziele mit menschlichen Werten übereinstimmen). Bill Hibbard befürwortet die öffentliche Bildung über Superintelligenz und öffentliche Kontrolle über die Entwicklung von Superintelligenz. Siehe auch Citations Bibliographie Bostrom, Nick (2002), "Existential Risks", Journal of Evolution and Technology, 9, abgerufen 2007-08-07 Bostrom, Nick (2014). Superintelligence: Pfade, Gefahren, Strategien. Oxford University Press. Chalmers, David (2010). " Die Singularität: Eine philosophische Analyse" (PDF). Journal of Consciousness Studies.17: 7–65.Hibbard, Bill (2002).Superintelligente Maschinen. Kluwer Academic/Plenum Publishers. Legg, Shane (2008). Machine Super Intelligence (PDF) (PhD). Abteilung Informatik, Universität Lugano. Retrieved September 19, 2014.Müller, Vincent C.; Bostrom, Nick (2016). " Zukunftsfortschritt im Künstlichen Intelligenz: Eine Umfrage der Expertenmeinung." In Müller, Vincent C. (Hrsg.). Grundlagen der Künstlichen Intelligenz.Springer.pp.553–571.Santos-Lang, Christopher (2014). "Unsere Verantwortung, evaluative Vielfalt zu verwalten" (PDF). ACM SIGCAS Computer & Society.44 (2): 16–19.doi:10.1145/2656870.2656874 Externe Links Bill Gates verbindet Stephen Hawking in Angst vor einem kommenden Bedrohung von Superintelligence Wird Superintelligente Maschinen Zerstören Menschlichkeit? Apple Mitbegründer hat Sinn für Foreboding über Künstliche Superintelligenz