In der Statistik ist die Verallgemeinerung der kleinsten Quadrate (GLS) eine Technik zur Schätzung der unbekannten Parameter in einem linearen Regressionsmodell, wenn eine gewisse Korrelation zwischen den Resten in einem Regressionsmodell besteht. In diesen Fällen können gewöhnliche kleinste Quadrate und gewichtete kleinste Quadrate statistisch ineffizient sein oder sogar irreführende Inferenzen geben. GLS wurde 1936 erstmals von Alexander Aitken beschrieben. Erläuterung In Standard-Linear-Regressionsmodellen beobachten wir die Daten { y i , x i j } i = 1 , ... , n , j = 2 , ... y_{i},x_{ij}\}_{i=1,\dots n,j=2,\dots ,k} auf n statistischen Einheiten. Die Antwortwerte werden in einem Vektor y = ( y 1 , ..., y n ) gesetzt. T {\displaystyle \mathbf {y} =left(y_{1},\dots ,y_{n}\right)^{\mathsf {T} und die Prädiktorwerte in der Designmatrix X = ( x 1 T, ..., x n T ) T {\displaystyle \mathbf {X} =left(\mathbf {x}_1}^{\mathsf {T},\dots ,\mathbf {x}_n}^{\mathsf T}\right)^{\mathsf {T}, wobei x i=style 1, x i 2 k, ... Das Modell zwingt das bedingte Mittel von y {\displaystyle \mathbf {y} X {\displaystyle \mathbf {X} ist eine lineare Funktion von X {\displaystyle \mathbf {X} und nimmt die bedingte Varianz des Fehlerterms bei X {\displaystyle \mathbf {X} an, ist eine bekannte nicht-singulare Kovarianzmatrix Ω {\displaystyle \mathbf {\Omega } . Dies wird in der Regel als y = X β + ε , E [ ε ∣ X ] = 0 , Cov [ ε ∣ X ] = Ω . {\displaystyle \mathbf {y} =\mathbf {X} \mathbf {\beta } \+mathbf {\varepsilon} ,\qquad \operatorname {E} \[varepsilon \mid \mathbf {X} ]=0,\ \operatorname {Cov} \[varepsilon \mid \mathbf (X} )=\mathbf {\Omega}}Hier ist β ε R k {\displaystyle \beta \in \mathbb {R} ^^{k} ein Vektor unbekannter Konstanten (bekannt als "Regressionskoeffizienten"), der aus den Daten geschätzt werden muss. Angenommen b {\displaystyle \mathbf {b} ist eine Kandidatenschätzung für β {\displaystyle \mathbf {\beta } .Dann der Restvektor für b {\displaystyle \mathbf {b} wird y - sein X b {\displaystyle \mathbf {y} -\mathbf {X} \mathbf {b} .Die verallgemeinerte Methode der kleinsten Quadrate schätzt β {\displaystyle \mathbf {\beta } durch Minimierung der quadratischen Mahalanobis Länge dieses Restvektors: ^ = argmin b ( y - X b ) T Ω - 1 ( y - X b ) = argmin b y T Ω - 1 y + (X b ) T Ω - 1 X b - y T Ω - 1 X b - (X b ) T Ω - 1 y , {\displaystyle \mathbf {\hat {\beta }} ~ b}{\operatorname {argmin} }(\mathbf {y} - Ja. {\Omega} {-^1}(\mathbf {y} -\mathbf {X} \mathbf {=)\underset b}{\operatorname {argmin}}}\mathbf {y} {\\mathsf}\\\\mathbf {\Omega} {-^1}\mathbf {y} (+\mathbf {X} \mathbf {b} {\mathf}}\mathbf}}\mathbf (Omega) {X} \mathbf {b} -\mathbf {y} {\\mathf}\mathbf (Omega) {X} \mathbf {b} -(\mathbf {X} \mathbf {b} {)^\mathsf {T}\mathbf {\Omega } {-^1}\mathbf {y} \,} wobei die letzten beiden Begriffe Skalare auswerten, was zu β ^ = argmin führt b y T Ω - 1 y + b T X T Ω - 1 X b - 2 b T X T Ω - 1 y.{\displaystyle \mathbf} } ~ b}{\operatorname {argmin},\mathbf {y} {\\mathsf}\\\\mathbf {\Omega} {-^1}\mathbf {y} \+mathbf {b} {\\mathf}\mathbf {X} (Omega) {X} -2\mathbf {b} {\\mathf}\mathbf {X} {\Omega} {-^1}\mathbf {y} \.} Dieses Ziel ist eine quadratische Form in b {\displaystyle \mathbf {b} .Taking the Gradient dieser quadratischen Form in Bezug auf b {\displaystyle \mathbf {b} } und gleichzusetzen es auf Null (wenn b = β ^ {\displaystyle \mathbf {b} {=\hat {\beta }}} gibt 2 X T Ω - 1 X β ^ - 2 X T Ω - 1 y = 0 {\displaystyle 2\mathbf {X} {^\mathf {T}\mathbf (Omega) - Ja. {X} (Omega) =0} Das Minimum des objektiven Spaßes kann daher nach der expliziten Formel berechnet werden: β ^ = (X T Ω - 1 X) - 1 X T Ω - 1 y . {\displaystyle \mathbf {\hat {\beta }}} =link(\mathbf) {X} (Omega) (X} Recht) {X} (Omega) Eigenschaften Der GLS-Schätzer ist unvoreingenommen, konsistent, effizient und asymptotisch normal mit E ‡ [β ^ ∣ X] = β {\displaystyle \operatorname {E} {\hat {\beta }\mid \mathbf (X} )=\beta } und Cov [β ^ ∣ X ] = (X T Ω − 1 X ) − 1 {\displaystyle \operatorname {Cov} {\hat {\beta }\mid \mathbf {X}(=\mathbf {X}\mathf {T}\Omega {-^1}\mathbf {X} {-)^1} .GLS entspricht der Anwendung gewöhnlicher kleinster Quadrate auf eine linear transformierte Version der Daten. Um dies zu sehen, Faktor Ω = C T {\displaystyle \mathbf {\Omega } =\mathbf {C} \mathbf {C} {^\mathsf {T}, z.B. unter Verwendung der Cholesky Zersetzung. Wenn wir dann beide Seiten der Gleichung y = X vormultiplizierenβ + ε {\displaystyle \mathbf {y} =\mathbf {X} \mathbf {\beta } \+mathbf {\varepsilon } von C - 1 {\displaystyle \mathbf {-^1} erhalten wir ein äquivalentes lineares Modell y ≠style = X χ β + ε ε χ {\cH00FF} {\cH00FF} {\cH00FF}} {\cH00FF}}} {\cH00FF}} {\cH00FF}}} C - 1 y {\displaystyle \mathbf {y} {y} {=^*}\mathbf {C} {-^1}\mathbf} }, X χ = C - 1 X {\displaystyle \mathbf {X} {=^*}\mathbf {C} {-^1}\mathbf {X} } und ε χ = C - 1 ε {\displaystyle \mathbf {\varepsilon} {\cHFF} {C} {-^1}\mathbf {\varepsilon}} In diesem Modell Var [ ε χ ∣ X ] = C - 1 Ω (C - 1 ) T = I '\displaystyle \operatorname {Var} \[varepsilon}\mid \mathbf {\cH00FF}\cH00FF}{\cH00FF}{\cH00FF}{\cH00FF}\cH00FF}\cH00FF}\cH00FF} Links (\mathbf) {C} -1}\right)^{\mathsf {T}=\mathbf {I} }, wobei I {\displaystyle \mathbf {I} die Identitätsmatrix ist. So können wir die β {\displaystyle \mathbf {\beta } effizient schätzen, indem wir auf die transformierten Daten, die eine Minimierung erfordert ( y ≠ - X χ β ) T ( y χ - X χ χ β ) = ( y - X b ) T Ω - 1 ( y - X b ) . {\displaystyle left(\mathbf {y} {*^}-\mathbf} {*^}\mathbf {\beta } recht){\mathf {T}}(\mathbf {y}-\mathbf {X}}\mathbf}\mathbf}} {\mathbf} {\beta}}}} {\}}}}}}\\\\}}}\\\\\\\\\\\\\\\\\\\\\\}}}}}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}}}}}}}}}\\\\\\\\\\\ -\mathbf {X} \mathbf {b} {)^\mathbf {T}\,\mathbf {\Omega } {-^1}(\mathbf {y} -\mathbf {X} \mathbf {b}}}}Dies hat die Wirkung, die Skala der Fehler zu standardisieren und "de-korrelieren" sie. Da OLS auf Daten mit homoszegischen Fehlern angewendet wird, gilt der Gauss-Markov theorem, und daher ist die GLS-Schätzung der beste lineare unvoreingenommene Schätzwert für β. Gewichtete kleinste Quadrate Ein spezieller Fall von GLS genannt gewichtete kleinste Quadrate (WLS) tritt auf, wenn alle außerdiagonalen Einträge von Ω 0 sind. Diese Situation ergibt sich, wenn die Varianzen der beobachteten Werte ungleich sind (d.h. die Heteroszenizität ist vorhanden), wobei jedoch keine Korrelationen unter den beobachteten Varianzen vorhanden sind. Das Gewicht für die Einheit i ist proportional zur Reziproke der Varianz der Antwort für die Einheit i. Wenn die Kovarianz der Fehler Ω {\displaystyle \Omega } unbekannt ist, kann man eine konsistente Schätzung von Ω {\displaystyle \Omega } erhalten, sagen Ω ^ {\displaystyle {\widehat {\Omega }} unter Verwendung einer durchführbaren Version von GLS bekannt als die durchführbare verallgemeinerte kleinste Quadrate (FGLS)-Schätzer. In FGLS verläuft die Modellierung in zwei Stufen: (1) das Modell wird von OLS oder einem anderen konsistenten (aber ineffizienten) Schätzer geschätzt, und die Reste werden verwendet, um einen konsistenten Schätzer der Fehler-Kovarianz-Matrix zu bauen (dazu muss man oft das Modell mit zusätzlichen Einschränkungen untersuchen, beispielsweise wenn die Fehler einem Zeitreihenprozess folgen, ein Statistiker braucht in der Regel einige theoretische Annahmen auf diesem Prozess zu implementieren (2). Während GLS unter Heteroszedasticität oder Autokorrelation effizienter als OLS ist, gilt dies nicht für FGLS. Der mögliche Schätzwert ist, sofern die Fehler-Kovarianz-Matrix konsequent geschätzt wird, asymmetrisch effizienter, aber für eine kleine oder mittlere Größe Probe kann es tatsächlich weniger effizient als OLS sein. Aus diesem Grund bevorzugen einige Autoren OLS zu verwenden und ihre Inferenzen zu reformieren, indem man einfach einen alternativen Schätzwert für die Varianz des Schätzers betrachtet, der für die Heteroszemetizität oder die serielle Autokorrelation robust ist. Für große Proben ist FGLS jedoch gegenüber OLS unter Heteroskektizität oder serieller Korrelation bevorzugt. Ein Vorsichtshinweis ist, dass der FGLS-Schätzer nicht immer konsistent ist. Ein Fall, in dem FGLS unvereinbar sein könnte, ist, wenn es einzelne spezifische Fixeffekte gibt. Im allgemeinen hat dieser Schätzer unterschiedliche Eigenschaften als GLS. Für große Proben (d.h. asymmetrisch) sind alle Eigenschaften (unter geeigneten Bedingungen) in Bezug auf GLS, aber für endliche Proben sind die Eigenschaften von FGLS-Schätzern unbekannt: Sie variieren dramatisch mit jedem einzelnen Modell, und in der Regel können ihre exakten Verteilungen nicht analytisch abgeleitet werden. Bei endlichen Proben kann FGLS in einigen Fällen sogar weniger effizient sein als OLS. Während also GLS durchführbar gemacht werden kann, ist es nicht immer ratsam, dieses Verfahren anzuwenden, wenn die Probe klein ist. Ein Verfahren zur Verbesserung der Genauigkeit der Schätzwerte in endlichen Proben ist dabei zu iterieren, d.h. die Restmengen von FGLS zur Aktualisierung des Fehlerkovarianzschätzers zu nehmen, und dann die FGLS-Schätzung zu aktualisieren, dieselbe Idee iterativ anzuwenden, bis die Schätzwerte weniger als eine gewisse Toleranz variieren. Aber diese Methode verbessert nicht unbedingt die Effizienz des Schätzers sehr, wenn die ursprüngliche Probe klein war. Eine vernünftige Möglichkeit, wenn Proben nicht zu groß sind, besteht darin, OLS anzuwenden, aber den klassischen Varianz-Schätzer σ 2 χ (X' X) - 1 {\displaystyle \sigma 2}*(X'X)^{-1 (was in diesem Rahmen nicht konsistent ist) wegzuwerfen und einen HAC (Heteroskedasticity and Autocorrelation Consistent)-Schätzer zu verwenden. Zum Beispiel können wir im Autokorrelationskontext den Bartlett-Schätzer (oft als Newey-West-Schätzer bekannt, da diese Autoren die Verwendung dieses Schätzers unter Ökonometrikern in ihrem 1987 Econometrica-Artikel populär gemacht haben) und in heteroskedastic Kontext können wir den Eicker-White-Schätzer verwenden. Dieser Ansatz ist viel sicherer, und es ist der geeignete Weg zu nehmen, es sei denn, die Probe ist groß, und groß ist manchmal ein rutschiges Problem (z.B. wenn die Fehlerverteilung asymmetrisch ist, wäre die benötigte Probe viel größer.)Der gewöhnliche kleinste Quadrat (OLS)-Schätzer wird wie üblich durch β ^ OLS = (X' X) - 1 X' y {\displaystyle berechnet {\widehat} \_text{OLS}=(X'X)^{-1}X'y und Schätzungen der Restmengen u ^ j = ( Y - X β ^ OLS ) j {\displaystyle {\widehat u}_{j}=(Y-X{\widehat {\beta \text{OLS})_{j werden aufgebaut. Einfachheit halber das Modell für heteroske Fehler. Annahme, dass die Varianz-Kovarianz-Matrix Ω {\displaystyle \Omega } des Fehlervektors ist diagonal, oder gleichwertig, dass Fehler von verschiedenen Beobachtungen unkorreliert sind. Dann kann jeder Diagonaleintrag durch die eingebauten Restmengen u ^ j {\displaystyle {\widehat u}_{j so Ω ^ O L S {\displaystyle {\hat {\Omega {_OLS kann durch Ω ^ OLS = diag ≠ 1 2 , σ ^ 2 2 , ... σ ^ n 2 ) erstellt werden. {\displaystyle {\widehat {\Omega _text{OLS}=\operatorname {diag} {(\widehat {\sigma _1}^{2},{\widehat {\sigma _2},\dots {\sigma _n}^{2.) Es ist wichtig zu bemerken, dass die quadratischen Reststoffe nicht im vorherigen Ausdruck verwendet werden können; wir brauchen einen Schätzer der Fehlervarianzen. Dazu können wir ein parametrisches Heteroskedasticity-Modell oder einen nichtparametrischen Schätzer verwenden. Sobald dieser Schritt erfüllt ist, können wir fortfahren: Schätzung β F G L S 1 {\displaystyle \beta {_FGLS1} mit Ω ^ OLS {\displaystyle {\widehat {\Omega \_text{OLS mit gewichteten kleinsten Quadraten β ^ F G L S 1 = (X' Ω ^ OLS - 1 X ) - 1 X' Ω ^OLS − 1 y {\displaystyle {\widehat} _FGLS1}=(X'{\widehat (\Omega _text{OLS}{-1}X)^{-1}X'{\widehat {\Omega \_text{OLS}{-1}y Das Verfahren kann iteriert werden. Die erste Iteration wird durch u ^ F G L S 1 = Y - X β ^ F G L S 1 gegeben {\displaystyle {\widehat σ σ σ σ σ σ ^ F G L S 1 , 1 2 σ σ ^ F G L S 1 , 2 , ... σ σ ^ F G L S 1 , 1 2 , σ σ ^ F G L S 1 , 2 , ... Omega {_FGLS1}=\Betreibername {diag} {\widehat} {\sigma} ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ {\widehat} _FGLS2}=(X'{\widehat (\Omega _FGLS1}^{-1}X{-1}X'{\widehat (Omega) Diese Schätzung von Ω ^ {\displaystyle {\widehat {\Omega }}}} kann auf Konvergenz iteriert werden. Unter Regelbedingungen wird der FGLS-Schätzer (oder der seiner Iterationen, wenn wir eine endliche Anzahl von Zeiten iterieren) asymptotisch als n (β ^ F G L S - β ) → d N (0, V ) verteilt. {\displaystyle}({\hat {\beta {_FGLS}-\beta)\\xrightarrow {d}\ - Ja.wobei n die Stichprobengröße und V = p - l i m ≠ (X' Ω - 1 X / T ) {\displaystyle V=\operatorname {p-lim} (X'\Omega {-^1}X/T) ist, hier p-lim bedeutet Wahrscheinlichkeitsgrenze Siehe auch Confidence Region Effektive Freiheitsgrade Referenzen Weiter lesen Amemiya, Takeshi (1985)."Allgemeine Least Squares Theory". Advanced Econometrics. Harvard University Press.ISBN 0-674-00560-0.Johnston, John (1972)."Generalized Least-squares". Ökonometrische Methoden (Second ed.). New York: McGraw-Hill.pp.208–242.Kmenta, Jan (1986)."Generalized Linear Regression Model and its Applications". Elemente der Ökonometrie (Second ed.). New York: Macmillan.pp.607–650.ISBN 0-472-10886-7.