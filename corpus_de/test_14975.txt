Entropy ist ein wissenschaftliches Konzept sowie ein messbares physisches Eigentum, das am häufigsten mit einem Zustand der Störung, der Randomität oder der Unsicherheit verbunden ist. Der Begriff und das Konzept werden in unterschiedlichen Bereichen, von klassischen Thermodynamiken, wo es zuerst anerkannt wurde, zur mikroskopischen Beschreibung der Natur in der statistischen Physik und zu den Grundsätzen der Informationstheorie verwendet. Es hat weit reichende Anwendungen in der Chemie und der Physik, in biologischen Systemen und ihrem Zusammenhang mit dem Leben, in Kosmologie, Wirtschaft, Soziologie, Wetterwissenschaft, Klimawandel und Informationssystemen, einschließlich der Übertragung von Informationen in der Telekommunikation, gefunden. Das thermodynamische Konzept wurde von schottischen Wissenschaftlern und Ingenieur Macquotenrn Rankin im Jahr 1850 mit den Namen Thermodynamische Funktion und Wärmepotenzial bezeichnet. 1865 definierte der deutsche Physiker Rudolf Clausius, einer der führenden Gründer des Bereichs der Thermodynamik, als das Verhältnis einer unendlichen Hitzemenge zur unmittelbaren Temperatur. Er bezeichnete es zunächst als Transformations-Inhalte, in deutscher Verwandlungs-Drucker, und später hat er den Begriff "Entropy" aus einem griechischen Umwandlungswort gelobt. Clausius verwies auf die mikroskopische Verfassung und Struktur im Jahre 1862 auf das Konzept als Degregation. Infolge der Eutropie ist, dass bestimmte Prozesse unumkehrbar oder unmöglich sind, abgesehen von der Anforderung, den Schutz der Energie nicht zu verletzen, die letztere im ersten Gesetz der Thermodynamik ausgedrückt werden. Entropy ist ein zentrales Element des zweiten Thermodynamikgesetzes, das besagt, dass die Eutropy von isolierten Systemen, die der spontanen Entwicklung überlassen sind, mit der Zeit nicht abnehmen kann, da sie immer zu einem Zustand des thermodynamischen Gleichgewichts kommen, wo die Eutropy am höchsten ist. Österreichischer Physiker Ludwig Bolzmann erläuterte die Eutropy als Maßnahme der Anzahl möglicher mikroskopischer Vereinbarungen oder Staaten von einzelnen Atomen und Molekülen eines Systems, das dem makroökonomischen Zustand des Systems entspricht. Er führte dadurch das Konzept der statistischen Störung und der Wahrscheinlichkeitsverteilung in ein neues Feld der Thermodynamik ein, das als statistisches Instrument bezeichnet wird, und stellte den Zusammenhang zwischen den mikroskopischen Interaktionen fest, die sich über eine durchschnittliche Konfiguration auf das makroökonomische observierbare Verhalten in Form eines einfachen Logarithmiegesetzes, mit einer Verhältnismäßigkeitskontinui, dem Kaszmann, aus, das zu einer der Definition universeller Konstanten für das moderne Internationale System der Einheiten (SI) geworden ist. Bell Labs Wissenschaftler Claude Shannon entwickelte 1948 ähnliche statistische Konzepte zur Messung mikroskopischer Unsicherheiten und der Vielfalt an dem Problem zufälliger Verluste von Informationen in Telekommunikationssignalen. Nach dem Vorschlag von John von Neumann nannte Shannon diese Einrichtung fehlender Informationen in ähnlicher Weise wie die Verwendung in statistischen Mechanismen als Eutropy und gab Geburten in der Informationstheorie. Diese Beschreibung wurde als universelle Definition des Begriffs der Entropy vorgeschlagen. Geschichte In seinem 1803-Papier haben die Grundprinzipien von Equilibrium und Bewegung vorgeschlagen, dass in jedem Fall die Beschleunigungen und Schocks der beweglichen Teile Verluste darstellen; in jedem natürlichen Prozess gibt es eine inhärente Tendenz zur Dissipation nützlicher Energie. 1824, der auf dieser Arbeit baut, veröffentlichte der Sohn Sadi Carnot, Sadi Carnot, Überlegungen über die Wettleistung des Feuers, die darauf hinwiesen, dass in allen Wärmemotoren, wann immer Kaloric (das ist jetzt bekannt als Wärme) durch einen Temperaturunterschied, die Arbeit oder die Motivationskraft aus den Maßnahmen des Sturzes von einem heißen bis kalten Körper erzeugt werden können. Er benutzte eine Analogie, wie Wasser in ein Wasserrad fällt. Dies war ein frühzeitiger Einblick in das zweite Gesetz der Thermodynamik. Carnot basierte seine Ansichten von Hitze teilweise auf dem frühen 18. Jahrhundert "Neutonian hypothesis", dass sowohl Wärme als auch Lichtart undurchlässige Formen von Materie sind, die durch andere Materie angezogen und gebremst werden, und zum Teil auf den zeitgenössischen Standpunkt von Grafford, der in 1789 zeigte, dass die Wärme durch Reibung verursacht werden könnte, da Nicht-Flagge maschinelliert werden. Carnot begründete, dass, wenn der Körper des Arbeitsmittels, wie ein Dampf, am Ende eines kompletten Motorzyklus in den ursprünglichen Zustand zurückgeführt wird, "keine Änderung im Zustand des Arbeitsorgans". Das erste Gesetz der Thermodynamik, das 1843 von den Hitzeversuchen von James Joule abgeschwächt wurde, zeigt das Energiekonzept und seine Erhaltung in allen Prozessen; das erste Gesetz ist jedoch nicht in der Lage, die Auswirkungen von Reibung und Entflechtung zu quantifizieren. In den siebziger und 1860er Jahren hat der deutsche Physiker Rudolf Clausius daran gehindert, dass keine Änderung im Arbeitsorgan stattfindet, und gab zu bedenken, dass eine mathematische Auslegung geändert wird, indem er die Art des inhärenten Verlusts von unsabler Wärme bei der Arbeit bezweifelt, z.B. die durch Reibung erzeugte Wärme. Er bezeichnete seine Beobachtungen als eine unteilbare Energienutzung, die zu einem Transformations-Inhalt (Verwandlungszentrale in Deutsch) eines thermodynamischen Systems oder eines Arbeitsorgans chemischer Arten während einer Änderung des Staates führt. Im Gegensatz zu früheren Ansichten, die auf den Theorien von Isaac Newton basieren, war die Wärme ein undurchlässiges Partikel, das Masse hatte. Clausius entdeckte, dass der nicht brauchbare Energieanstieg, da er aus der Einspeise bis zur Erschöpfung in einem Dampfmotor kommt. Aus dem Vorsatz en, wie im Energiebereich, und aus dem griechischen Wort  [ [tropē,], das in einem etablierten lexicon als Wende oder Wandel übersetzt wird, und dass er in deutscher Sprache als Verwandlung, ein Wort, das häufig in englischer Sprache übersetzt wurde, 1865 hat Clausius den Namen dieses Eigentums als Eutropy. Das Wort wurde 1868 in die englische Sprache aufgenommen. Später gab es Wissenschaftler wie Ludwig Schneiderzmann, Jacqueline Willard Gibbs und James Clerk Maxwell eine statistische Grundlage. Im Jahr 1929 hat sich die Probstabilisierung eines Pakets von idealen Gaspartikeln bewährt, in dem er als Verhältnis zum natürlichen Logarithm der Anzahl der Mikrostaaten eines solchen Gases definiert hat. Künftig ist das wesentliche Problem der statistischen Thermodynamik darin bestanden, die Verteilung einer bestimmten Menge Energie E über N identische Systeme zu bestimmen. Constantin Carathéodory, ein griechischer mathematician, verbunden mit einer mathematischen Definition der Unumkehrbarkeit, in Bezug auf Trawege und Integration. Etymologie 1865 nannte Clausius das Konzept der "Verschiedenheit einer Menge, die von der Konformierung des Systems abhängt", entropy (Entropie) nach dem griechischen Umwandlungswort. Er gibt "transformationsinhalte" (Verwandlungszentrale) als Synonym, parallel zu seinem "geothermischen und ergonalen Inhalt" (Schöne- und Werksbezeichnung) als Namen von U {\displaystyle U}, bevorzugt aber die Begriffstropy als eine enge Parallelität des Wortes, da er die Begriffe nahezu gleich in ihrer körperlichen Bedeutung feststellte. Dieser Begriff wurde durch die Ersetzung der Wurzel derἔργον (Arbeit) durch die von ). (Transformation) gebildet. Definitionen und Beschreibungen Das Konzept der Entropy wird durch zwei Hauptansätze, die makroskopische Perspektive der klassischen Thermodynamik und die mikroskopische Beschreibung Zentral für statistische Mechanismen beschrieben. Der klassische Ansatz definiert entropy in Bezug auf mikroskopisch messbare physische Eigenschaften wie Masse, Volumen, Druck und Temperatur. Die statistische Definition von entropy definiert sie in Bezug auf die Statistiken der Bewegungen der mikroskopischen Komponenten eines Systems, das zunächst klassischer Art, wie z.B. die in der Natur vorkommenden natürlichen Partikel, und später Quantenmechanisch (Foton, Tonnen, Spins usw.). Die beiden Ansätze bilden einen kohärenten, einheitlichen Standpunkt des gleichen Phänomens wie im zweiten Thermodynamikgesetz, das universale Anwendbarkeit auf physikalische Prozesse gefunden hat. Funktion des StaatesMany Thermodynamische Eigenschaften haben ein besonderes Merkmal, dass sie eine Reihe von physikalischen Variablen bilden, die einen Zustand des Gleichgewichts definieren; sie sind Funktionen des Staates. Häufig, wenn zwei Eigenschaften eines Systems bestimmt sind, reichen sie aus, um den Zustand des Systems und damit die Werte anderer Immobilien zu bestimmen. z.B. eine bestimmte Menge, Temperatur und Druck von Gas bestimmen ihren Staat und damit auch sein Volumen. Ein anderes Beispiel ist ein System, das aus einem reinen Stoff einer einzigen Phase in einer bestimmten einheitlichen Temperatur und einem bestimmten Druck besteht (und somit ein bestimmter Staat ist) und nicht nur ein bestimmtes Volumen, sondern auch eine bestimmte Eutropie ist. Die Tatsache, dass Eutropy eine Funktion des Staates ist, ist ein Grund, warum es sinnvoll ist. In der Autonot-Zyklus kehrte die Arbeitsflüssigkeit zu dem gleichen Zustand zurück, den sie zu Beginn des Zyklus hatte, so dass die Leitung jeder staatlichen Funktion, wie z.B. Eutropy, über diesen umkehrbaren Zyklus Null ist. Revertibler Prozess Entropy wird für einen reversiblen Prozess bewahrt. Ein reversibler Prozess ist eines, der nicht von einem thermodynamischen Gleichgewicht abweicht und gleichzeitig die maximale Arbeit produziert. Jeder Prozess, der schnell genug ist, um von thermischem Gleichgewicht abzuweichen, kann nicht umkehren. In diesen Fällen wird Energie verloren, die gesamte Eutrophierung erhöht, und das Potenzial für eine maximale Arbeit im Übergang wird ebenfalls verloren. Konkret wird die Gesamttropy in einem umkehrbaren Prozess bewahrt und in einem unumkehrbaren Prozess nicht bewahrt. Im Autonot-Zyklus bedeutet der Wärmefluss vom Warmspeicher zum Kalten Speicher einen Anstieg der Eutropie, die Arbeitsleistung, wenn in einigen Energiespeichern wiederversorgt und vollständig gelagert wird, eine Verringerung der Eutropie, die genutzt werden könnte, um den Wärmemotor im Umkehren zu bedienen und wieder in den vorherigen Staat zurückzukehren, so dass die Gesamttropy-Veränderung immer noch Null ist, wenn der gesamte Prozess umkehrbar ist. Ein unumkehrbarer Prozess erhöht die Eutropy. Autonot-Zyklus Das Konzept der Entropy entstand aus der Studie von Rudolf Clausius über den Carnot-Zyklus. Wärme QH wird in einem Carnot-Zyklus auf Temperatur, die er von einem heißen Reservoir erhitzt und instandgesetzt wird, als Wärme QC zu einem Kältespeicher bei TC. Nach dem Prinzip von Carnot kann die Arbeit nur dann durch das System hergestellt werden, wenn es einen Temperaturunterschied gibt, und die Arbeit sollte eine Funktion des Unterschieds in Temperatur und Wärme (QH) sein. Carnot hatte nicht zwischen QH und QC unterschieden, da er die falsche Hypothese nutzte, dass die Kaustheorie gültig war und daher Wärme erhalten wurde (der falsche Annahme, dass QH und QC gleich waren), wenn QH tatsächlich höher ist als QC. Durch die Bemühungen von Clausius und Kelvin ist jetzt bekannt, dass die maximale Arbeit, die eine Wärmeleistung erzeugen kann, das Produkt der Carnot-Wirksamkeit und die aus dem heißen Speicher absorbierte Wärme ist: Zur Abfederung der Carnoteffizienz, die 1 ‐ TC/TH (eine Zahl weniger als eine) ist, musste Kelvin das Verhältnis der Arbeitsleistung in die während der isothermischen Expansion mit Hilfe der Carnot-Choleeyron-Formel, die eine unbekannte Funktion als Carnot-Funktion enthielt, bewerten. Die Möglichkeit, dass die Carnot-Funktion aus einer Nulltemperatur gemessen werden könnte, wurde von Joule in einem Brief an Kelvin vorgeschlagen. So konnte Kelvin seinen absoluten Temperaturgrad festlegen. Kennzeichnend ist auch, dass die durch das System hergestellten Arbeiten die Differenz zwischen der Wärme, die aus dem Warmspeicher und der Wärme aus dem Kalten Speicher entnommen wird: Da Letztere über den gesamten Zyklus gültig sind, gab es Clausius den Hinweis, dass die Arbeit und Wärme in jeder Phase des Zyklus nicht gleich wären, sondern ihre Differenz eine staatliche Funktion sein würde, die nach dem Abschluss des Zyklus schädlich wäre. Die staatliche Funktion wurde als interne Energie bezeichnet und wurde zum ersten Gesetz der Thermodynamik. Jetzt (1) und (2) geben Q H T H - Q C T C = 0 KINGstyle 7.8frac Q_TONtext{HTT_Polytext{H}}}}-Fitfrac Q_TONtext{CTT_Polytext{C==0 oder Q H T H = Q C T C Memestyle Memefrac Q_TONtext{HTT_TONtext{H== LANDfrac Q_TONtext{CTT_Fittext{C Dies bedeutet, dass es eine Funktion des Staates gibt, die über einen kompletten Zyklus des Carnot-Zyklus erhalten wird. Clausius nannte diese staatliche Funktion Eutropy. Man kann sehen, dass Eutropy nicht durch Laborergebnisse entdeckt wurde. Es ist ein mathematischer Bau und hat keine einfache physische Analogie. Dies macht das Konzept etwas verschleiern oder abstrakt, was darauf zurückzuführen ist, wie das Konzept der Energie entsteht. Clausius fragte dann, was passieren würde, wenn das System weniger funktionieren sollte als das, was Carnot-Prinzip vorhergesagt hat. Die rechte Seite der ersten Formel wäre die Obergrenze der Arbeitsleistung durch das System, die nun in eine Ungleichbehandlung umgewandelt würde W  1 ( 1 − T C T H ) Q H WELLdisplaystyle W(left(Spafrac) T_7.8text{CTT_Kaffeetext{H)right)Q_ 7.8text{H Wenn die zweite Formel verwendet wird, um die Arbeit als Unterschied in der Wärme auszudrücken, erhalten wir Q H  – Q C < ( 1 − T C T H ) Q H KINGstyle Q_TONtext{H}}-Q_TONtext{C(left(1-8) T_7.8text{CTT_Kaffeetext{H)right)Q_TONtext{H oder Q C > T H Q H 7.8displaystyle Q_TONtext{C>} Tel.: Mehr Wärme wird also auf das Kältereservoir als im Carnot-Zyklus gelegt. Wenn wir die Eutropen von Si = Qi/Ti für die beiden Staaten verleugnen, können die oben genannten Ungleichheiten als Rückgang der Entropy S H − S C  C 0 KINGstyle geschrieben werden. S_TONtext{H}}-S_Polytext{C}}<0 oder S H · S displaystyle S_TONtext{H}}<S_Polytext{C Die Eutropie, die das System verlässt, ist größer als die Eutropie, die das System einführt, was bedeutet, dass ein unwiderruflicher Prozess den Zyklus daran hindert, den maximalen Arbeitsaufwand zu erzeugen, der von der Carnot-Formel vorhergesagt wird. Der Carnot-Zyklus und die Effizienz sind sinnvoll, weil sie die Obergrenze der möglichen Arbeitsleistung und die Effizienz eines klassischen thermodynamischen Systems bestimmen. Andere Zyklen, wie der Otto-Zyklus, der Dieselzyklus und der Brayton-Zyklus, können vom Stand des Carnot-Zyklus analysiert werden. Maschinen oder Verfahren, die Wärme ins Arbeitsleben umwandeln und eine größere Effizienz als die Carnot-Effizienz erzeugen, sind nicht tragfähig, weil sie gegen das zweite Gesetz der Thermodynamik verstößt. Für sehr kleine Partikel im System müssen statistische Thermodynamik verwendet werden. Die Effizienz von Geräten wie Photovoltaikzellen erfordert eine Analyse des Standpunkts der Quantentechnik. Klassische Thermodynamik In den frühen 1850er Jahren wurde die Thermodynamische Definition von Eutropy von Rudolf Clausius entwickelt und beschreibt im Wesentlichen, wie die Eutropie eines isolierten Systems in Thermodynamik mit seinen Teilen gemessen werden kann. Clausius hat den Begriff „Entropy“ als eine umfassende thermodynamische Variablen eingeführt, die als nützlich für die Charakterisierung des Carnot-Zyklus erwiesen wurde. Wärmetransfer entlang der Erdwärmestufen des Carnot-Zyklus wurde festgestellt, dass sie proportional zur Temperatur eines Systems (bekannt als absolute Temperatur) sind. Diese Beziehung wurde in Erhöhungen der Eutropie ausgedrückt, die dem Verhältnis der incrementalen Wärmeübertragung, der durch Temperatur geteilt wurde, entsprechen, was im thermodynamischen Zyklus variieren konnte, aber schließlich am Ende jedes Zyklus auf denselben Wert zurückkommen konnte. So wurde festgestellt, dass es sich um eine Funktion des Staates handelt, insbesondere um einen thermodynamischen Zustand des Systems. Clausius basiert auf seiner Definition auf einem umkehrbaren Prozess, es gibt auch unumkehrbare Prozesse, die die Eutropy verändern. Nach dem zweiten Thermodynamikgesetz erhöht sich die Eutropy eines isolierten Systems immer auf unumkehrbare Prozesse. Unterschied zwischen einem isolierten System und einem geschlossenen System besteht darin, dass Wärme nicht in ein isoliertes System fließen kann, aber Wärmefluss in und aus einem geschlossenen System ist möglich. Jedoch können sowohl geschlossene als auch isolierte Systeme, auch in offenen Systemen, unumkehrbare thermodynamische Prozesse auftreten. Laut Clausius Gleichbehandlung für einen reversiblen zyklischen Prozess:  Q v Q rev T = 0 7.8textstyle \oint SSOfrac Memedelta Q_TONtext{rev}}}{T==0 . Dies bedeutet, dass die Linie integraler Bestandteil der L   Q rev T 7.8textstyle \int _Lffrac ggiodelta Q_TONtext{rev}}}{T ist pfadunabhängig. Wir können also eine staatliche Funktion S namens entropy definieren, die d S = s Q rev T 7.8textstyle dS= SSOfrac SSOdelta Q_TONtext{rev}}}{TTT . Um den Unterschied zwischen den beiden Staaten eines Systems zu finden, muss der integrale Faktor für einen umkehrbaren Weg zwischen den ersten und den endgültigen Staaten bewertet werden. Da es sich um eine staatliche Funktion handelt, ist dietropy Änderung des Systems für einen unumkehrbaren Pfad die gleiche wie für einen umkehrbaren Weg zwischen den beiden Staaten.Jedoch unterscheidet sich die entropy Veränderung der Umgebung. Wir können nur durch die Integration der oben genannten Formel die Änderung der Eutropie erhalten. Um den absoluten Wert der Entropy zu erhalten, brauchen wir das dritte Thermodynamikgesetz, das heißt, dass S = 0 bei absoluter Null für perfekte Kristalle. Aus makroskopischer Sicht wird die Entropy in klassischer Thermodynamik als staatliche Funktion eines thermodynamischen Systems ausgelegt: das ist ein Eigentum, je nachdem, wie dieser Staat erreicht werden soll. In jedem Verfahren, in dem das System Energie EE und seine Eutropie durch enS abnimmt, muss eine Menge mindestens TR SS dieser Energie in der Umgebung des Systems als unbrauchbare Wärme (TR ist die Temperatur der äußeren Umgebung des Systems). Kommt der Prozess nicht voran. In klassischer Thermodynamik wird die Eutropy eines Systems nur definiert, wenn es in Thermodynamischem Gleichgewicht ist. Statistischer Mechanismus Ludwig Poszmann wurde in den 80er Jahren von der statistischen Erfassung der mikroskopischen Komponenten des Systems entwickelt. Poszmann zeigte, dass diese Definition von Eutropy der thermodynamischen Entropy innerhalb eines ständigen Faktors entspricht – bekannt als ständiges Poszmann. Zusammenfassung der thermodynamischen Definition von entropy enthält die experimentelle Definition von Eutropy, während die statistische Definition von Eutropy das Konzept erweitert, eine Erklärung und ein tieferes Verständnis seiner Natur enthält. Die Auslegung von Eutropy in statistischen Mechanismen ist die Maßnahme der Unsicherheit oder der Vereinheitlichung des Ausdrucks von Gibbs, der nach seinen konservierbaren makroökonomischen Eigenschaften, wie Temperatur, Druck und Volumen, nach wie vor ein System ist. Für eine Reihe makroskopischer Variablen ist der Grad, an dem die Wahrscheinlichkeit des Systems über verschiedene mögliche Mikrostaaten verteilt wird, durch die Eutropy-Maßnahmen bestimmt. Anders als der Makrostaat, der die durchschnittlichen Mengen deutlich vernachlässigbar ist, legt ein Mikrostaat alle molekularen Details über das System einschließlich Position und Geschwindigkeit jedes Moleküls fest. Je mehr diese Staaten dem System zur Verfügung stehen, wobei die Wahrscheinlichkeit spürbar ist, desto größer ist die Eutropie. In statistischen Mechanismen ist die Eutropy eine Maßnahme der Anzahl der Möglichkeiten, ein System zu schaffen, die oft zu einer Störungsmaßnahme getroffen werden können (das höher ist die Eutropy, die höhere Störung). In dieser Definition wird die Eutropie als proportional zum natürlichen Logarithm der Anzahl möglicher mikroskopischer Konfigurationen der einzelnen Atome und Moleküle des Systems (Mikrostaaten) beschrieben, die den beobachteten makroökonomischen Zustand (Makrostate) des Systems verursachen könnten. Die konstante Verhältnismäßigkeit ist die Dauer von Poszmann. Levzmanns ständige und daher Eutropy haben Abmessungen von Energie, die durch Temperatur geteilt werden, die eine Einheit von joules pro kelvin (JKK-1) im Internationalen System der Einheiten (oder kg⋅m2⋅s -2KK -1 in Bezug auf Basiseinheiten. Die Neutralität eines Stoffes wird in der Regel als Intensivobjekt – entweder entropy pro Masse (SI-Einheit: JKK -1⋅kg -1) oder entropy pro Stoffmenge (SI-Einheit: J⋅K -1⋅mol -1) gewährt. Konkret ist entropy eine Logarithmie der Zahl der Staaten mit erheblicher Wahrscheinlichkeit, besetzt zu werden: S = − k B  i i Log  i p i , {\displaystyle S=-k_ggiorm Mathematik {B}  isum i}p_{i}\log p_{i,} ( p i Memestyle p_{i} ist die Wahrscheinlichkeit, dass das System in i {\displaystyle i} ist; wenn Staaten kontinuierlich definiert werden, wird die Zusammenfassung durch einen integralen Bestandteil über alle möglichen Staaten ersetzt) oder in gleicher Weise der erwartete Wert des Logarithmus der Wahrscheinlichkeit, dass ein Mikrostaat besetzt ist S = k B⟩ Log ⟩ Log ⟩ p S {\ {\  S {\  S  S  S  S S=  S  S  S  S {\ S{\ {\  S  S {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {B} \}langle \log p\rwinkel } dort, wo die kB die Poszmann- konstant ist, gleich 1,38065 x10 -23 J/K. Die Zusammenfassung ist über alle möglichen Mikrostaaten des Systems, und Pirate ist die Wahrscheinlichkeit, dass das System im i-th-Mikrostaat ist. In dieser Definition wird davon ausgegangen, dass die Basis von Staaten aufgegriffen wurde, damit keine Informationen über ihre relativen Phasen vorliegen. Mehr allgemeine Ausdruck ist S = k B Tr ⁡ (^ ^ Log ) ^ ^ ^ {\ {\ ^ ^ {\ {\ {\ ) {\ {\ {\ {\ ) ) ) ) ) ) {B} )operatorname {Tr} {(\largehat ggiorho }log( ggiorho }})),}, wo   ^ KINGstyle ggiohat ggiorho }}} die Dichtematrix ist, Tr {\displaystyle \operatorname {Tr} } ist ein Rückverfolg und Log KINGstyle \log } ist die Matrix Logarithm. Diese Dichtematrix-Formulierung ist in Fällen von thermischem Gleichgewicht nicht erforderlich, solange die Basisstaaten sich dafür entscheiden, Energie-Abschlüsse zu sein. In den meisten praktischen Zwecken kann dies als grundlegende Definition von Eutropy betrachtet werden, da alle anderen Formeln für S mathematisch von ihm abgeleitet werden können, nicht umgekehrt. In welchem Maße die fundamentale Annahme statistischer Thermodynamik oder der grundlegenden Posten in statistischen Mechanismen genannt wurde, wird die Besetzung eines jeden Mikrostaates gleichermaßen wahrscheinlich sein (z.B.: 1/kWh, wo ITO die Zahl der Mikrostaaten beträgt); diese Annahme ist in der Regel für ein isoliertes, ausgewogenes System gerechtfertigt. Dann verringert sich die frühere Formel auf S = k B Log ⁡ ITO . KINGstyle S=k_ {B} .log \® .} In Thermodynamik ist ein solches System eines, in dem das Volumen, die Zahl der Moleküle und die interne Energie festgelegt sind (das Mikrocanonical-Paket). Für ein bestimmtes thermodynamisches System wird die Übertrophierung als Eutropy minus der eines idealen Gases in der gleichen Dichte und Temperatur definiert, eine Menge, die immer negativ ist, weil ein ideales Gas höchst störungsfrei ist. Dieses Konzept spielt eine wichtige Rolle in der flüssigen Theorie. Zum Beispiel führt das übertropische Abwrackungsprinzip von Rosenfeld aus, dass ermäßigte Verkehrskoeffizienten während des zweidimensionalen Abschnitts einzigartig durch die Übertropy bestimmt sind. Die allgemeine Auslegung von Eutropy ist eine Maßnahme unserer Unsicherheit über ein System. Der Gleichwertigkeitszustand eines Systems maximiert die Eutropie, weil wir alle Informationen über die ursprünglichen Bedingungen außer den erhaltenen Variablen verloren haben; Maximierung der Eutropy maximiert unsere Unkenntnis über die Einzelheiten des Systems. Diese Unsicherheit ist nicht der alltäglichen subjektiven Art, sondern die Unsicherheit im Zusammenhang mit der experimentellen Methode und dem Auslegungsmodell. Das Auslegungsmodell spielt bei der Bestimmung der Eutropie eine zentrale Rolle. „für eine Reihe makroskopischer Variablen“ hat der qualifizierende Gutachter tiefe Auswirkungen: wenn zwei Beobachter unterschiedliche makroökonomische Variablen verwenden, sehen sie unterschiedliche Entropies. Wenn Beobachter A beispielsweise die Variablen U, V und W verwendet, und Beobachter B verwendet U, V, W, X, dann kann Beobachter B durch Änderung von X einen Effekt verursachen, der wie ein Verstoß gegen das zweite Thermodynamik-Gesetz für Beobachter A.In anderen Worten: Die Reihe makroskopischer Variablen muss alle umfassen, die den Versuch ändern könnten, ansonsten kann man eine abnehmende Eutropie sehen. Entropy kann für alle Markov-Prozesse mit reversibler Dynamik und dem detaillierten Gleichgewichtsvermögen definiert werden. In den 1896 Vorlesungen zu Gastheorien von Poszmann zeigte er, dass dieser Ausdruck eine Maßnahme der Entropy für die Systeme von Atomen und Molekülen in der Gasphase enthält und somit eine Maßnahme für die Eutropy der klassischen Thermodynamik darstellt. Entropy entsteht direkt aus dem Carnot-Zyklus. Sie kann auch als die durch Temperatur geteilte reversible Wärme beschrieben werden. Entropy ist eine grundlegende Aufgabe des Staates. In einem thermodynamischen System werden Druck, Dichte und Temperatur im Laufe der Zeit einheitlich, da der Gleichwertigkeitszustand höhere Wahrscheinlichkeit (mögliche Kombinationen von Mikrostaaten) hat als jeder andere Staat. Als Beispiel für ein Glas von Eiswasser in der Luft bei Raumtemperatur beginnt der Temperaturunterschied zwischen einem warmen Raum (die Umgebung) und kaltem Glas von Eis und Wasser (das System und nicht Teil des Raums) als Teil der thermischen Energie aus der warmen Umgebung auf das Kühlsystem von Eis und Wasser zu gleichen. Während der Temperatur des Glases und seines Inhalts und der Temperatur des Raums gleich sind. Mit anderen Worten ist die Eutropy des Raums zurückgegangen, da einige seiner Energie auf Eis und Wasser verteilt sind, von denen die Eutropy zugenommen hat. Wie im Beispiel berechnet, hat die Eutropy des Systems von Eis und Wasser mehr als die Eutropie des Umgebungsraums verringert. In einem isolierten System wie dem gemeinsam genutzten Raum und Eiswasser führt die Disparation der Energie von warmem bis Kühler immer zu einer Nettoerhöhung der Eutropy. Wenn das Universum des Raums und des Eiswassersystems ein Temperaturgleichgewicht erreicht hat, ist die Eutropy-Änderung vom Anfangsstaat maximal. Die Eutropie des thermodynamischen Systems ist eine Maßnahme, wie weit die Gleichbehandlung voranschreitet. Thermodynamische Entropy ist eine nicht konservierte staatliche Funktion, die in den Bereichen Physik und Chemie von großer Bedeutung ist. Historisch hat sich das Konzept der Entropy entwickelt, um zu erklären, warum einige Prozesse (die durch die Naturschutzgesetze abgedeckt werden) spontan auftreten, während ihre Zeitumkehrungen (auch durch Naturschutzgesetze zulässig) nicht; die Systeme sind eher in Richtung zunehmender Eutropy. Für isolierte Systeme verringert sich die Eutropy nie. Diese Tatsache hat mehrere wichtige Konsequenzen in der Wissenschaft: Erstens verbietet sie "perpetual way"-Maschinen, und zweitens bedeutet, dass der Pfeil der Eutropy die gleiche Richtung hat wie der Pfeil der Zeit. Erhöhungen der Entropy entsprechen unumkehrbaren Veränderungen in einem System, da einige Energie als Abfallwärme ausgeschaltet wird und die Arbeitsmenge eines Systems begrenzt werden kann. Im Gegensatz zu vielen anderen staatlichen Funktionen kann Eutropy nicht direkt beobachtet werden, sondern muss berechnet werden. Entropy kann für einen Stoff berechnet werden, der als Standard-Molar-Entropy aus absoluten Null (auch als absolute Eutropy bekannt) oder als Unterschied in Eutropy aus einem anderen Referenzstaat, der als Nulltropy definiert ist. Entropy hat die Dimension der durch Temperatur geteilten Energie, die im internationalen System der Einheiten eine Einheit von joules pro kelvin (J/K) besitzt. Obwohl dies die gleichen Einheiten wie die Wärmeleistung sind, sind die beiden Begriffe unterschiedlich. Entropy ist keine erhaltene Menge: zum Beispiel in einem isolierten System mit nicht-uniformer Temperatur, Wärme kann unumkehrbar und die Temperatur wird einheitlicher, so dass die Eutropie erhöht. Mit dem zweiten Thermodynamik-Gesetz wird festgestellt, dass ein geschlossenes System eine Eutropie hat, die sich erhöhen oder sonst konstant bleiben kann. Chemikalienreaktionen verursachen Veränderungen bei Eutropy und Eutropy spielt eine wichtige Rolle bei der Bestimmung, in welcher Richtung eine spontane chemische Reaktion führt. Laut einer Definition von Entropy ist es "eine Maßnahme der thermischen Energie pro Maßeinheit, die nicht für sinnvolle Arbeiten zur Verfügung steht". Zum Beispiel ist ein Stoff mit einheitlicher Temperatur bei maximaler Eutropie und kann nicht zu einem Wärmemotor führen. Eine Substanz bei nicht-uniformer Temperatur befindet sich in einer niedrigeren Eutropie (sofern die Wärmeverteilung sogar ausreicht) und einige der thermischen Energie kann einen Wärmemotor steuern. Ein Sonderfall der Entropy-Erhöhung, der Vermischungsgefahr, kommt, wenn zwei oder mehr verschiedene Stoffe gemischt werden. Liegt der Stoff an der gleichen Temperatur und Druck, gibt es keinen Nettoaustausch von Wärme oder Arbeit – der durch die Mischung der verschiedenen Stoffe bewirkte Temperaturwechsel ist vollständig. Auf statistischer mechanischer Ebene ergibt sich dies aufgrund der Änderung des verfügbaren Volumens pro Partikel mit Mischmischung. Gleichwertigkeit der Definitionen der Gleichwertigkeit zwischen der Definition von Entropy in statistischen Mechanismen (die Gibbs entropy Formel S = − k B  i i Log  i p i {\text S=-k_ Haushaltsrm Mathematik {B}p_{i ilog p_{i} {i}log p_{i}) und klassischen Thermodynamik ( d S = v revv T 7.8textstyle dS=ggiofrac Memedelta Q_TONtext{rev}}}{T zusammen mit dem fundamentalen thermodynamischen Zusammenhang) sind für das Mikrocanonical-Paket, das canonische Ensemble, das große canonisches Ensemble und das iso-isobarische Paket bekannt. Diese Nachweise basieren auf der Wahrscheinlichkeitsdichte von Mikrostaaten der allgemeinisierten Kaszmann-Vertrieb und der Bestimmung der thermodynamischen internen Energie als Ganzes U =  i E i  steuerlichedisplaystyle U=\left\langle E_{i}\right\rwinkel } Thermodynamische Beziehungen werden dann zur Auslösung der bekannten Gibbs entropy Formel eingesetzt. Jedoch ist die Gleichwertigkeit zwischen der Gibbs-Entropy-Formel und der thermodynamischen Definition von Entropy kein grundlegendes thermodynamisches Verhältnis, sondern eine Folge der Form der allgemeinen Verteilung von Poszmann. Zweites Recht der Thermodynamik Das zweite Thermodynamikgesetz verlangt, dass im Allgemeinen die gesamte Eutropie eines jeden Systems nicht anders als durch die Erhöhung der Eutropie einiger anderer Systeme verringert wird. In einem System, das von seiner Umwelt isoliert ist, wird die Eutropie dieses Systems nicht abnehmen. Es folgt, dass die Wärme nicht von einem kalteren Körper zu einem heißen Körper gelangen kann, ohne dass die Arbeit an den Kalten Körper angewendet wird. Zweitens ist es für alle Geräte, die auf einem Zyklus betrieben werden, unmöglich, Nettoarbeit aus einem einzigen Temperaturlager herzustellen; die Produktion von Nettoarbeit erfordert einen Wärmefluss aus einem heißen Lager in ein Kältereservoir oder ein einziges, expandierendes Reservoir, das einer adiabatic Kühlung unterzogen wird. Infolgedessen besteht keine Möglichkeit einer ständigen Bewegungsmaschine. Es folgt, dass eine Verringerung der Eutropie in einem bestimmten Prozess, wie einer chemischen Reaktion, eine energische Effizienz bedeutet. Nach dem zweiten Gesetz der Thermodynamik kann die Eutropy eines Systems, das nicht isoliert ist, abnehmen. Ein Klimaanlager kann zum Beispiel die Luft in einem Raum kühlen und somit die Luftverunreinigung dieses Systems verringern. Die Wärme aus dem Raum (das System), das die Luftzustands- und Ableitungen in die Außenluft transportiert, leistet immer einen größeren Beitrag zur Umweltverschmutzung als die Verringerung der Luftverunreinigung dieses Systems. Mit dem zweiten Gesetz über Thermodynamik erhöht sich also die Gesamttropy des Raums und die Umweltverschmutzung. In der Praxis beschränkt das zweite Gesetz in Verbindung mit den grundlegenden thermodynamischen Verhältnissen die Fähigkeit eines Systems, nützliche Arbeiten durchzuführen. Die entropy Veränderung eines Systems bei Temperatur T Memetext T}, die eine unendliche Menge von Hitze δ q {\textstyle \delta q} auf wiederverwendbare Weise aufnimmt, wird von δ q / T VILLEtext \delta q/T} . Mehr explizit, eine Energie T R S S VILLETextstyle T_{R}S ist nicht verfügbar, wenn R log text text text nl T_{R} die Temperatur des kalten oder des äußeren Systems ist. Für weitere Diskussionen siehe Exergy. Statistische Mechanismen zeigen, dass die Eutropie von der Wahrscheinlichkeit abhängig ist und somit auch in einem isolierten System zu einer Verringerung der Störung führt. Obwohl dies möglich ist, hat eine solche Veranstaltung eine geringe Wahrscheinlichkeit, dass sie unwahrscheinlich ist. Die Anwendbarkeit eines zweiten Thermodynamikgesetzes ist auf Systeme in der Nähe oder im Gleichgewichtszustand beschränkt. Gleichzeitig sind die Rechtsvorschriften, die Systeme weit von Gleichwertigkeit regeln, noch debatierbar. Eines der Leitprinzipien für solche Systeme ist das höchste Prinzip der Eutropieproduktion. Er behauptet, dass sich die nicht-equilibrium-Systeme entwickeln, wie die Maximierung ihrer Eutropieproduktion. Anwendungen Kernthermie Die Neutralität eines Systems hängt von seiner internen Energie und seinen externen Parametern wie ihrem Volumen ab. In der Thermodynamischen Grenze führt dies zu einer Formel, die die Änderung der internen Energie U KINGstyle U} auf Veränderungen der Eutropie und der externen Parameter betrifft. Diese Beziehung ist als grundlegende Thermodynamik bekannt. Liegt der externe Druck p Memedisplaystyle p} auf dem Volumen V {\displaystyle V} als einziger externer Parameter, ist dieses Verhältnis: d U = T d S − p d V Memestyle dU=T\,dS-p\,dV, da beide interne Energie und Entropy monotonische Funktionen der Temperatur T Memestyle T} sind, was bedeutet, dass die interne Energie bei einer Bestimmung des entropy und des Volumens fixiert ist, wenn dieses Verhältnis von einem anderen System und einem anderen System nicht vollstämmbar ist. Die fundamentale Thermodynamische Beziehung impliziert viele thermodynamische Identitäten, die allgemein gültig sind, unabhängig von den mikroskopischen Details des Systems. Wichtige Beispiele sind die Beziehungen zwischen Maxwell und die Beziehungen zwischen den Wärmekapazitäten. Antitropy in der chemischen Thermodynamik Thermodynamik Thermodynamik ist in der chemischen Thermodynamik von zentraler Bedeutung, wodurch Änderungen quantifiziert und das Ergebnis der Reaktionen vorhergesagt werden kann. Mit dem zweiten Thermodynamik-Gesetz wird festgestellt, dass die Austrophierung in einem isolierten System – die Kombination eines Subsystems, das in der Studie und ihrer Umgebung durchgeführt wird – bei allen spontanen chemischen und physikalischen Prozessen zunimmt.  of q rev / T =  S SKINGstyle \delta q_text{rev}}/T=\Delta S} führt die Messung des entropischen Wandels ein,  S S {\displaystyle \Delta S} .Entropy change beschreibt die Richtung und quantifiziert das Ausmaß der einfachen Veränderungen wie Wärmetransfer zwischen den Systemen – immer von Warmter bis zum Kühler spontan. Die thermodynamische Entropy hat daher die Dimension der Energie, die durch Temperatur geteilt wird, und die Einheit Joule pro kelvin (J/K) im Internationalen System der Einheiten (SI). Thermodynamische Entropy ist ein umfangreiches Eigentum, was bedeutet, dass es mit der Größe oder dem Umfang eines Systems übereinstimmt. In vielen Prozessen ist es sinnvoll, die Entropy als ein von der Größe unabhängiges Intensivobjekt als ein spezifisches entropisches Merkmal der untersuchten Systemart anzugeben. Konkrete Entropy kann im Verhältnis zu einer Masse ausgedrückt werden, in der Regel der Kilogramm (Einheit: J⋅kg -11K-1). Alternativ wird in der Chemie auch ein Stoff genannt, in dem es den molarentropy mit einer Einheit von JKmol -1⋅K-1. Wenn ein Stoff in etwa 0 K durch seine Umgebung auf 298 K erhitzt wird, ist die Summe der inkrementellen Werte von q rev / T RICHtext q_ 7.8text{rev}}/T ein Indikator für die Menge der von einem Stoff auf 298 K gespeicherten Energie. Entropy ändert auch die Mischung von Stoffen als Zusammenfassung ihrer relativen Mengen in der Endmischung. Entropy ist ebenso wichtig, um den Umfang und die Richtung komplexer chemischer Reaktionen vorherzusagen. Für solche Anwendungen muss  S S WELLdisplaystyle \Delta S} in einen Ausdruck aufgenommen werden, der sowohl das System als auch die Umgebung umfasst,  S S Universum =  S S System KINGstyle \Delta S_7.8text{universe==\\ Delta S_7.8text{surrunden++\ Delta S_ILLAtext{System}. Dieser Ausdruck wird über einige Schritte die Gibbs kostenlose Energie für Reaktionsanten und Produkte im System:  G G {\displaystyle \Delta G} [der Gibbs free energy change des Systems] =  H H {\displaystyle \=Delta H} [der enthalpy change] − T  S S KINGstyle -T\,\Delta S} [der entropy change.] Die technische Kapazität der Welt zur Speicherung und Kommunikation entroper Informationen Eine Studie von 2011 in der Wissenschaft (journal) hat die technologische Kapazität der Welt geschätzt, die weltweit besten Druckinformationen auf die wirksamsten Kompressionsgorithmen im Jahr 2007 zu speichern und zu übermitteln, wodurch die Eutropie der technisch verfügbaren Quellen eingeschätzt wird. Laut Schätzungen des Autors stieg die technische Kapazität der menschlichen Art, Informationen zu speichern, von 2,6 (imtropischen Druck) Exabyte im Jahre 1986 auf 295 (imtropischen Druck) Exabyte im Jahr 2007. Die technologische Fähigkeit der Welt, Informationen über ein-Wege-TV-Netze zu erhalten, betrug im Jahr 1986 432 Exabyte von (entropischen) Informationen bis 1,9 zettabyte im Jahr 2007. Die wirksame Fähigkeit der Welt, Informationen über zwei Telekommunikationsnetze auszutauschen, war im Jahr 1986 281 Heimtiere (entropischer Druck) bis 65 (entropischer Druck) Exabyte. Kontropie für offene Systeme In der Chemietechnik werden die Grundsätze der Thermodynamik häufig auf "offene Systeme" angewandt, d.h. auf diejenigen, in denen Wärme, Arbeit und Massenfluss über die Systemgrenze liegen. Wärmeströme ( Q   WELLdisplaystyle {Q} ) und Arbeiten, d. h. W  S S ggiodisplaystyle 7.8dot W W_ Fitext{S (Swellenarbeit) und P ( d V / d t ) Memedisplaystyle P(dV/dt)} (Druckarbeiten), über die Systemgrenzen hinweg, in der Regel Änderungen der Entropy des Systems. Übertragung als Wärme bedeutet die entropy Transfer Q  T / T {\displaystyle {Q}}/T , wo T {\displaystyle T} die absolute thermodynamische Temperatur des Systems zum Zeitpunkt des Wärmeflusses ist. Wenn es Massenströme über die Systemgrenzen gibt, beeinflussen sie auch die gesamte Eutropie des Systems. In Bezug auf Wärme und Arbeit gilt dieses Konto nur für Fälle, in denen die Arbeits- und Wärmeübertragungen per Wege physisch von den Eingangs- und Ausstiegspfaden des Systems unterscheiden. Um eine allgemeinisierte, ausgewogene Formel zu schließen, beginnen wir mit der allgemeinen Gleichgewichtsform für den Wandel in jeder umfangreichen Menge   displaystyle \theta } in einem thermodynamischen System, einer Menge, die entweder erhalten werden kann, wie Energie oder nichtkonservierte, wie z.B. Entropy. Der grundlegende Ausdruck des generischen Gleichgewichts stellt fest, dass d  t / d t Memedisplaystyle d\theta/dt}, d. h. die Änderung der   Memestyle \theta } im System entspricht, der dem Satz entspricht, an dem θ Memestyle \theta } das System an den Grenzen einführt, abzüglich des Kurses, an dem {\ KINGstyle \theta \theta \theta \theta } über die Systemgrenzen hinwegtrifft. Für ein offenes Thermodynamisches System, in dem Wärme und Arbeit durch unterschiedliche Pfade für die Übertragung von Materie übertragen werden, unter Verwendung dieser generischen Gleichgewichtsform, in Bezug auf die Änderungsgeschwindigkeit mit Zeit t Memestyle t} der umfangreichen Menge entropy S HANAdisplaystyle S}, die entropy Balance ist: d S d t =  k k S ^ k S ^ + Q  T T S   gen KINGstyle SSOfrac dS=dt==\sum _k=1K M__{k}{\hat S__{k} Q}}{T++S S__Text{gen, wo ∑ k = 1 K M ̇ k S ^ k displaystyle \sum _k=1}^{ K}{\dot M__{k}{\hat S__{k ist die Nettorate des entropischen Stroms aufgrund der Massenströme in das System und aus dem System (wo S ^ {\displaystyle Memehat {S} pro Einheit ist). Q   T RARstyle Memefrac Memedot Q}}{ T ist die Rate des entropischen Stroms aufgrund des Wärmeflusses in der Systemgrenze. S . gen {\displaystyle WELLdot S S_ggiotext{gen ist die Rate der entropischen Produktion im System. Diese entropische Produktion entsteht aus Prozessen innerhalb des Systems, einschließlich chemischer Reaktionen, der internen Materienverbreitung, der internen Wärmeübertragung und der im System auftretenden widersprüchlichen Auswirkungen von mechanischem Arbeitstransfer bis zum System. Wenn es mehrere Wärmeströme gibt, wird der Begriff Q ̇ / T {\displaystyle {Q}}/T durch  j Q  j j / T j , Memetext \sum \sum Memedot Q__{j}/T_{j, wo Q  j j RARstyle WELLdot Qj_{j ist der Wärmefluss und T j {\displaystyle T_{j} die Temperatur am j Memestyle j} den Wärmestromhafen in das System. Hinweis darauf, dass die Nomenklatur "entropy Balance" irreführend ist und häufig als unangemessen angesehen wird, weil die Eutropie keine erhaltene Menge ist. In anderen Worten ist der Begriff S ̇ Genom WELLdisplaystyle Memedot S S_ Kapitalmarkt[gen] nie eine bekannte Menge, sondern immer ein abgeleiteter auf dem oben genannten Ausdruck. Infolgedessen wird die offene Systemversion des zweiten Gesetzes besser als die "tropische Erzeugungsform" beschrieben, da sie klarstellt, dass S . gen ≥ 0 Memedisplaystyle Memedot S__ figen}}\geq 0}, mit Null für umkehrbare Prozesse oder mehr als für unwiderrufliche Prozesse. Formeln für einfache Prozesse Manche einfache Umwandlungen in Systeme der konstanten Zusammensetzung werden durch einfache Formeln vorgenommen. Isothermische Expansion oder Kompression eines idealen Gases für die Expansion (oder Kompression) eines idealen Gases von einem anfänglichen Volumen V 0 Memestyle V_{0} und Druck P 0 {\displaystyle P_{0} auf ein Endvolumen V HANAdisplaystyle V} und Druck P Memedisplaystyle P} auf einer konstanten Temperatur, wird der Wechsel der Eutropy durch:  S S = n R V 0 = n R R V = n R R P 0  l P  l P 0 . P . P 0 . KINGstyle \Delta S=nR\ln Memefrac VVV_{0==-nR\ln Memefrac P.P_{0. n Memedisplaystyle n} ist die Zahl der Muscheln von Gas und R Memedisplaystyle R} ist die ideale Erdgaskontinuität. Diese Gleichungen gelten auch für die Erweiterung in ein Flossen-Schlag oder einen dreistufigen Prozess, bei dem die Temperatur, die interne Energie und die enthalpy für ein ideales Gas konstant bleiben. Kühlen und Heizungen für Heizung oder Kühlung eines beliebigen Systems (Gas, Flüssigkeit oder feste) mit konstantem Druck von einer anfänglichen Temperatur T 0 Memestyle T_{0} auf eine Endtemperatur T Memestyle T}, die Entropy Veränderung ist  S S = n C P l  T T 0 . KINGstyle \Delta S=nC_{P}\ln ggiofrac TTT_{0. vorausgesetzt, dass die ständige Druckkraft (oder bestimmte Wärme) KP konstant ist und in diesem Temperaturintervall keine Übergangsphase stattfindet. Gleiches bei konstantem Volumen ist der entropy Wandel  S S = n C v ln  T T 0 , \Delta S=nC_{v}\ln ggiofrac TTT_{0, wo die ständigen Molybdänkapazitäten für Kleinkraftanlagen Cv ist konstant und es gibt keine Phasewechsel. Niedrige Temperaturen in der Nähe von absolut Null fallen die Wärmekapazitäten von Feststoffen schnell auf nahezu Null, so dass die Annahme der ständigen Wärmekapazität nicht anwendbar ist. Da es sich bei der Eutropie um eine staatliche Funktion handelt, ist dietropy Änderung eines jeden Verfahrens, in dem Temperatur und Volumen beide variieren, die gleiche wie für einen Pfad, der in zwei Stufen unterteilt ist – Heizung bei konstantem Volumen und Erweiterung bei konstanter Temperatur. Für ein ideales Gas ist die Gesamttropy Veränderung . S = n C v ln  T T 0 + n R ln  V V 0. KINGstyle \Delta S=nC_{v}\ln ggiofrac TTT_{0}}}+nR\ln Memefrac VVV_{0. . S = n C P ln   T 0  - n R ln  C P 0 . KINGstyle \Delta S=nC_{P}\ln ggiofrac T}{T_{0}}}-nR\ln Memefrac P.P_{0. Übergangsphase Umkehrbare Übergangsphasen treten bei konstanter Temperatur und Druck auf. Die reversible Wärme ist der enthalpy Wandel für den Übergang, und der entropy Wandel ist der durch die Thermodynamische Temperatur geteilte Wolfswechsel. Fusion (Verschmelzung) einer festen Flüssigkeit am Schmelzpunkt Tm, die Entropy of Fusion ist  S Sfus =  H H Fus T m . KINGstyle \Delta S_ggiotext{fus==ggiofrac 7.8Delta H_Fittext{fus}}}{T_ggiotext{m. Gleiches gilt für die Verdampfung einer Flüssigkeit zu einem Gas an der Kochstelle Tb, die Entropy von Verdampfung ist  S S vap =  H H vap Tb . 7.8displaystyle \Delta S_ggiotext{vap== 7.8Delta H_ggiotext{vap}}}{T_ggiotext{b. Konzepte zum Verständnis von Eutropy Als grundlegender Aspekt der Thermodynamik und der Physik gelten mehrere unterschiedliche Ansätze zur Eutropie über die von Clausius und Poszmann. Standardtextbuch-Definitionen Liste der zusätzlichen Definitionen von Entropy aus einer Sammlung von Textbüchern: eine Maßnahme der Energiedistribution bei einer bestimmten Temperatur.a Maßnahme der Störung im Universum oder der Verfügbarkeit der Energie in einem System, um zu arbeiten.a Maßnahme der thermischen Energie pro Einheitstemperatur, die für nützliche Arbeiten nicht verfügbar ist. entropy ist eine Maßnahme der Anzahl möglicher mikroskopischer Staaten (oder Mikrostaaten) eines Systems in Thermodynamik. Im Einklang mit der Definition von Poszmann muss das zweite Thermodynamikgesetz neu formuliert werden, da die Eutropie im Laufe der Zeit zunimmt, obwohl das zugrunde liegende Prinzip unverändert bleibt. Ordnung und Störung Entropy sind oft lose mit der Auftrags- oder Störungsmenge oder dem Chaos in einem thermodynamischen System verbunden. Die traditionelle qualitative Beschreibung von Eutropy bezieht sich auf Veränderungen des Status quo des Systems und ist eine Maßnahme der "molekularen Störung" und die Menge der verschuldeten Energie in einer dynamischen Energieumwandlung aus einem Staat oder einer Form. In dieser Richtung haben mehrere Autoren genaue Eutropy-Formeln abgeleitet, um Störungen und Ordnung in Atom- und Molekularversammlungen zu berücksichtigen und zu messen. Einer der einfacheren entropyen Order/disorder Formeln ist, die 1984 von Thermodynamischen Physikern Peter Landsberg auf der Grundlage einer Kombination aus Thermodynamik und Informationstheorie gewonnen wurden. Er plädiert dafür, dass bei Problemen, die auf einem System betrieben werden, verhindert wird, dass ein oder mehrere seiner möglichen oder zulässigen Staaten, wie mit den verbotenen Staaten, die Maßnahme der Gesamtzahl der Störungen im System von: Disorder = C CI . KINGstyle Disorder.={C_customtext{D \over C_customtext{I.,} Ebenso wird der Gesamtauftragsbetrag im System von: Order = 1 − C O C I . KINGstyle Text Order==1-{C_customtext{O \over C_customtext{I,} CD ist die Störungskapazität des Systems, das die Eutropie der im erlaubten Paket enthaltenen Teile ist, CI ist die Informationskapazität des Systems, ein Ausdruck ähnlich wie die Kanalkapazität von Shannon, und CO ist die Auftragskapazität des Systems. Energieverteilung In qualitativer Hinsicht kann das Konzept der Entropy als Maßnahme der Energiedistribution bei einer bestimmten Temperatur beschrieben werden. Ähnliche Begriffe wurden bereits früh in der Geschichte der klassischen Thermodynamik verwendet, und mit der Entwicklung statistischer Thermodynamik und Quantentheorie wurden die entropischen Veränderungen im Hinblick auf die Mischung oder Verbreitung der Gesamtenergie jedes einzelnen Bestandteils eines Systems über seine spezifischen quantifizierten Energieniveaus beschrieben. Ambiguities in Bezug auf Begriffsstörungen und Chaos, die in der Regel direkt gegen das Gleichgewicht verstoßen, tragen zu einer weit verbreiteten Verwechslung bei und behindern das Verständnis von Eutropy für die meisten Studierenden. Wie das zweite Thermodynamikgesetz zeigt, werden in einem isolierten System interne Portionen an verschiedenen Temperaturen tendenziell an eine einheitliche Temperatur angepasst und stellen somit ein Gleichgewicht her. Ein unlängst entwickelter pädagogischer Ansatz vermeidet unmissverständliche Bedingungen und beschreibt diese Ausbreitung von Energie als Dispersion, was zu einem Verlust der für die Arbeit erforderlichen Unterschiede führt, auch wenn die Gesamtenergie im Einklang mit dem ersten thermodynamischen Recht (vergleichbare Diskussion im nächsten Abschnitt). Physikalisch Peter Atkins, in seinem Textbuch Physikalische Chemie, führt mit der Erklärung ein, dass "spontane Veränderungen immer mit einer Streuung der Energie einhergehen". Übergreifen der Energienutzung Nach oben ist es möglich (in einem thermischen Kontext), niedrigere Eutropie als Indikator oder Maß an Wirksamkeit oder Nutzen einer bestimmten Energiemenge zu betrachten. Dies ist, weil Energie, die an einer höheren Temperatur (d. h. mit niedrigem Entropy) geliefert wird, eher nützlich ist als die gleiche Energiemenge, die bei niedrigerer Temperatur zur Verfügung steht. Mischen Sie ein heißes Paket einer Flüssigkeit mit einem Kalten ein Paket von Zwischentemperaturen, in dem die Gesamterhöhung von Eutropy einen Verlust darstellt, der nie ersetzt werden kann. So ist die Tatsache, dass die Eutropy des Universums stetig wächst, bedeutet, dass seine Gesamtenergie weniger nützlich ist: schließlich führt dies zu dem „Heizenster des Universums“. E.H.Lieb und J. Yngvason haben 1999 eine Definition von Eutropy auf der Grundlage einer adiabaticen Zugänglichkeit zwischen Gleichgewichtsstaaten erhalten. Dieser Ansatz umfasst mehrere Vorgänger, darunter die Pionierarbeit von Constantin Carathéodory aus dem Jahr 1929 und die Monographie von R. Giles. Bei der Festsetzung von Lith und Yngvason beginnt, für eine Menge der betreffenden Substanz zwei Referenzstaaten X 0 HANAstyle X_{0} und X 1 {\displaystyle X_1}, so dass die letztere von der früheren, aber nicht umgekehrt adiabattisch zugänglich ist. Bestimmung der Eutropen der Referenzstaaten, die 0 bzw. 1 sind, der Eutropy eines Staates X HANAstyle X} ist definiert als die größte Zahl der {\ \lambda }, so dass X {\displaystyle X} von einem zusammengesetzten Staat, bestehend aus einem Betrag {\ Memestyle \lambda } im Staat X1 {1} und einem zusätzlichen Betrag ( 1  1 .  1  apart  1,  the dia) ist ein reiner Bestandteil, der  X }  0  X  X  X  0  0  0  X  X {\  X  X  X  X  X  X  X  X  X  X  X  X  X . . . . . . . . . } Es ist monotonisch in Bezug auf die adiabatic Barrierefreiheit, den Zusatz von Verbundsystemen und die umfangreiche Unterstufung. Eutropy in Quantentechnik In Quantenstatistiken wurde das Konzept der Entropy von John von Neumann entwickelt und wird allgemein als "von Düsseldorf entropy" bezeichnet. − k B Tr ⁡ (ρ Log     ) ) , HANAdisplaystyle S=-k_chinorm {B} .operator {Tr} (\rho \rho \rho \rho )}, wo . die Dichtematrix und Tr ist der Rückverfolger. Das Prinzip des Briefwechsels, weil in der klassischen Grenze die Phasen zwischen den für die klassische Probabilities verwendeten Basisstaaten rein zufällig sind, entspricht dieser Ausdruck der vertrauten klassischen Definition von entropy, S = − k B  i i p i Log p i , {\displaystyle S=-k_9.5rm {B}  isum i}p_{i.,\log \p_{i}, d. h. in einer solchen Basis ist die Dichtematrix diagonal. Von Neumann hat einen strengen mathematischen Rahmen für Quantenmechaniken mit seinem Werk Mathematische Grundlagen der Kunst geschaffen. Er stellte in dieser Arbeit eine Messtheorie vor, in der die übliche Begriffsbestimmung der Wellenfunktion als unwiderruflicher Prozess bezeichnet wird (die sogenannten von Neumann oder die projizierte Messung). In Verbindung mit der Dichtematrix verlängerte er das klassische Konzept der Eutropie in den Quantenbereich. Informationstheorie In Bezug auf die Informationstheorie ist die Funktion des Eutropischen Staates die Menge der Informationen im System, die erforderlich ist, um den Mikrostaat des Systems vollständig zu bestimmen. Entropy ist das Maß fehlender Informationen vor dem Empfang. Häufig bezeichnete Shannon entropy, es wurde ursprünglich von Claude Shannon im Jahr 1948 entworfen, um die Größe der Informationen einer übermittelten Botschaft zu untersuchen. Die Definition von Informationen wird in Bezug auf ein gesondertes Paket von Probabilities p i {\displaystyle p_{i} ausgedrückt, so dass H ( X) =  i i = 1 n p ( x i) Log . p ( x i) . KINGstyle H(X)=-\sum i=1}^{n}p(x_{i})\log p(x_{i)} Im Falle der übermittelten Nachrichten waren diese Probabilities die Voraussetzungen, dass eine bestimmte Botschaft tatsächlich übertragen wurde und die Eutropie des Nachrichtensystems eine Maßnahme der durchschnittlichen Größe der Informationen einer Botschaft darstellte. Im Falle der Chancengleichheit (d.h. jede Botschaft ist gleichermaßen wahrscheinlich), ist die Shannon-Entropy (in Bits) nur die Anzahl der binären Fragen, die zur Bestimmung des Inhalts der Botschaft erforderlich sind. Die meisten Forscher betrachten Informationen, die direkt mit dem gleichen Konzept verbunden sind, und andere argumentieren, dass sie voneinander getrennt sind. Beide Begriffe sind mathematisch ähnlich. W KINGstyle W} ist die Zahl der Mikrostaaten, die einen bestimmten gesamtstaatlichen Zustand erzielen können, und jeder Mikrostaat hat die gleiche Vorlaufzeit, dann ist diese Wahrscheinlichkeit 1 = p = 1 / W {\displaystyle 1=p=1/W} .Die Shannon entropy (in nats) ist: H =  i i = 1 P Log   ( p ) = Log W ( W ) 7.8displaydisplaystyle H=-\sum _i=1}^{W}p\log(p)=\log(W) und entropy wird in Blöcken der k Memestyle k} pro nat gemessen, dann wird die Eutropy von: H = k Log . (W ) Memestyle H=k\log(W)}, die die Formel "Bozmann entropy" ist, wo k Memenstyle k} ist, dass es sich ständig um die thermotropyische Formel handelt. Manche Autoren plädieren dafür, dass das Wort entropy für die H {\displaystyle H} Funktion der Informationstheorie und Verwendung des anderen Begriffs Shannon, Unsicherheit, statt. Messung Die Eutropie eines Stoffes kann gemessen werden, auch mittelbar. Die Messung, die als entropymetrie bekannt ist, erfolgt auf einem geschlossenen System (mit Partikelzahl N und Volumen V ist konstant) und verwendet die Definition von Temperatur im Hinblick auf Entropy, während der Energieaustausch auf Wärme ( d U → d Q KINGstyle dU\rightmark dQ} ) T:= ( of U  U Sight ) V  N } } } }  S {\  S  S  S  = {\  =  =  =  =  =  /  /  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  In diesem Zusammenhang wird beschrieben, wie entropy Veränderungen d S Memestyle dS} vorgenommen werden, wenn ein kleiner Teil der Energie d Q {\displaystyle dQ} in das System mit einer bestimmten Temperatur T Memestyle T} eingeführt wird. Der Messprozess geht wie folgt. Erstens wird eine Probe des Stoffes so nah wie möglich auf absolute Null abgekühlt. In solchen Temperaturen sind die Eutropy-Ansätze Null – aufgrund der Definition der Temperatur. Dann werden kleine Wärmemengen in die Probe genommen und die Temperaturänderung wird aufgezeichnet, bis die Temperatur einen gewünschten Wert erreicht (normalerweise 25 °C). Die erhaltenen Daten ermöglichen es dem Benutzer, die vorstehende Formel zu integrieren, was den absoluten Wert der Eutropie des Stoffes bei der Endtemperatur ergibt. Dieser Wert von Eutropy wird als kalorimetrische Entropy bezeichnet. interdisziplinäre Anwendungen Obwohl das Konzept der Entropy ursprünglich ein thermodynamisches Konzept war, wurde es in anderen Bereichen der Studie, einschließlich der Informationstheorie, der Psychodynamik, der Thermoökonomie und der Entwicklung, angepasst. Zum Beispiel wurde ein entropisches Argument vorgeschlagen, um die Präferenz von Schikanen bei der Wahl eines geeigneten Gebiets für die Festsetzung ihrer Eier zu erklären. Mit dieser Erweiterung der Felder/Systeme, auf denen das zweite Thermodynamikgesetz anwendbar ist, hat sich auch die Bedeutung des Wortes ausgebreitet und basiert auf der Antriebsenergie für dieses System. Mit dem Konzept werden die Systeme in drei Kategorien unterteilt: natürliche, hybride und vom Menschen hergestellte Systeme, basierend auf der Kontrollmenge, die der Menschen den unüberwindbaren Sprung der Eutropy und die Zeitspanne jeder Kategorie bremst, um eine maximale Eutropie zu erreichen. Liste der entropy-Themen in Thermodynamik und statistischen Mechanismen Entropy Einheit – eine nicht-S.I-Einheit von Thermodynamik, in der Regel e.u und gleich einer Kalorien pro kelvin pro mole oder 4.184 joules pro kelvin pro mole. Gibbs entropy – die übliche statistische mechanische Entropy eines thermodynamischen Systems. Ziegenzmann entropy – eine Art von Gibbs entropy, die interne statistische Korrelationen im Gesamtteil der Partikelverteilung vernachlässigt. Tsallis entropy – eine Generalisierung der Standard-Sentropy-Bozmann-Gibby. Standardmolar entropy – ist der entropische Inhalt eines Stoffes unter Standardtemperatur und Druck. Rückfall – das Eutropy-Produkt nach einem Stoff wird willkürlich in der Nähe von absoluten Null abgekühlt. Kontamination der Mischung – die Änderung der Eutropie, wenn zwei verschiedene chemische Stoffe oder Komponenten gemischt werden. entropy – ist die Eutropy, die zwei Rückstände eines Polymers innerhalb einer vorgeschriebenen Entfernung vereint. Conformational entropy – ist die Eutropy, die mit der physischen Anordnung einer Polymerkette verbunden ist, die einen kompakten oder klimatischen Zustand in der Lösung übernimmt. Tropische Kraft – eine mikroskopische Kraft oder Reaktionsneigung im Zusammenhang mit Systemorganisationänderungen, molekularen Relikationen und statistischen Schwankungen. Free entropy – ein entropisches thermodynamisches Potenzial, das dem freien Energie entspricht. Entrop Explosion – eine Explosion, bei der die Reaktionsanten eine große Veränderung des Volumens durchlaufen, ohne eine große Wärmemenge zu mieten. Entropy Change – eine Änderung der Entropy dS zwischen zwei Gleichgewichtsstaaten erfolgt durch die von der absoluten Temperatur T des Systems in diesem Zeitraum geteilte Wärmemenge. Sackur-Tetrode entropy – die Eutropy eines monatomischen Idealgases, das über Quantenbelange bestimmt ist. Pfeil der Zeit Entropy ist die einzige Menge in den physikalischen Wissenschaften, die offenbar eine besondere Richtung des Fortschritts mit sich bringt, manchmal als Pfeiler der Zeit. In Zeiten der Fortschritte stellt das zweite Thermodynamikgesetz fest, dass die Eutropie eines isolierten Systems in großen Systemen nie über wichtige Zeiträume hinweg sinkt. Aus dieser Perspektive wird daher die Eutropy-Messung als Uhr in diesen Bedingungen betrachtet. Entropy von DNA-Sequenzen hat sich bei der Analyse von DNA-Sequen als nützlich erwiesen. Viele entropy-basierte Maßnahmen haben gezeigt, dass zwischen den verschiedenen Strukturregionen des Genoms unterschieden wird, zwischen den Kodierungs- und Nicht-Kodierungsregionen von DNA unterscheiden und auch für die Erholung von evolutionären Bäumen angewendet werden können, indem die Entfernung zwischen verschiedenen Arten bestimmt wird. Mit dem zweiten Gesetz der Thermodynamik wird festgestellt, dass die Gesamttropy stetig zunimmt. Seit dem 19. Jahrhundert ist es spekuliert, dass das Universum zu einem Wärmetod, in dem alle Energie als homogene Verteilung der Wärmeenergie endet, so dass keine Arbeit aus jeder Quelle gewonnen werden kann. Wenn das Universum in der Regel zunehmend entropy haben kann, so wie Roger Penrose hervorgehoben hat, spielt die Schwere eine wichtige Rolle in der Erhöhung, da die Schwerlast zu einer akkumulierten Materie führt, die schließlich zu schwarzen Löchern kam. Die Eutropie eines schwarzen Lochs ist proportional zur Oberflächenfläche des schwarzen Lochs. Jacob Bekenstein und Stephen Johnsoning haben gezeigt, dass schwarz Löcher die größtmögliche Eutropie eines Gegenstands gleicher Größe aufweisen. Dies führt zu Endepunkten aller entropy-Erhöhungsprozesse, wenn sie absolut wirksam sind und Energiefallen. Jedoch könnte der Energieausbruch aus schwarzen Löchern aufgrund von Quantenaktivität möglich sein (vgl. Die Rolle der Entropy in der Kosmologie bleibt seit dem Zeitpunkt des Ludwig-Schuzmanns ein kontroverses Thema. Jüngste Arbeiten haben Zweifel an der Wärmesterblichkeit und der Anwendbarkeit eines einfachen thermodynamischen Modells für das Universum im Allgemeinen. Obwohl die Entropy das Modell eines expandierenden Universums erhöht, erhöht sich die höchstmögliche Eutropy-Erhöhung viel schneller und bewegt das Universum mit Zeit, nicht näher. In einem "entropy Gap" wird das System weiter weg von der gemahlenenen Wärmesterblichkeit gedrängt. Anderelic Faktoren, wie die Energiedichte des Vakuums und makroskopische Quanteneffekte, sind schwer zu vereinbaren mit Thermodynamischen Modellen, was Vorhersagen großer Thermodynamik extrem schwierig macht. Aktuelle Theorien deuten darauf hin, dass die entropy Lücke zunächst durch die rasche exponentielle Expansion des Universums eröffnet wurde. Wirtschaft rumänischer US-Wirtschaftswissenschaftler Nicholas Georgescu-Roegen, ein Progenitor in Wirtschaft und ein Paradigma- Gründer von ökologischen Wirtschaften, nutzte das Konzept der Entropy in seiner Größenordnung über das Entropy-Gesetz und den wirtschaftlichen Prozess. Infolge der Arbeit von Georgescu-Roegen bilden die Gesetze der Thermodynamik nun ein integraler Bestandteil der ökologischen Wirtschaft. Obwohl seine Arbeit durch Fehler etwas abgeschwächt wurde, wurde ein vollständiges Kapitel über die Wirtschaft von Georgescu-Roegen in ein elementares Physik-Textbuch über die historische Entwicklung von Thermodynamik aufgenommen. Georgescu-Roegens Arbeit hat den Begriff „Entropy pessimism“ erzeugt. Seit den 90er Jahren war der führende ökologische Wirtschaftswissenschaftler und der stetige Gast Herman Daly – ein Studenten von Georgescu-Roegen – der einflussreichste Befürworter der entropy pessimism Position. Hermeneutics In Hermeneutics, Arianna Béatrice Fabbricatore hat den Begriff „Entropy“ verwendet, der sich auf die Werke von Umberto Eco stützt, um den Verlust der Bedeutung zwischen der verbalen Beschreibung von Tanz und dem Choreotext (die durch den Tanzfans in der Durchführung des dokumentarischen Schriftschreibens) zu ermitteln und zu bewerten, der durch intersemiotische Übersetzungsoperationen entsteht. Diese Verwendung ist mit den Begriffen Logotext und choreotext verknüpft. Im Übergang vom Logotext zu choreotext ist es möglich, zwei Typologien von Entropy zu identifizieren: die erste, als Natur bezeichnete, ist mit der Einzigartigkeit des Durchführungsrechts und seinem ephemeralen Charakter verbunden. Die zweite wird durch Unstimmigkeiten mehr oder weniger wichtig im Logotext (d. h. den mündlichen Text, der den Akt widerspiegelt). Siehe auch HinweiseFurther Lesen Adam, Gerhard; Otto Hittmair (1992). Wärmephysik. Blickeg, Braunschweig.ISBN gegen 3-528-33311-9.Atkins, Peter; Julio De Paula (2006) Physikalische Chemie (8. Oxford University Press. JAHRESSPRODUKTE, JAHR 2003. Wärmephysik. Cambridge University Press.ISBN gegen0-521-65838-6.Ben-Naim, Arieh (2007) Entropy Demys ratifiziert. World Scientific.ISBN gegen 981-270-055-1.Callen, Herbert, B (2001). Thermodynamik und Einführung zu Thermostaten (2. John Kuhn und Sons.ISBN gegen0-471-86256-7.Chang, Raymond (1998). Chemie (6. New York: McGraw Hill.ISBNUR0-07-115221-1. John, D; Johnson, Kenneth, J. (1998). Physik (4. John Kuhn und Sons, Inc. ISBN @0-471-19113-1.Dugdale, J. S. (1996). Entropy und seine materielle Bedeutung (2. ed). Taylor und Francis (UK); CRC (US). [0-7484-0569-5.Fermi, Enrico (1937). Thermodynamik. Prentice Hall.ISBNGoldstein, Martin; Inge, F (1993). Kühlschränke und Universum. Harvard University Press.ISBN gegen0-674-75325-9.Gyftopoulos, E.P; G.P roller (2010) Thermodynamik. Stiftungen und Anwendungen. Dover.ISBN gegen0-486-43932-7.Haddad, Wassim M;. Chellaboina, VijaySekhar; Nersov, Sergey G. (2005). Thermodynamik – Ein dynamisches Systemkonzept.Princeton Universitätspresse.ISBN gegen0-691-12327-1.Johnson, Eric Abdul. Angst und die Equation: Verständnis für die Entropy von Danzmann. The MIT Press. ISBN:0-262-03861-4.Kroemer, Herbert; Charles Kittel. Wärmephysik (2. W H. Freeman Company.ISBN Vor0-7167-1088-2.Lambert, Frank L; entropysite.oxy. Müller-Kirsten, Harald J. W. (2013) Grundlagen der statistischen Physik (2. ed). Singapur: World Scientific.ISBN gegen 981-4449-53-3.Penrose, Roger (2005). Realität: Ein vollständiger Leitfaden für die Gesetze des Universums. New York: A. A. Knopf.ISBN UV0-679-45443-4.Reif, F. (1993). Grundlagen der statistischen und thermischen Physik. McGraw-Hill.ISBN:30-07-051800-1.Schroeder, Daniel V. (2000). Einführung in Wärmephysik. New York: Roc Longman.ISBN UV0-201-38027-9.Serway, Raymond, A. (1992). Physik für Wissenschaftler und Ingenieure. Saunders Golden Subburst Serie.ISBN NACH0-036026-0.Sharp, Kim (2019). Entropy und der Tao of Counting: Eine kurze Einführung in statistische Anlagen und das zweite Gesetz über Thermodynamik (Frühe in Physik). Springer Nature.ISBN:30354596.Spirax-Sarco Limited, Entropy – eine grundlegende Vereinbarung Leitmotiv für Dampftechnik vonBaeyer; Hans Christian (1998). Maxwell's Demonstrierung: Warme und Time Passes. Random House.ISBN:0-679-43342-2. Außenbeziehungen Entropy und das zweite Gesetz über Thermodynamik – ein A-Level-physik-Lehrstuhl mit der Entivation von entropy auf der Carnot Zyklus Khan Academy: entropy Vorlesungen, Teil der Chemie-SpiellisteProof: S (oder Entropy) ist ein gültiger variabeler Begriff des thermodynamischen Entropy Definition Clarification Reconciling Thermodynamik und Definitionen von Entropy Entropy Entropy Entropy Entropy Second Law der Thermodynamik und Entropy Entropy – Entropy 20 (JI) Entdeckung von Entropy durch Adam Shulman. Langes Video, Januar 2013. Moriarty, Philip; Merrifield, Michael (2009)."S Entropy"Sixty-Symbole. Brady Haran für die Universität Nottingham. Entropy at Scholarpä