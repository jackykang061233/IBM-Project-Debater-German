Unicode ist ein Informationstechnologie-Standard für die konsequente Kodierung, Darstellung und Handhabung von Texten, die in den meisten der weltweiten Schreibsysteme zum Ausdruck gebracht werden. Der Standard, der durch das Unicode-Konsortium erhalten wird, definiert 143,859 Zeichen, die 154 moderne und historische Skripte sowie Symbole, Emoji und nicht-visuelle Steuerungs- und Formatierungscodes abdecken. Unicode 14.0 ist jetzt in Beta mit 144.697 Zeichen, darunter 5 neue Skripte und 37 neue Emoji Zeichen. Das Unicode-Zeichenrepertoire wird mit ISO/IEC 10646 synchronisiert, wobei jeder Code-für-Code identisch mit dem anderen ist. Der Unicode Standard enthält jedoch mehr als nur den Basiscode. Neben den Zeichencodierungen enthält die offizielle Publikation des Konsortiums eine Vielzahl von Details zu den Skripten und wie sie angezeigt werden können: Normalisierungsregeln, Zersetzung, Kollation, Rendering und bidirektionale Textanzeigeauftrag für mehrsprachige Texte usw. Der Standard enthält auch Referenzdatendateien und visuelle Diagramme, um Entwicklern und Designern bei der korrekten Umsetzung des Repertoires zu helfen. Unicodes Erfolg bei der Vereinheitlichung von Zeichensätzen hat zu seiner weit verbreiteten und überwiegenden Verwendung bei der Internationalisierung und Lokalisierung von Computersoftware geführt. Der Standard wurde in vielen neueren Technologien implementiert, darunter moderne Betriebssysteme, XML, Java (und andere Programmiersprachen), und das .NET Framework. Unicode kann durch verschiedene Zeichencodierungen realisiert werden. Der Unicode-Standard definiert Unicode Transformation Formate (UTF): UTF-8, UTF-16 und UTF-32 und mehrere andere Codierungen. Die am häufigsten verwendeten Kodierungen sind UTF-8, UTF-16 und der veraltete UCS-2 (ein Vorläufer von UTF-16 ohne volle Unterstützung für Unicode);GB18030, während kein offizieller Unicode Standard, ist in China standardisiert und implementiert Unicode vollständig. UTF-8, die dominante Kodierung auf dem World Wide Web (verwendet in über 95% der Webseiten ab 2020 und bis zu 100% für einige Sprachen) und auf den meisten Unix-ähnlichen Betriebssystemen, verwendet ein Byte (8 Bits) für die ersten 128 Codepunkte und bis zu 4 Bytes für andere Zeichen. Die ersten 128 Unicode-Codepunkte stellen die ASCII-Zeichen dar, was bedeutet, dass jeder ASCII-Text auch ein UTF-8-Text ist. UCS-2 verwendet zwei Bytes (16 Bits) für jedes Zeichen, kann aber nur die ersten 65,536 Codepunkte, die sogenannte Basic Multilingual Plane (BMP) kodieren. Mit 1,112,064 möglichen Unicode-Code-Punkten entsprechend Zeichen (siehe unten) auf 17 Ebenen und mit über 143,000 Code-Punkten definiert als Version 13.0, UCS-2 ist nur in der Lage, weniger als die Hälfte aller codierten Unicode-Zeichen zu repräsentieren. Daher ist UCS-2 obsolet, obwohl noch in Software verwendet. UTF-16 erweitert UCS-2, indem die gleiche 16-Bit-Codierung wie UCS-2 für die Basic Multilingual Plane verwendet wird, und eine 4-Byte-Codierung für die anderen Ebenen. Solange es keine Codepunkte im reservierten Bereich U+D800–U+DFFF enthält, ist ein UCS-2 Text gültig UTF-16 Text. UTF-32 (auch als UCS-4) bezeichnet) verwendet vier Bytes, um einen beliebigen Codepunkt zu kodieren, jedoch nicht notwendigerweise ein bestimmtes Benutzer-perceived-Zeichen (sprachlich, ein Graphem), da ein Benutzer-perceived-Zeichen durch einen Graphem-Cluster (eine Sequenz von mehreren Codepunkten) dargestellt werden kann. Wie UCS-2 wird die Anzahl der Bytes pro Codepunkt festgelegt, wodurch die Codepunkt-Indizierung erleichtert wird; im Gegensatz zu UCS-2 kann UTF-32 alle Unicode-Codepunkte kodieren. Da jedoch jeder Codepunkt vier Bytes verwendet, nimmt UTF-32 deutlich mehr Platz als andere Codierungen und ist nicht weit verbreitet. Obwohl UTF-32 für jeden Codepunkt eine feste Größe hat, ist es auch hinsichtlich benutzerdefinierter Zeichen variable Länge. Beispiele sind: die Devanagari kshi, die mit 4 Codepunkten kodiert ist, und nationale Flag emojis, die aus zwei Codepunkten bestehen. Alle Kombinationen von Zeichenfolgen sind Grapheme, aber es gibt auch andere Sequenzen von Codepunkten, wie z.B. r\n. Herkunft und Entwicklung Unicode hat das ausdrückliche Ziel, die Grenzen traditioneller Charaktercodierungen, wie sie durch die Norm ISO/IEC 8859 definiert sind, zu überwinden, die in verschiedenen Ländern der Welt weit verbreitet ist, aber weitgehend unvereinbar bleiben. Viele traditionelle Zeichencodierungen teilen ein gemeinsames Problem, indem sie eine zweisprachige Computerverarbeitung (in der Regel mit lateinischen Zeichen und dem lokalen Skript) erlauben, aber nicht mehrsprachige Computerverarbeitung (Computerverarbeitung von beliebigen Skripten gemischt miteinander). Unicode, intent, kodiert die zugrunde liegenden Zeichen - graphemes und graphemartige Einheiten - anstelle der Variante glyphs (Renderings) für solche Zeichen. Bei chinesischen Charakteren führt dies manchmal zu Kontroversen über die Unterscheidung des zugrunde liegenden Charakters von seiner Variante Glyphen (siehe Han-Einheit). Bei der Textverarbeitung übernimmt Unicode die Aufgabe, für jedes Zeichen einen eindeutigen Codepunkt - eine Zahl, nicht einen Glyphen - bereitzustellen. Mit anderen Worten stellt Unicode abstrakt ein Zeichen dar und lässt das visuelle Rendering (Größe, Form, Schrift oder Stil) an andere Software, wie einen Webbrowser oder Wortprozessor. Dieses einfache Ziel wird jedoch aufgrund von Konzessionen von Unicodes Designern in der Hoffnung, eine schnellere Einführung von Unicode zu fördern, kompliziert. Die ersten 256 Codepunkte wurden identisch mit dem Inhalt der ISO/IEC 8859-1 gemacht, um es zu veranlassen, bestehende westliche Texte zu konvertieren. Viele im Wesentlichen identische Zeichen wurden mehrfach an verschiedenen Codepunkten kodiert, um Auszeichnungen zu erhalten, die durch ältere Kodierungen verwendet werden und somit die Umwandlung von diesen Kodierungen zu Unicode (und zurück) ohne jegliche Informationen zu verlieren. Zum Beispiel umfasst der Abschnitt "Vollbreite Formen" von Codepunkten ein vollständiges Duplikat des lateinischen Alphabets, weil chinesische, japanische und koreanische (CJK) Schriften zwei Versionen dieser Buchstaben enthalten, die Vollbreite der CJK-Zeichen und die normale Breite. Weitere Beispiele finden Sie in Unicode. Unicode Bulldog Award-Empfänger kategorisieren viele Namen einflussreich in der Entwicklung von Unicode und beinhalten Tatsuo Kobayashi, Thomas Milo, Roozbeh Pournader, Ken Lunde, und Michael Everson History Basierend auf Erfahrungen mit dem Xerox Character Code Standard (XCCS) seit 1980, die Ursprünge von Unicode Datum bis 1987, wenn Joe Becker aus Xerox mit Lee Collins und Mark Davis von Apple begann eine Untersuchung eines universalen Charakter zu erstellen. Mit zusätzlichen Beiträgen von Peter Fenwick und Dave Opstad veröffentlichte Joe Becker im August 1988 einen Vorschlag für ein "internationales/multilinguales Textzeichencodiersystem", das vorläufig Unicode genannt wird. Er erklärte, dass ["t] der Name Unicode eine einzigartige, einheitliche, universelle Kodierung vorschlagen soll". In diesem Dokument mit dem Titel Unicode 88 skizzierte Becker ein 16-Bit-Zeichenmodell: Unicode soll die Notwendigkeit einer arbeitbaren, zuverlässigen Welttextcodierung ansprechen. Unicode könnte grob als "wide-body ASCII" beschrieben werden, die auf 16 Bit gestreckt wurde, um die Zeichen aller lebendigen Sprachen der Welt zu umfassen. In einer richtig konstruierten Ausführung sind hierfür 16 Bit pro Zeichen mehr als ausreichend. Sein ursprüngliches 16-Bit-Design basierte auf der Annahme, dass nur diese Skripte und Zeichen in der modernen Nutzung kodiert werden müssten: Unicode gibt höhere Priorität, um das Nutzen für die Zukunft zu gewährleisten, als frühere Antiquitäten zu bewahren. Unicode zielt in der ersten Instanz auf die im modernen Text veröffentlichten Zeichen (z.B. in der Gewerkschaft aller Zeitungen und Zeitschriften, die 1988 in der Welt gedruckt wurden), deren Zahl zweifellos weit unter 214 = 16,384 liegt. Jenseits dieser modernen Charaktere können alle anderen als veraltet oder selten definiert werden; dies sind bessere Kandidaten für die private Anmeldung als für die Verstopfung der öffentlichen Liste von allgemein nützlichen Unicodes. Anfang 1989 erweiterte sich die Unicode-Arbeitsgruppe um Ken Whistler und Mike Kernaghan von Metaphor, Karen Smith-Yoshimura und Joan Aliprand von RLG und Glenn Wright von Sun Microsystems, 1990 traten Michel Suignard und Asmus Freytag von Microsoft und Rick McGowan von NeXT der Gruppe bei. Ende 1990 waren die meisten Arbeiten zur Kartierung bestehender Charaktercodierungsstandards abgeschlossen, und ein abschließender Überprüfungsentwurf von Unicode war fertig. Das Unicode Consortium wurde am 3. Januar 1991 in Kalifornien aufgenommen und im Oktober 1991 wurde das erste Band des Unicode-Standards veröffentlicht. Das zweite Volumen, Abdeckung Han-Ideographen wurde im Juni 1992 veröffentlicht. 1996 wurde in Unicode 2.0 ein Surrogat-Zeichenmechanismus implementiert, so dass Unicode nicht mehr auf 16 Bit beschränkt war. Dadurch wurde der Unicode-Coderaum auf über eine Million Codepunkte erhöht, die die Kodierung vieler historischer Skripte (z.B. Ägyptische Hieroglyphen) und Tausende selten verwendeter oder veralteter Zeichen erlaubten, die nicht als Notwendigkeit der Kodierung erwartet wurden. Unter den ursprünglich nicht für Unicode vorgesehenen Zeichen werden selten Kanji oder chinesische Zeichen verwendet, von denen viele Teil der persönlichen und Ortsnamen sind, so dass sie selten verwendet, aber viel wichtiger als in der ursprünglichen Architektur von Unicode vorgesehen. Die Microsoft True Typ Spezifikation Version 1.0 von 1992 verwendet den Namen 'Apple Unicode' anstelle von Unicode für die Plattform-ID in der Namenstabelle. Unicode Konsortium Das Unicode Consortium ist eine gemeinnützige Organisation, die Unicodes Entwicklung koordiniert. Vollständige Mitglieder umfassen die meisten Computer-Software- und Hardware-Unternehmen, die Interesse an Textverarbeitungsstandards haben, darunter Adobe, Apple, Facebook, Google, IBM, Microsoft, Netflix und SAP SE. Im Laufe der Jahre sind mehrere Länder oder Regierungsstellen Mitglied des Unicode Consortiums. Derzeit ist nur das Ministerium für Endowmente und religiöse Angelegenheiten (Oman) ein Vollmitglied mit Stimmrechten. Das Konsortium hat das ehrgeizige Ziel, die bestehenden Charaktercodierungssysteme durch Unicode und die Standard-Systeme Unicode Transformation Format (UTF) zu ersetzen, da viele der bestehenden Systeme in Größe und Umfang begrenzt sind und mit mehrsprachigen Umgebungen unvereinbar sind. Scripts abgedeckt Unicode deckt fast alle Skripte (Schreibsysteme) in der heutigen Nutzung ab. Insgesamt 154 Skripte sind in der neuesten Version von Unicode enthalten (Bedeckung von Alphabeten, Abugidas und Silben), obwohl es noch Skripte gibt, die noch nicht codiert sind, insbesondere diejenigen, die hauptsächlich in historischen, liturgischen und akademischen Kontexten verwendet werden. Es treten auch weitere Ergänzungen von Zeichen zu den bereits codierten Skripten sowie Symbole, insbesondere für Mathematik und Musik (in Form von Noten und rhythmischen Symbolen) auf. Das Unicode Roadmap Committee (Michael Everson, Rick McGowan, Ken Whistler, V.S Umamaheswaran) hält die Liste der Skripte, die Kandidaten oder potenzielle Kandidaten für die Kodierung und ihre vorläufigen Code Blockzuweisungen auf der Unicode Roadmap Seite der Unicode Consortium Website sind. Für einige Skripte auf der Roadmap, wie Jurchen und Khitan kleine Skript, wurden Kodierungsvorschläge erstellt und sie arbeiten ihren Weg durch den Genehmigungsprozess. Für andere Skripte, wie Mayan (beide Zahlen) und Rongorongo, ist noch kein Vorschlag gemacht worden, und sie erwarten eine Vereinbarung über Charakterrepertoire und andere Details der beteiligten Nutzergemeinschaften. Einige moderne erfinderische Skripte, die noch nicht in Unicode (z.B. Tengwar) enthalten sind oder aufgrund mangelnder realer Nutzung (z.B. Klingon) nicht in Unicode aufgenommen werden können, sind in der ConScript Unicode Registry zusammen mit inoffiziellen, aber weit verbreiteten Private Use Areas Code-Beauftragungen aufgeführt. Es gibt auch eine Medieval Unicode Font Initiative konzentriert sich auf spezielle lateinische mittelalterliche Charaktere. Ein Teil dieser Vorschläge wurde bereits in Unicode aufgenommen. Script Encoding Initiative Die Script Encoding Initiative, ein Projekt von Deborah Anderson an der University of California, Berkeley wurde 2002 mit dem Ziel gegründet, Vorschläge für Skripte zu finanzieren, die noch nicht im Standard codiert sind. Das Projekt wurde in den letzten Jahren zu einer großen Quelle von vorgeschlagenen Ergänzungen zum Standard. Versionen Das Unicode Consortium und die Internationale Organisation für Normung haben ein gemeinsames Repertoire nach der ersten Veröffentlichung des Unicode Standard 1991 entwickelt; Unicode und das ISO's Universal Coded Character Set verwenden identische Zeichennamen und Codepunkte. Die Unicode-Versionen unterscheiden sich jedoch von ihren ISO-Äquivalenten auf zwei signifikante Weise. Während das UCS eine einfache Zeichenkarte ist, gibt Unicode die Regeln, Algorithmen und Eigenschaften an, die erforderlich sind, um Interoperabilität zwischen verschiedenen Plattformen und Sprachen zu erreichen. So Der Unicode Standard enthält mehr Informationen, die - in der Tiefe - Topiken wie bitweise Codierung, Collation und Rendering abdecken. Es bietet auch einen umfassenden Katalog von Charaktereigenschaften, einschließlich derjenigen, die für die Unterstützung von bidirektionalen Text benötigt werden, sowie visuelle Diagramme und Referenzdatensätze für Hilfsgeräte. Zuvor, Der Unicode Standard wurde als Druckvolumen verkauft, das die komplette Kernspezifikation, Standard-Anschlüsse und Code-Charts enthält. Unicode 5.0, veröffentlicht im Jahr 2006, war jedoch die letzte Version auf diese Weise gedruckt. Ab Version 5.2 kann nur die Kernspezifikation, die als Print-on-Demand-Papierback veröffentlicht wird, erworben werden. Der vollständige Text wird hingegen als kostenloses PDF auf der Unicode-Website veröffentlicht. Ein praktischer Grund für diese Veröffentlichungsmethode ist der zweite signifikante Unterschied zwischen UCS und Unicode – die Frequenz, mit der aktualisierte Versionen freigegeben werden und neue Zeichen hinzugefügt werden. Der Unicode Standard hat regelmäßig jährliche erweiterte Versionen veröffentlicht, gelegentlich mit mehr als einer Version in einem Kalenderjahr veröffentlicht und mit seltenen Fällen, in denen die geplante Veröffentlichung verschoben werden musste. Zum Beispiel, im April 2020, nur einen Monat nach Version 13.0 veröffentlicht wurde, kündigte das Unicode Consortium an, dass sie das beabsichtigte Veröffentlichungsdatum für Version 14.0 geändert hatten und es sechs Monate vom März 2021 bis September 2021 aufgrund der COVID-19 Pandemie zurückdrängten. Unicode 14.0 wird versprochen, mindestens fünf neue Skripte sowie siebenunddreißig neue Emoji-Zeichen aufzunehmen. Bisher wurden die folgenden großen und kleinen Versionen des Unicode-Standards veröffentlicht. Update-Versionen, die keine Änderungen des Charakterrepertoires enthalten, werden durch die dritte Nummer (z.B. "Version 4.0.1") gekennzeichnet und in der nachfolgenden Tabelle weggelassen. Architektur und Terminologie Der Unicode Standard definiert einen Codespace, einen Satz von Zahlenwerten von 0 bis 10FF16, genannt Codepunkte und bezeichnet als U+0000 bis U+10FF (U+ plus Code-Point-Wert in hexadezimal, vorausgesetzt mit führenden Nullen als notwendig, um ein Minimum von vier Ziffern, z.B. zu erzielen. U+00F7 für das Teilungszeichen ÷but U+13254 für den ägyptischen Hieroglyph, der einen Reed-Bunker oder eine Wickelwand bezeichnet. Von diesen 216 + 220 definierten Codepunkten sind die Codepunkte von U+D800 bis U+DFFF, die zur Codierung von Surrogatpaaren in UTF-16 verwendet werden, vom Standard reserviert und dürfen nicht verwendet werden, um gültige Zeichen zu kodieren, was zu einer Nettosumme von 216 - 211 + 220 = 1,112,064 möglichen Codepunkten entsprechend gültigen Unicodezeichen führt. Nicht alle diese Codepunkte entsprechen zwangsläufig sichtbaren Zeichen; mehrere sind beispielsweise Steuercodes, wie z.B. Schlittenrückgabe, zugeordnet. Codeebenen und Blöcke Der Unicode-Coderaum ist in 17 Ebenen unterteilt, nummeriert 0 bis 16: Alle Codepunkte im BMP werden als einzige Codeeinheit in der UTF-16 Kodierung aufgerufen und können in einem, zwei oder drei Bytes in UTF-8 kodiert werden.Code-Punkte in den Planen 1 bis 16 (Ergänzungsebenen) werden als Surrogat-Paare in UTF-16 aufgerufen und in vier Bytes in UTF-8 kodiert. Innerhalb jeder Ebene werden Zeichen innerhalb der genannten Blöcke von zugehörigen Zeichen zugewiesen. Obwohl Blöcke eine beliebige Größe sind, sind sie immer ein Vielfaches von 16 Codepunkten und oft ein Vielfaches von 128 Codepunkten. Für ein bestimmtes Skript benötigte Zeichen können über mehrere verschiedene Blöcke verteilt werden. Allgemeine Kategorie Jeder Codepunkt hat eine einzige Eigenschaft der Kategorie General. Die wichtigsten Kategorien sind angegeben: Buchstaben, Mark, Nummer, Punctuation, Symbol, Separator und andere. Innerhalb dieser Kategorien gibt es Unterteilungen. In den meisten Fällen müssen andere Eigenschaften verwendet werden, um die Eigenschaften eines Codepunktes hinreichend festzulegen. Die möglichen allgemeinen Kategorien sind: Codepunkte im Bereich U+D800–U+DBFF (1,024 Codepunkte) sind als hoch-surrogate Codepunkte bekannt und Codepunkte im Bereich U+DC00–U+DFFF (1,024 Codepunkte) werden als niedrig-surrogate Codepunkte bezeichnet. Ein High-surrogate-Codepunkt, gefolgt von einem Low-surrogate-Codepunkt, bilden ein Surrogate-Paar in UTF-16, um Codepunkte größer als U+FFFF zu repräsentieren. Diese Codepunkte können andernfalls nicht verwendet werden (diese Regel wird in der Praxis oft ignoriert, besonders wenn nicht UTF-16 verwendet wird). Ein kleiner Satz von Code-Punkten wird garantiert nie für die Codierung von Zeichen verwendet werden, obwohl Anwendungen können diese Code-Punkte intern verwenden, wenn sie wollen. Es gibt sechsundsechzig dieser Nichtkennzeichen: U+FDD0–U+FDEF und jeder Codepunkt, der im Wert FFFE oder FFFF endet (d.h. U+FFFE, U+FFFF, U+1FFFE, U+1FFFF,... U+10FFFE, U+10FFFFFE). Der Satz der Nicht-Kennzeichen ist stabil, und keine neuen Nicht-Kennzeichen werden je definiert. Wie Surrogate wird die Regel, die diese nicht verwendet werden können, oft ignoriert, obwohl der Betrieb der Byte-Bestellmarke davon ausgeht, dass U+FFFE niemals der erste Codepunkt in einem Text sein wird. Ausschließlich Surrogate und Nicht-Charakter sind 1.111.998 Codepunkte für die Verwendung verfügbar. Private-Use-Code-Punkte werden als zugewiesene Zeichen betrachtet, aber sie haben keine von der Unicode-Standard spezifizierte Interpretation, so dass jeder Austausch solcher Zeichen eine Vereinbarung zwischen Sender und Empfänger über ihre Interpretation erfordert. Es gibt drei private Bereiche im Unicode-Codespace: Private Use Area: U+E000–U+F8FF (6.400 Zeichen,) Zusätzliche private Nutzungsbereich-A: U+F0000–U+FFD (65,534 Zeichen), ergänzende private Nutzungsbereich-B: U+100000–U+10FFFD (65,534 Zeichen). Graphische Zeichen sind Zeichen, die von Unicode definiert werden, um eine bestimmte Semantik zu haben, und haben entweder eine sichtbare Glyphenform oder stellen einen sichtbaren Raum dar. Ab Unicode 13.0 gibt es 143.696 Grafikzeichen. Format-Zeichen sind Zeichen, die kein sichtbares Aussehen haben, können aber Auswirkungen auf das Aussehen oder Verhalten benachbarter Zeichen haben. Beispielsweise können U+200C ZERO WIDTH NON-JOINER und U+200D ZERO WIDTH JOINER verwendet werden, um das Standard-Formverhalten benachbarter Zeichen zu ändern (z.B. um Ligaturen zu hemmen oder Ligaturbildung zu fordern). Es gibt 163 Formatzeichen in Unicode 13.0.Sechzig-fünf Codepunkte (U+0000–U+001F und U+007F–U+009F) sind als Steuercodes reserviert und entsprechen den in ISO/IEC 6429.U+0009 (Tab,) U+000A (Line Feed) und U+000D (Carriage Return) definierten C0-Steuercodes. In der Praxis sind die C1-Code-Punkte oft unangemessen (mojibake) als die alten Windows-1252 Zeichen, die von einigen englischen und westeuropäischen Texten verwendet werden. Grafikzeichen, Formatzeichen, Steuerzeichen und private Nutzungszeichen sind als zugeordnete Zeichen bezeichnet. Reservierte Codepunkte sind diejenigen Codepunkte, die zur Verwendung zur Verfügung stehen, aber noch nicht zugewiesen sind. Ab Unicode 13.0 gibt es 830.606 reservierte Codepunkte. Abstrakte Zeichen Der von Unicode definierte Satz von Grafik- und Formatzeichen entspricht nicht direkt dem Repertoire von abstrakten Zeichen, die unter Unicode darstellbar sind. Unicode kodiert Zeichen durch Zuordnung eines abstrakten Zeichens mit einem bestimmten Codepunkt. Jedoch werden nicht alle abstrakten Zeichen als einzelne Unicode-Zeichen kodiert, und einige abstrakte Zeichen können in Unicode durch eine Folge von zwei oder mehr Zeichen dargestellt werden. Beispielsweise wird ein lateinischer kleiner Buchstabe i mit einem Ogonek, einem Punkt oben und einem in litauischer Sprache erforderlichen akuten Akzent durch die Zeichenfolge U+012F, U+0307, U+0301 dargestellt. Unicode unterhält eine Liste von eindeutig benannten Zeichenfolgen für abstrakte Zeichen, die nicht direkt in Unicode kodiert sind.Alle Grafik-, Format- und privaten Gebrauchszeichen haben einen einzigartigen und unveränderlichen Namen, mit dem sie identifiziert werden können. Diese Unmutbarkeit ist seit Unicode Version 2.0 durch die Name Stability Policy garantiert. In Fällen, in denen der Name schwer defekt und irreführend ist oder einen schwerwiegenden typografischen Fehler hat, kann ein formaler Alias definiert werden, und Anträge werden ermutigt, die formalen Alias anstelle des offiziellen Charakternamens zu verwenden. Zum Beispiel hat U+A015 ꀕ YI SYLLABLE WU die formalen Alias YI SYLLABLE ITERATION MARK, und U+FE18 〗 PRESENTATION FORM FOR VERTICALRIGHT WHITE LENTICULAR BRAKCET (sic) hat die formalen Alias PRESENTATION FORM FÜR VERTICALRIGHT WHITE LENTICULAR BRACKET. Fertig gemachte versus zusammengesetzte Zeichen Unicode umfasst einen Mechanismus zur Änderung von Zeichen, die das unterstützte Glyph Repertoire stark erweitert. Dies deckt die Verwendung der Kombination von diakritischen Markierungen ab, die nach dem Grundzeichen des Benutzers hinzugefügt werden können. Mehrere Kombinationsdiakritiken können gleichzeitig auf denselben Charakter angewendet werden. Unicode enthält auch vorkonstituierte Versionen der meisten Buchstaben/diakritischen Kombinationen im normalen Gebrauch. Diese machen die Umstellung auf und von Legacy-Encodings einfacher, und ermöglichen Anwendungen Unicode als internes Textformat zu verwenden, ohne Kombinationszeichen zu implementieren. Beispielsweise kann é in Unicode als U+0065 (LATIN SMALL LETTER E) dargestellt werden, gefolgt von U+0301 (COMBINING ACUTE ACCENT), aber es kann auch als vorkomponiertes Zeichen U+00E9 (LATIN SMALL LETTER E MIT ACUTE) dargestellt werden. So haben Benutzer in vielen Fällen mehrere Möglichkeiten, das gleiche Zeichen zu kodieren. Um damit zu umgehen, bietet Unicode den Mechanismus der kanonischen Äquivalenz. Ein Beispiel dafür entsteht mit Hangul, dem koreanischen Alphabet. Unicode bietet einen Mechanismus zum Komponieren von Hangul Silben mit ihren einzelnen Teilkomponenten, bekannt als Hangul Jamo. Es bietet jedoch auch 11,172 Kombinationen von vormontierten Silben aus der häufigsten Jamo. Die CJK-Zeichen haben derzeit nur Codes für ihr vorkomponiertes Formular. Dennoch enthalten die meisten dieser Zeichen einfachere Elemente (genannte Radikale), so dass Unicode sie im Prinzip wie mit Hangul zersetzt hätte. Dies hätte die Anzahl der benötigten Codepunkte stark reduziert, während die Anzeige von nahezu jedem denkbaren Charakter (was mit einigen der Probleme, die durch die Han-Vereinigung verursacht werden könnten, weggehen könnte). Eine ähnliche Idee wird von einigen Eingabemethoden wie Cangjie und Wubi verwendet. Versuche, dies für die Charaktercodierung zu tun, haben jedoch über die Tatsache gestolpert, dass chinesische Charaktere nicht wie einfach oder regelmäßig zerfallen, wie Hangul es tut. In Unicode 3.0 (CJK-Radikale zwischen U+2E80 und U+2EFF, KangXi-Radikale in U+2F00 bis U+2FDF und ideographische Beschreibungszeichen von U+2FF0 bis U+2FFB) warnen aber die Unicode-Norm (ch.12.2 von Unicode 5.2) davor, ideografische Beschreibungssequenzen als alternative Darstellung für vorher kodierte Zeichen zu verwenden: Dieser Vorgang unterscheidet sich von einer formalen Kodierung eines Ideographen. Es gibt keine kanonische Beschreibung von uncodierten Ideographen; es gibt keine semantische Zuordnung zu beschriebenen Ideographen; es gibt keine für beschriebene Ideographen definierte Äquivalenz. Konzeptionell sind ideographische Beschreibungen eher der englischen Phrase "an e with akut accent on it" als der Charaktersequenz <U+0065, U+0301.> Ligaturen Viele Skripte, einschließlich Arabisch und Devanāgarī, haben spezielle orthographische Regeln, die bestimmte Kombinationen von Briefformen erfordern zu speziellen Ligaturformen kombiniert werden. Die Regeln für die Ligaturbildung können recht komplex sein, erfordern spezielle Script-Shaping-Technologien wie ACE (Arabic Calligraphic Engine von DecoType in den 1980er Jahren und verwendet, um alle arabischen Beispiele in den gedruckten Editionen des Unicode-Standards zu generieren), die zum Konzept für OpenType (von Adobe und Microsoft,) Graphite (von SIL International,) oder AAT (von Apple) wurde. Anweisungen sind auch in Schriften eingebettet, um dem Betriebssystem zu sagen, wie man richtig verschiedene Zeichenfolgen ausgibt. Eine einfache Lösung für das Setzen von Markierungen oder Diakritik ist die Zuordnung der Markierungen zu einer Breite von Null und das Setzen des Glyphens selbst nach links oder rechts des linken Seitenlagers (je nach Richtung des Skripts, mit dem sie verwendet werden sollen). Eine so gehandhabte Marke erscheint über das, was ihr vorausgeht, aber wird ihre Position nicht in Bezug auf die Breite oder Höhe des Basisglyphens einstellen; sie kann visuell awkward sein und sie kann einige Glyphen überdecken. Reales Stapeln ist unmöglich, kann aber in begrenzten Fällen angenähert werden (z.B. thailändische Top-kombinierende Vokale und Tonmarken können nur in unterschiedlichen Höhen sein, um mit zu beginnen). In der Regel ist dieser Ansatz nur in monospaced Schriften wirksam, kann aber als Fallback-Rendering-Verfahren verwendet werden, wenn komplexere Methoden ausfallen. Standardisierte Teilmengen Mehrere Teilmengen von Unicode sind standardisiert: Microsoft Windows seit WindowsNT 4.0 unterstützt WGL-4 mit 657 Zeichen, die alle zeitgenössischen europäischen Sprachen mit dem lateinischen, griechischen oder kyrillischen Skript unterstützen. Weitere standardisierte Teilmengen von Unicode umfassen die mehrsprachigen europäischen Teilmengen:MES-1 (nur latente Skripte, 335 Zeichen), MES-2 (lateinische, griechische und kyrillische 1062 Zeichen) und MES-3A & MES-3B (zwei größere Teilmengen, hier nicht dargestellt). Beachten Sie, dass MES-2 jedes Zeichen in MES-1 und WGL-4 enthält. Rendering-Software, die ein Unicode-Zeichen nicht verarbeiten kann, zeigt es entsprechend oft als offenes Rechteck an, oder das Unicode "Ersatzzeichen" (U+FFFD, ), um die Position des nicht erkannten Zeichens anzuzeigen. Einige Systeme haben versucht, mehr Informationen über solche Zeichen zu liefern. Apple's Last Resort Schrift zeigt einen Ersatz-Glyphen, der den Unicode-Bereich des Zeichens angibt, und die SIL International Unicode Fallback Schriftart zeigt eine Box mit dem hexadezimalen Skalarwert des Zeichens. Mapping und Codierung Mehrere Mechanismen wurden für die Speicherung einer Reihe von Codepunkten als eine Reihe von Bytes festgelegt. Unicode definiert zwei Mapping-Methoden: die Unicode Transformation Format (UTF) Kodierungen und die Universal Coded Character Set (UCS) Kodierungen. Eine Codierkarte (möglicherweise eine Untermenge) weist den Bereich von Unicode-Code auf Wertefolgen in einem Festwertbereich, bezeichnet als Codeeinheiten, hin. Alle UTF Kodierungen map Code Punkte auf eine einzigartige Folge von Bytes. Die Zahlen in den Namen der Kodierungen zeigen die Anzahl der Bits pro Codeeinheit (für UTF-Kodierungen) oder die Anzahl der Bytes pro Codeeinheit (für UCS-Kodierungen und UTF-1). UTF-8 und UTF-16 sind die am häufigsten verwendeten Codierungen. UCS-2 ist eine obsolete Teilmenge von UTF-16; UCS-4 und UTF-32 sind funktionell gleichwertig. UTF-Kodierungen umfassen: UTF-8, verwendet für jeden Codepunkt ein bis vier Bytes, maximiert die Kompatibilität mit ASCII UTF-EBCDIC, ähnlich UTF-8, aber für die Kompatibilität mit EBCDIC (nicht Teil des Unicode Standard) UTF-16, verwendet eine oder zwei 16-Bit-Codeeinheiten pro Code-Wechselpunkt, kann nicht UTF-32 kodieren Es wird von FreeBSD und den neuesten Linux-Distributionen als direkten Ersatz für Legacy-Encodings im allgemeinen Texthandling verwendet. Die UCS-2 und UTF-16 Kodierungen geben die Unicode Byte Order Mark (BOM) für die Verwendung an den Anfangen von Textdateien an, die zur Byte-Ordnung (oder Byte-Endianness-Erkennung) verwendet werden können. Die BOM, Codepunkt U+FEFF, hat die wichtige Eigenschaft der Unklarheit auf Byte-Reorder, unabhängig von der verwendeten Unicode-Kodierung; U+FFFE (das Ergebnis von Byte-Swapping U+FEFF) entspricht nicht einem rechtlichen Charakter, und U+FEFF an anderen Stellen als dem Beginn des Textes vermittelt die Nullbreite Nicht-Breiten-Raum (ein Zeichen ohne Aussehen und Verhinderung). Das gleiche, in UTF-8 umgewandelte Zeichen wird zur Byte-Sequenz EF BB BF. Der Unicode Standard ermöglicht es, dass die BOM "als Signatur für UTF-8 codierten Text dienen kann, in dem der Zeichensatz nicht markiert ist". Einige Software-Entwickler haben es für andere Codierungen, einschließlich UTF-8, angenommen, um UTF-8 von lokalen 8-Bit-Codeseiten zu unterscheiden. RFC 3629, der UTF-8-Standard, empfiehlt jedoch, dass Byte-Bestellmarken in Protokollen mit UTF-8 verboten werden, diskutiert aber die Fälle, in denen dies möglicherweise nicht möglich ist. Darüber hinaus bedeutet die große Beschränkung auf mögliche Muster in UTF-8 (z.B. es kann keine Einzelbytes mit dem hohen Bitsatz geben) dass es möglich sein sollte, UTF-8 von anderen Zeichencodierungen zu unterscheiden, ohne auf die BOM zu verlassen. In UTF-32 und UCS-4 dient eine 32-Bit-Codeeinheit als ziemlich direkte Darstellung eines beliebigen Zeichencodepunktes (obwohl die Endianität, die sich über verschiedene Plattformen hinweg ändert, beeinflusst, wie sich die Codeeinheit als Byte-Sequenz manifestiert). In den anderen Codierungen kann jeder Codepunkt durch eine variable Anzahl von Codeeinheiten dargestellt werden. UTF-32 ist weit verbreitet als interne Darstellung von Text in Programmen (im Gegensatz zu gespeicherten oder übertragenen Text), da jedes Unix-Betriebssystem, das die gcc-Compiler verwendet, um Software zu erzeugen, es als Standard-"breite Zeichen"-Codierung verwendet. Einige Programmiersprachen wie Seed7 verwenden UTF-32 als interne Darstellung für Zeichen und Zeichen. Neuere Versionen der Python Programmiersprache (anfangs mit 2.2) können auch konfiguriert werden, um UTF-32 als Darstellung für Unicode-Strings zu verwenden, um eine solche Codierung in hochrangiger codierter Software effektiv zu verbreiten. Punycode, ein weiteres Codierformular, ermöglicht die Codierung von Unicode-Strings in das durch das ASCII-basierte Domain Name System (DNS) unterstützte eingeschränkte Zeichen. Die Codierung wird als Teil der IDNA verwendet, die ein System ist, das die Verwendung von Internationalized Domain Names in allen Skripten ermöglicht, die von Unicode unterstützt werden. Zu den früheren und heute historischen Vorschlägen gehören UTF-5 und UTF-6.GB18030 eine weitere Kodierungsform für Unicode, von der Standardisierungsverwaltung Chinas. Es ist der offizielle Charakter der Volksrepublik China (PRC). BOCU-1 und SCSU sind Unicode Kompressionssysteme. Im April Fools' Day RFC von 2005 wurden zwei Parodie-UTF-Encodings, UTF-9 und UTF-18 spezifiziert. Adoption Betriebssysteme Unicode ist das dominante System für die interne Verarbeitung und Speicherung von Text. Obwohl in den Legacy-Encodings noch viel Text gespeichert ist, wird Unicode fast ausschließlich zum Aufbau neuer Informationsverarbeitungssysteme verwendet. Frühe Adopter neigten dazu, UCS-2 (der fest-breite Zwei-Byte-Vorläufer zu UTF-16) zu verwenden und später zu UTF-16 (der variable-breite-Strom-Standard), da dies die am wenigsten störende Weise war, Unterstützung für Nicht-BMP-Zeichen hinzuzufügen. Das bekannteste solches System ist Windows NT (und seine Nachkommen, Windows 2000, Windows XP, Windows Vista, Windows 7, Windows 8 und Windows 10), die UTF-16 als einzige interne Zeichencodierung verwendet. Das Java und . NET Bytecode-Umgebungen, macOS und KDE verwenden es auch für interne Darstellung. Teilunterstützung für Unicode kann unter Windows 9x über die Microsoft Layer für Unicode installiert werden. UTF-8 (ursprünglich für Plan 9 entwickelt) ist die Hauptspeichercodierung auf den meisten Unix-ähnlichen Betriebssystemen geworden (obwohl andere auch von einigen Bibliotheken verwendet werden), weil es ein relativ einfacher Ersatz für traditionelle erweiterte ASCII-Zeichensätze ist. UTF-8 ist auch die häufigste Unicode-Kodierung, die in HTML-Dokumenten im World Wide Web verwendet wird. Mehrsprachige Text-Rendering-Engines, die Unicode verwenden, umfassen Uniscribe und DirectWrite für Microsoft Windows, ATSUI und Core Text für macOS, und Pango für GTK+ und den GNOME-Desktop. Eingangsverfahren Da Tastaturlayouts keine einfachen Tastenkombinationen für alle Zeichen haben können, bieten mehrere Betriebssysteme alternative Eingabemethoden, die den Zugriff auf das gesamte Repertoire ermöglichen. Die ISO/IEC 14755, die Methoden für die Eingabe von Unicode-Zeichen aus ihren Codepunkten standardisiert, gibt mehrere Methoden an. Es gibt das Basisverfahren, an das sich eine Anfangssequenz anschließt, die hexadezimale Darstellung des Codepunktes und der Endsequenz. Es wird auch ein Screen-Selektion-Eintrags-Verfahren angegeben, bei dem die Zeichen in einer Tabelle in einem Bildschirm aufgelistet sind, beispielsweise mit einem Zeichenkarten-Programm. Online-Tools zum Auffinden des Codepunktes für einen bekannten Charakter sind Unicode Lookup von Jonathan Hedley und Shapecatcher von Benjamin Milde. In Unicode Lookup gibt man einen Suchschlüssel (z.B. Fraktionen) ein und eine Liste entsprechender Zeichen mit ihren Codepunkten wird zurückgegeben. In Shapecatcher, basierend auf Shape-Kontext, zieht man das Zeichen in einer Box und eine Liste von Zeichen, die die Zeichnung mit ihren Codepunkten annähern, wird zurückgegeben. E-Mail MIME definiert zwei verschiedene Mechanismen zur Codierung nicht-ASCII-Zeichen in E-Mail, je nachdem, ob die Zeichen in E-Mail-Headern (wie dem Betreff,:) oder im Textkörper der Nachricht sind; in beiden Fällen wird der ursprüngliche Zeichensatz als auch eine Transfercodierung identifiziert. Für die E-Mail-Übermittlung von Unicode werden der UTF-8-Zeichensatz und der Base64 oder die Zitat-druckbare Transfercodierung empfohlen, je nachdem, ob ein Großteil der Nachricht aus ASCII-Zeichen besteht. Die Details der beiden unterschiedlichen Mechanismen sind in den MIME-Standards angegeben und werden im Allgemeinen von Benutzern von E-Mail-Software versteckt. Die Annahme von Unicode in E-Mail war sehr langsam. Einige ostasiatische Texte werden noch in Kodierungen wie ISO-2022 kodiert, und einige Geräte, wie Mobiltelefone, können Unicode-Daten noch nicht korrekt verarbeiten. Die Unterstützung wurde jedoch verbessert. Viele große kostenlose Mail-Anbieter wie Yahoo, Google (Gmail,) und Microsoft (Outlook.com) unterstützen es. WebAll W3C-Empfehlungen haben Unicode als ihren Dokumentenzeichensatz seit HTML 4.0 verwendet. Web-Browser haben Unicode, insbesondere UTF-8, seit vielen Jahren unterstützt. Es gab früher Probleme, die vor allem aus schriftsbezogenen Problemen resultieren; z.B. v 6 und älter von Microsoft Internet Explorer nicht viele Codepunkte, es sei denn, explizit gesagt, eine Schrift zu verwenden, die sie enthält. Obwohl Syntax-Regeln die Reihenfolge beeinflussen können, in der Zeichen angezeigt werden dürfen, enthalten XML-Dokumente (einschließlich XHTML) definitionsgemäß Zeichen aus den meisten Unicode-Codepunkten, mit Ausnahme der meisten C0-Steuercodes, der permanent nicht zugewiesenen Codepunkte D800–DFFF, FFFE oder FFFF. HTML-Zeichen manifestieren sich entweder direkt als Bytes nach Dokument-Kodierung, wenn die Kodierung sie unterstützt, oder Benutzer können sie als numerische Zeichenreferenzen basierend auf dem Unicode-Punkt des Zeichens schreiben. Die Referenzen #&916, #&1049, #&1511,; #&1605,; #&3671,; #&12354, #&21494, #&33865 und #&47568; (oder die gleichen Zahlenwerte, ausgedrückt in hexadezimal, mit #&x als HTTP-Präfix) sollten auf allen Browsern als Δ, 叶 Fonts Unicode beschäftigt sich grundsätzlich nicht mit Schriftarten an sich und sieht sie als Implementierungsoptionen. Jeder gegebene Charakter kann viele Allographen haben, von den häufigeren kühnen, italischen und Grundbuchstaben bis hin zu komplexen dekorativen Stilen. Eine Schriftart ist "Unicode konform", wenn die Glyphen in der Schrift mit im Unicode-Standard definierten Codepunkten aufgerufen werden können. Der Standard gibt keine Mindestanzahl an Zeichen an, die in der Schrift enthalten sein müssen; einige Schriftarten haben recht ein kleines Repertoire. Freie und Einzelhandelsschriftarten auf Basis von Unicode sind weit verbreitet, da TrueType und OpenType Unicode unterstützen. Diese Schriftformate Landkarte Unicode-Codepunkte auf Glyphs, aber TrueType Schriftart ist auf 65,535 Glyphen beschränkt. Tausende Schriftarten gibt es auf dem Markt, aber weniger als ein Dutzend Schriften - manchmal als pan-Unicode Schriften beschrieben - versuchen, die Mehrheit des Unicode-Zeichenrepertoires zu unterstützen. Stattdessen konzentrieren sich Unicode-basierte Schriften typischerweise auf die Unterstützung von nur grundlegenden ASCII und bestimmten Skripten oder Zeichensätzen oder Symbolen. Mehrere Gründe rechtfertigen diesen Ansatz: Anwendungen und Dokumente müssen selten Zeichen von mehr als einem oder zwei Schreibsystemen machen; Schriftarten neigen dazu, Ressourcen in Rechenumgebungen zu verlangen; und Betriebssysteme und Anwendungen zeigen zunehmende Intelligenz in Bezug auf die Gewinnung von Glyph-Informationen von separaten Schriftdateien nach Bedarf, d.h. Schriftsubstitution. Darüber hinaus stellt die Gestaltung einer konsequenten Reihe von Rendering-Anweisungen für Zehntausende von Glyphen eine monumentale Aufgabe dar; ein solches Risiko geht über den Punkt der abnehmenden Rückkehr für die meisten Schriftarten hinaus. Newlines Unicode befasst sich teilweise mit dem Problem der Neuzeile, das beim Versuch einer Textdatei auf verschiedenen Plattformen auftritt. Unicode definiert eine große Anzahl von Zeichen, die übereinstimmende Anwendungen als Zeilenterminatoren erkennen sollen. In Bezug auf die neue Linie, Unicode eingeführt U+2028 LINE SEPARATOR und U+2029 PARAGRAPH SEPARATOR. Dies war ein Versuch, eine Unicode-Lösung zur Kodierung von Absätzen und Zeilen semantisch bereitzustellen, die möglicherweise alle verschiedenen Plattformlösungen ersetzen könnte. Dabei bietet Unicode einen Weg um die historischen plattformabhängigen Lösungen. Dennoch, wenige, wenn irgendwelche Unicode-Lösungen haben diese Unicode-Linie und Absatz-Separatoren als die einzige kanonische Linie ending Zeichen angenommen. Ein gemeinsamer Ansatz, dieses Problem zu lösen, ist jedoch durch eine Neueinstellung. Dies wird mit dem Cocoa Textsystem in Mac OS X sowie mit W3C XML und HTML Empfehlungen erreicht. In diesem Ansatz wird jeder mögliche Neuliniencharakter intern in eine gemeinsame Neulinie umgewandelt (die nicht wirklich wichtig ist, da es sich um eine interne Operation nur zum Rendern handelt). Mit anderen Worten, das Textsystem kann das Zeichen richtig als eine neue Zeile behandeln, unabhängig von der eigentlichen Kodierung der Eingabe. Themen Philosophie- und Vollständigkeitskritiken Die Vereinigung Han (die Identifikation von Formen in den ostasiatischen Sprachen, die man als stilistische Variationen des gleichen historischen Charakters behandeln kann) wurde zu einem der kontroversialsten Aspekte des Unicodes, trotz der Anwesenheit einer Mehrheit von Experten aus allen drei Regionen in der Ideographischen Forschungsgruppe (IRG), die das Konsortium und ISO zu Ergänzungen zum Repertoire und zur Han-Einheit berät. Unicode wurde kritisiert, weil sie nicht getrennt kodieren ältere und alternative Formen von kanji, die, Kritiker argumentieren, kompliziert die Verarbeitung von alten japanischen und ungewöhnlichen japanischen Namen. Dies liegt häufig daran, dass Unicode Zeichen anstelle von Glyphen kodiert (die visuellen Darstellungen des Grundcharakters, die oft von einer Sprache zur anderen variieren). Die Vereinigung von Glyphen führt zur Wahrnehmung, dass die Sprachen selbst, nicht nur die Grundcharakterdarstellung, zusammengeführt werden. Es gab mehrere Versuche, alternative Kodierungen zu schaffen, die die stilistischen Unterschiede zwischen chinesischen, japanischen und koreanischen Zeichen im Gegensatz zu Unicodes Politik der Han-Vereinigung bewahren. Ein Beispiel dafür ist TRON (obwohl es in Japan nicht weit verbreitet ist, gibt es einige Benutzer, die den historischen japanischen Text handhaben müssen und ihn bevorzugen). Obwohl das Repertoire von weniger als 21.000 Han-Zeichen in der frühesten Version von Unicode war weitgehend auf Zeichen in der gemeinsamen modernen Nutzung beschränkt, Unicode umfasst jetzt mehr als 92.000 Han-Zeichen und Arbeit wird weiterhin Tausende historischer und dialektaler Zeichen in China, Japan, Korea, Taiwan und Vietnam verwendet. Moderne Schrifttechnologie bietet ein Mittel, um die praktische Frage der Notwendigkeit, einen einheitlichen Han-Charakter in Bezug auf eine Sammlung von alternativen Glyph-Darstellungen in Form von Unicode-Variationssequenzen darzustellen. Die Advanced Typographic Tabellen von OpenType erlauben es beispielsweise, eine von mehreren alternativen Glyph-Darstellungen bei der Durchführung des Zeichens zum Glyph-Mapping zu wählen. In diesem Fall können innerhalb eines Klartextes Informationen bereitgestellt werden, um zu bezeichnen, welche alternative Zeichenform zur Auswahl bildet. Wenn sich der Unterschied in den entsprechenden Glyphen für zwei Zeichen im gleichen Skript nur im italischen unterscheiden, hat Unicode sie im Allgemeinen vereinheitlicht, wie im Vergleich zwischen russischen (markierten Standard) und serbischen Zeichen rechts zu sehen ist, was bedeutet, dass die Unterschiede durch intelligente Schrifttechnologie oder manuell wechselnde Schriften angezeigt werden.Unicode wurde entwickelt, um Code-Point-by-Code-Point-Rund-Trip-Format-Konvertierung zu und von allen vorhandenen Zeichencodierungen bereitzustellen, so dass Textdateien in älteren Zeichensätzen in Unicode umgewandelt werden können und dann zurück und die gleiche Datei zurück erhalten, ohne kontextabhängige Interpretation zu verwenden. Das bedeutete, dass inkonsistente Legacy-Architekturen, wie die Kombination von Diakritik und vorkomponierten Zeichen, beide existieren in Unicode, geben mehr als eine Methode der Darstellung eines Textes. Dies ist in den drei verschiedenen Kodierungsformen für koreanischen Hangul am ausgeprägtesten. Seit Version 3.0 können alle vorkomponierten Zeichen, die durch eine Kombination von bereits vorhandenen Zeichen repräsentiert werden können, nicht mehr dem Standard hinzugefügt werden, um die Interoperabilität zwischen Software mit verschiedenen Versionen von Unicode zu erhalten. Injizierende Mappings müssen zwischen Zeichen in bestehenden Legacy-Zeichensätzen und Zeichen in Unicode bereitgestellt werden, um die Umwandlung in Unicode zu erleichtern und Interoperabilität mit Legacy-Software zu ermöglichen. Mangel an Konsistenz in verschiedenen Mappings zwischen früheren japanischen Codierungen wie Shift-JIS oder EUC-JP und Unicode führte zu Round-Trip-Format-Konvertierung Fehlanpassungen, vor allem die Mapping des Charakters JIS X 0208 ～ (1-33, WAVE DASH), die stark in Legacy-Datenbankdaten verwendet wird, entweder U+FF5E ～ FULLWIDTH TILDE (inAVE1 Microsoft Windows-Anbieter) oder Us + Us). Einige japanische Computer Programmierer wandten sich gegen Unicode, weil es sie erfordert, die Verwendung von U+005C \ REVERSE SOLIDUS (Backslash) und U+00A5 ¥ YEN SIGN, die auf 0x5C in JIS X 0201 abgebildet wurde, zu trennen, und eine Menge Legacy-Code mit dieser Nutzung existiert. ( Diese Kodierung ersetzt auch Tilde '~' 0x7E durch Makron ̄, jetzt 0xAF.) Die Trennung dieser Zeichen besteht in ISO 8859-1, von lang vor Unicode. Indische Skripte wie Tamil und Devanagari werden jeweils nur 128 Codepunkte zugewiesen, die dem ISCII-Standard entsprechen. Die korrekte Wiedergabe von Unicode Indic Text erfordert die Umwandlung der gespeicherten logischen Ordnungszeichen in visuelle Ordnung und die Bildung von Ligaturen (aka conjuncts) aus Komponenten. Einige lokale Gelehrte argumentierten zugunsten von Zuordnungen von Unicode-Punkte auf diese Ligaturen, gegen die Praxis für andere Schreibsysteme, obwohl Unicode enthält einige Arabische und andere Ligaturen für Rückwärtskompatibilität nur. Die Kodierung neuer Ligaturen in Unicode wird nicht geschehen, zum Teil weil der Satz der Ligaturen schriftartabhängig ist und Unicode eine von Schriftvariationen unabhängige Kodierung ist. Die gleiche Art von Problem entstand für das tibetische Drehbuch im Jahr 2003, als die Standardisierungsverwaltung Chinas die Kodierung von 956 vorbesetzten tibetischen Silben vorgeschlagen hatte, die jedoch für die Kodierung durch das betreffende ISO-Komiteee (ISO/IEC JTC 1/SC 2) abgelehnt wurden. Die thailändische Alphabet-Unterstützung wurde wegen der Bestellung von thailändischen Charakteren kritisiert. Die links vom vorangehenden Konsonanten geschriebenen Vokale sind im Gegensatz zu den Unicode-Darstellungen anderer Indic-Skripte in visueller Reihenfolge anstelle der phonetischen Ordnung. Diese Komplikation ist darauf zurückzuführen, dass Unicode den Thai Industrial Standard 620 erbte, der in der gleichen Weise funktionierte, und war die Art, wie Thai immer auf Tastaturen geschrieben worden war. Dieses Bestellproblem kompliziert den Unicode-Kollationsprozess leicht und erfordert Tabellenbetrachtungen, um Thai-Zeichen für die Kollation wiederherzustellen. Selbst wenn Unicode die Kodierung nach gesprochener Ordnung angenommen hätte, wäre es immer noch problematisch, Wörter in der Wörterbuchordnung zu kollatieren. E.g., das Wort bestimmungดуς [sa dуς] perform beginnt mit einem konsonanten Cluster สς (mit einem inhärenten Vokal für den Konsonanten ส,) das Vokal ≠,- in gesprochener Reihenfolge würde nach dem ◄ kommen, aber in einem Wörterbuch wird das Wort zusammengestellt, wie es geschrieben ist, mit dem Vokal nach ส. Kombinierende Zeichen Charaktere mit diakritischen Markierungen können in der Regel entweder als ein einziges vorkomponiertes Zeichen oder als zersetzte Folge eines Basisbriefes plus ein oder mehrere nicht-spacierende Markierungen dargestellt werden. So sollten z.B. schwerpunkt (vorausgesetzt e mit Makron und akut oben) und schwerpunkt (e, gefolgt von der Kombination von Makron oben und der Kombination von Akut oben) identisch dargestellt werden, wobei beide als e mit einem Makron- und Akutakzent erscheinen, aber in der Praxis kann ihr Aussehen je nach dem, was Rendering Engine und Schriften verwendet werden, um die Zeichen anzuzeigen. In ähnlicher Weise werden Unterpunkte, wie sie bei der Romanisierung von Indic benötigt werden, oft falsch platziert.. Unicode-Zeichen, die Karte zu vormontierten Glyphen verwenden können, können in vielen Fällen verwendet werden, so dass das Problem vermieden wird, aber wo kein vorkomponiertes Zeichen kodiert wurde, kann das Problem oft durch die Verwendung einer spezialisierten Unicode-Schrift wie Charis SIL, die Graphite, OpenType oder AAT-Technologien für fortgeschrittene Rendering-Funktionen verwendet werden. Anomalie Der Unicode-Standard hat Regeln zur Gewährleistung der Stabilität eingeführt. Je nach strenger Regel kann eine Änderung untersagt oder erlaubt werden. Zum Beispiel kann ein Name, der einem Codepunkt angegeben wird, nicht und wird sich nicht ändern. Aber eine Skript-Eigenschaft ist flexibler, nach Unicodes eigenen Regeln. In Version 2.0 hat Unicode viele Codepoint-Namen von Version 1 geändert. Im selben Moment erklärte Unicode, dass sich von da an ein zugewiesener Name an einen Codepunkt nie mehr ändern würde. Dies bedeutet, dass, wenn Fehler veröffentlicht werden, diese Fehler nicht korrigiert werden können, auch wenn sie trivial sind (wie in einem Beispiel mit der Schreibweise BRAKCET für BRACKET in einem Zeichennamen passiert). Im Jahr 2006 wurde eine Liste von Anomalien in Zeichennamen veröffentlicht, und ab Juni 2021 gab es 104 Zeichen mit identifizierten Problemen, zum Beispiel: U+2118 ℘ SCRIPT CAPITAL P: Das ist ein kleiner Brief. Die Hauptstadt ist U+1D4AB P MATHEMATICAL SCRIPT CAPITAL P. U+034F ͏ COMBINING GRAPHEME JOINER: Nicht bei Graphemen. U+A015 ꀕ YI SYLLABLE WU: Das ist kein Yi syllable, sondern ein Yi Iterationszeichen. U+FE18 〗 PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRAKCET: Klammer wird falsch geschrieben. Spelling-Fehler werden mit Unicode-Aliasnamen und Abkürzungen behoben. Siehe auch Vergleich der Unicode-Encodings Religiöse und politische Symbole in Unicode International Components for Unicode (ICU), jetzt als ICU-TC ein Teil der Unicode-Liste der Binärcodes Liste der Unicode-Zeichen Liste der XML- und HTML-Zeichen-Entitätsreferenzen Open-Source Unicode-Typfaces im Zusammenhang mit Unicode Unicode Unicode-Symbolen Universal Coded Character Set Lotus Multi-Multi-Byte-2019-Zeichen-Zeichen-Zeichen-Zeichen-Zeichen-Zeichen-Zeichen-Zeichen-Zeichen (LM-Zeichen-Zeichen-Zeichen) " Unicode aus einem Linguistischen Blickpunkt." In Haralambous, Yannis (Hrsg.). Proceedings of Graphemics im 21. Jahrhundert, Brest 2018. Brest: Fluxus Editions.pp.167–183.ISBN 978-2-9570549-1-6. Externe Links Offizielle Website · Offizielle technische Website Unicode bei Curlie Alan Wood's Unicode Resources – enthält Listen von Textverarbeitungsgeräten mit Unicode-Fähigkeit; Schriften und Zeichen werden nach Typ gegruppiert; Zeichen werden in Listen dargestellt, nicht Gitter. Unicode BMP Fallback Font – zeigt den Unicode-Wert eines beliebigen Zeichens in einem Dokument, einschließlich im Private Use Area, anstatt der Glyph selbst.