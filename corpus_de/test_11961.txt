Eliezer Shlomo Yudkowsky (born September 11, 1979) ist eine amerikanische künstliche Intelligenz (AI) der Orist und Schriftsteller, die am besten bekannt sind, um die Idee einer freundlichen künstlichen Intelligenz zu verbreiten. Er ist ein gemeinsamer Gründer und Forscher im Forschungs- und Technologie-Forschungsinstitut (MIRI), einer privaten Forschung, die in Berkeley, Kalifornien ansässig ist. Seine Arbeit an der Aussicht auf eine andauernde Intelligenz Explosion war ein Einfluss auf die Superintelligence von Nick Bostrom: Paths, Dangers, Strategien. Arbeiten im Bereich der künstlichen Sicherheitsziele Lernen und Anreize in Softwaresystemen Yudkowsky's Ansichten zu den sicherheitspolitischen Herausforderungen, die künftige Generationen von AI-Systemen mit sich bringen, werden im unteratelliten Textbuch in AI, Stuart Russell und Peter Norvig's Künstliche Intelligenz diskutiert: Ein moderner Ansatz. Lassen Sie sich feststellen, dass es schwierig ist, die allgemeinen Zielvorgaben durch Hand, Russell und Norvig den Vorschlag von Yudksky zu präzisieren, dass autonome und anpassbare Systeme für das richtige Verhalten im Laufe der Zeit entwickelt werden: Yudkowsky (2008) geht näher auf die Gestaltung einer freundschaftlichen AI ein. Er stellt fest, dass die Benutzerfreundlichkeit (ein Wunsch, den Menschen nicht zu schädigen) von Anfang an entwickelt werden sollte, aber dass die Designer erkennen sollten, dass ihre eigenen Designs entweicht werden können und dass der Roboter im Laufe der Zeit lernen und weiterentwickeln wird. Man stellt daher eine Herausforderung dar – es handelt sich um einen Mechanismus zur Entwicklung von AI im Rahmen eines Systems von Kontrollen und Gleichgewichten und um die Versorgungsfunktionen der Systeme zu gewährleisten, die angesichts dieser Veränderungen weiterhin freundschaftlich bleiben. In Reaktion auf die maßgeblichen Konvergenzanliegen, in denen autonome Entscheidungssysteme mit schlecht konzipierten Zielen Standardanreize für den Menschen haben würden, haben Yudkowsky und andere MIRI-Forscher empfohlen, Softwareagenten zu benennen, die sich auf sichere Standardverhalten verständigen, selbst wenn ihre Ziele missachtet sind. Kapazitätsprognose In dem explosionsgefährdeten Explosionsszenario von I. J. Good, recursive Selbstverbesserung der AI-Systeme schnell Übergang von submenschlichen allgemeinen Erkenntnissen zu Superintelligent. Nick Bostroms Buch Superintelligence: Paths, Dangers, Strategien zeigen im Detail ein gutes Argument, während sie von Yudkowsky schriftlich auf das Risiko hinweisen, dass anthropomorphisieren fortgeschrittene AI-Systeme Menschen dazu führen, die Natur einer Intelligence-Explosion zu untergraben. " AI könnte einen offensichtlich starken Sprung in die Intelligenz allein aufgrund von anthropomorphismus machen, die menschliche Tendenz, "village I" und Einstein als die extremsten Ende der Nachrichtenskala zu betrachten, anstatt nahezu unzerstörbare Punkte auf dem Ausmaß der Köpfe im Allgemeinen zu vermeiden." InArtificial Intelligence: Ein modernes Konzept, die Autoren Stuart Russell und Peter Norvig weisen den Widerspruch auf, dass es Grenzen für intelligente Problemlösung aus der rechnerischen Komplexitätstheorie gibt; wenn es starke Grenzen dafür gibt, wie effizient Algorithmen verschiedene computerwissenschaftliche Aufgaben lösen können, dann kann die Intelligenz nicht möglich sein. Leitmotiv der Rationalität zwischen 2006 und 2009 waren Yudkowsky und Robin Hanson die wichtigsten Beiträge zur Überwindung von Bias, einem kognitiven und gesellschaftlichen Blog, der von dem Institut für Humanität der Universität Oxford unterstützt wird. Yudkowsky gründete im Februar 2009 wenigerWrong, ein "Gemeinschafts-Blog zur Raffination der Art der menschlichen Rationalität". Überwindung von Bias hat sich seitdem als Hansons persönliches Blog bewährt. Mehr als 300 Blogposten von Yudkowsky zu Philosophie und Wissenschaft (ursprünglich mit weniger Falschbüchern und überwindbaren Bias) wurden als E-Mail mit dem Titel Rationalität veröffentlicht: Von der AI bis zu den Spielen des Forschungsinstituts für maschinelle Intelligenz (MIRI) 2015. MIRI hat auch Inadequate Equilibria, Yudksky's 2017 E-Books zum Thema gesellschaftliche Ineffizienzen veröffentlicht. Yudkowsky hat auch mehrere Kunstwerke geschrieben. Harry Potter, Harry Potter und die Methoden der Rationalität, verwendet die Elemente der J.K Hackling-Reihe Harry Potter, um Themen in der Wissenschaft zu illustrieren. Harry Potter und die Methoden der Rationalität bezeichneten das New Yorker Harry Potter als ein Zeichen für das Original von Hackling "in einem Versuch, Harry's Zauberry über die wissenschaftliche Methode zu erklären". Personalleben Yudkowsky ist ein autodidakt und hat nicht an der Schule oder Hochschule teilgenommen. Er wurde in der orthodoxen Judentum. Veröffentlichungen Yudkowsky, Eliezer (2007). "Levels of Organization in General Intelligence" (PDF). Künstliche allgemeine Intelligenz. Berlin: Springer. Yudkowsky, Eliezer (2008). " Kognitive Biases potenziell bevollmächtigte Richterin für globale Risiken (PDF). In Bostrom, Nick; Ćirković, Mailand (eds). Global Catastroph Risiken. Oxford University Press.ISBNUNG AM0199606504. Yudkowsky, Eliezer (2008)." In Bostrom, Nick; Ćirković, Mailand (eds). Global Catastroph Risiken. Oxford University Press.ISBNUNG AM0199606504. Yudkowsky, Eliezer (2011). " Komplexe Wertesysteme in umweltfreundlicher AI" (PDF). Künstliche allgemeine Intelligenz: 4. Internationale Konferenz, AGI 2011, Bergblick, CA, USA, August 3–6, Berlin: Springer. Yudkowsky, Eliezer (2012).“ freundschaftliche Intelligenz. Garten, Ammon; Moor, James; Søraker, John; et al.(eds). Farbstoff Hypothesen: Eine wissenschaftliche und theoretische Bewertung. Berliner: Springer.pp.181-195.doi:10.1007/978-3-642-32560-1_10.ISBN @3-642-32559-5.Bostrom, Nick; Yudkowsky, Eliezer (2014). "Die Ethik der künstlichen Intelligenz" (PDF). In Frankish, Keith; Ramsey, William (eds). Cambridge Handbuch der künstlichen Intelligenz. New York: Cambridge University Press.ISBN UV0-521-87142-6.LaVictoire, Patrick; Fallenstein, Benja; Yudkowsky, Eliezer; Bárász, Mihály; Christiano, Paul; Herreshoff, Marcello (2014) Programm Equilibrium in der Tat des Strafgefangenen über Löb's Theorem. Multiagenzien-Wechselwirkungen ohne vorherige Koordinierung: Papiere aus dem AAAI-14-Workshop. AAAI Veröffentlichungen. Soares, Nate; Fallenstein, Benja; Yudkowsky, Eliezer (2015)."Corrigibility (PDF). AAAI-Workshops: Workshops auf der 20-Ninth AAAI-Konferenz über künstliche Intelligenz, Austin, TX, Januar 25–26, 2015.AAAI-Veröffentlichungen. Siehe auch AI-Box-freundliche künstliche Intelligenz offenes Schreiben zu künstlichen Datenreferenzen Außenbeziehungen offizielle Website Rationalität: von AI bis Game (entire Book online)