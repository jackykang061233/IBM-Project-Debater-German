Spracherkennung ist ein interdisziplinärer Teilbereich von Informatik und Informatik, der Methoden und Technologien entwickelt, die die Anerkennung und Übersetzung der gesprochenen Sprache durch Computer ermöglichen. Sie ist auch als automatische Sprachanerkennung (ASR), Anerkennung von Computern oder Rede zum Text (STT). Wissen und Forschung in den Bereichen Computerwissenschaften, Sprachen und Computertechnik. Manche Spracherkennungssysteme erfordern eine Ausbildung (auch „Rolling“), bei der ein einzelner Redner Text lesen oder ein isoliertes Votum in das System einführt. Das System untersucht die spezifische Stimme der Person und nutzt es, um die Anerkennung der Rede dieser Person zu verbessern, was zu einer höheren Genauigkeit führt. Systeme, die keine Ausbildung verwenden, werden als „redunabhängige Systeme“ bezeichnet. Systeme, die die Ausbildung nutzen, sind "speaker abhängig". Spracherkennungsanwendungen umfassen Sprachanwenderschnittstellen wie Sprachwahlen (z.B. "Aufrufsland")," Anrufführung (z.B.: "Ich möchte einen Sammelruf erstellen")," domotische Gerätekontrolle, Suche nach Schlüsselworten (z.B. ein Podcast, wo bestimmte Wörter gesprochen wurden), einfacher Dateneintrag (z.B. bei Eintritt einer Kreditkartennummer), Erstellung strukturierter Dokumente (z.B. ein Radiologie-Bericht), Festlegung von Kommentaren, Red-to-Text-Verarbeitung (z.B. Textverarbeiter oder E-Mails) und (normalerweise direktes). Der Begriff der Spracherkennung oder -identifikation bezieht sich auf die Identifizierung des Sprechers und nicht auf das, was sie sagen. Anerkennung des Sprechers kann die Aufgabe der Übersetzung von Reden in Systeme, die in einer bestimmten Sprache ausgebildet wurden, vereinfachen oder es kann genutzt werden, um die Identität eines Sprechers als Teil eines Sicherheitsprozesses zu bescheinigen oder zu überprüfen. Aus technologischer Sicht hat die Redeerkennung eine lange Geschichte mit mehreren Wellen großer Innovationen. Kürzlich hat das Feld von Fortschritten im tiefen Lernen und großen Daten profitiert. Die Vorschüsse werden nicht nur durch das Aufkommen von Fachpapieren, die auf dem Gebiet veröffentlicht werden, sondern vor allem durch die weltweite Einführung einer Vielzahl tiefer Lernmethoden bei der Konzeption und Einführung von Spracherkennungssystemen belegt. Geschichte Kernbereiche des Wachstums waren: Vokulargröße, Sprecher Unabhängigkeit und Verarbeitungsgeschwindigkeit. Pre-1970 1952 – drei Bell Labs-Forscher, Stephen Balashek, R. Biddulph und K. H. Davis haben ein System namens Laurent für die digitale Anerkennung von Single-Speaker aufgebaut. Ihr System befindet sich in den Formanten im Stromspektrum jedes Kopfes. 1960 –Gunnar Fant entwickelte und veröffentlichte das Modell der Sprachproduktion. 1962 – IBM hat seine 16-Formel-Stibilitäts-Fähigkeit zur Anerkennung der Rede auf der Weltmesse von 1962 demonstriert. 1966 – Linear prädiktive Kodierung (LPC) wurde zunächst von Fumitada Itakura von Nagoya University und Shuzo Saito von Nippon Research und Telefon (NTT) vorgeschlagen, während sie an der Spracherkennung arbeiten. 1969 – Die Finanzierung von Bell Labs, die seit mehreren Jahren getrocknet wurde, als das einflussreiche John-Pflanzen 1969 ein offenes Schreiben schrieb, das kritisch und defundiert war. Letztendlich dauerte diese Defundierung bis der britische und James L. Flanagan. Raj Reddy war die erste Person, in den späten 60er Jahren als Hochschulabsolvent an der Universität von Stanford eine ständige Rede zu nehmen. Frühere Systeme mussten die Nutzer nach jedem Wort Pausen vornehmen. Reddy's System hat gesprochene Befehle für den Wettbewerb. In dieser Zeit haben die sowjetischen Forscher den dynamischen Zeitkrieg (DTW)-Algorithmus entwickelt und es genutzt, um einen anerkennbaren Anscheinwerfer zu schaffen, der auf einem 200-Vorwort-Vaktor arbeitet. DTW verarbeitete Rede, indem es in kurze Rahmen, z.B. 10ms-Segmente, unterteilt wird und jeder Rahmen als einheitliche Einheit verarbeitet wird. DTW würde zwar durch spätere Algorithmen ersetzt, aber die angewandte Technik. In dieser Zeit blieb die Unabhängigkeit der Sprecher ungelöst. 1970-90 1971 – DARPA finanzierte fünf Jahre für Sprachforschung, Spracherkennungsforschung, die eine Mindest-Vaktur von 1.000 Wörtern anstrebt. Sie würden die Meinungsverschiedenheit als Schlüssel für Fortschritte bei der Sprachanerkennung betrachten, doch dies hat sich später als unwahr erwiesen. BBN, IBM, Carnegie Mellon und Stanford Research Institute nahmen alle an dem Programm teil. Diese neu belebte Redeerkennungsforschung nach dem Schreiben des John-Cyber. 1972 – The Canons, Rede und Signalverarbeitungsgruppe veranstalteten eine Konferenz in Newton, Massachusetts. 1976 – Das erste ICASSP wurde in Philadelphia abgehalten, das seither ein wichtiger Ort für die Veröffentlichung von Forschungsarbeiten zur Spracherkennung war. Leonard Baum entwickelte in den späten 1960er Jahren die Mathematik der Markov-Ketten am Institut für Verteidigungsanalyse. Nach einem Jahrzehnt begannen die Schüler von Raj Reddy James Baker und Janet M. Baker mit dem versteckten Markov-Modell (HMM) für die Spracherkennung. James Baker hatte während seines Studiums über HMMs von einer Sommerstelle im Institut für Verteidigungsanalyse erfahren. Die Verwendung von HMMs ermöglichte es Forschern, verschiedene Wissensquellen wie akustische, Sprache, Sprache und Elektronik in einem einheitlichen probabilistischen Modell zu kombinieren. Mitte der 80er Jahre gründete das Team von IBM Fred Jelinek eine von der Stimme aktivierte Typautorin namens Tira, die mit einem 20-Punkte-Vaknomin Jelinek's statistischen Ansatz weniger auf die Gestaltung der menschlichen Gehirnprozesse hinweisen und die Rede zur Verwendung statistischer Modelltechniken wie HMMs verstehen. (Jelinek-Gruppe entdeckte unabhängig die Anwendung von HMMs auf Rede.) Das war umstritten, da die HMM zu simuliert sind, um viele gemeinsame Merkmale menschlicher Sprachen zu berücksichtigen. Jedoch hat sich der HMM als sehr nützliches Mittel für die Modellierung von Reden erwiesen und die dynamische Zeitspanne ersetzt, um in den 80er Jahren zum marktbeherrschenden Sprachanerkennungsgorithmus zu werden. 1982 – Dragon Systems, gegründet von James und Janet M. Baker, war einer der wenigen Konkurrenten von IBM. Praktische Spracherkennung In den 80er Jahren wurde auch das n-gram-Sprachmodell eingeführt. 1987 – Das Back-off-Modell erlaubte es Sprachmodellen, mehrere Länge n-grams zu verwenden, und CSELT verwendet HMM, um Sprachen zu erkennen (bei Software und bei Hardware spezialisierten Prozessoren, z.B. RIPAC). Viele der Fortschritte auf dem Gebiet sind auf die rasant wachsenden Fähigkeiten von Computern zurückzuführen. Am Ende des DARPA-Programms im Jahr 1976 war der beste Computer für die Forscher die PDP-10 mit 4 MB-ram. Es könnte bis zu 100 Minuten dauern, bis es nur 30 Sekunden dauern kann. Zwei praktische Produkte waren: 1984 – wurden mit bis zu 4096 Worten unterstützt, von denen nur 64 zu einer Zeit in RAM gehalten werden könnten. 1987 – ein anerkennter von Kurzweil Applied Intelligence 1990 – Dragon Diktaturate, ein im Jahr 1990 veröffentlichtes Verbraucherprodukt AT &T hat 1992 den Voice-Anmelungs-Service eingeführt, um Telefongespräche ohne den Einsatz eines menschlichen Betreibers zu führen. Lawrence Rabiner und andere in Bell Labs entwickelt. In diesem Punkt war das Vokular des typischen kommerziellen Spracherkennungssystem größer als das durchschnittliche menschliche Vokular. Raj Reddy's ehemaliger Student, Xuedong Huang, entwickelte das Sphinx-II-System an der Kapitalmarktunion. Das Sphinx-II-System war das erste Mal, wenn er Sprecherin, großem Vokabular, ständige Spracherkennung und die beste Leistung in der Bewertung von DARPA von 1992. Umgang mit ständiger Rede mit einem großen Vokabular war ein wichtiger Meilenstein in der Geschichte der Spracherkennung. Huang wurde 1993 in Microsoft die Redeerkennungsgruppe gefunden. Raj Reddy's Student Kai-Fu Lee kam zu Apple, wo er 1992 einen Sprach-Schnittstellen-Proto für den Apple-Computer namens Casper entwickelt hatte. Lernout & Hauspie, ein belgisches Sprachanerkennungsunternehmen, erwarb mehrere andere Unternehmen, darunter Kurzweil Applied Intelligence 1997 und Dragon Systems im Jahr 2000. L &H-Sprachtechnik wurde im Windows XP-Betriebssystem eingesetzt. L &H war Marktführer, bis 2001 ein Buchskandal dem Unternehmen ein Ende gebracht hat. L &H wurde 2005 von ScanSoft gekauft. Apple ursprünglich lizenzierte Software von Nuance zur Bereitstellung von Spracherkennungsfähigkeit für seine digitale Assistent Siri 2000s In den 2000er Jahren unterstützte DARPA zwei Spracherkennungsprogramme: effektive, bezahlbare, wiederverwendbare Red-to-Text (EARS) im Jahr 2002 und globale autonome Sprachverwendung (GALE). Vier Teams nahmen am EARS-Programm teil: IBM, ein Team von BBN mit LIMSI und Univ.of Pittsburgh, Cambridge University und ein Team aus ICSI, SRI und University of Washington. EARS finanzierte die Sammlung der Telefonanschrift Korpus mit 260 Stunden registrierter Gespräche von über 500 Rednern. Das GALE-Programm konzentrierte sich auf arabische und chinesische Nachrichtennachrichten. Googles erste Bemühungen zur Spracherkennung kam 2007 nach der Einstellung einiger Forscher aus Nuance. Das erste Produkt war GOOG-411, ein Telefonverzeichnisdienst. Die Aufzeichnungen von GOOG-411 haben wertvolle Daten produziert, die Google bei der Verbesserung ihrer Anerkennungssysteme helfen. Google Voice Jetzt wird die Suche in über 30 Sprachen unterstützt. In den Vereinigten Staaten hat die Nationale Sicherheitsbehörde seit mindestens 2006 eine Art von Redeerkennung für Schlüsselspoting genutzt. Diese Technologie ermöglicht es Analysten, über große Mengen registrierter Gespräche zu suchen und die Erwähnung von Stichwörtern zu isolieren. Aufzeichnungen können indexiert werden und Analysten können Anfragen über die Datenbank führen, um Interessenkonflikte zu finden. Manche Forschungsprogramme der Regierung konzentrierten sich auf Nachrichtenanwendungen der Redeerkennung, z.B. das EARS-Programm von DARPA und das Programm IARPA. In den frühen 2000er Jahren wurde die Redeerkennung immer noch von traditionellen Ansätzen wie versteckten Markov-Modelle in Kombination mit künstlichen Neuralnetzen dominiert. Heute wurden jedoch viele Aspekte der Spracherkennung durch eine vertiefte Lernmethode namens Long-term Memory (LSTM), ein von Sepp Hochreserv & Jürgen Schmidhuber 1997 veröffentlichtes neurales Netz übernommen. LSTM RNNs vermeiden das schwindende Bewegungsproblem und können "Very Deep Learning"-Aufgaben lernen, die eine Erinnerung an Ereignisse erfordern, die vor Tausenden von Zeitschritten geschehen, was für die Rede wichtig ist. Lediglich 2007 begann die LSTM, die von der Verbindungsisten Temporalklassifikation (CTC) ausgebildet wurde, die traditionelle Spracherkennung in bestimmten Anwendungen auszuschalten. Im Jahr 2015 berichtete Googles Redeerkennung über einen dramatischen Leistungssprung von 49 % durch CTC-trained LSTM, der nun über Google Voice für alle Smartphone-Nutzer verfügbar ist. Im späteren Teil des Jahres 2009 wurde die Nutzung von tiefen Feedforward-Netzen für akustisches Modelling von Geoffrey Hinton und seinen Studenten an der Universität Toronto und von Li Deng und Kollegen in Microsoft Research eingeführt, zunächst in der gemeinsamen Arbeit zwischen Microsoft und der Universität Toronto, die später auf IBM und Google ausgeweitet wurde (Gegenheit "Die gemeinsamen Standpunkte von vier Forschungsgruppen" im Prüfungspapier 2012). Microsoft Research Executive nannte diese Innovation „die dramatischste Veränderung der Genauigkeit seit 1979“. Im Gegensatz zu den stetigen inkrementellen Verbesserungen der letzten Jahrzehnte verringerte die Anwendung des tiefen Lernens die Fehlerquote um 30 %. Diese Innovation wurde schnell auf dem Gebiet angenommen. Forscher haben damit begonnen, auch für das Sprachmodell umfassende Lerntechniken zu nutzen. In der langen Geschichte der Redeerkennung wurden in den 80er Jahren, 1990er Jahren und einigen Jahren in den 2000er Jahren sowohl flache Form als auch tiefe Form (z.B. wiederkehrende Netze) künstlicher Neuralnetze untersucht. Diese Methoden wurden jedoch nie über das nicht-uniforme interne Handhandwerking Gausssian-Mixmodell/Hidden Markov-Modell (GMM-HMM) gewonnen, das auf physikalisch-technischen Modellen der Rede beruht. In den 90er Jahren wurden eine Reihe von Schlüsselschwierigkeiten methodisch analysiert, darunter die sich abschwächende und schwache zeitliche Korrelationsstruktur in den prädikativen Neuralmodellen. All diese Schwierigkeiten waren in diesen frühen Tagen neben dem Mangel an großen Ausbildungsdaten und der großen Rechenleistung. Die meisten Reden erkennen Forscher an, die solche Barrieren verstanden haben, und weg von Neuralnetzen, um physikalisch modellierende Ansätze zu verfolgen, bis die jüngste Resurgence des tiefen Lernens von 2009 bis 2010 begonnen hat, die alle diese Schwierigkeiten überwunden haben. Hinton et al.and Deng et al. überprüfte Teil dieser jüngsten Geschichte, wie ihre Zusammenarbeit mit einander und dann mit Kollegen in vier Gruppen (Universität Toronto, Microsoft, Google und IBM) eine Renaissance der Anwendungen von tiefen Feedforward-Neural-Netzen zur Spracherkennung bewirkt. 2010sBy früh 2010s Rede anerkennt, auch die Spracherkennung wurde deutlich von der Anerkennung von Sprechern unterschieden, und die Sprecherabhängigkeit wurde als wichtiger Durchbruch angesehen. Bis dahin erforderten die Systeme eine Ausbildungszeit. A 1987 Ad für eine Puppen hatte die Markierung "Finally, die Puppen, die Sie verstehen" – trotz der Tatsache, dass sie als "die Kinder auf ihre Stimme sprechen könnten". Im Jahr 2017 erreichten Microsoft-Forscher einen historischen Meilenstein der transkrbingen Telefonansprache auf der weitverbreiteten Benchmark-Aufstellung. Mehrere Modelle des tiefen Lernens wurden genutzt, um die Richtigkeit der Redeanerkennung zu optimieren. Die Fehlerquote bei der Sprachanerkennung wurde gemeldet, dass es sich bei 4 berufstätigen Human-Tranchen, die gemeinsam an demselben Benchmark arbeiten, um so gering wie möglich zu sein, was von IBM Watson-Sprachteam zu derselben Aufgabe finanziert wurde. Modelle, Methoden und Algorithmen Sowohl akustische Modellierung als auch Sprachmodellierung sind wichtige Bestandteile moderner statistisch fundierter Sprachanerkennungsgorithmen. versteckte Markov Modelle (HMMs) werden in vielen Systemen weit verbreitet. Sprachmodellierung wird auch in vielen anderen natürlichen Sprachverarbeitungsanwendungen wie Dokumentenklassifikation oder statistische Übersetzung verwendet. versteckte Markov Modelle moderne allgemeine Spracherkennungssysteme basieren auf versteckten Markov Modellen. Dies sind statistische Modelle, die eine Sequenz von Symbolen oder Mengen darstellen. HMMs werden in Redeerkennung verwendet, weil ein Redesignal als ein stückhaftes Signal oder ein kurzzeitiges Signal angesehen werden kann. Kurzfristig (z.B. 10 Millis) kann die Rede als stationärer Prozess angeglichen werden. Rede kann als Markov-Modell für viele katastrophale Zwecke betrachtet werden. Ein weiterer Grund, warum HMMs populär sind, ist, dass sie automatisch ausgebildet werden können und einfach und rechnerisch machbar sind. In der Redeerkennung würde das versteckte Markov-Modell eine Sequenz von n-dimensionalen realen Wertträgern (mit einer kleinen Zahl, wie 10,) erzeugen, die eine dieser 10 Milleniums bilden. Die Vektor würden aus cepstralen Koeffizienten bestehen, die durch die Einführung eines Vierer-Umwandlungsfensters und die Ausprägung des Frequenzspektrums durch eine Kosin-Umwandlung gewonnen werden, dann aber die ersten (meist signifikanten) Koeffizienten. Das versteckte Markov-Modell wird in jedem Staat eine statistische Verteilung haben, die eine Mischung aus diagonaler Kovarianz Gaussians ist, die für jeden beobachteten Vektor eine Wahrscheinlichkeit bieten wird. Jedes Wort oder (für allgemeinere Spracherkennungssysteme) wird jede Telefonme eine andere Outputverteilung haben; ein verstecktes Markov-Modell für eine Sequenz von Wörtern oder Telefonen wird durch Vermittlung der individuellen versteckten Markov-Modelle für die getrennten Wörter und Telefone hergestellt. oben beschrieben sind die Kernelemente des am häufigsten auf HMM basierenden Ansatzes zur Spracherkennung. moderne Spracherkennungssysteme verwenden verschiedene Kombinationen verschiedener Standardtechniken, um die Ergebnisse im oben beschriebenen Grundkonzept zu verbessern. Ein typisches großflächiges System würde eine Kontextabhängigkeit für die Telefone (sowie Telefone mit unterschiedlichem linken und rechten Kontext haben unterschiedliche Umsetzungen wie HMM-Staaten); es würde die Normalisierung von Hormonen zum Normalisieren für einen anderen Redner und Aufnahmebedingungen verwenden; für weitere Redner-Normung könnte es eine Normalisierung der Votumslänge (VTLN) für die Normalisierung von männlichen Frauen und maximal linearen Regression (MLLR) für allgemeinere Redneranpassung verwenden. Die Merkmale hätten so genannte Delta- und delta-delta-Koeffizienten, um die Rededynamik zu erfassen, und könnten außerdem heteroscede lineare Diskriminierende Analysen (HLDA) verwenden oder die Delta- und Delta-delta-Koeffizienten auslösen und die Milz und eine LDA-basierte Projektion verwenden, gefolgt vielleicht durch heteroscede lineare Diskriminierende Analyse oder eine globale halbtiierte Kovarianz (auch bekannt als Maximalwahrscheinlich lineare Umwandlung oder MLLT). Viele Systeme nutzen so genannte diskriminierende Ausbildungstechniken, die mit einem rein statistischen Ansatz für die HMM-Parameterschätzung umgehen und stattdessen eine klassifikationsbezogene Maßnahme der Ausbildungsdaten optimieren. Beispiele sind maximal gegenseitige Informationen (MMI), Mindestklassifikationsfehler (MCE) und Mindest-Telefonfehler (MPE). Dekodierung der Rede (der Begriff für das, was geschieht, wenn das System mit einer neuen Empfängnis präsentiert wird und die wahrscheinlichste Quellenstrafe berechnet werden muss), würde wahrscheinlich den Viterbi-Algorithmus nutzen, um den besten Weg zu finden, und hier gibt es eine Wahl zwischen dynamischer Herstellung eines kombinierten versteckten Markov-Modells, das sowohl die akustischen als auch die sprachlichen Modellinformationen umfasst und ihn statisch vorsieht (der Finnit-Transduktor oder FST-Ansatz). Eine mögliche Verbesserung der Entschlüsselung ist die Beibehaltung einer Reihe guter Kandidaten anstelle des besten Kandidaten und die Verwendung einer besseren Messfunktion (re-Punkt) zur Erhöhung dieser guten Kandidaten, damit wir nach diesem verfeinerten Fortschritt am besten gewinnen können. Die Auswahl der Kandidaten kann entweder als Liste (der N-best-Liste-Ansatz) oder als Teil der Modelle (a lattice) gehalten werden. Leistungspunkte werden in der Regel durchgeführt, indem versucht wird, das Risiko der Buchten (oder eine Annäherung) zu minimieren: Anstatt die Quellenstrafe mit maximaler Wahrscheinlichkeit zu verhängen versuchen wir, den Satz zu verhängen, der die Wahrscheinlichkeit einer bestimmten Verlustfunktion im Hinblick auf alle möglichen Umschreibungen minimiert (d. h. wir nehmen den Satz an, der die durchschnittliche Entfernung zu anderen möglichen Sätzen, die durch ihre geschätzte Wahrscheinlichkeit gewichtet werden). Die Verlustfunktion ist in der Regel die Entfernung von Levenshtein, obwohl sie unterschiedliche Entfernungen für bestimmte Aufgaben darstellen kann; die Reihe möglicher Umladungen ist natürlich zur Aufrechterhaltung der Trassenbarkeit. Leistungsfähige Algorithmen wurden entwickelt, um die Lattices als gewichtete Finite-Staatstransduktoren mit der Bearbeitung von Entfernungen zu überprüfen, die sich als Finite-Staatstransdukter, der bestimmte Annahmen überprüft. Dynamische Zeitwarnsysteme (DTW)-basierte Redeerkennung dynamischer Zeiten ist ein Ansatz, der historisch für die Anerkennung der Rede verwendet wurde, aber inzwischen weitgehend durch den erfolgreicheren HMM-basierten Ansatz vertrieben wurde. Dynamische Zeitkriege sind ein Algorithmus zur Messung der Ähnlichkeit zwischen zwei Sequenzen, die sich in Zeit oder Geschwindigkeit unterscheiden können. Beispielsweise würden Ähnlichkeiten bei den Wandermustern festgestellt, auch wenn in einem Video die Person langsam und wenn sie in einem anderenhe oder schneller unterwegs war, oder auch wenn im Verlauf einer Beobachtung Beschleunigung und Verlangsamung bestanden. DTW wurde auf Video, Audio und Grafik angewandt – tatsächlich können alle Daten, die in eine lineare Darstellung umgewandelt werden können, mit DTW analysiert werden. Ein bekannter Antrag wurde automatisch als Spracherkennung anerkannt, um verschiedene Sprachgeschwindigkeiten zu bewältigen. Insgesamt ist es eine Methode, die es einem Computer ermöglicht, ein optimales Gleichgewicht zwischen zwei bestimmten Sequenzen (z.B. Zeitreihen) mit bestimmten Einschränkungen zu finden. Dies ist, die Sequenzen sind nicht linear zueinander gewarnt. Diese Sequenzierungsmethode wird oft im Zusammenhang mit versteckten Markov-Modellen verwendet. Neural-Netze Neural-Netze haben sich in den späten 80er Jahren als attraktives akustisches Modellkonzept entwickelt. Seither wurden in vielen Aspekten der Spracherkennung wie Telefonnummernklassifikation, Telefonnummernklassifikation durch multi-zielle evolutionäre Entwicklungsgorithmen, isolierte Worterkennung, audiovisuelle Sprachanerkennung, audiovisuelle Sprecheranerkennung und Sprecheranpassung genutzt.Neural-Netze machen weniger explizite Annahmen bezüglich statistischer Eigenschaften als HMMs und verfügen über mehrere Qualitäten, die sie für die Anerkennung von Reden attraktiv machen. Bei der Schätzung der Wahrscheinlichkeit eines Sprachmerkmalsegments ermöglichen neurale Netze eine natur- und effiziente Ausbildung. Trotz ihrer Wirksamkeit bei der Klassifizierung von Kurzzeiteinheiten wie einzelne Telefone und isolierte Wörter waren die frühen Neuralnetze jedoch selten erfolgreich für kontinuierliche Anerkennungsaufgaben, da sie nur begrenzt in der Lage sind, zeitliche Abhängigkeiten zu modellieren. Ein Ansatz für diese Beschränkung war die Nutzung von Neuralnetzen als Vorverarbeitung, Merkmalsumwandlung oder Eindämmung der Dimension vor der HMM-basierten Anerkennung. Kürzlich haben LSTM und damit verbundene Neuralnetze (RNNs) und zeitverzögerte Neuralnetze (TDNN) in diesem Bereich jedoch eine bessere Leistung bewiesen. Deep Feedforward und wiederkehrende Neuralnetze Deep Neural Networks und Denoising Autoencoders werden ebenfalls untersucht. Ein tiefes Feedforward-Neuralnetz (DNN) ist ein künstliches Neuralnetz mit mehreren versteckten Einheiten zwischen den Eingangs- und Produktionsschichten. ähnlich wie flache Neuralnetze können DNNs komplexe nichtlineare Beziehungen gestalten. DNN-Architekturen erzeugen Zusammenstellungsmodelle, in denen zusätzliche Schichten die Zusammensetzung von Merkmalen aus niedrigeren Schichten ermöglichen, eine enorme Lernkapazität und damit das Potenzial der Modellierung komplexer Muster von Sprachdaten. Im Jahr 2010 kam es in Zusammenarbeit mit Wissenschaftlern zu einem Erfolg der DNNs in der großen bulgarischen Sprachanerkennung, wo große Outputschichten der DNN auf der Grundlage von durch die Entscheidungsbäume errichteten Kontextstaaten angenommen wurden. umfassende Bewertungen dieser Entwicklung und des Stands der Kunst vom Oktober 2014 im jüngsten Springer-Buch von Microsoft Research. Siehe auch den Kontext der automatischen Spracherkennung und die Auswirkungen verschiedener maschinenlesbarer Lernparate, insbesondere des tiefen Lernens, in den letzten Übersichtsartikeln. Ein grundlegendes Prinzip des tiefen Lernens ist es, mit handhandwerklicher Funktionstechnik zu umgehen und Rohstoffe zu nutzen. Dieses Prinzip wurde zunächst erfolgreich in der Architektur von tiefen autoencoder auf dem Rohstoffspektrogramm oder linearen Filterbanken untersucht, was seine Überlegenheit über die Mel-Cepstral-Funktionen zeigt, die einige Phasen der festen Transformation von Spektrogrammen enthalten. Die wahren Rohstoffmerkmale von Rede, Wellenforms, haben sich vor kurzem gezeigt, hervorragende, umfassendere Spracherkennungsergebnisse zu erstellen. End-to-end automatische Redeerkennung Seit 2014 gibt es viel Forschungsinteresse an der End-to-end ASR. traditionelle Telefon-basierte (d. h. alle HMM-basierten Modelle) Ansätze erfordern getrennte Komponenten und Schulungen für die Aussprache, akustische und sprachliches Modell. End-to-End-Modelle lernen gemeinsam alle Komponenten des Sprachanlegers. Dies ist wertvoll, da es den Ausbildungsprozess und den Einsatz erleichtert. Zum Beispiel ist ein n-gram-Sprachmodell für alle HMM-basierten Systeme erforderlich, und ein typisches n-gram-Sprachmodell führt häufig mehrere Gigabits in Erinnerung, die sie für den Einsatz auf mobilen Geräten unpraktisch sind. Infolgedessen werden moderne kommerzielle ASR-Systeme von Google und Apple (Stand 2017) auf der Cloud eingesetzt und erfordern eine Netzverbindung gegenüber dem Gerät vor Ort. Der erste Versuch zum Ende des ASR war mit den von Alex Grabs von Google DeepMind und Navdeep Jaitly der University of Toronto im Jahr 2014 eingeführten Anschlusssystemen der Temporalklassifikation (CTC) verbunden. Das Modell bestand aus wiederkehrenden Neuralnetzen und einer CTC-Ebene. Gemeinsam lernt das RNN-CTC-Modell die Aussprache und das akustische Modell zusammen, aber es ist nicht in der Lage, die Sprache zu lernen, weil die Annahmen der Unabhängigkeit, ähnlich wie ein HMM, abhängig sind. CTC-Modelle können sich daher direkt an englische Figuren orientieren, aber die Modelle machen viele gemeinsame Fehler und müssen auf ein separates Sprachmodell zurückgreifen, um die Transkripte zu sanieren. Baidu hat sich später auf die Arbeit mit extrem großen Datensets ausgeweitet und hat einen kommerziellen Erfolg in chinesischer Sprache und Englisch bewiesen. Im Jahr 2016 stellte die Universität Oxford LipNet, das erste end-to-end-s-Ligungs-Modell, das Spatiotemporal convolutions in Verbindung mit einer RNN-CTC-Architektur, die die Leistung auf menschlicher Ebene in einem eingeschränkten Gehaltstest übertraf. Im Jahr 2018 wurde eine groß angelegte CNN-RNN-CTC-Architektur von Google DeepMind vorgestellt, die sechsmal bessere Leistung als menschliche Experten erreicht. Ein alternativer Ansatz für CTC-basierte Modelle sind maßgeschneiderte Modelle. Betonbasierte ASR-Modelle wurden gleichzeitig von Chan et al.of Carnegie Mellon University und Google Brain und Bahdanau et al. der Universität Montrealin 2016 eingeführt. Das Modell mit dem Titel "Aufzeichnungen, Anbietungen und Teller" (LAS), das genau dem akustischen Signal gehört, widmet sich unterschiedlichen Teilen des Signals und legt die Transkripte zu einer Zeit heraus. Im Gegensatz zu CTC-basierten Modellen haben maßgeschneiderte Modelle keine an Auflagen gebundenen Annahmen und können alle Komponenten eines Sprachanerkennungers, einschließlich der Aussprache, des akustischen und sprachlichen Modells, direkt kennen. Man braucht während des Einsatzes kein Sprachmodell, das es für Anwendungen mit eingeschränktem Gedächtnis sehr praktisch macht. Ende 2016 haben die auf der Website basierenden Modelle einen erheblichen Erfolg erzielt, einschließlich der Ausmusterung der CTC-Modelle (mit oder ohne Modell einer Fremdsprache). Seit dem ursprünglichen LAS-Modell wurden verschiedene Erweiterungen vorgeschlagen. Latent Sequence Decompositions (LSD) wurde von der Carnegie Mellon University, MIT und Google Brain vorgeschlagen, direkt Unterworteinheiten zu vergeben, die mehr natürliche als englische Zeichen sind; University of Oxford und Google DeepMind erweiterte LAS auf "Überwachung, Hör-, Teilnehmer- und Schenkung" (WLAS) um die Leseleistung auf menschlicher Ebene zu übertreffen. AnwendungenIn-Auto-Systeme In der Regel ermöglicht ein manuelles Kontrollmaterial, z.B. mittels einer Fingerkontrolle auf dem Lenkrad, das Spracherkennungssystem, und dies wird dem Fahrer per Audio-Notruf signalisiert. Nach dem Audio-Notruf verfügt das System über ein "Aufhörfenster", bei dem es einen Redebeitrag zur Anerkennung akzeptieren kann. einfache Sprachanweisungen können verwendet werden, um Telefongespräche zu starten, Radiostationen auszuwählen oder Musik von einem kompatiblen Smartphone, MP3 Spieler oder Musik-loaded-Flash-Laufwerk zu spielen. Spracherkennungskapazitäten unterscheiden sich zwischen Autoherstellung und Modell. Manche der jüngsten Automodelle bieten eine natürliche Sprachanerkennung im Rahmen eines festen Satzes von Befehlen und ermöglichen dem Fahrer vollständige Sätze und gemeinsame Sätze. Mit solchen Systemen ist es daher nicht notwendig, dass der Nutzer eine Reihe von festen Befehlsnummern vorlegt. Medizinische Versorgung Im Gesundheitswesen kann die Redeerkennung vor dem Ende oder dem Ende des medizinischen Dokumentationsverfahrens umgesetzt werden. Front-end-Ansprache-Anerkennung ist, wo der Anbieter in einen Sprachanerkennungsmotor setzt, die anerkannten Wörter werden als gesprochen, und der Angeklagte ist für die Bearbeitung und Unterzeichnung des Dokuments verantwortlich. Back-end oder dehnungsbezogene Anerkennung ist, wenn der Anbieter in ein digitales Urteilssystem setzt, wird die Stimme durch eine Spracherkennungsmaschine geleitet, und der anerkannte Entwurf des Dokuments wird mit der Originalsprache an den Herausgeber weitergeleitet, wo der Entwurf veröffentlicht und abschließend berichtet wird. Zurzeit wird in der Branche weit verbreitet die Anerkennung der Redefreiheit verwendet. Eines der wichtigsten Fragen im Zusammenhang mit der Verwendung der Spracherkennung in der Gesundheitsversorgung ist, dass das amerikanische Konjunktur- und Reinvestment Act von 2009 (ARRA) erhebliche finanzielle Vorteile für Ärzte vorsieht, die einen EMR gemäß den „Meaningful Use“-Normen verwenden. Diese Normen erfordern, dass ein erheblicher Anteil der Daten vom EMR erhalten bleibt (jetzt als elektronisches Gesundheitsregister oder EHR bezeichnet). Die Verwendung der Sprachanerkennung ist im Rahmen einer radiologischen/pathologischen Auslegung, Fortschrittsvermerkung oder Ableitungsübersicht natürlich geeigneter für die Erstellung von Texttexten: Die ergonomischen Vorteile der Verwendung von Sprachanerkennung zur Aufnahme strukturierter diskreter Daten (z.B. numerische Werte oder Codes aus einer Liste oder einem kontrollierten Vokular) sind für Menschen, die eine Tastatur und Maus betreiben können. Ein wichtiger Punkt ist, dass die meisten EHRs nicht ausdrücklich auf die Nutzung von Spracherkennungsfähigkeiten zugeschnitten sind. Ein großer Teil der Interaktion zwischen dem Kliniker und dem EHR umfasst Navigation durch die Benutzeroberfläche mit Menüs und Tab/Shirts und ist stark von Tastatur und Maus abhängig: Sprachnavigation bietet nur geringe ergonomische Vorteile. Jedoch werden viele sehr kundenspezifische Systeme für Radiologie oder Pathologie-Sektion Sprach Makros, in denen die Verwendung bestimmter Begriffe – z.B. „normaler Bericht“ – automatisch eine große Zahl von Standardwerten ausfüllen und/oder Kesselbleche erzeugen, die mit der Art der Prüfung variieren – z.B. eine Brust Röntgen-Darm-Reihe gegen eine gastrointestinale Serie für ein Funksystem. therapeutische Verwendung von Spracherkennungssoftware in Verbindung mit Textverarbeitern hat Vorteile für eine kurzfristige Stärkung der Gehirn-AVM-Patienten, die mit einem Abschnitt behandelt wurden. Weitere Forschungsarbeiten müssen durchgeführt werden, um kognitive Vorteile für Personen zu ermitteln, deren AVM mit radiologischen Techniken behandelt wurden. Militärische Hochleistungs-Trägerflugzeuge sind im letzten Jahrzehnt auf die Prüfung und Bewertung der Redeerkennung in Kampfflugzeugen ausgerichtet. Insbesondere wurde das US-Programm in der Rede Anerkennung für die Advanced-F-16-Flugzeuge (F-16 VISTA), das Programm in Frankreich für die Flugzeuge von Abu Dhabi und andere Programme im Vereinigten Königreich, die sich mit einer Vielzahl von Flugzeugplattformen befassen. In diesen Programmen wurden Sprachanschauer erfolgreich in Kampfflugzeugen betrieben, wobei Funkfrequenzen festgelegt werden, ein autopilotsystem zu beherrschen, Parameter für die Lenkpunkt-Koordinierung und Waffenfreigabe festzulegen und Fluganzeige zu kontrollieren. Mit schwedischen Piloten, die im JAS-39 Gripen Cockpit fliegen, fand Englund (2004) die Anerkennung mit zunehmenden G-loads verschlechtert. In diesem Bericht kam auch zu dem Schluss, dass die Anpassung die Ergebnisse in allen Fällen erheblich verbessert hat und dass die Einführung von Modellen für die Atembewältigung deutlich verbessert wurde. Im Gegensatz zu dem, was zu erwarten wäre, wurden keine Auswirkungen der zerbrochenen englischen Redner gefunden. Es war offensichtlich, dass die spontane Rede Probleme für den Anerkennungserreger verursachte, wie erwartet. Ein eingeschränktes Vokabular und vor allem eine richtige DNA-Diagnose könnten daher die Anerkennungsgenauigkeit erheblich verbessern. Der Eurofighter Typhoon, der derzeit mit der UK RAF in Betrieb ist, verfügt über ein redaktionsunabhängiges System, das jeden Piloten verpflichtet, ein Muster zu erstellen. Das System wird nicht für sicherheitskritische oder waffenkritische Aufgaben verwendet, wie z.B. die Waffenfreigabe oder die Verringerung des Unterwagens, sondern für eine breite Palette anderer Cockpitfunktionen. Sprachbefehle werden durch visuelles und/oder aurales Feedback bestätigt. Das System wird als wichtiges Design bei der Reduzierung der Pilotenbelastung angesehen und ermöglicht es dem Piloten sogar, seine Flugzeuge mit zwei einfachen Sprachanweisungen oder einem seiner Flügel mit nur fünf Befehlen zuzuweisen. Sprecherunabhängige Systeme werden auch entwickelt und werden derzeit für den F35-Syber-II (JSF) und den Alenia Aermacchi M-346 Master-Vorreiter getestet. Diese Systeme haben bei über 98 % Wortgenauigkeits-Ergebnisse produziert. Hubschrauber Insbesondere in der Hubschrauberumgebung sowie in der Umgebung der Jet-Bekämpfung sind die Probleme, eine hohe Anerkennungsgenauigkeit unter Stress und Lärm zu erreichen. Das akustische Lärmproblem ist in der Hubschrauberumgebung tatsächlich schwerer, nicht nur wegen des hohen Lärmpegels, sondern auch, weil der Hubschrauber-Pilot im Allgemeinen keine Gesichtsmaske trägt, die akustische Lärm im Mikrofon verringern würde. Substantielle Test- und Bewertungsprogramme wurden in den letzten zehn Jahren in Spracherkennungssystemenanwendungen in Hubschraubern durchgeführt, insbesondere durch die Forschungs- und Entwicklungstätigkeit der US-Arme Avionics (AVRADA) und die Royal Aerospace-Einrichtung (RAE) im Vereinigten Königreich. Arbeiten in Frankreich haben die Spracherkennung im Puma Hubschrauber aufgenommen. Kanada hat auch viel nützliche Arbeit geleistet. Ergebnisse sind ermutigend, und Sprachanwendungen sind enthalten: Kontrolle von Kommunikationsradios, Festlegung von Navigationssystemen und Kontrolle eines automatischen Ziel-Abgabesystems. Wie bei den Kampfanwendungen ist das Hauptthema für die Stimme in Hubschraubern die Wirkung auf die Wirksamkeit von Piloten. ermutigende Ergebnisse werden für die AVRADA-Tests gemeldet, obwohl diese nur eine Durchführbarkeit in einem Testumfeld darstellen. Man muss noch viel sowohl in der Sprachanerkennung als auch in der allgemeinen Sprachtechnologie tun, um Leistungsverbesserungen in Betriebssituationen konsequent zu erzielen. Fluglotsen Training für Fluglotsen (ATC) stellt eine ausgezeichnete Anwendung für Spracherkennungssysteme dar. Viele ATC-Ausbildungssysteme verlangen derzeit, dass eine Person als Pseudo-Pilot handeln muss, in einem Sprachdialog mit dem für den Zug Verantwortlichen, der den Dialog simuliert, den der für die Verarbeitung Verantwortliche mit Piloten in einer echten ATC-Situation durchführen muss. Spracherkennungs- und Synthesetechniken bieten das Potenzial, die Notwendigkeit einer Person, als Pseudo-Pilot zu agieren, zu beseitigen und somit das Personal zu verringern. Theorie, Fluglotsen zeichnen sich auch durch eine hoch strukturierte Rede als primäre Leistung des für die Verarbeitung Verantwortlichen aus, so dass die Schwierigkeit der Spracherkennungsaufgabe möglich sein sollte. In der Praxis ist dies selten der Fall. In dem FAA Dokument 7110.65 sind die Begriffe aufgeführt, die von Fluglotsen verwendet werden sollten. In diesem Dokument gibt es weniger als 150 Beispiele solcher Begriffe, doch die Zahl der von einer der Simulationshersteller unterstützten Sätze beträgt über 500 000. USAF, USMC, US-amerikanische Armee, US-amerikanische Marine und FAA sowie eine Reihe internationaler ATC-Trainingsorganisationen wie die Royal australische Air Force und die Zivilluftfahrtbehörden in Italien, Brasilien und Kanada verwenden derzeit ATC- Simulatoren mit Redeerkennung verschiedener Anbieter. Telephony und andere Bereiche ASR sind jetzt im Bereich der Telefonie üblich und werden im Bereich Computerspiel und -simulation weit verbreitet. In den Telefonsystemen wird ASR nun überwiegend in Kontaktzentren eingesetzt, indem sie es mit den IVR-Systemen integriert. Trotz des hohen Integrationsgrads mit der Textverarbeitung im allgemeinen persönlichen Datenverarbeitung im Bereich der Dokumentenherstellung hat ASR nicht die erwartete Erhöhung der Verwendung zu erkennen. Die Verbesserung der Mobilfunk-Vertriebsgeschwindigkeiten hat die Spracherkennung in Smartphones praktisch gemacht. Kommentare werden hauptsächlich als Teil einer Benutzeroberfläche verwendet, um vordefinierte oder individuelle Sprachanweisungen zu erstellen. Nutzung in Bildung und Alltag Sprachunterricht kann für das Erlernen einer zweiten Sprache nützlich sein. Sie kann eine korrekte Aussprache unterrichten, um eine Person dabei zu unterstützen, Grippe mit ihren Sprachkompetenzen zu entwickeln. Studenten, die blind sind (siehe Blindheit und Bildung) oder sehr niedrige Visionen haben, können von der Nutzung der Technologie profitieren, um Wörter zu vermitteln und dann den Computer zu hören und einen Computer zu benutzen, indem sie mit ihrer Stimme betraut werden, anstatt den Bildschirm und die Tastatur zu prüfen. Studierende, die körperlich behindert sind oder unter einer Wiederholungsbelastung leiden, können durch die Verwendung von Sprach-to-Text-Programmen entlastet werden, weil sie sich Sorgen um Handaufnahme, Typierung oder Zusammenarbeit mit Schulbesuchen machen. Sie können auch die Spracherkennungstechnologie nutzen, um das Internet frei zu finden oder einen Computer zuhause zu benutzen, ohne eine Maus und Tastatur physisch zu betreiben. Spracherkennung kann es Studierenden mit Lernhindernissen ermöglichen, bessere Schriftsteller zu werden. Indem sie die Wörter Aloud angeben, können sie die Fluidität ihres Schreibens erhöhen und die Bedenken hinsichtlich der Angabe, Pünktierung und anderer Schreibweisen mildern. Man sehe auch eine Behinderung. Die Verwendung von Spracherkennungssoftware, in Verbindung mit einem digitalen Audioaufzeichnungsgerät und einer persönlichen computergestützten Textverarbeitungssoftware hat sich als positiv erwiesen, um beschädigte kurzfristige Speicherkapazitäten in Schlaganfall und Crianiotomy- Einzelnen wiederherzustellen. Menschen mit Behinderungen können von Spracherkennungsprogrammen profitieren. Personen, die Deaf oder Hard von Anhörungen sind, werden die Spracherkennungssoftware verwendet, um automatisch eine geschlossene Aufnahme von Gesprächen wie Diskussionen in Konferenzsälen, Vorträgen und/oder religiöse Dienstleistungen zu ermöglichen. Spracherkennung ist auch sehr nützlich für Menschen, die Schwierigkeiten haben, ihre Hände zu nutzen, von leichten wiederholten Stressverletzungen bis hin zu Behinderungen, die die Nutzung konventioneller Computer-Instrukturen verhindern. Menschen, die die Tastatur sehr viel benutzten und entwickelte RSI, wurden in der Tat zu einem dringenden frühen Markt für die Anerkennung von Reden. Spracherkennung wird in deaf Telephonie verwendet, wie Sprachmail an Text, Relaisdienste und gedeckeltes Telefon. Einzelpersonen mit Lernhindernissen, die Probleme mit der Think-to-Papier-Kommunikation haben (im wesentlichen denken sie an eine Idee, aber sie werden falsch verarbeitet, die sie zu Ende bringen) können möglicherweise von der Software profitieren, aber die Technologie ist kein Fehler. Auch die gesamte Idee, zu Text zu sprechen, kann wegen der Tatsache, dass es selten ist, dass jemand versucht, die Technologie zu lernen, um die Person mit Behinderung zu unterrichten, schwer sein. Diese Art von Technologie kann denjenigen mit Dyslexia helfen, aber andere Behinderungen sind noch in Frage. Wirksamkeit des Produkts ist das Problem, das es behindert, wirksam zu sein. Obwohl ein Kind in der Lage sein kann, je nachdem, wie klar sie sagen, dass die Technologie ein anderes Wort und ein falsches Wort sagen kann. Mehr zu tun, damit sie mehr Zeit mit der Festsetzung des falschen Wortes nehmen müssen. Weitere Anwendungen Aerospace (z.B. Weltraumforschung, Raumfahrzeug usw.)NASA Mars Polar Lander verwendet Spracherkennungstechnologie von Sensory, Inc. im Mars Mikrofon auf dem Lander automatische Subtitation mit Redeerkennung automatische Anerkennung automatische Anerkennung des automatischen Übersetzungsgerichts (Real timevoice)e Entdeckung (Legal Entdeckung) Spracherkennung Computer-Benutzeroberfläche Home Automation Interactive Voice response Mobile Telephony, einschließlich mobiler E-Mail-Multimodal-Diszipation in computergestützten Sprachlernanwendungen Real TimeCaping Robotics Security, einschließlich Verwendung mit anderen biometrischen Scannern für multi-Faktorielle Authentifizierungsrede zum Text (Transkription von Rede in Text, Echtzeit-Video-Capation, Hofberichterstattung )Telematics (z.B. Fahrzeugnavigationssysteme) Transkription (digitale Rede-to-text)Videospiele, mit Tom Clancys End-Waren und Lifeline (Beispiele von Sir Apple). Die Leistung der Spracherkennungssysteme wird in der Regel an Präzision und Geschwindigkeit bewertet. Genauigkeit wird in der Regel mit Wortfehlerquote (WER) bewertet, während die Geschwindigkeit mit dem tatsächlichen Zeitfaktor gemessen wird. Weitere Maßnahmen der Genauigkeit umfassen die einheitliche Fehlerquote (SWER) und die Erfolgsbilanz von Befehls (CSR). Spracherkennung durch Maschinen ist jedoch ein sehr komplexes Problem. Kurzbeschreibungen unterscheiden sich im Hinblick auf Akzent, Aussprache, Kunstikulation, Rauheit, Sterblichkeit, Spiel, Volumen und Geschwindigkeit. Rede wird durch ein Hintergrundgeräusch und Echos, elektrische Merkmale verzerrt. Genauigkeit der Spracherkennung kann variieren mit: Vocabuläre Größe und konfusability Sprecherabhängigkeit versus Unabhängigkeit Isolated, Diskontinuier oder ständige Rede und Sprachzwänge Lesen versus spontane Rede Adverse Bedingungen Wie bereits in diesem Artikel erwähnt, kann die Richtigkeit der Spracherkennung je nach folgenden Faktoren variieren: Fehlerraten steigen, da die bulgarische Größe wächst: z.B. die 10stelligen Null auf neun können im Wesentlichen vollkommen anerkannt werden, aber die bulgarischen Größen von 200, 5000 oder 100000 könnten Fehlerraten von 3,% 7,% oder 45% haben.Vocabulary ist schwer zu erkennen, wenn es verwirrende Worte enthält: z.B. die 26 Buchstaben des englischen Alphabets sind schwer zu diskriminieren, weil sie verwirrend sind (meist nicht zutreffend, das E-set: "B, C, D, E, E, G, G, P, T, V, Z -, wenn Z stark ze, anstatt abhängig von der englischen Region; eine 8% Fehlerquote gilt für dieses Vokular. Abhängigkeit von Sprechern gegenüber der Unabhängigkeit: Ein sprecherunabhängiges System soll von einem einzigen Sprecher genutzt werden. Jedes Sprecher (schwerer) soll ein sprecherunabhängiges System verwenden. Isolierte, abschreckende oder ständige Rede Mit isolierter Rede werden einzelne Wörter verwendet, weshalb es einfacher wird, die Rede zu erkennen. Mit einer nicht fristgerechten Rede werden alle durch Schweigen getrennten Sätze verwendet, weshalb es einfacher wird, die Rede sowie die isolierte Rede zu erkennen. Mit ständiger Rede werden natürlich gesprochene Sätze verwendet, weshalb es schwieriger wird, die Rede zu erkennen, die sich von isolierten und absetzenden Reden unterscheidet. Task- und Sprachzwänge, z.B. die Querying-Anwendung kann die Hypothesis „Der Apfel ist rot“. z.B. Constraints kann semantisch sein; weigern Sie sich "Der Apfel ist verdächtig". z.B. Syntactic; Abweisung von "Red ist Apfel die"Constraints ist oft durch Grammatik vertreten. Lesen Sie vs. Spontaneische Rede – Wenn eine Person sie in der Regel in einem Kontext gelesen hat, der zuvor vorbereitet wurde, aber wenn eine Person die spontane Rede benutzt, ist es schwierig, die Rede wegen der Grippen (wie uh und Um, falsche Start, unvollständige Sätze, Stuttering, Husten und Lachen) und begrenztem Votum zu erkennen. Umweltgeräusche (z.B. Lärm in einem Auto oder einer Fabrik). akustische Verzerrungen (z.B. Echos, Raumgeräusche) Spracherkennung ist eine multi-Level-Diagnostik. akustische Signale werden in eine Rangfolge von Einheiten, z.B. von Telefonmes, Wörtern, Texten und Empfindungen, auf jeder Ebene werden zusätzliche Zwänge bereitgestellt; z.B. Aussprachen des Wissens oder Rechtstextsequenzen, die Fehler oder Unsicherheiten auf einem niedrigeren Niveau kompensieren können; Diese Rangordnung der Zwänge wird genutzt. Indem Entscheidungen probabilist auf allen niedrigeren Ebenen kombiniert werden und mehr deterministische Entscheidungen nur auf höchster Ebene getroffen werden, ist die Spracherkennung durch eine Maschine ein Prozess, der in mehrere Phasen unterteilt ist. Cynly, es ist ein Problem, in dem ein solides Muster anerkannt oder in eine Kategorie eingestuft werden muss, die eine Bedeutung für den Menschen darstellt. Jedes akustische Signal kann in kleinere Basissignale unterteilt werden. Da das komplexere Schallsignal in die kleineren Sub-Risikos zerbrochen wird, werden unterschiedliche Ebenen geschaffen, wo wir auf höchster Ebene komplexe Geräusche aufweisen, die aus einfacheren Geräuschen auf dem niedrigeren Niveau bestehen, und noch mehr, wir schaffen grundlegende und kürzere und einfachere Geräusche. Auf der niedrigsten Ebene, wo die Geräusche die fundamentalsten sind, würde eine Maschine nach einfachen und mehr probabilistischen Regeln prüfen, was sinnvoll ist. Wenn diese Geräusche in komplexere Geräusche auf der oberen Ebene zusammengefasst werden, sollte eine neue Reihe von deterministischen Regeln vorhersagen, was das neue komplexe Geräusch darstellen sollte. Die höchste Ebene einer deterministischen Regel sollte die Bedeutung komplexer Ausdrucksformen hervorbringen. Um unser Wissen über die Anerkennung von Reden zu erweitern, müssen wir neurale Netze berücksichtigen. Es gibt vier Stufen von Neuralnetzkonzepten: Digitalisierung der Rede, die wir für die Telefonanschrift anerkennen wollen, beträgt 8000 Proben pro Sekunde; Komputete Merkmale der spektralen Dimension der Rede (mit vierter Umform); alle 10 ms, mit einem 10 ms-Abschnitt, die als Rahmen bezeichnet werden; Analyse von vierstufigen Neuralnetzansätzen lässt sich durch weitere Informationen erklären. Schall wird durch Luft (oder ein anderes Medium) Vibrationen hergestellt, die wir von Ohren, aber Maschinen von Empfängern registrieren. Grundgedanke schafft eine Welle, die zwei Beschreibungen enthält: Amplitude (wie stark es ist), und Häufigkeit (wie oft erschüttert pro Sekunde). Genauigkeit kann mit Hilfe von Wortfehlerquote (WER) berechnet werden. Fehlerquote kann berechnet werden, indem das anerkannte Wort und das Referenzwort mit dynamischer Strenge angeglichen werden. Das Problem kann auftreten, während die Wortfehlerquote aufgrund der Differenz zwischen den Sequenzlängen des anerkannten Wortes und dem Referenzwort berechnet wird. Lassen Sie S die Anzahl der Substitutionen sein, D ist die Anzahl der Streichungen, ich ist die Zahl der Einbringungen, N die Zahl der Wortreferenzen. WER (=S+D+I)÷N Bei der Berechnung der Worterkennungsrate (WRR) wird die Fehlerquote (WER) verwendet und die Formel ist WRR =1- WER (=N-S-D-I) N N (=H-I) (=N hier H ist die Zahl der korrekt anerkannten Wörter. H= N-(S+D). Sicherheitsanliegen der Anerkennung können ein Mittel für Angriff, Diebstahl oder zufälliger Betrieb werden. Aktivierungsbegriffe wie die in einer Audio- oder Video-Übertragung gesprochene, können Geräte in Haushalten und Büros auslösen, die sich für unangemessene Beiträge einsetzen oder möglicherweise unerwünschte Maßnahmen ergreifen. Voice-kontrollierte Geräte sind auch für Besucher des Gebäudes oder sogar für diejenigen außerhalb des Gebäudes zugänglich, wenn sie im Inneren gehört werden. Angriffe können Zugang zu personenbezogenen Daten wie Kalender, Adressbuchinhalte, Privatnachrichten und Dokumenten erhalten. Sie können den Nutzer auch in der Lage sein, Nachrichten zu senden oder Online-Käufe vorzunehmen. Zwei Angriffe haben gezeigt, dass künstliche Geräusche verwendet werden. One übermittelt Ultraschall und versucht, Befehle ohne nahestehende Menschen zu senden. Die andere führt zu kleinen, unkontrollierbaren Verzerrungen in anderen Reden oder Musik, die speziell entwickelt werden, um das spezielle Spracherkennungssystem zu verwechseln, um Musik als Rede zu erkennen oder zu machen, was wie ein Befehl auf ein menschliches Gut wie ein anderes Kommando zum System ist. Weitere Informationskonferenzen und Fachzeitschriften Volksabstimmungen, die jedes Jahr oder zwei Jahre abgehalten wurden, umfassen die SprachTEK und die RedeTEK Europa, ICASSP, Interspeech/Eurospeech und die MR ASRU. Konferenzen im Bereich der natürlichen Sprachverarbeitung, wie ACL, NAACL, EMNLP und HLT, beginnen mit Papieren zur Sprachverarbeitung. Wichtige Fachzeitschriften umfassen die EV-Transaktionen über Sprach- und Audioverarbeitung (die in den Fachabteilungen für Audio-, Sprach- und Sprachverarbeitung umbenannt wurden, und seit September 2014 in den Bereichen Audio-, Sprach- und Sprachverarbeitung – nach dem Zusammenschluss mit einer ACM-Veröffentlichung), Computer- und Sprache- und Sprachkommunikation umbenannt. Bücher Bücher wie "Grundsätze der Spracherkennung" von Lawrence Rabiner können nützlich sein, grundlegendes Wissen zu erwerben, aber möglicherweise nicht vollständig bis heute (1993). Eine weitere gute Quelle kann von Frederick Jelinek und "Spoken Language Processing (2001)" von Xuedong Huang etc. sein. "Computer Rede", von Manfred R. Schroeder, zweite Ausgabe im Jahr 2004, und "Speech Verarbeitung: Ein dynamisches und optimiertes Konzept", veröffentlicht 2003 von Li Deng und Doug O'Shaugh. Laut dem aktualisierten Textbuch Sprachverarbeitung (2008) von Jurafsky und Martin legen die Grundlagen und den Stand der Technik für ASR vor. Sprecherin Anerkennung nutzt auch die gleichen Merkmale, die meisten der gleichen Frontendverarbeitung und Klassifikationstechniken wie in der Spracherkennung. Ein umfassendes Textbuch, "Grundsätze der Anerkennung von Sprechern" ist eine eingehende Quelle für bis zu aktuelle Informationen über Theorie und Praxis. Ein guter Einblick in die Techniken, die in den besten modernen Systemen eingesetzt werden, kann gewonnen werden, indem staatlich geförderte Bewertungen wie diejenigen, die im Rahmen von DARPA organisiert wurden (das größte Projekt zur Anerkennung von Reden, das 2007 läuft, berücksichtigt werden, das GALE-Projekt, das sowohl die Anerkennung als auch die Übersetzungskomponenten umfasst. Eine gute und leicht zugängliche Einführung zur Spracherkennungstechnik und ihre Geschichte wird vom allgemeinen Publikumsbuch "Die Stimme in der Maschine" bereitgestellt. Aufbau von Computern Dieses Unterstandsgespräch" von Roberto Pieraccini (2012). Jüngstes Buch über die Anerkennung von Reden ist die automatische Spracherkennung: Ein vertieftes Lernkonzept (Publisher: Springer), das von den Microsoft-Forschern D. Yu und L. Deng geschrieben wurde und in der Nähe Ende 2014 veröffentlicht wurde, mit sehr mathematisch orientierten technischen Details darüber, wie tiefe Lernmethoden in modernen Spracherkennungssystemen auf der Grundlage von DNNs und damit zusammenhängenden Methoden des tiefen Lernens abgeleitet und umgesetzt werden. L. Deng und D. ein ähnliches Buch, das im Jahr 2014 veröffentlicht wurde. Yu bietet einen weniger technischen, aber mehr methodenorientierten Überblick über die DNN-basierte Sprachanerkennung in den Jahren 2009–2014, der im allgemeineren Kontext von Anwendungen des tiefen Lernens, einschließlich nicht nur der Spracherkennung, sondern auch der Imageerkennung, der natürlichen Sprachverarbeitung, der Informationsrückgabe, der multimodalen Verarbeitung und des Multi-task Learning, platziert wird. Software In Bezug auf frei verfügbare Ressourcen ist die Sphinx-Toolkit der Universität der Universität von Carnegie Mellon ein Ort, um sowohl über die Anerkennung der Rede als auch über das Experimentieren zu lernen. Eine weitere Ressource (frei, aber urheberrechtlich geschützt) ist das HTK-Buch (und das dazugehörige HTK-Toolkit). Mehr neueste und hochmoderne Techniken können Kaldi-Toolkit verwendet werden. Im Jahr 2017 startete das Open-Source-Projekt „Gemeinsame Stimme“, um große Sprachdatenbanken zu sammeln, die zum Aufbau eines kostenlosen Spracherkennungsprojekts DeepSpeech (verfügbar kostenlos bei GitHub) beitragen könnten, das auf Google Open Source Plattform TensorFlow verwendet wird. Die kommerziellen Cloud-basierten Reding-Diagnostika sind im Großen und Ganzen von WI, Azure, Drehbuchix, IBM und GCP verfügbar. Eine Demonstration einer Online-Sprachanerkennung ist auf der Webseite von Kobal erhältlich. Mehr Software-Ressourcen siehe Liste der Spracherkennungssoftware. Siehe auch weitere Lesung Pieraccini, Roberto (2012). Die Stimme in der Maschine. Aufbau von Computern, die unterstanden sind. The MIT Presse. Mai 2009; McDonough, John (26. Mai 2009). Kritik an der Anerkennung.Wiley.ISBN UV0470517048.Karat, Clare-Marie; Vergo, John; Nahamoo, David (2007)."Konversational Interface Technologies. In Sears, Andrew; Jacko, Julie A. (eds.). Human-Computer-Interaktionshandbuch: Grundlagen, Evolvierende Technologien und neue Anwendungen (Human Factors und Mathematik). Lawrence Erlbaum Associates Inc. [0-8058-5870-9.Cole, Ronald; Mariani, Joseph; Uszkoreit, Hans; Varile, Giovanni Battista; Zaenenen, Annie; Zampolli; Zue, Victor, eds. (1997). Erhebung des Stands der Kunst in der menschlichen Sprache. Cambridge Studies in Natural Language Processing.XII-XIII.Cambridge University Press.ISBN UV0-521-59277-2.Juniqua, J.-C; Haton, J.-P (1995). Robustheit bei der automatischen Spracherkennung: Grundlagen und Anwendungen. Kluwer Academic Publishers.ISBN UV0-7923-9646-8.Pirani, Giancarlo, ed. (2013) fortgeschrittene Algorithmen und Architekturen für das Meinungsverständnis. Springer Science & Business Media.ISBN gegen 3-442-84341-9. Außenbeziehungen Signer, Beat und Hoste, Lode: SpeeG2: Eine Sprach- und Gesture-basierte Schnittstelle für einen effizienten kontrollefreien Texteintrag, In Proceedings of ICMI 2013, 15. Internationale Konferenz über Multimodale Interaktionen, Sydney, Australien, Dezember 2013