Q-Learning ist ein modellfreier Verstärkungslernalgorithmus, um den Wert einer Aktion in einem bestimmten Zustand zu erlernen. Es erfordert kein Modell der Umgebung (hence model-free), und es kann Probleme mit stochastischen Übergängen und Belohnungen ohne Anpassungen zu behandeln. Für jeden endlichen Markov-Entscheidungsprozess (FMDP) findet Q-Learning eine optimale Politik im Sinne der Maximierung des Erwartungswerts der Gesamtbelohnung über alle aufeinanderfolgenden Schritte, ausgehend vom aktuellen Zustand. Q-learning kann eine optimale Aktionsauswahl-Politik für jedes gegebene FMDP identifizieren, angesichts unendlicher Explorationszeiten und einer teilweise zufälligen Politik. " Q bezeichnet die Funktion, die der Algorithmus berechnet – die erwarteten Belohnungen für eine Aktion in einem bestimmten Zustand. Verstärktes Lernen Verstärktes Lernen beinhaltet einen Agenten, einen Satz von Zuständen S {\displaystyle S} und einen Satz A {\displaystyle A} von Handlungen pro Staat. Durch die Durchführung einer Aktion a ε A {\displaystyle a\in A} geht der Agent von Staat zu Staat über. Die Durchführung einer Aktion in einem bestimmten Zustand bietet dem Agenten eine Belohnung (eine numerische Punktzahl). Ziel des Agenten ist es, seine Gesamtbelohnung zu maximieren. Dies führt dazu, dass die von zukünftigen Staaten erreichte maximale Belohnung zur Belohnung für den aktuellen Zustand hinzugefügt wird und die aktuelle Aktion durch die potenzielle zukünftige Belohnung wirksam beeinflusst wird. Diese potenzielle Belohnung ist eine gewichtete Summe der erwarteten Werte der Belohnungen aller zukünftigen Schritte ausgehend vom aktuellen Zustand. Betrachten Sie beispielsweise den Prozess des Einsteigens eines Zuges, bei dem die Belohnung durch das Negativ der gesamten Zeit des Einsteigens gemessen wird (alternativ sind die Kosten des Einsteigens des Zuges gleich der Einsteigenszeit). Eine Strategie ist es, die Zugtür zu betreten, sobald sie sich öffnen, die anfängliche Wartezeit für sich zu minimieren. Wenn der Zug aber überfüllt ist, dann haben Sie einen langsamen Einstieg nach der ersten Aktion, die Tür zu betreten, da die Leute kämpfen, um den Zug zu verlassen, während Sie an Bord versuchen. Die Gesamteinstiegszeit oder Kosten ist dann: 0 Sekunden Wartezeit + 15 Sekunden Kampfzeit Am nächsten Tag, durch zufällige Chance (Erklärung,) Sie entscheiden, zu warten und lassen andere Leute zuerst gehen. Dies führt zunächst zu einer längeren Wartezeit. Die Zeit gegen andere Passagiere ist jedoch geringer. Insgesamt hat dieser Weg eine höhere Belohnung als der des Vortags, da die Gesamteinstiegszeit jetzt ist: 5 zweite Wartezeit + 0 zweite KampfzeitDurch die Exploration, trotz der anfänglichen (Patient) Aktion, die zu einer größeren Kosten (oder negative Belohnung) als in der kraftvollen Strategie führt, sind die Gesamtkosten niedriger, so dass eine lohnendere Strategie enthüllt. AlgorithmAfter Δ t {\displaystyle \Delta t} Schritte in die Zukunft wird der Agent einen nächsten Schritt entscheiden. Das Gewicht für diesen Schritt wird als γ Δ t {\displaystyle \gamma {^\Delta t} berechnet, wobei γ {\displaystyle \gamma } (der Diskontfaktor) eine Zahl zwischen 0 und 1 ist ( 0 ≤ γ ≤ 1 {\displaystyle 0\leq \gamma \leq 1}) und die Wirkung der Bewertung der Prämien früher als die empfangenen Werte arefte γ {\displaystyle \gamma } kann auch als Wahrscheinlichkeit interpretiert werden, bei jedem Schritt Δ t {\displaystyle \Delta t} erfolgreich zu sein (oder zu überleben).Der Algorithmus hat daher eine Funktion, die die Qualität einer Zustand-Aktions-Kombination berechnet: Q : S × A → R {\displaystyle Q:S\times A\to \mathbb {R} .Beforelearning beginnt, Q {\displaystyle Q} wird auf einen eventuell beliebigen Festwert initialisiert (ausgewählt durch den Programmierer). Dann, zu jeder Zeit t {\displaystyle t} wählt der Agent eine Aktion a t {\displaystyle a_{t}, beobachtet eine Belohnung r t {\displaystyle r_{t}, betritt einen neuen Zustands t + 1 {\displaystyle s_{t+1} (die sowohl vom vorherigen Zustand s t {\displaystyle s_{t} als auch von der ausgewählten Aktion abhängig sein kann) und Q {\displaystyle Q} werden aktualisiert. Der Kern des Algorithmus ist eine Bellman-Gleichung als einfaches Wert iteration-Update, mit dem gewichteten Durchschnitt des alten Wertes und den neuen Informationen: Q n e w ( s t , a t ) ← Q ( s t , a t ) ⏟ alter Wert + α ⋅ Lernrate ∙ ( r t ⏟ Belohnung + γ ⋅ Diskontfaktor ⋅ max a Q ( s t + 1 , a ) ⏟ Schätzung des optimalen zukünftigen Wertes ⏟ neuer Wert (Temporal Differenz Target) - Q ( s t ⏟ alter Wert )⏞ Zeitdifferenz {\displaystyle Q^{new}(s_{t},a_{t})\leftarrow \underbrace Q(s_{t},a_{t) \_text{al value}+\underbrace {\alpha } \_text{learning rate}}\cdot \overbrace {\bigg }(\underbrace {\underbrace r_{t}_text{reward}}+\underbrace\gamma } \_text{discount factor}}\cdot \underbrace {max a}Q(s_ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Werte der Lernrate bei 1 machen die Veränderungen in Q {\displaystyle Q} schneller. α r t {\displaystyle \alpha ,\r_{t}: die Belohnung r t = r ( s t, a t ) {\displaystyle r_{t}=r(s_{t},a_{t) zu erhalten, wenn die Aktion a t\displaystyle a_{t} im Zustand s {\displaystyle s_{t} genommen wird, die maximale Belohnung, die aus dem Zustand s t + 1 erhalten werden kann {\displaystyle s_{t+1} (gewichtet durch Lernrate und Diskontfaktor) Eine Folge des Algorithmus endet, wenn der Zustand s t + 1 {\displaystyle s_{t+1} ein End- oder Endzustand ist. Q-Learning kann aber auch in nicht-episodischen Aufgaben (als Folge der Eigenschaft der konvergenten unendlichen Serie) lernen. Wenn der Rabattfaktor kleiner als 1 ist, sind die Aktionswerte endlich, auch wenn das Problem unendliche Schlaufen enthalten kann. Für alle Endzustände s f {\displaystyle s_{f}, Q ( s f , a ) {\displaystyle Q(s_{f},a) wird nie aktualisiert, sondern wird auf den für den Zustand s f {\displaystyle s_{f} beobachteten Belohnungswert r {\displaystyle r} gesetzt.In den meisten Fällen kann Q (s f , a ) {\displaystyle Q(s_{f},a) gleich Null genommen werden. Einfluss von Variablen Lernrate Die Lernrate oder Stufengröße bestimmt, inwieweit neu erworbene Informationen alte Informationen überwiegen. Ein Faktor von 0 macht den Agenten nichts lernen (ausschließlich die Nutzung von Vorkenntnissen), während ein Faktor von 1 macht den Agenten betrachten nur die neuesten Informationen (ignorieren vor dem Wissen, Möglichkeiten zu erkunden). In voll deterministischen Umgebungen ist eine Lernrate von α t = 1 {\displaystyle \alpha {_t}=1 optimal. Wenn das Problem stochastisch ist, konvergiert der Algorithmus unter einigen technischen Bedingungen auf der Lernrate, die es erfordern, auf Null zu sinken. In der Praxis wird oft eine konstante Lernrate verwendet, wie α t = 0,1 {\displaystyle \alpha {_t}=0.1 für alle t {\displaystyle t} . Rabattfaktor Der Rabattfaktor γ {\displaystyle \gamma } bestimmt die Bedeutung zukünftiger Belohnungen. Ein Faktor von 0 wird den Agenten myopic (oder kurzsichtig) nur unter Berücksichtigung aktueller Belohnungen, d.h. r t {\displaystyle r_{t} (in der Update-Regel oben), während ein Faktor, der 1 nähert, wird es für eine langfristige hohe Belohnung streben. Wenn der Rabattfaktor 1 erfüllt oder überschreitet, können die Aktionswerte divergieren. Für γ = 1 {\displaystyle \gamma = 1}, ohne Endzustand, oder wenn der Agent nie einen erreicht, werden alle Umgebungsgeschichten unendlich lang, und Nutzungen mit additiven, ungezählten Belohnungen werden im Allgemeinen unendlich. Auch bei einem Rabattfaktor nur geringfügig kleiner als 1 führt Q-Funktions-Erlernen zu einer Ausbreitung von Fehlern und Instabilitäten, wenn die Wertfunktion mit einem künstlichen neuronalen Netz angenähert wird. In diesem Fall, beginnend mit einem niedrigeren Diskontfaktor und erhöhen ihn auf seinen endgültigen Wert beschleunigt das Lernen. Erstbedingungen (Q0) Da Q-Lernen ein iterativer Algorithmus ist, nimmt er implizit eine Anfangsbedingung an, bevor das erste Update stattfindet. Hohe Ausgangswerte, auch als "optimistische Ausgangsbedingungen" bezeichnet, können die Exploration fördern: Egal, welche Aktion ausgewählt wird, die Aktualisierungsregel wird dazu führen, dass sie niedrigere Werte als die andere Alternative hat und somit ihre Wahlwahrscheinlichkeit erhöht. Die erste Belohnung r {\displaystyle r} kann verwendet werden, um die ursprünglichen Bedingungen zurückzusetzen. Nach dieser Idee wird zum ersten Mal eine Aktion genommen, die Belohnung wird verwendet, um den Wert von Q {\displaystyle Q} einzustellen.Dies ermöglicht sofortiges Lernen bei festen deterministischen Belohnungen. Ein Modell, das Rücksetzen von Anfangsbedingungen (RIC) beinhaltet, wird voraussichtlich das Verhalten der Teilnehmer besser vorhersagen als ein Modell, das jede beliebige Anfangsbedingung (AIC) annimmt. RIC scheint in wiederholten binären Experimenten mit menschlichem Verhalten konsistent zu sein. Implementierung Q-Learning an seinen einfachsten Speicherdaten in Tabellen. Dieser Ansatz verschlechtert sich mit zunehmender Anzahl von Zuständen/Aktionen, da die Wahrscheinlichkeit des Agenten, der einen bestimmten Zustand besucht und eine bestimmte Aktion durchführt, zunehmend gering ist. Funktion Näherung Das Q-Lernen kann mit Funktionsapplikation kombiniert werden. Dies ermöglicht es, den Algorithmus auf größere Probleme anzuwenden, auch wenn der Zustandsraum kontinuierlich ist. Eine Lösung besteht darin, ein (angepasstes) künstliches neuronales Netz als Funktions-Aktuator zu verwenden. Die Funktions-Approximation kann das Lernen in endlichen Problemen beschleunigen, da der Algorithmus frühere Erfahrungen zu bisher ungesehenen Zuständen verallgemeinern kann. Menge Eine weitere Technik zur Verringerung des Zustands/Aktionsraums quantisiert mögliche Werte. Betrachten Sie das Beispiel des Lernens, einen Stick auf einem Finger auszugleichen. Um einen Zustand zu einem bestimmten Zeitpunkt zu beschreiben, ist die Lage des Fingers im Raum, seine Geschwindigkeit, der Winkel des Stiftes und die Winkelgeschwindigkeit des Stiftes miteinbezogen. Dies ergibt einen viergliedrigen Vektor, der einen Zustand beschreibt, d.h. einen Snapshot eines in vier Werte codierten Zustands.Das Problem ist, dass unendlich viele mögliche Zustände vorhanden sind. Um den möglichen Raum gültiger Aktionen zu schrumpfen, können mehrere Werte einem Eimer zugeordnet werden. Der genaue Abstand des Fingers von seiner Ausgangsposition (Infinity to Infinity) ist nicht bekannt, sondern ob er weit weg ist oder nicht (Near, Far). Die Geschichte Q-Learning wurde 1989 von Chris Watkins vorgestellt. 1992 präsentierten Watkins und Peter Dayan einen Konvergenznachweis. Watkins adressierte „Lernen von verzögerten Belohnungen“, den Titel seiner Doktorarbeit. Acht Jahre zuvor im Jahre 1981 wurde das gleiche Problem unter dem Namen „Verzögertes Verstärkungslernen“ von Bozinovskis Crossbar Adaptive Array (CAA) gelöst. Die Speichermatrix W = , w (a, s ) zusammengestellt {\displaystyle W=\|w(a,s}) war die gleiche wie die acht Jahre spätere Q-Tabelle von Q-Learning. Die Architektur führte den Begriff „State-Evaluation“ bei der Stärkung des Lernens ein. Der in dem Papier in mathematischem Pseudocode geschriebene Crossbar-Lernalgorithmus führt bei jeder Iteration folgende Berechnung durch: In dem Zustand s Durchführung Aktion a; Empfang Konsequenz Zustand s;' Berechnung Zustandsauswertung v ( s' ) {\displaystyle v(s}') ; Update crossbar value w' (a, s ) = w ( a, s ) + v ( s' ) {\displaystyle w' (a,s) = w(a,s)+v(s} CAA berechnet Zustandswerte vertikal und wirkt horizontal (die Querschiene"). Demonstrationsgraphen mit verzögertem Verstärkungslernen enthielten Zustände (unerwünschte, unerwünschte und neutrale Zustände), die von der Zustandsbewertungsfunktion berechnet wurden. Dieses Lernsystem war ein Vorläufer des Q-Learning-Algorithmus. Im Jahr 2014 patentierte Google DeepMind eine Anwendung von Q-Learning auf Deep Learning, mit dem Titel "Deep Verstärkung Learning" oder "Deep Q-learning", die Atari 2600 Spiele auf erfahrenen menschlichen Ebenen spielen kann. Varianten Deep Q-Learning Das DeepMind-System nutzte ein tiefes konvolutionales neuronales Netz, mit Schichten von gefliesten Faltungsfiltern, um die Effekte von empfänglichen Feldern zu mimieren. Verstärkungslernen ist instabil oder divergierend, wenn ein nichtlinearer Funktions-Atmator wie ein neuronales Netz zur Darstellung von Q verwendet wird. Diese Instabilität ergibt sich aus den in der Reihenfolge der Beobachtungen vorhandenen Korrelationen, der Tatsache, dass kleine Aktualisierungen zu Q die Politik des Agenten und die Datenverteilung und die Korrelationen zwischen Q und den Zielwerten erheblich verändern können. Die Technik nutzte Erfahrungsreplay, einen biologisch inspirierten Mechanismus, der eine zufällige Probe von vorherigen Aktionen anstelle der neuesten Aktion verwendet, um fortzufahren. Dies entfernt Korrelationen in der Beobachtungssequenz und glättet Änderungen in der Datenverteilung. Aktualisierungen anpassen Q auf Zielwerte, die nur periodisch aktualisiert werden, weiter reduzieren Korrelationen mit dem Ziel. Doppeltes Q-Lernen Da der zukünftige maximale angenäherte Aktionswert im Q-Learning mit der gleichen Q-Funktion wie in der aktuellen Aktionsauswahlpolitik bewertet wird, kann Q-Learning in lauten Umgebungen manchmal die Aktionswerte überschätzen und das Lernen verlangsamen. Eine Variante namens Double Q-learning wurde vorgeschlagen, dies zu korrigieren. Double Q-learning ist ein Off-Policy-Verstärkungs-Erlernungsalgorithmus, bei dem eine andere Politik zur Wertauswertung verwendet wird als das, was zur Auswahl der nächsten Aktion verwendet wird. In der Praxis werden zwei separate Wertefunktionen durch separate Erfahrungen, Q A {\displaystyle Q^{A} und Q B {\displaystyle Q^{B} gegenseitig symmetrisch ausgebildet. Der doppelte Q-Learning-Updateschritt ist dann wie folgt: Q t + 1A (s t , a t ) = Q t A (s t , a t ) + α t (s t , a t ) ( r t + γ Q t B ( s t + 1 , a r m a x a ζ a ( s t + 1 ) - Q t A (s t , a t ) ) {\displaystyle Q_{t+1}{A}(s_{t},a_{t})=Q_{t}(s_{t},a_{t})+\alpha t}(s_{t},a_{t})\left(r_{t}+\gamma t} ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^( s t + 1 , a ) - Q t B ( s t , a t ) {\displaystyle Q_{t+1}{B}(s_{t},a_{t})=Q_{t}{B}(s_{t},a_{t})+\alpha t}(s_{t},a_{t})\left(r_{t}+\gamma t} Q_{t}{A}\left(s_{t+1},\mathop {\operatorname {arg~max} a}Q_{t}{B}(s_{t+1},a)\right)-Q_{t}(s_{t},a_{t})\right. Nun wird der geschätzte Wert der ermäßigten Zukunft mit einer anderen Politik bewertet, die die Überschätzungsfrage löst. Dieser Algorithmus wurde später im Jahr 2015 geändert und mit Deep Learning kombiniert, wie im DQN-Algorithmus, was zu Double DQN führt, der den ursprünglichen DQN-Algorithmus übertrifft. Sonstiges Delayed Q-learning ist eine alternative Implementierung des Online-Q-Learning-Algorithmus, mit wahrscheinlich ungefähr korrektem (PAC) Lernen. Greedy GQ ist eine Variante des Q-Lernens, um in Kombination mit (linearer) Funktion Approximation zu verwenden. Der Vorteil von Greedy GQ besteht darin, dass die Konvergenz auch dann gewährleistet ist, wenn die Funktion Approximation zur Schätzung der Aktionswerte verwendet wird. Einschränkungen Der Standard Q-Lernalgorithmus (unter Verwendung einer Q {\displaystyle Q}-Tabelle) gilt nur für diskrete Aktions- und Zustandsräume. Die Diskretisierung dieser Werte führt zu ineffizientem Lernen, vor allem durch den Fluch der Dimensionalität. Es gibt jedoch Anpassungen von Q-Learning, die versuchen, dieses Problem wie Wire-fitted Neural Network Q-Learning zu lösen. Siehe auch Verstärktes Lernen Temporaler Unterschied Lernen SARSA Iterated Häftling Dilemma Game Theorie Referenzen Externe Links Watkins, C.J.C.H (1989). Lernen von Delayed Rewards. Doktorarbeit, Cambridge University, Cambridge, England. Strehl, Li, Wiewiora, Langford, Littman (2006). PAC modellfreies Verstärkungslernen Verstärktes Lernen:Eine Einführung von Richard Sutton und Andrew S. Barto, einem Online-Lehrbuch. Siehe "6.5 Q-Learning: Off-Policy TD Control".Piqle: eine Generische Java-Plattform für Verstärkung Lernverstärkung Lernmaze, eine Demonstration der Führung einer Ameise durch ein Labyrinth mit Q-learning Q-learning-Arbeit von Gerald TesauroComputer Power und menschlicher Ursache:Vom Urteil zur Berechnung (1976) von Joseph Weizenbaumivale sollte die ambence Computer-Technologie des menschlichen Autors nie wichtige Computer-Falls Weizenbaum unterscheidet entscheidend zwischen Entscheidung und Wahl. Entscheiden ist eine rechnerische Aktivität, etwas, das letztlich programmiert werden kann. Es ist die Fähigkeit zu wählen, die uns letztendlich menschlich macht. Wahl ist jedoch das Produkt des Urteils, nicht Berechnung. Umfassendes menschliches Urteil ist in der Lage, nicht-mathematische Faktoren wie Emotionen einschließen. Urteil kann Äpfel und Orangen vergleichen, und kann dies ohne jede Fruchtart zu quantifizieren und dann reduktiv jede zu Faktoren zu quantifizieren, die für den mathematischen Vergleich erforderlich sind. Siehe auch Ethik der künstlichen Intelligenz Kritik an der Technologie Externe Links Review of Computer Power and Human Reason Excerpt from Computer Power and Human Reason Documentary Film on Joseph Weizenbaum, sein Leben, sein Buch und sein Humor Weizenbaum. Rebel at Work.(25C3 Video Recording) Joseph Weizenbaum, Famed Programmer, Is Dead at 85, New York Times Plug & Pray, Documentary Film on Joseph Weizenbaum und die Ethik von TechnologyEMC Atmos ist eine Cloud-Speicherservice-Plattform, die von EMC Corporation entwickelt wurde. Atmos kann als Hardwaregerät oder als Software in einer virtuellen Umgebung eingesetzt werden. Die Atmos-Technologie nutzt eine Objektspeicher-Architektur, um Petabytes von Informationen und Milliarden von Objekten über mehrere geographische Standorte als ein einziges System zu verwalten. Atmos kann als Datenspeicher für benutzerdefinierte oder verpackte Anwendungen verwendet werden, die entweder eine REST- oder SOAP-Daten-API oder mehrere traditionelle Speicherschnittstellen wie NFS und SMB verwenden.Es stellt einen einzigen einheitlichen Namensraum oder Objekt-Raum dar, speichert Informationen als Objekte (Dateien + Metadaten) und verwaltet Informationen durch Benutzer- oder Administrator-definierte Richtlinien. History Atmos wurde von EMC Corporation entwickelt und im November 2008 allgemein verfügbar gemacht. Eine zweite große Veröffentlichung im Februar 2010 fügte eine GeoProtect verteilte Datenschutzfunktion, schnellere Prozessoren und dichtere Festplatten hinzu. Während des EMV World im Mai 2011, EMC kündigte die 2.0-Version von Atmos mit besserer Leistung, effizienterem GeoParity-Datenschutz und erweitertem Zugriff mit Windows-Client-Software (Atmos GeoDrive) und einem Atmos SDK mit Centera/XAM und Apple iOS-Kompatibilität. Referenzen Externe Links Offizielle Atmos Produktseite EMC Atmos (Maui) ist hier Aufbau EMC Atmos Das OceanStore Projekt