Ein Videocodierformat (oder manchmal Videokompressionsformat) ist ein Content-Darstellungsformat zur Speicherung oder Übertragung von digitalen Videoinhalten (z.B. in einer Datendatei oder Bitstream). Es verwendet in der Regel einen standardisierten Videokompressionsalgorithmus, am häufigsten basierend auf diskreten Cosinus-Transformation (DCT) Codierung und Bewegungskompensation. Beispiele für Videocodierformate sind H.262 (MPEG-2 Teil 2,) MPEG-4 Teil 2, H.264 (MPEG-4 Teil 10,) HEVC (H.265,) Theora, RealVideo RV40, VP9 und AV1. Eine spezielle Software oder Hardware-Implementierung, die komprimierbar oder dekomprimierbar auf/aus einem bestimmten Videocodierformat ist, wird als Video-Codec bezeichnet; ein Beispiel eines Video-Codecs ist Xvid, das einer von mehreren verschiedenen Codecs ist, die Kodierung und Decodierung von Videos im MPEG-4 Part 2 Video-Codierung Format in Software implementiert. Einige Videocodierformate werden durch ein detailliertes technisches Spezifikationsdokument, das als Videocodierungsspezifikation bekannt ist, dokumentiert. Einige dieser Spezifikationen werden von Normungsorganisationen als technische Standards geschrieben und genehmigt und sind daher als Video-Codierung Standard bekannt. Der Begriff Standard wird manchmal auch für de facto Standards sowie formale Standards verwendet. Videoinhalte, die mit einem bestimmten Videocodierformat codiert werden, werden in der Regel mit einem Audiostream (codiert mit einem Audiocodierformat) innerhalb eines Multimedia-Containerformats wie AVI, MP4, FLV, RealMedia oder Matroska gebündelt. Als solche hat der Benutzer normalerweise keine H.264-Datei, sondern eine .mp4-Videodatei, die ein MP4-Container mit H.264-codiertem Video ist, normalerweise neben AAC-codiertem Audio. Multimedia-Container-Formate können jede von einer Reihe von verschiedenen Video-Codierung Formate enthalten; zum Beispiel kann das MP4-Container-Format Video in entweder dem MPEG-2 Teil 2 oder dem H.264 Video-Codierung Format enthalten. Ein weiteres Beispiel ist die anfängliche Spezifikation für den Dateityp WebM, die das Containerformat (Matroska) spezifiziert, aber auch genau das Video (VP8) und Audio (Vorbis) Kompressionsformat im Matroska-Container verwendet wird, obwohl das Matroska-Containerformat selbst in der Lage ist, andere Videocodierungsformate (VP9 Video und Opus Audio-Unterstützung wurde später der WebM-Spezifikation hinzugefügt.) Unterscheidung zwischen Format und Codec Obwohl Videocodierformate wie H.264 manchmal als Codecs bezeichnet werden, besteht ein klarer konzeptioneller Unterschied zwischen einer Spezifikation und deren Implementierungen. Video-Kodierungsformate werden in Spezifikationen beschrieben, und Software oder Hardware, um Daten in einem bestimmten Video-Codierung Format von / zu unkomprimiertem Video zu kodieren / zu dekodieren, sind Implementierungen dieser Spezifikationen. Analog ist das Videocodierformat H.264 (Spezifikation) auf den Codec OpenH264(spezifische Implementierung) das, was die C Programming Language (Spezifikation) auf den Compiler GCC (spezifische Implementierung) ist. Beachten Sie, dass für jede Spezifikation (z.B. H.264) viele Codecs diese Spezifikation implementieren können (z.B. x264, OpenH264, H.264/MPEG-4 AVC Produkte und Implementierungen). Diese Unterscheidung spiegelt sich nicht terminologisch in der Literatur wider. Die H.264-Spezifikation ruft H.261, H.262, H.263 und H.264-Videocodierungsstandards auf und enthält nicht den Wortcodec. Die Alliance for Open Media unterscheidet eindeutig zwischen dem AV1-Videocodierformat und dem dazugehörigen Codec, den sie entwickeln, aber nennt das Videocodierformat selbst eine Video-Codec-Spezifikation. Die VP9-Spezifikation nennt das Videocodierformat VP9 selbst ein Codec. Als Beispiel der Konfilation, Chromiums und Mozillas Seiten, die ihre Videoformat-Unterstützung auflisten, rufen beide Videocodierformate wie H.264 Codecs. Als weiteres Beispiel bezieht sich die Pressemitteilung in Ciscos Ankündigung eines Free-as-in-beer-Videocodecs auf das H.264-Videocodierformat als Codec ("Auswahl eines gemeinsamen Videocodecs"), ruft aber Ciscos Umsetzung eines H.264-Encoders/Decoders kurz darauf ("open-source our H.264 codec") auf. Ein Videocodierformat diktiert nicht alle Algorithmen, die von einem Codec verwendet werden, der das Format implementiert. Ein großer Teil der Funktionsweise der Videokompression besteht beispielsweise darin, dass Ähnlichkeiten zwischen Videorahmen (Block-matching) gefunden werden und dann eine Komprimierung erreicht wird, indem zuvor codierte ähnliche Subbilder (z.B. Makroblöcke) kopiert und bei Bedarf kleine Unterschiede hinzugefügt werden. Optimale Kombinationen solcher Vorhersager und Differenzen zu finden, ist ein NP-hartes Problem, so dass es praktisch unmöglich ist, eine optimale Lösung zu finden. Während das Video-Coding-Format eine solche Komprimierung über Frames im Bitstream-Format unterstützen muss, haben die Codecs, die die Video-Coding-Spezifikation implementieren, nicht unnötig bestimmte Algorithmen zum Auffinden solcher Block-Matches und anderer Kodierungsschritte manipulieren, einige Freiheit, die Algorithmen zu optimieren und innovieren. Beispielsweise sagt Abschnitt 0.5 der H.264-Spezifikation, dass Codierungsalgorithmen nicht Teil der Spezifikation sind. Die freie Wahl des Algorithmus ermöglicht auch unterschiedliche Raum-Zeit-Komplexitäts-Austausche für das gleiche Video-Codierung Format, so dass ein Live-Feed einen schnellen aber platzineffizienten Algorithmus verwenden kann, während eine einmalige DVD-Codierung für die spätere Massenproduktion lange Kodierungszeit für eine platzsparende Codierung tauschen kann. Geschichte Das Konzept der analogen Videokompression stammt aus dem Jahre 1929, als R.D Kell in Großbritannien das Konzept vorgeschlagen hat, nur die von Frame zu Frame veränderten Szenenabschnitte zu übertragen. Das Konzept der digitalen Videokompression stammt aus dem Jahr 1952, als Bell Labs-Forscher B.M Oliver und C.W Harrison die Verwendung von differentiellen Puls-Code-Modulation (DPCM) in der Videocodierung vorgeschlagen haben. Das Konzept der Inter-Frame-Motion-Kompensation stammt aus dem Jahr 1959, als NHK-Forscher Y. Taki, M. Hatori und S. Tanaka prognostizierende Inter-Frame-Videocodierung in der zeitlichen Dimension vorgeschlagen haben. 1967 schlugen die Forscher der University of London A.H Robinson und C. Cherry eine Laufzeitcodierung (RLE) vor, um die Übertragungsbandbreite von analogen Fernsehsignalen zu reduzieren. Die frühesten digitalen Video-Codierung Algorithmen waren entweder für unkomprimiertes Video oder verwendet verlustfreie Kompression, beide Methoden ineffizient und unpraktisch für digitale Video-Codierung. Digitales Video wurde in den 1970er-Jahren eingeführt, zunächst mit unkomprimierter Puls-Code-Modulation (PCM) mit hohen Bitraten von 45–200 Mbit/s für Standard-Definition (SD)-Video, das bis zu 2.000 mal größer war als die bis in die 1990er-Jahre verfügbare Telekommunikationsbandbreite (bis zu 100 kbit/s). Ebenso erfordert unkomprimiertes High-Definition (HD) 1080p-Video Bitraten von mehr als 1 Gbit/s, deutlich größer als die in den 2000er Jahren verfügbare Bandbreite. Bewegungskompensierte DCT Durch die Entwicklung von bewegungskompensierten DCT (MC DCT) Codierungen, auch Block-Bewegungskompensation (BMC) oder DCT-Bewegungskompensation wurde eine praktische Videokompression ermöglicht. Dies ist ein hybrider Kodierungsalgorithmus, der zwei Schlüsseldatenkompressionstechniken kombiniert: diskrete Cosinus-Transformation (DCT), die in der Raumdimension kodiert, und prädiktive Bewegungskompensation in der zeitlichen Dimension. DCT-Codierung ist eine verlustbehaftete Blockkompressions-Transformations-Codierung, die zunächst von Nasir Ahmed vorgeschlagen wurde, die es zunächst für die Bildkompression bestimmte, während er 1972 an der Kansas State University arbeitete. Er wurde 1973 von Ahmed mit T. Natarajan und K. R. Rao an der University of Texas zu einem praktischen Bildkompressionsalgorithmus entwickelt und 1974 veröffentlicht. Die andere Schlüsselentwicklung war bewegungskompensierte Hybridcodierung. Im Jahr 1974 führte Ali Habibi an der University of Southern California Hybrid Codierung ein, die prädiktive Codierung mit Transformation Codierung kombiniert. Er untersuchte mehrere transformierende Codierungstechniken, einschließlich der DCT, Hadamard-Transformation, Fourier-Transformation, Slant-Transformation und Karhunen-Loeve-Transformation. Sein Algorithmus war jedoch zunächst auf die Intra-Frame-Codierung in der Raumdimension beschränkt. Im Jahr 1975 erweiterten John A. Roese und Guner S. Robinson Habibis hybride Codierungsalgorithmus auf die zeitliche Dimension, indem er die Codierung in der Raumdimension und die prädiktive Codierung in der zeitlichen Dimension transformierte und interframe motionkompensierte Hybrid Codierung entwickelt. Für die räumliche Transformationscodierung experimentierten sie mit unterschiedlichen Transformationen, darunter die DCT und die schnelle Fourier-Transformation (FFT), die für sie interframe-Hybrid-Codeer entwickelt, und fanden heraus, dass die DCT aufgrund ihrer reduzierten Komplexität die effizienteste ist, um Bilddaten bis zu 0,25-bit pro Pixel für eine Videotelephone-Szene mit Bildqualität zu komprimieren, die einem typischen Intra-frame-Codeer vergleichbar ist, der 2-bit pro Pixel benötigt. Die DCT wurde auf Videocodierung von Wen-Hsiung Chen angewendet, die 1977 einen schnellen DCT-Algorithmus mit C.H Smith und S.C Fralick entwickelt und Compression Labs gegründet, um DCT-Technologie zu vermarkten. 1979 entwickelten Anil K. Jain und Jaswant R. Jain die bewegungskompensierte DCT-Videokompression weiter. Dies führte dazu, dass Chen 1981 einen praktischen Videokompressionsalgorithmus entwickelt hat, genannt Bewegungkompensierte DCT oder adaptive Szenencodierung. Motion-kompensierte DCT später wurde die Standard-Codierungstechnik für Videokompression ab Ende der 1980er Jahre. Normen der Videocodierung Der erste digitale Videocodierungsstandard war H.120, entwickelt von der CCITT (jetzt ITU-T) im Jahr 1984. H.120 war in der Praxis nicht nutzbar, da seine Leistung zu schlecht war. H.120 verwendete bewegungskompensierte DPCM-Codierung, einen verlustfreien Kompressionsalgorithmus, der für die Videocodierung ineffizient war. In den späten 1980er Jahren begann eine Reihe von Unternehmen, mit diskreten Cosinus-Transformationen (DCT) zu experimentieren, eine viel effizientere Form der Kompression für Video-Codierung. Die CCITT erhielt 14 Vorschläge für DCT-basierte Videokompressionsformate, im Gegensatz zu einem einzigen Vorschlag auf der Grundlage der Vektorquantisierung (VQ) Komprimierung. Der H.261 Standard wurde auf der Grundlage der bewegungskompensierten DCT-Verdichtung entwickelt. H.261 war der erste praktische Videocodierungsstandard und wurde mit Patenten entwickelt, die von einer Reihe von Unternehmen lizenziert wurden, darunter Hitachi, PictureTel, NTT, BT und Toshiba. Seit H.261 wurde die bewegungskompensierte DCT-Kompression durch alle großen Video-Codierungsstandards (einschließlich der H.26x- und MPEG-Formate) angenommen. MPEG-1, entwickelt von der Motion Picture Experts Group (MPEG), gefolgt im Jahr 1991, und es wurde entwickelt, um VHS-Qualität Video zu komprimieren. Es wurde 1994 von MPEG-2/H.262, die mit Patenten entwickelt wurde, die von einer Reihe von Unternehmen lizenziert wurden, in erster Linie Sony, Thomson und Mitsubishi Electric. MPEG-2 wurde das Standard-Videoformat für DVD und SD-Digital-TV. Sein bewegungskompensierter DCT-Algorithmus konnte ein Kompressionsverhältnis von bis zu 100:1 erreichen, das die Entwicklung digitaler Medientechnologien wie Video-on-Demand (VOD) und High-Definition-TV (HDTV) ermöglichte. Im Jahr 1999 folgte MPEG-4/H.263, ein großer Sprung nach vorne für die Videokompressionstechnologie. Es wurde mit Patenten entwickelt, die von einer Reihe von Unternehmen lizenziert wurden, vor allem Mitsubishi, Hitachi und Panasonic. Das am weitesten verbreitete Videocodierformat ab 2019 ist H.264/MPEG-4 AVC. Es wurde 2003 entwickelt, mit Patenten, die von einer Reihe von Organisationen lizenziert wurden, vor allem Panasonic, Godo Kaisha IP Bridge und LG Electronics. Im Gegensatz zum Standard DCT, der von seinen Vorgängern verwendet wird, verwendet AVC die ganze Zahl DCT. H.264 ist einer der Videocodierungsstandards für Blu-ray Discs; alle Blu-ray Disc-Player müssen in der Lage sein, H.264 zu decodieren. Es ist auch weit verbreitet durch das Streamen von Internet-Quellen, wie Videos von YouTube, Netflix, Vimeo, und den iTunes Store, Web-Software wie den Adobe Flash Player und Microsoft Silverlight, sowie verschiedene HDTV-Übertragungen über terrestrische (Advanced Television Systems Committee Standards, ISDB-T, DVB-T oder DVB-T2,) Kabel (DVB-C,) und Satelliten (DVB-S2). Ein Hauptproblem für viele Video-Codierung Formate war Patente, so dass es teuer ist, einen Patentanspruch aufgrund von U-Boot-Patententen zu verwenden oder potenziell zu riskieren. Die Motivation hinter vielen kürzlich gestalteten Videocodierformaten wie Theora, VP8 und VP9 bestand darin, einen (libre) Videocodierungsstandard zu schaffen, der nur von lizenzfreien Patenten abgedeckt ist. Patentstatus war auch ein wichtiger Punkt der Zufriedenheit, für die Wahl, welche Videoformate die Mainstream-Webbrowser im HTML5-Video-Tag unterstützen. Das aktuelle Video-Coding-Format der Generation ist HEVC (H.265,) im Jahr 2013 eingeführt. Während AVC die ganze Zahl DCT mit 4x4 und 8x8 Blockgrößen verwendet, verwendet HEVC ganzzahlige DCT- und DST-Transformationen mit unterschiedlichen Blockgrößen zwischen 4x4 und 32x32. HEVC ist stark patentiert, mit der Mehrheit der Patente von Samsung Electronics, GE, NTT und JVC Kenwood. Es wird derzeit durch das zielorientierte AV1-Format herausgefordert. Ab 2019 ist AVC bei weitem das am häufigsten verwendete Format für die Aufnahme, Kompression und Verteilung von Videoinhalten, die von 91% von Videoentwicklern verwendet werden, gefolgt von HEVC, die von 43% von Entwicklern verwendet wird. Liste der Video-Codierung Standards Lossless, lossy, und unkomprimierte Video-Codierung Formate Verbrauchervideo wird in der Regel mit verlustigen Video-Codecs komprimiert, da dies zu deutlich kleineren Dateien als verlustfreie Kompression führt. Während es Video-Coding-Formate speziell für eine verlustige oder verlustlose Kompression konzipiert sind, unterstützen einige Video-Coding-Formate wie Dirac und H.264 beide. Unkomprimierte Videoformate, wie Clean HDMI, ist eine Form von verlustfreien Video, das unter Umständen verwendet wird, wie zum Beispiel beim Senden von Video an ein Display über eine HDMI-Verbindung. Einige High-End-Kameras können auch Video direkt in diesem Format erfassen. Intra-frame-Video-Codierung Formate Interframe-Kompression erschwert die Bearbeitung einer codierten Videosequenz. Eine Unterklasse relativ einfacher Videocodierformate sind die Intra-Frame-Videoformate, wie DV, bei denen jeder Rahmen des Videostreams unabhängig komprimiert wird, ohne sich auf andere Frames im Stream zu beziehen, und es wird nicht versucht, Korrelationen zwischen aufeinanderfolgenden Bildern im Laufe der Zeit zur besseren Kompression auszunutzen. Ein Beispiel ist Motion JPEG, das ist einfach eine Sequenz von einzelnen JPEG-komprimierten Bildern. Dieser Ansatz ist schnell und einfach, auf Kosten das codierte Video ist viel größer als ein Video-Codierung Format, das Inter Frame Codierung unterstützt. Da die Interframe-Kompression Daten von einem Rahmen zum anderen kopiert, wenn der Originalrahmen einfach ausgeschnitten wird (oder bei der Übertragung verloren geht), können die folgenden Frames nicht richtig rekonstruiert werden. Die Schnitte im Intraframe-komprimierten Video zu machen, während die Videobearbeitung fast so einfach ist, wie das Bearbeiten von unkomprimierten Videos: Man findet den Anfang und das Ende jedes Frames, und kopiert einfach Bit-für-Bit jeden Frame, den man halten will, und verworfen die Frames, die man nicht will. Ein weiterer Unterschied zwischen Intraframe und Interframe-Kompression besteht darin, dass bei Intraframe-Systemen jeder Frame eine ähnliche Datenmenge verwendet. In den meisten Interframe-Systemen sind bestimmte Frames (wie "I frames" in MPEG-2) nicht erlaubt, Daten von anderen Frames zu kopieren, so dass sie viel mehr Daten benötigen als andere Frames in der Nähe. Es ist möglich, einen computerbasierten Video-Editor zu erstellen, der Probleme entdeckt, die bei der Bearbeitung von Frames entstehen, während andere Frames sie benötigen. Damit können neuere Formate wie HDV für die Bearbeitung verwendet werden. Dieser Prozess erfordert jedoch viel mehr Rechenleistung als die Bearbeitung von intraframe komprimiertem Video mit der gleichen Bildqualität. Aber diese Kompression ist nicht sehr effektiv für jedes Audioformat zu verwenden. Profile und Levels Ein Video-Codierung Format kann optionale Einschränkungen definieren, um Video, genannt Profile und Levels zu codieren. Es ist möglich, einen Decoder zu haben, der nur die Decodierung einer Teilmenge von Profilen und Ebenen eines bestimmten Videoformats unterstützt, beispielsweise das Decoder-Programm/Hardware kleiner, einfacher oder schneller zu machen. Ein Profil beschränkt die Kodierungstechniken. Zum Beispiel umfasst das H.264-Format die Profile Basislinie, Haupt und Hoch (und andere). Während in allen Profilen P-Slices (die auf vorausgehenden Scheiben prognostiziert werden können) unterstützt werden, werden B-Slices (die auf beiden vorausgehenden und nachfolgenden Scheiben prognostiziert werden können) in den Haupt- und Hochprofilen, aber nicht in der Basis unterstützt. Ein Pegel ist eine Einschränkung von Parametern wie Maximalauflösung und Datenraten. Siehe auch Vergleich der Containerformate Datenkompression#Video Liste der Videokompressionsformate Videodateiformat == Referenzen und Anmerkungen =The Review of Economics and Statistics is a peer-reviewed Academic Journal for Applied quantitative Economics. Es wurde 1919 als The Review of Economic Statistics gegründet und erhielt 1948 seinen aktuellen Namen. In der Einleitungsfrage wurde der Zweck der Zeitschrift genannt: Förderung der Erhebung, Kritik und Interpretation der Wirtschaftsstatistik ... durch Untersuchung der Quellen und wahrscheinlicher Genauigkeit der vorhandenen Statistiken ... und durch die Entwicklung der Anwendung auf die Wirtschaftsstatistik der modernen Methoden der statistischen Analyse, die bisher in anderen Wissenschaften stärker genutzt wurden als in der Wirtschaft. Die Zeitschrift wird an der Kennedy School of Government der Harvard University herausgegeben und von MIT Press veröffentlicht. Die aktuellen Herausgeber sind Olivier Coibion, Raymond Fisman, Benjamin R. Händel, Rema N. Hanna, Brian A. Jacob, Shachar Kariv, Amit K. Khandelwal und Xiaoxia Shi. Referenzen Externe Links Offizielle Website