Bei der Fragebeantwortung (QA) handelt es sich um eine Informatik-Disziplin in den Bereichen Information Retrieval and Natural Language Processing (NLP), die sich mit Gebäudesystemen befasst, die von Menschen in einer natürlichen Sprache gestellte Fragen automatisch beantworten. Überblick Eine Frage, die die Implementierung, in der Regel ein Computerprogramm, beantwortet, kann seine Antworten erstellen, indem eine strukturierte Datenbank von Wissen oder Informationen, in der Regel eine Wissensbasis. Häufiger können Fragebeantwortungssysteme Antworten von einer unstrukturierten Sammlung von natürlichen Sprachdokumenten ziehen. Einige Beispiele für natürliche Sprachdokumentsammlungen, die für Fragebeantwortungssysteme verwendet werden, umfassen: eine lokale Sammlung von Referenztexten interne Organisationsdokumente und Web-Seiten kompiliert newswire berichtet eine Reihe von Wikipedia-Seiten eine Untergruppe von World Wide Web-SeitenFrage beantworten Forschungsversuche mit einer Vielzahl von Fragetypen wie: Tatsache, Liste, Definition, Wie, Warum, hypothetisch, semantisch eingeschränkt, und Cross-lingual Fragen. Eine Closed-Domain-Fragenbeantwortung befasst sich mit Fragen unter einem bestimmten Bereich (z.B. Medizin oder Automotive Maintenance) und kann Domain-spezifisches Wissen häufig formalisiert in Onlogien ausnutzen. Alternativ könnte sich die Closed-Domain auf eine Situation beziehen, in der nur eine begrenzte Art von Fragen akzeptiert werden, wie z.B. Fragen, die nach beschreibenden und nicht verfahrenstechnischen Informationen gestellt werden. Fragenbeantwortungssysteme im Zusammenhang mit maschinellen Leseanwendungen wurden auch im medizinischen Bereich konstruiert, beispielsweise im Zusammenhang mit der Alzheimer-Krankheit. Open-Domain-Fragen, die Fragen über fast alles beantworten, und können sich nur auf allgemeine Ontologien und Weltwissen verlassen. Andererseits haben diese Systeme in der Regel viel mehr Daten zur Verfügung, aus denen die Antwort extrahiert wird. Geschichte Zwei frühe Fragestellungssysteme waren BASEBALL und LUNAR. BASEBALL beantwortete Fragen zu Major League Baseball Liga über einen Zeitraum von einem Jahr. LUNAR beantwortete wiederum Fragen zur geologischen Analyse der von den Apollo-Mondmissionen zurückgekehrten Felsen. Beide Fragebeantwortungssysteme waren in ihren gewählten Domänen sehr effektiv. In der Tat, LUNAR wurde auf einem Mond-Wissenschafts-Übereinkommen im Jahr 1971 gezeigt und es war in der Lage, 90% der Fragen in seiner Domäne von Menschen, die nicht ausgebildet auf dem System. In den folgenden Jahren wurden weitere eingeschränkte Fragenbeantwortungssysteme entwickelt. Das gemeinsame Merkmal all dieser Systeme ist, dass sie eine Kerndatenbank oder ein Wissenssystem hatten, das von Experten der gewählten Domäne handgeschrieben wurde. Die Sprachfähigkeiten von BASEBALL und LUNAR verwendet Techniken ähnlich ELIZA und DOCTOR, die ersten Chatterbot-Programme. SHRDLU war in den späten 1960er- und frühen 1970er-Jahren ein sehr erfolgreiches Frageberatungsprogramm von Terry Winograd. Es simulierte den Betrieb eines Roboters in einer Spielzeugwelt (die "Blocks-Welt), und es bot die Möglichkeit, die Roboterfragen über den Zustand der Welt zu stellen. Auch hier war die Stärke dieses Systems die Wahl einer sehr spezifischen Domäne und einer sehr einfachen Welt mit physikalischen Regeln, die in einem Computerprogramm leicht kodierbar waren. In den 1970er Jahren wurden Wissensbasen entwickelt, die engere Wissensdomänen zielen. Die zur Schnittstelle zu diesen Expertensystemen entwickelten Fragebeantwortungssysteme haben wiederholte und gültige Antworten auf Fragen in einem Bereich des Wissens erbracht. Diese Expertensysteme ähnelten den modernen Fragestellungssystemen mit Ausnahme ihrer internen Architektur. Expertensysteme verlassen sich stark auf fachlich aufgebaute und organisierte Wissensbasen, während viele moderne Fragebeantwortungssysteme auf die statistische Verarbeitung eines großen, unstrukturierten, natürlichen Sprachtextkorpus vertrauen. In den 70er und 80er Jahren wurde die Entwicklung umfassender Theorien in rechnerischen Linguistiken entwickelt, die zur Entwicklung ambitionierter Projekte im Textverstehen und der Fragebeantwortung führten. Ein Beispiel für ein solches System war der Unix Consultant (UC), der Ende der 1980er Jahre von Robert Wilensky bei U.C. Berkeley entwickelt wurde. Das System beantwortete Fragen zum Unix-Betriebssystem. Es hatte eine umfassende handgefertigte Wissensbasis seiner Domain, und es zielte darauf ab, die Antwort auf verschiedene Arten von Benutzern zu phrasieren. Ein weiteres Projekt war LILOG, ein Text-Verstehen-System, das auf dem Gebiet der touristischen Informationen in einer deutschen Stadt betrieben. Die in den UC- und LILOG-Projekten entwickelten Systeme gingen nie an die Bühne einfacher Demonstrationen vorbei, aber sie halfen der Entwicklung von Theorien über rechnerische Linguistik und Argumentation. Spezielle Fragen der natürlichen Sprache wurden entwickelt, wie EAGLi für Gesundheits- und Lebenswissenschaftler. Architektur Ab 2001 umfassten die Fragebeantwortungssysteme typischerweise ein Frage-Klassifikator-Modul, das die Art der Frage und die Art der Antwort bestimmt. Fragebeantwortungsverfahren Die Fragebeantwortung ist sehr abhängig von einem guten Suchkorpus – denn ohne die Antwort enthaltende Dokumente gibt es kaum Fragen, die das System beantworten kann. Es ist daher sinnvoll, dass größere Sammelgrößen in der Regel gut zur besseren Beantwortung der Leistung leihen, es sei denn, der Fragebereich ist orthogonal zur Sammlung. Der Begriff der Datenredundanz in massiven Sammlungen, wie dem Web, bedeutet, dass Nuggets von Informationen in unterschiedlichen Kontexten und Dokumenten in vielerlei Hinsicht formuliert werden, was zu zwei Vorteilen führt: Indem die richtigen Informationen in vielen Formen erscheinen, wird die Belastung des Fragebeantwortungssystems, komplexe NLP-Techniken durchzuführen, um den Text zu verstehen, verringert. Richtige Antworten können aus falschen Positiven gefiltert werden, indem man auf die richtige Antwort zurückgreift, um mehr Male in den Dokumenten zu erscheinen als Fälle falscher. Einige Fragebeantwortungssysteme verlassen sich stark auf automatisierte Argumentation. Frage der offenen Domain Im Informationsabruf zielt ein offenes Domain-Fraging-System darauf ab, eine Antwort auf die Frage des Nutzers zurückzugeben. Die zurückgegebene Antwort ist in Form von Kurztexten statt einer Liste relevanter Dokumente. Das System verwendet eine Kombination von Techniken aus rechnerischen Linguistiken, Informationsabruf und Wissensdarstellung, um Antworten zu finden. Das System nimmt eine natürliche Sprachfrage als Eingabe statt als eine Reihe von Schlüsselwörtern, zum Beispiel "Wann ist der nationale Tag von China? " Der Satz wird dann durch seine logische Form in eine Abfrage umgewandelt. Die Eingabe in Form einer natürlichen Sprachfrage macht das System benutzerfreundlicher, aber schwieriger zu implementieren, da es verschiedene Fragetypen gibt und das System die richtige identifizieren muss, um eine vernünftige Antwort zu geben. Die Zuordnung eines Fragetyps zur Frage ist eine entscheidende Aufgabe, der gesamte Antwortextraktionsprozess beruht auf der Suche nach dem richtigen Fragetyp und damit dem richtigen Antworttyp. Keyword-Extraktion ist der erste Schritt zur Identifizierung des Eingabefragetyps. In einigen Fällen gibt es klare Wörter, die den Fragetyp direkt angeben, d.h., Wer, Wo oder "Wie viele", diese Wörter sagen dem System, dass die Antworten von Typ Person, Standort oder Nummer sein sollten. Im Beispiel oben, das Wort Wenn anzeigt, dass die Antwort von Typ Date sein sollte". POS (part-of-speech) Etikettierung und syntaktische Parsing-Techniken können auch verwendet werden, um den Antworttyp zu bestimmen. In diesem Fall ist das Thema "Chinesischer Nationaltag", das Prädikat ist und der Adverbial-Modifier ist, wenn, daher der Antwort-Typ ist Datum". Leider, einige verhörte Wörter wie Was, Was oder Wie geben Sie nicht klare Antwort-Typen. Jedes dieser Wörter kann mehr als einen Typ darstellen. In solchen Situationen müssen andere Wörter in der Frage berücksichtigt werden. Zunächst ist es, die Worte zu finden, die den Sinn der Frage angeben können. Ein lexisches Wörterbuch wie WordNet kann dann zum Verständnis des Kontexts verwendet werden. Sobald der Fragetyp identifiziert wurde, wird ein Informationsabrufsystem verwendet, um eine Reihe von Dokumenten zu finden, die die richtigen Keywords enthalten. Ein Tagger- und NP/Verb-Gruppen-Chunker kann verwendet werden, um zu überprüfen, ob die richtigen Einheiten und Beziehungen in den gefundenen Dokumenten erwähnt werden. Bei Fragen wie Who oder Wo wird ein benannter Gottheits-Erkenner verwendet, um relevante Personen- und Ortsnamen aus den abgerufenen Dokumenten zu finden. Für das Ranking werden nur die entsprechenden Absätze ausgewählt. Ein Vektor-Raummodell kann als Strategie zur Klassifikation der Kandidatenantworten verwendet werden. Prüfen Sie, ob die Antwort der richtigen Art ist, wie sie in der Frage-Typ-Analysestufe bestimmt ist. Eine Inferenztechnik kann auch zur Validierung der Kandidatenantworten verwendet werden. Jeder dieser Kandidaten wird dann eine Partitur nach der Anzahl der Fragewörter gegeben, die sie enthält und wie nah diese Wörter dem Kandidaten sind, umso mehr und je näher desto besser. Die Antwort wird dann durch Parsing in eine kompakte und aussagekräftige Darstellung übersetzt. Im vorherigen Beispiel ist die erwartete Ausgabeantwort "1. Okt." Mathematische Fragestellung 2018 wurde ein Open Source math-aware-Fragebeantwortungssystem basierend auf Ask Platypus und Wikidata veröffentlicht. Das System nimmt eine englische oder hindi natürliche Sprache Frage als Eingabe und gibt eine mathematische Formel aus Wikidata als succinct Antwort zurück. Die resultierende Formel wird in eine rechnerische Form übersetzt, so dass der Benutzer Werte für die Variablen einfügen kann. Namen und Werte von Variablen und gemeinsamen Konstanten werden von Wikidata abgerufen, wenn verfügbar. Es wird behauptet, dass das System eine kommerzielle rechnerische mathematische Wissensmaschine auf einem Testsatz übergibt. MathQA Methoden müssen natürliche und Formel Sprache kombinieren. Ein möglicher Ansatz besteht darin, eine überwachte Annotation über Entity Linking durchzuführen. Die "ARQMath Task" bei CLEF 2020 wurde gestartet, um das Problem zu lösen, neu geschriebene Fragen von der Plattform Math Stack Exchange (MSE) an bestehende zu verknüpfen, die bereits von der Community beantwortet wurden. Das Labor wurde von der Tatsache motiviert, dass Mansouri et al. entdeckte, dass 20% der mathematischen Abfragen im allgemeinen Suchmaschinen als gut formulierte Fragen zum Ausdruck gebracht werden. Es enthielt zwei separate Teilaufgaben. Aufgabe 1: "Answer retrieval" mit alten Post-Antworten auf neu gestellte Fragen und Aufgabe 2: "Formula retrieval" mit alten Post-Formeln zu neuen Fragen. Ausgehend von der Domäne der Mathematik, die Formelsprache umfasst, ist es das Ziel, die Aufgabe später auf andere Bereiche zu erweitern (z.B. STEM-Disziplinen, wie Chemie, Biologie, etc.), die andere Arten von Spezialnotation (z.B. chemische Formeln) verwenden. Progress Fragebeantwortung Systeme wurden in den letzten Jahren erweitert, um zusätzliche Bereiche des Wissens zu umfassen Zum Beispiel wurden Systeme entwickelt, um zeitliche und geospatiale Fragen, Fragen der Definition und Terminologie, biografische Fragen, mehrsprachige Fragen und Fragen zum Inhalt von Audio, Bildern und Video automatisch zu beantworten. Aktuelle Fragebeantwortung Forschungsthemen umfassen: Interaktivität – Klärung von Fragen oder Antworten Beantwortung Wiederverwendung oder Caching semantisches Parsing Antwort Präsentation Wissensdarstellung und Argumentation Social Media Analyse mit Fragebeantwortung Systeme Stimmungsanalyse Nutzung der thematischen Rollen semantische Auflösung: die Lücke zwischen syntaktisch verschiedenen Fragen und antworttragenden Texten Nutzung von sprachlichen Ressourcen, wie WordNet, FrameNet, und die ähnliche Bildbeschriftung für visuelle Watson-Beantwortung gegen Brad Rutter und Ken Jennings, gewinnen durch eine erhebliche Marge. Facebook Research hat ihr DrQA-System unter einer Open Source-Lizenz zur Verfügung gestellt. Dieses System wurde für offene Domain-Fragebeantwortung mit Wikipedia als Wissensquelle verwendet. Weitere Informationen Dragomir R. Radev, John Prager und Valerie Samn. Ranking verdächtigte Antworten auf natürliche Sprachfragen mit prädiktiver Annotation. In den Proceedings der 6. Konferenz über Angewandte natürliche Sprache Verarbeitung, Seattle, WA, Mai 2000. John Prager, Eric Brown, Anni Coden und Dragomir Radev. Fragen, die durch vorausschauende Anmerkungen beantwortet werden. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athen, Griechenland, Juli 2000. Hutchins, W. John; Harold L. Somers (1992). Eine Einführung in die maschinelle Übersetzung.London: Academic Press. ISBN 978-0-12-362830-5.L Fortnow, Steve Homer (2002/2003.) Eine kurze Geschichte der Computational Complexity. In D. van Dalen, J. Dawson und A. Kanamori, Herausgeber, Die Geschichte der mathematischen Logik. Nord-Holland, Amsterdam. Externe Links Fragebeantwortung Bewertung bei NTCIR Fragebeantwortung Bewertung bei TREC Fragebeantwortung Bewertung bei CLEF Quiz Fragebeantwortungen Online-FrageBeantwortung System Die Einkommensteuerschwelle ist das Einkommensniveau, auf dem eine Person beginnt, Einkommensteuern zu zahlen. Die Einkommensteuerschwelle entspricht dem: Personalzuschlag im Vereinigten Königreich, der 12,500 £ für 2019/20 beträgt. Grundbeihilfe in Deutschland, das ist 9,408 € im Jahr 2020. Einkommensteuerschwelle in Frankreich, die 2012 6,088 € betrug. Der Standardabzug in den USA, der 2018 $12.000 für eine einzelne Person war. Grundlegender persönlicher Betrag in Kanada, der 2018 bei C$11.809 lag. Steuerfreie Schwelle in Australien, das war 18.200 A$ in 2012-13. Steuerfreie Schwelle in Griechenland, das war €9,545 im Jahr 2016. Die steuerfreie Schwelle in Polen beträgt 2018 3091 PLN. Über 3091 PLN steuerfrei wird Steuerermäßigung und sinkt schrittweise Siehe auch Grundeinkommensgarantie Steuerermäßigung == Referenzen ==