Maschinenbau (ML) ist die Untersuchung von Computeralgorithmen, die sich automatisch durch Erfahrungen und die Verwendung von Daten verbessern können. Es wird als Teil der künstlichen Intelligenz gesehen. Werkzeugmaschinen Lernalgorithmen bauen ein Modell auf, das sich auf Stichprobendaten stützt, die als „Ausbildungsdaten“ bezeichnet werden, um Vorhersagen oder Entscheidungen zu treffen, ohne dies ausdrücklich zu planen. Werkzeugmaschinen Lernalgorithmen werden in einer Vielzahl von Anwendungen wie Medizin, E-Mail-Filter, Spracherkennung und Computervision verwendet, wo es schwierig oder unwiderruflich ist, konventionelle Algorithmen zu entwickeln, um die erforderlichen Aufgaben zu erfüllen. Ein Teil des maschinenlesbaren Lernens ist eng mit Rechenstatistiken verknüpft, die sich auf die Vorhersagen mit Computern konzentrieren, aber nicht das gesamte maschinelle Lernen ist statistisches Lernen. Studie über mathematische Optimierungsmethoden, Theorie und Anwendungsgebiete im Bereich des maschinellen Lernens. Data Mining ist ein verwandter Bereich der Studie, wobei der Schwerpunkt auf der Analyse von explorativen Daten durch unüberwachtes Lernen liegt. Manche Implementierungen des maschinenlesbaren Lernens verwenden Daten und Neuralnetze in einer Weise, die die Arbeit eines biologischen Gehirns verwechselt. In ihrer Anwendung über Geschäftsprobleme wird auch das maschinelle Lernen als prädiktive Analyse bezeichnet. Übersicht über das maschinelle Lernen umfasst Computer, die wissen, wie sie Aufgaben ausführen können, ohne dass sie dazu explizit programmiert werden. Computer werden von Daten lernen, damit sie bestimmte Aufgaben erfüllen. Für einfache Aufgaben, die Computern zugewiesen werden, ist es möglich, Algorithmen zu planen, die die Maschine angeben, wie alle zur Lösung des Problems erforderlichen Schritte ausgeführt werden können; zum Teil ist kein Lernen erforderlich. Mehr fortgeschrittene Aufgaben können für einen Menschen schwierig sein, die notwendigen Algorithmen manuell zu erstellen. In der Praxis kann es sein, wirksamer zu sein, um die Maschine zu helfen, ihren eigenen Algorithmus zu entwickeln, anstatt die Humanprogrammer jeden notwendigen Schritt anzugeben. Die Disziplin des Maschinenlernens umfasst verschiedene Ansätze, um Computer zu unterrichten, um Aufgaben zu erfüllen, wenn kein vollständig zufriedenstellender Algorithmus zur Verfügung steht. In Fällen, in denen viele mögliche Antworten vorliegen, besteht ein Ansatz darin, einige der korrekten Antworten als gültig zu kennzeichnen. Dies kann dann als Fortbildungsdaten für den Computer verwendet werden, um den Algorithmus(en) zu verbessern, der es verwendet, um korrekte Antworten zu finden. Zum Beispiel wurde oft der MNIST-Datenset von handschriftlichen digitalisierten digitalisierten digitalen Charakters genutzt. Geschichte und Beziehungen zu anderen Bereichen Arthur Samuel, ein amerikanischer IBMer und Pionier im Bereich des Computerspiels und der künstlichen Intelligenz, wurde 1959 mit dem Begriff Maschinenbau ausgezeichnet. Ein repräsentatives Buch der Maschinen- und Lernforschung in den 60er Jahren war das Buch der Nilsson über Lernmaschinen, das sich hauptsächlich mit dem Maschinenbaulernen für Musterklassifikation befasst. Interesse im Zusammenhang mit der Mustererkennung setzte sich in den siebziger Jahren fort, wie von Duda und Hart 1973 beschrieben. 1981 wurde ein Bericht über die Verwendung von Lehrstrategien veröffentlicht, damit ein Neuralnetz 40 Zeichen (26 Briefe, 10 Ziffern und 4 spezielle Symbole) von einem Computer-Terminal erkennt. Tom M. Mitchell hat eine allgemein zitierte, formellere Definition der im Bereich des Maschinenlernens untersuchten Algorithmen vorgelegt: „Ein Computerprogramm soll aus Erfahrung E in Bezug auf bestimmte Aufgabenklassen T und Leistung messen P lernen, wenn seine Leistung bei den T, gemessen durch P, verbessert mit Erfahrung E. Diese Definition der Aufgaben, bei denen das maschinelle Lernen eine grundsätzlich operative Definition bietet, anstatt den Bereich in kognitiven Bedingungen zu definieren. Dies folgt: Alan Turings Vorschlag in seinem Papier "Computing Maschinen und Nachrichten", in dem die Frage "Kannmaschinen denken?" durch die Frage "Kannmaschinen tun, was wir tun können?" Modernes maschinelles Lernen hat zwei Ziele, eines ist die Klassifizierung von Daten auf der Grundlage von Modellen, die entwickelt wurden, der andere Zweck ist es, Vorhersagen für künftige Ergebnisse auf der Grundlage dieser Modelle zu erstellen. Ein hypothetischer Algorithmus, der auf die Einstufung von Daten spezialisiert ist, kann die Computervision von moles in Verbindung mit überwachtem Lernen verwenden, um sie zur Einstufung der Krebstiere auszubilden. Kann ein maschinenlesbarer Lerngorithmus für den Börsenhandel den Händler über künftige Vorhersagen informieren. Künstliche Intelligenz Als wissenschaftliches Endeavor wuchs das maschinelle Lernen aus der Suche nach künstlichen Erkenntnissen. In den frühen Tagen der AI als akademische Disziplin waren einige Forscher daran interessiert, Maschinen aus Daten zu erhalten. Sie versuchten, das Problem mit verschiedenen symbolischen Methoden zu lösen, sowie was später als "neural Networks" bezeichnet wurde;" dies waren vor allem Perceptrons und andere Modelle, die später als Reventionen der allgemeinen linearen Modelle der Statistiken gefunden wurden. Probstabilistische Gründe wurden ebenfalls beschäftigt, insbesondere in der automatisierten medizinischen Diagnose. Jedoch führte ein zunehmender Schwerpunkt auf dem logischen, wissensbasierten Ansatz zu einer Kluft zwischen AI und Maschinenbau. Probabilistische Systeme wurden durch theoretische und praktische Probleme des Erwerbs und der Vertretung von Daten gebremst. Bis 1980 hatten die Expertensysteme die AI dominiert, und die Statistiken waren von Vorteil. Arbeiten im Bereich des symbolischen und wissensbasierten Lernens wurden innerhalb der AI fortgesetzt, was zu einer induktiven Planung der Logik führte, aber die mehr statistische Linie der Forschung war jetzt außerhalb des Bereichs der AI richtig, in der Mustererkennung und der Informationserfassung. Neural-Netz-Forschung wurde von der AI und der Computerwissenschaft rund um die gleiche Zeit aufgegeben. Auch diese Linie wurde außerhalb des AI/CS-Bereichs, als Anschlussismus, von Forschern anderer Disziplinen wie Hopfield, Rumelhart und Hinton fortgesetzt. Ihr Haupterfolg kam Mitte der 80er Jahre mit der Wiedererlangung der Gegenpropagation. Maschinenlernen (ML), die in einem separaten Bereich neu organisiert wurden, begannen in den 90er Jahren zu florieren. In diesem Bereich hat sich das Ziel verändert, künstliche Intelligenz zu erreichen, um so undurchlässige Probleme einer praktischen Natur zu bewältigen. Es verlagerte sich den Schwerpunkt weg von den symbolischen Ansätzen, die er von der AI übernommen hatte, und zu Methoden und Modellen, die aus Statistiken und Wahrscheinlichkeitstheorie finanziert wurden. Im Jahr 2020 behaupten viele Quellen weiterhin, dass das maschinelle Lernen ein Teilbereich von AI bleibt. Hauptverschiedenheit ist, ob alle ML Teil von AI ist, da dies bedeutet, dass jeder, der ML verwendet, die Verwendung von AI beantragen könnte. Andere sind der Ansicht, dass nicht alle ML Teil der AI ist, wo nur ein intelligenter Teil der ML Teil der AI ist. Die Frage, was der Unterschied zwischen ML und AI ist, wird von Judika Pearl im Buch der Gründe beantwortet. Laut ML lernen und vorhersagen, die auf passiven Beobachtungen basieren, während AI einen Agenten, der mit der Umwelt in Kontakt kommt, um Maßnahmen zu erfahren und zu ergreifen, die die Chance nutzen, seine Ziele erfolgreich zu erreichen. DatenMining Werkzeugmaschinen Lernen und Data Mining beschäftigen häufig dieselben Methoden und überschneiden sich erheblich, während das maschinelle Lernen auf der Grundlage bekannter Eigenschaften, die aus den Ausbildungsdaten gelernt wurden, auf der Entdeckung (vorher) unbekannter Eigenschaften in den Daten (dies ist der Analyseschritt der Wissensaufnahme in Datenbanken). Data Mining verwendet viele maschinelle Lernmethoden, aber mit unterschiedlichen Zielen; andererseits beschäftigt das maschinelle Lernen auch Data Mining-Methoden als „unbeaufsichtigtes Lernen“ oder als Vorverarbeitungsschritt zur Verbesserung der Lesegenauigkeit. Viele der Verwechslungen zwischen diesen beiden Forschungsgemeinschaften (die oft getrennte Konferenzen und gesonderte Fachzeitschriften haben, ist ECML PKDD eine große Ausnahme) stammen aus den grundlegenden Annahmen, die sie mit: maschinellem Lernen arbeiten, wird die Leistung in der Regel im Hinblick auf die Fähigkeit bewertet, bekannte Kenntnisse zu reproduzieren, während der Wissens- und Datenbergbau (KDD) die Hauptaufgabe ist die Entdeckung früher unbekannter Kenntnisse. Eine unüberlegte Methode (unbeaufsichtigte) wird leicht durch andere beaufsichtigte Methoden informiert, während in einer typischen KDD-Task-Force die beaufsichtigten Methoden aufgrund der Nichtverfügbarkeit von Ausbildungsdaten nicht genutzt werden können. Optimierung des Maschinenlernens hat auch enge Verbindungen zur Optimierung: Viele Lernprobleme werden als Minimierung einiger Verlustfunktion auf einem Ausbildungsplatz formuliert. Verlustfunktionen zeigen die Diskrepanz zwischen den Vorhersagen des verwendeten Modells und den eigentlichen Problemfällen (z.B. in der Einstufung, ein Etikett an Instanzen zu vergeben, und Modelle werden ausgebildet, um die vorab unterzeichneten Etiketten einer Reihe von Beispielen korrekt vorherzusagen). Generalisierung Die Differenz zwischen Optimierung und Maschinenlernen ergibt sich aus dem Ziel der Generalisierung: Während die Optimierungs-Algorithmen den Verlust auf einer Ausbildungseinrichtung minimieren können, ist das maschinelle Lernen mit einer Minimierung des Verlustes an uneen Proben besorgt. Kennzeichnend für die Generalisierung verschiedener Lernalgorithmen ist ein aktives Thema der aktuellen Forschung, insbesondere für tiefe Lernalgorithmen. Statistiken zum Erwerb von Maschinen und Statistiken sind eng verknüpfte Bereiche im Hinblick auf Methoden, die jedoch in ihrem Hauptziel unterscheiden: Statistiken ziehen Bevölkerungsunterschiede aus einer Probe, während das maschinelle Lernen allgemeine prädiktive Muster findet. Laut Michael I. Jordanien haben die Ideen des Maschinenlernens, von methodischen Prinzipien bis zu theoretischen Werkzeugen, eine lange Vorgeschichte in Statistiken. Er schlug auch die Begriffsdatenwissenschaft als Inhaber vor, den gesamten Bereich anzurufen. Leo Breiman hat zwei statistische Modell-Symptome unterschieden: Datenmodell und Algorithmen-Modell, bei dem „algorithmic Modell“ mehr oder weniger die maschinenlesbaren Lernalgorithmen wie Randomwald bedeutet. Manche Statistiker haben Methoden aus dem maschinellen Lernen angenommen, die zu einem kombinierten Bereich führen, das sie statistisches Lernen fordern. Theorie Ein Kernziel eines Lernenden ist die allgemeine Ausrichtung seiner Erfahrungen. Generalisierung in diesem Zusammenhang ist die Fähigkeit einer Lernmaschine, sich genau auf neue, uneen Beispiele/Aufgaben zu setzen, nachdem sie einen Lerndatensatz erlebt hat. Die Schulungsbeispiele kommen aus einer allgemein unbekannten Wahrscheinlichkeitsverteilung (überwiegend repräsentativ für den Raum der Ereignisse) und der Lernende muss ein allgemeines Modell für diesen Raum entwickeln, das es ermöglicht, in neuen Fällen hinreichend genaue Vorhersagen zu erstellen. Die rechnerische Analyse von maschinenlesbaren Lernalgorithmen und ihrer Leistung ist ein Bereich der theoretischen Computerwissenschaft, die als Rechentheorie bekannt ist. Da Ausbildungssets finite und die Zukunft unsicher sind, gibt es in der Regel keine Gewähr für die Leistung von Algorithmen. Stattdessen sind probabilistische Bindungen an die Leistung recht üblich. Ein Weg zur Quantifizierung des Allgemeinisierungsfehlers ist die einseitige -varianz. Für die beste Leistung im Zusammenhang mit der Generalisierung sollte die Komplexität der Hypothese der Komplexität der zugrunde liegenden Funktion entsprechen. Wenn die Hypothesis weniger komplex ist als die Funktion, hat das Modell die Daten geliefert. Wenn sich die Komplexität des Modells erhöht, verringert sich der Ausbildungsfehler. Wenn die Hypothesis zu komplex ist, unterliegt das Modell einer Überrüstung und Generalisierung. Neben Leistungskonzessionen studieren die Lehrer die Zeit komplexer und machbarer Lernprozesse. In der rechnerischen Lerntheorie wird eine Berechnung als möglich erachtet, wenn sie in Polynomialzeit durchgeführt werden kann. Es gibt zwei Arten von Zeitkomplexen. Positive Ergebnisse zeigen, dass eine bestimmte Klasse von Funktionen in Polynomialzeit gelernt werden kann. Negative Ergebnisse zeigen, dass bestimmte Klassen nicht in Polynomialzeit gelernt werden können. Konzepte für den Maschinenbau werden traditionell in drei breite Kategorien unterteilt, je nach Art des Signals oder Feedbacks, die dem Lernsystem zur Verfügung stehen: Supervised Learning: Der Computer wird mit Beispieleinsätzen und ihren gewünschten Outputs präsentiert, die von einem Lehrer angegeben werden, und das Ziel ist es, eine allgemeine Regel zu lernen, die Karten für Outputs enthält. Lernen: Keine Etiketten erhalten den Lerngorithmus, so dass es auf eigenem Weg ist, Struktur in seinem Input zu finden. Unüberwachtes Lernen kann ein Ziel in sich selbst sein (die versteckten Muster in Daten) oder ein Mittel zu einem Ende (Ausbildung). Lernförderung: In einem Computerprogramm geht es um ein dynamisches Umfeld, in dem es ein bestimmtes Ziel verfolgen muss (wie das Führen eines Fahrzeugs oder das Spiel gegen einen Gegner). Da es seinen Problemraum durchwegt, wird das Programm Rückmeldungen geben, die den Belohnungen entsprechen, die es zu maximieren versucht. Supervised Learning Supervised Learning Algorithmen entwickeln ein mathematisches Modell einer Reihe von Daten, die sowohl die Inputs als auch die gewünschten Ergebnisse enthalten. Diese Daten sind als Ausbildungsdaten bekannt und bestehen aus einer Reihe von Schulungsbeispielen. Jedes Ausbildungsbeispiel verfügt über einen oder mehrere Inputs und die gewünschte Leistung, auch als Aufsichtssignal bekannt. Im mathematischen Modell ist jedes Trainingsbeispiel durch eine Vielzahl oder Vektor vertreten, manchmal als Merkmal bezeichnet, und die Ausbildungsdaten werden durch eine Matrix repräsentiert. Durch die bessere Optimierung einer objektiven Funktion lernen überwachte Lernalgorithmen eine Funktion, die genutzt werden kann, um die mit neuen Inputs verbundene Produktion vorherzusagen. Eine optimale Funktion wird es dem Algorithmus ermöglichen, die Produktion von Inputs, die nicht Teil der Ausbildungsdaten waren, korrekt zu bestimmen. Ein Algorithmus, der die Richtigkeit seiner Outputs oder Prognosen im Laufe der Zeit verbessert, wird darauf hingewiesen, dass er gelernt hat, diese Aufgabe auszuführen. Arten von überwachten Lernalgorithmen umfassen aktives Lernen, Klassifizierung und Regression. Klassifikationsgorithmen werden verwendet, wenn die Outputs auf ein begrenztes Maß an Werten beschränkt sind und Regressionsgorithmen verwendet werden, wenn die Outputs einen numerischen Wert innerhalb einer Bandbreite haben. Als Beispiel für einen Klassifikations-Algorithmus, der E-Mailsfiltern, wäre der Input eine neue E-Mail, und das Ergebnis wäre der Name des Verzeichnisses, in dem die E-Mail-Adresse angezeigt wird. Ähnliches Lernen ist ein Bereich, in dem das Maschinenlernen eng mit Regression und Klassifizierung verknüpft ist, aber das Ziel besteht darin, aus Beispielen zu lernen, die eine ähnliche Funktion verwenden, die Maßnahmen wie ähnliche oder verwandte zwei Gegenstände. Sie hat Anwendungen in Ranking, Empfehlungssystemen, visuelle Identitätsverfolgung, Gesichtsüberprüfung und Sprecherprüfung. Unüberwachte Lernalgorithmen nehmen eine Reihe von Daten auf, die nur Inputs enthalten, und finden Struktur in den Daten, wie die Zusammenstellung oder Bündelung von Datenpunkten. Die Algorithmen lernen daher aus Testdaten, die nicht gekennzeichnet, klassifiziert oder kategorisiert wurden. anstatt auf Feedback zu reagieren, unüberwachte Lernalgorithmen erkennen die Gemeinsamkeiten in den Daten an und reagieren auf der Grundlage des Vorhandenseins oder Fehlens solcher Gemeinsamkeiten in jedem neuen Datenstück. Eine zentrale Anwendung des unkontrollierten Lernens befindet sich im Bereich der Dichteschätzung in Statistiken, wie z.B. die Feststellung der Wahrscheinlichkeitsdichte. Obwohl unüberwachtes Lernen andere Bereiche umfasst, die eine Zusammenfassung und Erläuterung der Datenmerkmale beinhalten. Clusteranalyse ist die Aufgabe einer Reihe von Beobachtungen in Untersets (genannte Cluster), damit Beobachtungen innerhalb desselben Clusters nach einem oder mehreren vorgefertigten Kriterien ähnlich sind, während Beobachtungen aus verschiedenen Clustern ungleich sind. Unterschiedliche Clusterbildungstechniken machen unterschiedliche Annahmen in Bezug auf die Struktur der Daten, die oft durch eine ähnliche Parameter definiert und bewertet werden, zum Beispiel durch interne Kompaktheit oder die Ähnlichkeit zwischen den Mitgliedern des gleichen Clusters und die Trennung, den Unterschied zwischen Clustern. Andere Methoden basieren auf einer geschätzten Dichte und einer grafischen Konnektivität. Halbgesteuertes Lernen fällt zwischen unüberwachtem Lernen (ohne etikettierte Ausbildungsdaten) und überwachtem Lernen (mit vollständig gekennzeichneten Ausbildungsdaten). Manche Schulungsbeispiele sind fehlende Ausbildungsetiketten, doch viele maschinenlesbare Forscher haben festgestellt, dass unlabelierte Daten, wenn sie in Verbindung mit einer kleinen Anzahl von Etiketten verwendet werden, eine erhebliche Verbesserung der Lerngenauigkeit bewirken können. In schwach überwachtem Lernen sind die Ausbildungsetiketten laut, begrenzt oder unpräzise; diese Etiketten sind jedoch oft billiger, um zu erhalten, was zu größeren effektiven Ausbildungssets führt. Stärkung des Lernens ist ein Bereich des maschinellen Lernens, der sich mit der Frage befasst, wie Software-Beauftragte Maßnahmen in einem Umfeld ergreifen sollten, um einen Teil der kumulativen Belohnung zu maximieren. Aufgrund seiner allgemeinen Ausrichtung wird das Feld in vielen anderen Disziplinen untersucht, wie Spieltheorie, Kontrolltheorie, Betriebsforschung, Informationstheorie, Simulationsoptimierung, Multi-agen-Systeme, sachkundige Intelligenz, Statistiken und genetische Algorithmen. In der Regel ist die Umwelt als Markov-Entscheidungsprozess (MDP) vertreten. Viele verstärkte Lernalgorithmen verwenden dynamische Programmierungstechniken. Verstärkte Lernalgorithmen nehmen nicht Kenntnis von einem genauen mathematischen Modell des MDP und werden verwendet, wenn genaue Modelle unwiderruflich sind. Verstärkte Lernalgorithmen werden in autonomen Fahrzeugen oder im Lernen verwendet, um ein Spiel gegen einen Menschenrechtsgegner zu spielen. Die Verringerung der Dimensionalität ist ein Prozess zur Verringerung der Anzahl der unter Berücksichtigung der Parameter zu berücksichtigenden Zufallsvariablen durch die Erlangung einer Reihe von Hauptvariablen. In anderen Worten, es ist ein Prozess zur Verringerung der Dimension des gesetzten Merkmals, auch „Anzahl der Merkmale“. Die meisten Methoden zur Verringerung der Dimension können entweder als Abschaffung oder Extraktion gelten. Eine der gängigen Methoden zur Verringerung der Dimension ist die wichtigste Komponenteanalyse (PCA). PCA beinhaltet eine Änderung von mehrdimensionalen Daten (z.B. 3D) auf einen kleineren Raum (z.B. 2D). Dies führt zu einer kleineren Dimension der Daten (2D statt 3D) und hält alle Originalvariablen im Modell ohne Änderung der Daten. In der Vielzahl von Hypothesen wird vorgeschlagen, dass hochdimensionale Datensets entlang von hochdimensionalen Vielfältigen liegen, und viele Methoden zur Eindämmung der Dimension machen diese Annahme, was zum Bereich des vielfältigen Lernens und der vielfältigen Regularisierung führt. Andere Arten Andere Ansätze wurden entwickelt, die nicht in diese dreifache Kategorisierung passen und manchmal mehr als ein von demselben Maschinenlernsystem genutzt wird. z.B. Themamodellierung, Meta-Learning. Im Jahr 2020 ist das tiefe Lernen zum marktbeherrschenden Ansatz für die viel laufenden Arbeiten im Bereich des Maschinenbaus geworden. Selbstlernen als maschinelernorientiertes Paradigma wurde 1982 mit einem neuralen Netzwerk eingeführt, das in der Lage ist, selbst zu lernen, das sogenannte Crossbar-App (CAA). Es ist ein Lernen ohne externe Belohnungen und keine externen Lehrerberatung. Der CAA-Selbst-Lern-Algorithmus berechnet in einer Kreuzbar-Diagnose beide Entscheidungen über Maßnahmen und Emotionen (Fälle). Das System wird durch die Wechselwirkung zwischen Ausscheidung und Emotionen angetrieben. Der Selbst-Lern-Algorithmus aktualisiert eine Speichermatrix W =w(a,s) so, dass in jedem Iteration die folgenden maschinellen Lerngewohnheiten ausgeführt werden: In der Situation führt dies zu einer Maßnahme, die Folgesituation s’; die erhoffte Überzeugung, dass es zu einer Folgesituation v(s) kommt; Aktualisierung des Crossbar-W’(a,s) =w(a,s) + v(s). Es ist ein System mit nur einem Input, einer Situation und nur einem Output (oder Verhalten) a. Es gibt weder einen separaten Verstärkungs Input noch einen Beitrag aus der Umwelt. Der rückführende Wert (zweite Verstärkung) ist die Emotionen gegenüber der Folgensituation. Das CAA besteht in zwei Umgebungen, einem ist das Verhaltensweisensumfeld, in dem es sich handelt, und der andere ist die genetische Umgebung, wo es zunächst und nur einmal erste Emotionen über Situationen erhält, die im Verhalten zu finden sind. Nach Erhalt des Genoms (Arten) aus der genetischen Umgebung lernt die CAA ein objektives Verhalten in einem Umfeld, das sowohl wünschenswerte als auch unerwünschte Situationen enthält. Spielfilme mit mehreren Lernalgorithmen wollen bessere Darstellungen der im Rahmen der Ausbildung bereitgestellten Inputs finden.Klassische Beispiele sind die Hauptkomponentenanalyse und die Clusteranalyse. Animations-Algorithmen, die auch die Repräsentation von Lernalgorithmen genannt haben, versuchen oft, die Informationen in ihrem Input zu erhalten, aber auch sie in einer Weise zu verwandeln, die es sinnvoll macht, oft als Vorverarbeitungsschritt vor der Einstufung oder Vorhersage. Diese Technik ermöglicht den Wiederaufbau der Beiträge, die aus der unbekannten Datenerzeugung stammen, während sie nicht unbedingt den unter diese Verteilung fallenden Konfigurationen entsprechen. Er ersetzt manuelles Merkmal Engineering und ermöglicht es einer Maschine, die Merkmale zu lernen und sie zu nutzen, um eine bestimmte Aufgabe auszuführen. Spielfilme können entweder beaufsichtigt oder unüberwacht werden. Kennzeichnend für das Erwerbsleben sind die Merkmale unter Verwendung etikettierter Inputdaten. Beispiele sind künstliche Neuralnetze, mehrschichtige Perceptrons und überwachtes Wörterbuchlernen. In unkontrollierter Funktion lernen die Merkmale mit unlabelten Inputdaten. Beispiele sind das Sprachenlernen, die unabhängige Komponenteanalyse, die Autoencoders, die Matrix Factorization und verschiedene Formen der Clusterbildung. Manifold Lernalgorithmen versuchen, dies unter dem Druck zu tun, dass die erlernte Darstellung niedrigdimensional ist. Sparse code-Algorithmen versuchen, dies unter dem Druck zu tun, dass die erlernte Darstellung gering ist, was bedeutet, dass das mathematische Modell viele Nullen aufweist. Multilineare Subspace Learning-Algorithmen zielen darauf ab, niedrigdimensionale Darstellungen direkt von zehnten Vertretungen für multidimensionale Daten zu lernen, ohne sie in mehrdimensionale Vektor umzugestalten. Deep Learning-Algorithmen entdecken mehrere Ebenen der Vertretung oder eine Hierarchie der Merkmale, mit höherer Ebene, mehr abstrakten Merkmalen, die in Bezug auf (oder erzeugende) niedrigere Merkmale definiert sind. Es wurde argumentiert, eine intelligente Maschine ist eine solche Darstellung, die die zugrunde liegenden Faktoren der Variationen, die die beobachteten Daten erklären, entwirft. Kommerzielles Lernen wird durch die Tatsache motiviert, dass maschinelle Lernaufgaben wie Klassifikation häufig Input erfordern, die mathematisch und rechnerisch verfahrensfreundlich sind. real-world-Daten wie Bilder, Video und Sensordaten haben jedoch nicht die Versuche erkundet, bestimmte Merkmale zu definieren. Alternative ist es, solche Merkmale oder Darstellungen durch Prüfung zu entdecken, ohne auf explizite Algorithmen zu zurückgreifen. Sparse Wörterbuch Learning Sparse Wörterbuch Learning ist ein Merkmal des Lernprozesses, bei dem ein Ausbildungsbeispiel als lineare Kombination von Basisfunktionen vertreten ist und als eine kleine Matrix gilt. Die Methode ist stark NP-hart und schwer zu lösen. Es ist der K-SVD-Algorithmus, ein beliebtes Hetourismus-Methode für das sparte Wörterbuchlernen. Sparse Wörterbuch Learning wurde in mehreren Kontexten angewandt. In der Einstufung besteht das Problem darin, die Klasse zu bestimmen, zu der ein zuvor ungeklärtes Ausbildungsbeispiel gehört. Für ein Wörterbuch, in dem jede Klasse bereits gebaut wurde, ist ein neues Ausbildungsbeispiel mit der Klasse verbunden, die am besten durch das entsprechende Wörterbuch vertreten ist. Sparse Wörterbuch Learning wurde auch in Bildabsicht angewandt. Kernelement ist, dass ein sauberes Bild-Pflanzen durch ein Bild-Format sehr gering sein kann, aber der Lärm kann nicht sein. Ungewöhnliche Erkennung In Data Mining, aomaly Erkennung, auch bekannt als Aufdeckung, ist die Identifizierung seltener Gegenstände, Ereignisse oder Beobachtungen, die den Verdacht deutlich machen, indem sie deutlich von der Mehrheit der Daten abweichen. In der Regel stellen die aomalischen Gegenstände ein Problem dar, wie Bankbetrug, struktureller Mangel, medizinische Probleme oder Fehler in einem Text. Anomalien werden als Auslierer, Romane, Lärm, Abweichungen und Ausnahmen bezeichnet. Insbesondere im Zusammenhang mit der Erkennung von Missbrauch und Netzeingriffen sind die interessanten Objekte oft nicht selten, aber unerwartete Einbrüche von Unaktivität. Dieses Muster steht nicht im Einklang mit der gemeinsamen statistischen Definition von Auslierer als seltenen Gegenstand, und viele unüberwindbare Nachweismethoden (insbesondere unüberwachte Algorithmen) werden solche Daten nicht verlieren, es sei denn, sie wurden entsprechend zusammengefasst. stattdessen kann ein Clusteranalysegorithmus die Mikrocluster, die durch diese Muster gebildet werden, entdecken. Drei große Kategorien von aomaly Erkennungstechniken bestehen. Unkontrollierte Anomaly-Erkennungstechniken erkennen Anomalien in einer nichtlabelierten Testdaten an, die unter der Annahme festgelegt sind, dass die Mehrheit der Fälle der Daten normal sind, indem sie nach Fällen suchen, die mindestens den Rest der Daten enthalten. Supervised aomaly Aufdeckungstechniken erfordern ein Datenpaket, das als normal und anormal gekennzeichnet wurde und eine Ausbildung beinhaltet (der Schlüssel für viele andere statistische Klassifikationsprobleme ist die inhärent unausgewogene Natur der Aufdeckung). Halbkontrollierte aomaly Erkennungstechniken schaffen ein Modell, das ein normales Verhalten aus einem bestimmten Standard-Ausbildungsdatensatz darstellt und dann die Wahrscheinlichkeit eines Testfalls durch das Modell testen kann. Lernen In Entwicklungsrobotern, wie Menschen, die servalische Aufgaben wahrnehmen und von den Computern ihren Teil des Computers und seinen Roboterlern-Algorithmen kontrolliert werden, erzeugen ihre eigenen Lernerfahrungen, auch als Lehrplan bekannt, kumulativ neue Fähigkeiten durch selbstgeführte Exploration und soziale Interaktion mit Menschen. Diese Roboter verwenden Leitfäden wie aktives Lernen, Laufzeit, Motor Synergien und Nachahmung. Association Regel Learning ist eine regelbasierte maschine Lernmethode zur Entdeckung von Beziehungen zwischen Variablen in großen Datenbanken. Es ist vorgesehen, in Datenbanken mit einer gewissen Maß an interessantem Charakter strenge Vorschriften zu identifizieren. Regelbasiertes Maschinenlernen ist ein allgemeiner Begriff für jede maschinelle Lernmethode, die Regeln für die Speicherung, die Manipulation oder die Anwendung von Wissen festlegt. Kennzeichnend für einen regelbasierten Maschinenbau-Egorithmus ist die Ermittlung und Nutzung einer Reihe von miteinander verknüpften Regeln, die gemeinsam das vom System erfasste Wissen repräsentieren. Im Gegensatz zu anderen maschinenlesbaren Lernalgorithmen, die häufig ein einzigartiges Modell erkennen, das auf jeden Fall allgemein angewendet werden kann, um eine Vorhersage zu machen. Regelbasierte maschinelle Lernkonzepte umfassen Lern-Klassen-Systeme, Assoziierungs- und künstliche Immunsysteme. Auf der Grundlage des Konzepts starker Regeln haben Rakesh Agrawal, Tomasz Imieliński und Arun Swami Assoziierungsregeln für die Feststellung von Ordnungsmäßigkeiten zwischen Produkten in Großkundengeschäftsdaten eingeführt, die von den Systemen von Point-of-sale (POS) in Supermärkten erfasst wurden. z.B. die Regel { o n i o n s s , p o t a t o e s } fla { b u r } {\displaystyle  fionions,potatoes}  Rightarrow Meme Mathematikrm {Hamburg} ., die in den Verkaufsdaten eines Supermarkts gefunden wurde, deutet darauf hin, dass sie, wenn ein Kunde Reinigungsmittel und Kartoffeln zusammen kauft, wahrscheinlich auch das Hamburger Fleisch kaufen. Diese Informationen können als Grundlage für Entscheidungen über Marketingaktivitäten wie Werbepreis oder Produktplatzierungen verwendet werden. Neben der Marktkorbanalyse werden heute in Anwendungsbereichen wie Web-Nutzungsabbau, Intrusionserkennung, kontinuierliche Produktion und Bioinformatik Assoziierungsregeln angewandt. Im Gegensatz zur Sequenzierung des Bergbaus berücksichtigt das Assoziierungsgesetz in der Regel nicht die Bestellung von Gegenständen innerhalb einer Transaktion oder über Transaktionen. Lern-Klassenifiersysteme (LCS) sind eine Familie von regelbasierten maschinenlesbaren Lernalgorithmen, die eine Entdeckungskomponente, in der Regel ein Genetikgorithmus, mit einer Lernkomponente, die entweder überwachtes Lernen, verstärktes Lernen oder unüberwachtes Lernen führt. Sie versuchen, eine Reihe von kontextabhängigen Regeln zu ermitteln, die gemeinsam gespeichert und das Wissen in vollem Umfang nutzen, um Vorhersagen zu machen. Induktive Logik-Programmplanung (ILP) ist ein Konzept für das Regellernen mit Logik-Programm als einheitliche Vertretung für Input-Beispiele, Hintergrundkenntnisse und Hypothesen. Angesichts der Kodierung des bekannten Hintergrundwissens und einer Reihe von Beispielen, die als logische Datenbank von Fakten dienen, wird ein ILP-System ein hypothetisches Logikprogramm, das alle positiven und nicht negativen Beispiele umfasst, auslösen. Induktive Programmierung ist ein verwandter Bereich, der jede Art von Programmierungssprache für die Darstellung von Hypothesen (und nicht nur Logik-Programmierung) wie funktionelle Programme betrachtet. Induktive Logik-Programmierung ist in der Bioinformatik und der natürlichen Sprachverarbeitung besonders nützlich. Gordon Plotkin und Ehud Shapiro haben die erste theoretische Grundlage für induktives Maschinenlernen in einer logischen Einrichtung geschaffen. Shapiro baut ihre erste Umsetzung (Modell-Inference System) im Jahr 1981 auf:a Prolog-Programm, das induktiv angewandte Logikprogramme aus positiven und negativen Beispielen. Induktiv ist hier die philosophische Einführung, die eine Theorie vorlegt, um beobachtete Tatsachen zu erklären, statt mathematische Vorführung, die eine Immobilie für alle Mitglieder eines gut organisierten Set vorweisen. Modelle, die das maschinelle Lernen durchführen, schaffen ein Modell, das in einigen Ausbildungsdaten ausgebildet wird und dann zusätzliche Daten verarbeiten kann, um Vorhersagen zu erstellen. Verschiedene Arten von Modellen wurden für maschinelle Lernsysteme verwendet und erforscht. Künstliche Neuralnetze künstliche Neuralnetze (ANNs) oder Anschlusssysteme,are Rechensysteme vagelich inspiriert von den biologischen Neuralnetzen, die ein Tier Gehirn bilden. Solche Systeme lernen, Aufgaben auszuführen, indem sie Beispiele erwägen, in der Regel nicht mit allen Aufgabenstellungen programmiert. Eine ANN ist ein Modell, das auf einer Sammlung von angeschlossenen Einheiten oder Knoten "artificial Neuronen" basiert, die die Neuronen in einem biologischen Gehirn lockern. Jede Verbindung, wie die synapses in einem biologischen Gehirn, kann Informationen, ein Signal, von einem künstlichen Neuron zu einem anderen übertragen. Ein künstlicher Neuron, der ein Signal erhält, kann es verarbeiten und dann zusätzliche künstliche Neuronen, die mit ihm verbunden sind, signalisieren. In gemeinsamen ANN-Durchführungen ist das Signal an eine Verbindung zwischen künstlichen Neuronen eine echte Zahl, und die Produktion jedes künstlichen Neurons wird durch eine nichtlineare Funktion der Summe seiner Beiträge berechnet. Die Verbindungen zwischen künstlichen Neuronen werden als Ränder bezeichnet. Künstliche Neuronen und Kanten verfügen in der Regel über ein Gewicht, das als Lernprozesse angepasst wird. Das Gewicht erhöht oder verringert die Stärke des Signals in einem Zusammenhang. Künstliche Neuronen können einen Schwellenwert haben, so dass das Signal nur übermittelt wird, wenn das Gesamtsignal diese Schwelle überschreitet. Typischerweise werden künstliche Neuronen in Schichten zusammengefasst. Verschiedene Ebenen können unterschiedliche Arten von Transformationen auf ihren Inputs durchführen. Signale reisen von der ersten Schicht (die Eingangsschicht) in die letzte Schicht (die Outputschicht), möglicherweise nach der Verdreifachung der Schichten. Das ursprüngliche Ziel des ANN-Ansatzes bestand darin, Probleme in derselben Weise zu lösen, dass ein menschliches Gehirn wäre. Im Laufe der Zeit wurde jedoch darauf geachtet, spezifische Aufgaben zu erfüllen, was zu Abweichungen von der Biologie führt. Künstliche Neuralnetze wurden auf einer Vielzahl von Aufgaben eingesetzt, darunter Computervision, Spracherkennung, maschinelle Übersetzung, soziales Netzwerkfiltern, Spiel- und Videospiele und medizinische Diagnose. Deep Learning besteht aus mehreren versteckten Schichten in einem künstlichen Neuralnetz. Dieser Ansatz versucht, die Art und Weise zu modellieren, wie die menschlichen Gehirnprozesse Licht und Ton in Vision und Höre zeigen. Manche erfolgreiche Anwendungen des tiefen Lernens sind Computervision und Spracherkennung. Entscheidung Baumunterricht verwendet einen Entscheidungsbaum als prädiktives Modell, um sich von Beobachtungen über einen Punkt (repräsentiert in den Zweigen) bis zu Schlussfolgerungen über den Zielwert des Gegenstands (repräsentiert in den Blätter) zu äußern. Es ist eines der prädiktiven Modellierungskonzepte, die in den Bereichen Statistik, Data Mining und Maschinenbau verwendet werden. Baummodelle, bei denen die Zielvariable eine bestimmte Reihe von Werten einnehmen kann, werden als Klassifikationsbäume bezeichnet; in diesen Baumstrukturen sind Klassenetiketten und Zweigniederlassungen, die zu diesen Klassenetiketten führen. Entscheidung Bäume, in denen die Zielvariable kontinuierliche Werte (typische Zahlen) einnehmen kann, werden als Regressionsbäume bezeichnet. In der Entscheidungsanalyse kann ein Beschlussbaum verwendet werden, um Entscheidungen und Entscheidungen ausdrücklich zu treffen. In Data Mining beschreibt ein Beschlussbaum Daten, aber der daraus resultierende Klassifikationsbaum kann ein Beitrag zur Entscheidungsfindung sein. Support-vector Maschinen Support-vector Maschinen (SVM), auch als Unterstützungsnetze bekannt, sind eine Reihe verwandter, überwachter Lernmethoden zur Einstufung und Regression. In Anbetracht einer Reihe von Ausbildungsbeispielen, die jeweils zu einer von zwei Kategorien gehören, baut ein SVM-Ausbildungsgorithmus ein Modell auf, das vorausstellt, ob ein neues Beispiel in eine Kategorie oder eine andere Kategorie fällt. Ein SVM-Ausbildungsgorithmus ist ein nicht-probstabilistischer, binärer, linearer Klassenifier, obwohl Methoden wie Platt-Saling vorhanden sind, um SVM in einer probabilistischen Einstufung zu verwenden. Neben der linearen Einstufung können die SVM eine nichtlineare Einstufung effizient durchführen, indem sie den Kernschwierigkeiten, implizite Kartierung ihrer Inputs in hochdimensionale Merkmalsräume verwenden. Regressionsanalyse umfasst eine Vielzahl statistischer Methoden zur Schätzung der Beziehung zwischen Inputvariablen und ihren zugehörigen Merkmalen. Seine häufigste Form ist lineare Regression, bei der eine einzige Linie aufgestellt wird, um die angegebenen Daten nach einem mathematischen Kriterium wie gewöhnliche mindestens Quadrate optimal anzupassen. Letztere werden häufig durch regelmäßige Methoden (mathematische) erweitert, um die Überrüstung und die Verzerrung zu verringern, wie bei der Aufladung von Tuben. Bei der Behandlung nicht-linearer Probleme umfassen die Elektronomen-Regression (z.B. für den Trend in Microsoft Exzellenz), die logistische Regression (oft in statistischer Einstufung) oder sogar die Regression von Kernen, die eine Nicht-linearität vorsieht, indem das Kernproblem genutzt wird, um implizite Inputvariablen für einen stärkerdimensionalen Raum zu erfassen. Bayesische Netze A Bayesian, Empfindungsnetz oder ein antizyklisches grafisches Modell sind ein probabilistisches grafisches Modell, das eine Reihe von Zufallsvariablen und deren bedingter Unabhängigkeit mit einem zyklischen Diagramm (DAG) darstellt. Beispielsweise könnte ein Bayesisches Netzwerk die probabilistischen Beziehungen zwischen Krankheiten und Symptomen darstellen. Angesichts von Symptomen kann das Netz genutzt werden, um die Wahrscheinlichkeit der Anwesenheit verschiedener Krankheiten zu berechnen. Leistungsfähige Algorithmen sind vorhanden, die in Gleichgültigkeit und Lernen führen. Bayesische Netze, die Modellsequenzen von Variablen, wie Redesignale oder Proteinsequenzen, sind dynamische Bayesische Netze. Generalisierungen der Bayesischen Netze, die Entscheidungsprobleme unter Unsicherheit vertreten und lösen können, werden als Einflussdiagramme bezeichnet. Genetische Algorithmen Ein Genetikalgorithmus (GA) ist ein Suchgorithmus und er touristische Technik, die den Prozess der natürlichen Auswahl mit Methoden wie Mutation und Crossover um neue Genotypen in der Hoffnung auf gute Lösungen für ein bestimmtes Problem in Gang setzen. In den 80er und 90er Jahren wurden genetische Algorithmen verwendet. Umgekehrt wurden maschinelle Lernmethoden verwendet, um die Leistung von genetischen und evolutionären Algorithmen zu verbessern. Ausbildungsmodelle erfordern in der Regel viel Daten, damit sie gut funktionieren können. In der Regel muss bei der Schulung eines Modells für das maschinelle Lernen eine große, repräsentative Stichprobe von Daten von einem Ausbildungsplatz gesammelt werden. Daten aus der Ausbildungseinrichtung können so unterschiedlich sein wie ein Textkorpus, eine Sammlung von Bildern und Daten, die von einzelnen Nutzern eines Dienstes erhoben werden. Überrüstung ist etwas, um bei der Ausbildung eines Modells für das maschinelle Lernen zu sehen. Zugierte Modelle aus unvoreingenommenen Daten können zu skewederen oder unerwünschten Vorhersagen führen. Algorithmic Verzerrung ist ein potenzielles Ergebnis von Daten, die nicht vollständig für die Ausbildung vorbereitet sind. Federiertes Lernen Federed Learning ist eine angepasste Form der verteilten künstlichen Intelligenz zur Schulung von Werkzeugmaschinenlernmodellen, die den Ausbildungsprozess dezentralisieren und den Schutz der Privatsphäre der Nutzer ermöglichen, indem sie ihre Daten nicht an einen zentralisierten Server senden. Dadurch wird auch die Effizienz erhöht, indem der Ausbildungsprozess auf viele Geräte verteilt wird. Gboard nutzt z.B. föderiertes maschinelles Lernen, um Suchmaschinen-Suchvorhersagemodelle auf Mobiltelefonen der Nutzer auszubilden, ohne einzelne Suchvorgänge an Google zu senden. Anwendungen Es gibt viele Anwendungen für das maschinelle Lernen, darunter: Im Jahr 2006 fand der Mediendienstleister Netflix den ersten „Netflix-Preis“ statt, um ein Programm zu finden, um die Nutzerpräferenzen besser vorherzusagen und die Richtigkeit seiner bestehenden Filmempfehlung für Filmanleger um mindestens 10 % zu verbessern. Ein gemeinsames Team aus Forschern aus AT &T-Research in Zusammenarbeit mit den Teams Big Chaos und Pragmatic Theorie hat ein Modell entwickelt, um den Großen Preis 2009 für 1 Mio. $ zu gewinnen. Kurz nach der Verleihung des Preises stellte Netflix fest, dass die Ratings der Zuschauer nicht die besten Indikatoren ihrer Sehgewohnheiten waren („jething ist eine Empfehlung“) und ihre Empfehlung entsprechend geändert haben. 2010 Der Wall Street Journal schrieb über die feste Revolutionsforschung und ihren Einsatz von Maschinenlern, um die Finanzkrise vorherzusagen. Im Jahr 2012 bewarf der Gründer von Sun Microsystems, Vinod Khosla, dass in den nächsten zwei Jahrzehnten 80 % der medizinischen Ärzte Arbeitsplätze verloren gehen würden, um eine automatisierte Software für das maschinelle Lernen zu entwickeln. Im Jahr 2014 wurde berichtet, dass ein maschinenlesbarer Lerngorithmus auf dem Gebiet der Kunstgeschichte angewandt wurde, um feine Kunst Gemälde zu untersuchen und dass es vorab unbemerkte Einflüsse zwischen Künstlern gezeigt haben könnte. Springer Nature hat das erste Forschungsbuch veröffentlicht, das mit dem maschinellen Lernen erstellt wurde. 2020 wurde die maschinelle Lerntechnik genutzt, um die Diagnosen und Helfer bei der Entwicklung einer Heilung für COVID-19 zu unterstützen. Maschinenlernen wird vor kurzem angewendet, um das grüne Verhalten des Menschen vorherzusagen. Kürzlich wird auch die maschinelle Lerntechnik zur Optimierung der Leistung und des thermischen Verhaltens von Smartphone auf der Grundlage der Interaktion mit dem Telefon eingesetzt. Grenzen Obwohl das maschinelle Lernen in einigen Bereichen transformativ ist, laufen maschinenlesbare Programme oft nicht zu erwartenden Ergebnissen. Gründe für dies sind zahlreiche: fehlende (geeignete) Daten, fehlender Zugriff auf Daten, Datenverzerrung, Datenschutzprobleme, schlecht gewählte Aufgaben und Algorithmen, falsche Werkzeuge und Menschen, fehlende Ressourcen und Bewertungsprobleme. 2018 versäumte ein Auto von Uber nicht, einen Fußgänger zu entdecken, der nach einer Kollision getötet wurde. Versuche, das maschinelle Lernen in der Gesundheitsversorgung mit dem IBM Watson-System zu nutzen, haben es nicht geschafft, selbst nach Jahren der Zeit und Milliarden von Dollar investiert. Maschinenlernen wurde als Strategie zur Aktualisierung der Beweismittel im Zusammenhang mit einer systematischen Überprüfung und erhöhten Überprüfungslast im Zusammenhang mit dem Wachstum der biomedizinischen Literatur verwendet. Obwohl es mit Ausbildungssets verbessert hat, hat es noch nicht ausreichend entwickelt, um die Arbeitsbelastung zu verringern, ohne die erforderliche Empfindlichkeit für die Forschungsergebnisse selbst zu beschränken. Bias Maschinen- und Lernkonzepte können insbesondere von unterschiedlichen Datenverzerrungen betroffen sein. Ein speziell auf aktuelle Kunden ausgebildetes Maschinenlernsystem kann nicht in der Lage sein, die Bedürfnisse neuer Kundengruppen vorherzusagen, die nicht in den Ausbildungsdaten vertreten sind. Bei der Ausbildung auf vom Menschen verursachten Daten wird das maschinelle Lernen wahrscheinlich die bereits in der Gesellschaft vorhandenen verfassungsrechtlichen und unbewussten Verzerrungen auslösen. Sprachmodelle, die aus Daten gelernt wurden, haben gezeigt, dass menschliche Verzerrungen enthalten. Maschinenlernsysteme, die für die strafrechtliche Risikobewertung verwendet werden, wurden gegen schwarze Menschen vorgeworfen.Im Jahr 2015 würden Google-Fotos oft schwarze Menschen als Gorillas bringen, und 2018 war dies noch nicht gut gelöst, aber Google berichtete, dass es immer noch die Arbeit nutzte, um alle Gorillas aus den Schulungsdaten zu entfernen, und konnte somit nicht wirkliche Gorillas an allen erkennen. Ähnliche Probleme mit der Anerkennung nichtweißer Menschen wurden in vielen anderen Systemen gefunden. Microsoft hat 2016 einen Chatbot getestet, der aus Twitter gelernt wurde und schnell rassistische und sexistische Sprache aufnahm. Infolge solcher Herausforderungen kann die effektive Nutzung des maschinellen Lernens in anderen Bereichen länger dauern. Fairness im Maschinenlernen, d. h. die Verringerung der Verzerrung des maschinellen Lernens und deren Verwendung für menschliches Wohl werden zunehmend von künstlichen Intelligenzwissenschaftlern, einschließlich Fei-Fei Li, die Ingenieure daran erinnern, dass "nicht künstliches über AI... Sie inspiriert von Menschen, die von Menschen geschaffen wurden, und – besonders wichtig – beeinflusst die Menschen. Es ist ein wirksames Werkzeug, das wir nur noch zu verstehen beginnen und eine tiefgreifende Verantwortung ist.“ Modellbewertungen Klassifizierung von Modellen des maschinenlesbaren Lernens kann durch genaue Bewertungstechniken wie die Einstellungsmethode validiert werden, die die Daten in einem Ausbildungs- und Testset (konventionell zwei Ausbildungs- und 1/3-Testset) aufgeteilt und die Leistung des Ausbildungsmodells auf der Testform bewertet. Vergleichlich werden die Daten in K-fold-cross-validierungsmethode zufällig in K-Untersets und dann werden K-Tests je nach einer Untergruppe für die Bewertung und den verbleibenden K-1-Untersätzen für die Ausbildung des Modells durchgeführt. Neben den Einstellungs- und Cross-validierungsmethoden kann Sprint, das aus dem Datenset ersetzte n-Stellen, zur Bewertung der Modellgenauigkeit verwendet werden. Neben der Gesamtgenauigkeit melden die Ermittler häufig Sensitivität und Spezifität, d. h. echte positive Rate (TPR) und echte negative Rate (TNR). Ebenso berichten Ermittler manchmal die falsche positive Rate (FPR) sowie die falsche negative Rate (FNR). In diesen Sätzen gibt es jedoch keine Zahlen und Nenner. Das gesamte Betriebsmerkmal (TOC) ist eine wirksame Methode, um die Diagnosefähigkeit eines Modells auszudrücken. TOC zeigt die Zahler und Nenner der zuvor genannten Sätze, so bietet TOC mehr Informationen als das am häufigsten verwendete Empfängerbetriebsmerkmal (ROC) und das assoziierte Gebiet ROC (AUC). Ethik-Diagnosetik ist ein Gastgeber ethischer Fragen. Systeme, die über die mit Unparteien gesammelten Daten ausgebildet werden, können diese Verzerrungen auf der Nutzung (algorithmie-Interessen) zeigen und damit kulturelle Vorurteile digitalisieren. 1988 stellte die britische Kommission für Racial Equality fest, dass St. George's Medical School ein Computerprogramm nutzte, das aus Daten früherer Zulassungen geschult wurde und dieses Programm fast 60 Kandidaten verweigert hatte, die entweder Frauen oder nichteuropäische Tonnagenamen gefunden haben. Durch die Nutzung von Stellenangeboten von einem Unternehmen mit rasssistischer Einstellung kann ein maschineller Lernsystem geschaffen werden, das die Verzerrung durch die Bewertung von Bewerbern durch die Ähnlichkeit mit früheren erfolgreichen Bewerbern auszeichnet. Verantwortungsvolle Sammlung von Daten und Dokumentation von Algorithmen-Vorschriften, die von einem System verwendet werden, ist somit ein kritischer Bestandteil des maschinellen Lernens. AI kann gut ausgestattet sein, um Entscheidungen in technischen Bereichen zu treffen, die sich stark auf Daten und historische Informationen stützen. Diese Entscheidungen stützen sich auf Objektivität und logischen Grund. Da menschliche Sprachen Verzerrungen enthalten, werden Maschinen, die auf der Sprache corpora ausgebildet sind, diese Verzerrungen unbedingt auch kennen. Andere Formen ethischer Herausforderungen, die nicht mit persönlichen Verzerrungen zusammenhängen, werden im Gesundheitswesen gesehen. Gesundheitsfachleute haben Bedenken, dass diese Systeme nicht im Interesse der Öffentlichkeit entwickelt werden könnten, sondern als einkommensschaffende Maschinen. Dies gilt insbesondere in den Vereinigten Staaten, in denen ein langjähriges ethisches Dilemma zur Verbesserung der Gesundheitsversorgung besteht, aber auch zur Steigerung der Gewinne. Beispielsweise könnten die Algorithmen konzipiert werden, um Patienten unnötige Tests oder Medikamente zu bieten, an denen die Inhaber des Algorithmus beteiligt sind. Es besteht Potenzial für das maschinelle Lernen in der Gesundheitsversorgung, um den Fachkräften ein zusätzliches Instrument zur Diagnose, Medizin und zur Planung von Rückgewinnungspfaden für Patienten bereitzustellen, aber dies erfordert, dass diese Verzerrungen abgeschwächt werden. Hardware Seit den 2010er Jahren haben Fortschritte sowohl bei den maschinellen Lernalgorithmen als auch bei der Computer-Hardware zu effizienteren Methoden für die Schulung von tiefen Neuralnetzen geführt (ein besonderer kleiner Teilbereich des maschinellen Lernens), die viele Schichten nichtlinearer versteckter Einheiten enthalten. Bis 2019 hatten die grafischen Verarbeitungseinheiten (GPUs), oft mit AI-spezifischen Aufwertungen, die CPUs als marktbeherrschende Methode zur Ausbildung groß angelegter kommerzieller Cloud-Identität vertrieben. OpenAI schätzte die Hardware, die in den größten Projekten des tiefen Lernens von AlexNet (2012) bis AlphaZero 2017 verwendet wurde, und stellte einen 300.000-fachen Anstieg der geforderten Rechenmenge mit einer Verdoppelungs-Zeit-Trend von 3.4 Monaten fest. Software-Software-Reihen mit einer Vielzahl von maschinenlesbaren Lernalgorithmen umfassen: Freie und offene Software Proprietäre Software mit kostenlosen und offenen Ausgaben KNIME RapidMiner Proprietary Software Journals Journals Journals Journal of Weral Computation Conferences Association for Computational Languages (ACL) European Conference on Maschinen Learning and Principles and Praxis of Knowledge in Databases (ECML) International Conference on Digital Learning (ICLR) International Conference on Datenschutz (ICLR) International Conference on Smart Roboter and Systems (IROS) on Knowledge Systems (D) Siehe auch automatisiertes maschinelles Lernen – Prozess der Automatisierung der Anwendung von maschinenlesbaren Daten – Informationen, die durch hohe Mengen, Geschwindigkeit und unterschiedliche Programmierung gekennzeichnet sind – Programm Paradigmenliste wichtiger Veröffentlichungen im Bereich des maschinenlesbaren Lernens – Liste der Daten für maschinelle Forschungsarbeiten – Laut der Liste der Daten für die digitale Liste der Daten über die weitere Lesequoten im Zusammenhang mit dem Computerlernen auf Wikilist International Learning Society m Loss ist eine akademische Datenbank der Open-Source-Software.