In der Computerarchitektur trennt die Speicherhierarchie den Computerspeicher in eine Hierarchie basierend auf der Antwortzeit. Da die Reaktionszeit, die Komplexität und die Kapazität miteinander verbunden sind, können die Niveaus auch durch ihre Leistungs- und Steuerungstechnologien unterschieden werden. Die Speicherhierarchie wirkt sich auf die Leistung im Computerarchitektur-Design, Algorithmus-Prädiktionen und niedrigere Level-Programmierkonstrukte mit Lokalität der Referenz. Die Gestaltung für hohe Leistung erfordert die Berücksichtigung der Einschränkungen der Speicherhierarchie, d.h. der Größe und Fähigkeiten jeder Komponente. Jede der verschiedenen Komponenten kann als Teil einer Hierarchie von Speichern (m1, m2,..., mn) betrachtet werden, in denen jedes Mitglied mi typischerweise kleiner und schneller ist als das nächsthöchste Mitglied mi+1 der Hierarchie. Um das Warten auf höhere Ebenen zu begrenzen, wird ein niedrigerer Pegel durch Füllen eines Puffers und dann Signalisierung für die Aktivierung der Übertragung ansprechen. Es gibt vier große Speicherstufen. Intern – Prozessor-Register und Cache. Main – das System RAM und Controller-Karten. On-line Massenspeicher – Sekundärlager. Off-line Schüttung – Tertiär- und Off-line-Speicher. Dies ist eine allgemeine Gedächtnishierarchiestrukturierung. Viele andere Strukturen sind nützlich. Beispielsweise kann ein Paging-Algorithmus bei der Gestaltung einer Computer-Architektur als Level für virtuellen Speicher betrachtet werden, und man kann eine Ebene der Nahline-Speicher zwischen Online- und Offline-Speicher umfassen. Eigenschaften der Technologien in der Speicherhierarchie Die Komplexität hinzufügen verlangsamt die Speicherhierarchie. CMOx Speichertechnologie dehnt den Flash-Raum in der Speicherhierarchie Eine der wichtigsten Möglichkeiten, die Systemleistung zu erhöhen, ist die Minimierung, wie weit unten die Speicherhierarchie muss man gehen, um Daten zu manipulieren. Latency und Bandbreite sind zwei Metriken, die mit Caches verbunden sind. Weder von ihnen ist einheitlich, sondern ist spezifisch für eine bestimmte Komponente der Speicherhierarchie. Die Vorhersage, wo in der Speicherhierarchie die Daten liegen, ist schwierig. .der Ort in der Speicherhierarchie diktiert die Zeit, die für die Präfetch erforderlich ist. Beispiele Die Anzahl der Ebenen in der Speicherhierarchie und die Leistung auf jeder Ebene hat sich im Laufe der Zeit erhöht. Auch die Art der Speicher- oder Speicherkomponenten ändert sich historisch. Beispielsweise ist die Speicherhierarchie eines Intel Haswell Mobile Prozessors um 2013: Prozessor-Register – der schnellste Zugriff (in der Regel 1 CPU-Zyklus). Ein paar tausend Bytes in der Größe Cache Level 0 (L0)Micro Operationen cache – 6,144 Bytes (6 KiB) in der Größe Level 1 (L1) Instruction cache – 128 KiB in der Größe Level 1 (L1) Daten cache – 128 KiB in der Größe. Die beste Zugriffsgeschwindigkeit beträgt ca. 700 GB/s Level 2 (L2) Instruction und data (shared) – 1 MiB in size. Die beste Zugriffsgeschwindigkeit beträgt ca. 200 GB/s Level 3 (L3)Cache teilen – 6 MiB in Größe. Die beste Zugriffsgeschwindigkeit beträgt ca. 100 GB/s Level 4 (L4)Cache teilen – 128 MiB in Größe. Die beste Zugriffsgeschwindigkeit beträgt ca. 40 GB/s Hauptspeicher (Primärspeicher) – GiB in Größe. Die beste Zugriffsgeschwindigkeit beträgt ca. 10 GB/s. Im Falle einer NUMA-Maschine können Zugriffszeiten keine einheitliche Festplattenspeicherung (Secondary Storage) -Terabytes in Größe sein. Ab 2017 ist die beste Zutrittsgeschwindigkeit von einem Verbraucher-Feststoff-Laufwerk ca. 2000 MB/s Nahverkehr (Tertiary Storage) – Bis zu Exabytes in Größe. Ab 2013 beträgt die beste Zutrittsgeschwindigkeit ca. 160 MB/s Angebot Die unteren Ebenen der Hierarchie – von den Scheiben nach unten – sind auch als binred Storage bekannt. Die formale Unterscheidung zwischen Online, Nearline und Offline-Speicher ist: Online-Speicher ist sofort für I/O verfügbar. Fastline-Speicher ist nicht sofort verfügbar, kann aber schnell ohne menschliche Intervention online gemacht werden. Offline-Speicher ist nicht sofort verfügbar und erfordert einige menschliche Intervention, um online zu bringen. Zum Beispiel sind immer auf Spinnscheiben online, während Spinnscheiben, die spin-down, wie massive Array der Leerscheibe (MAID) sind in der Nähe. Abnehmbare Medien wie Bandpatronen, die wie in einer Bandbibliothek automatisch geladen werden können, sind in der Nähe, während Patronen, die manuell geladen werden müssen, offline sind. modern CPUs sind so schnell, dass für die meisten Programm-Workloads der Engpass die Lokalität der Referenz von Speicherzugriffen und die Effizienz der Cache- und Speicherübertragung zwischen verschiedenen Ebenen der Hierarchie ist. Dadurch verbringt die CPU viel Zeit Leerlauf und wartet darauf, dass der Speicher I/O abgeschlossen ist. Dies nennt man manchmal die Raumkosten, da ein größeres Speicherobjekt eher eine kleine/schnelle Ebene überlaufen und eine größere/senkrechte Ebene benötigen. Die resultierende Speicherlast wird als Druck (bzw. Registerdruck, Cachedruck und (Haupt-)Speicherdruck) bezeichnet. Die Bedingungen für Daten, die auf einem höheren Niveau fehlen und von einem niedrigeren Level abgeholt werden müssen, sind: Register-Spilling (durch Registrierungsdruck: Registrieren auf Cache), Cache-Verfehlung (Cache to main Memory) und (hard) Seitenfehler (Hauptspeicher auf Festplatte). Moderne Programmiersprachen nehmen vor allem zwei Ebenen von Speicher, Hauptspeicher und Festplattenspeicher an, obwohl in Montagesprache und Inline-Assembler in Sprachen wie C, Register können direkt aufgerufen werden. Ein optimaler Vorteil der Speicherhierarchie erfordert die Zusammenarbeit von Programmierern, Hardware und Compilern (sowie der zugrunde liegenden Unterstützung durch das Betriebssystem:) Programmierer sind dafür verantwortlich, Daten zwischen Festplatte und Speicher über die Datei I/O zu bewegen. Hardware ist für das Verschieben von Daten zwischen Speicher und Cache verantwortlich. Optimierende Compiler sind für die Generierung von Code verantwortlich, der bei der Ausführung die Hardware dazu veranlassen wird, Caches und Register effizient zu verwenden. Viele Programmierer nehmen eine Ebene des Gedächtnisses an. Dies funktioniert gut, bis die Anwendung eine Performance-Wand trifft. Dann wird die Speicherhierarchie während der Code-Refactoring beurteilt. Siehe auch Cache Hierarchie Verwendung von räumlicher und zeitlicher Lokalität: hierarchischer Speicher Buffer vs. cache Cache Hierarchie in einem modernen Prozessor Speicherwand Computerspeicher Hierarchical Storage Management Cloud Speicher-Zugriffsmuster Kommunikationsvermeidungsalgorithmus == Referenzen ==