Freundliche künstliche Intelligenz (auch freundliche KI oder FAI) bezieht sich auf hypothetische künstliche allgemeine Intelligenz (AGI), die einen positiven (abgegrenzten) Effekt auf die Menschlichkeit haben oder zumindest mit menschlichen Interessen vereinbaren oder zur Verbesserung der menschlichen Spezies beitragen würde. Es ist ein Teil der Ethik der künstlichen Intelligenz und ist eng mit der Maschinenethik verbunden. Während sich die Maschinenethik darum kümmert, wie sich ein künstlich intelligenter Agent verhalten sollte, konzentriert sich die freundliche künstliche Intelligenzforschung darauf, wie man dieses Verhalten praktisch herbeiführt und dafür sorgt, dass es angemessen eingeschränkt ist. Etymologie und Nutzung Der Begriff wurde von Eliezer Yudkowsky geprägt, der am besten für die Popularisierung der Idee bekannt ist, über superintelligente künstliche Agenten zu diskutieren, die menschliche Werte zuverlässig umsetzen. Stuart J. Russell und Peter Norvigs führendes Lehrbuch für künstliche Intelligenz, Künstliche Intelligenz: Ein moderner Ansatz, beschreibt die Idee: Yudkowsky (2008) geht näher über die Gestaltung einer freundlichen KI. Er behauptet, dass Freundlichkeit (ein Wunsch, Menschen nicht zu schaden) von Anfang an entworfen werden sollte, aber dass die Designer beide erkennen sollten, dass ihre eigenen Designs fehlerhaft sein können, und dass der Roboter im Laufe der Zeit lernen und entwickeln wird. Die Herausforderung ist daher eine der Mechanismen, die einen Mechanismus für die Entwicklung von KI-Systemen unter einem System von Kontrollen und Waagen definieren und die System-Dienstleistungen zu geben, die angesichts solcher Veränderungen freundlich bleiben. Freundlich wird in diesem Zusammenhang als technische Terminologie verwendet, und holt Agenten, die sicher und nützlich sind, nicht unbedingt diejenigen, die im kolloquialen Sinne freundlich sind. Das Konzept wird in erster Linie im Kontext von Diskussionen von wiederkehrenden selbstverbessernden künstlichen Agenten, die in der Intelligenz schnell explodieren, aufgerufen, weil diese hypothetische Technologie einen großen, schnellen und schwer zu kontrollierenden Einfluss auf die menschliche Gesellschaft haben würde. Risiken unfreundlicher KIDie Wurzeln der Sorge um künstliche Intelligenz sind sehr alt. Kevin LaGrandeur zeigte, dass die für KI spezifischen Gefahren in der alten Literatur über künstliche humanoide Diener wie das Golem oder die Proto-Roboter von Gerbert von Aurillac und Roger Bacon zu sehen sind. In diesen Geschichten stoßen die extreme Intelligenz und die Macht dieser humanoiden Schöpfungen mit ihrem Status als Sklaven (die von der Natur als untermenschlich gesehen werden) und verursachen katastrophale Konflikte. Bis 1942 forderten diese Themen Isaac Asimov auf, die "Drei Gesetze der Robotik" zu schaffen - Prinzipien, die in alle Roboter in seiner Fiktion fest verdrahtet wurden, um zu verhindern, dass sie sich an ihre Schöpfer wenden, oder sie zu schaden zu lassen. In der modernen Zeit als die Aussicht auf superintelligente KI-Looms näher, hat der Philosoph Nick Bostrom gesagt, dass superintelligente KI-Systeme mit Zielen, die nicht mit menschlicher Ethik ausgerichtet sind, sind intrinsisch gefährlich, es sei denn, extreme Maßnahmen ergriffen werden, um die Sicherheit der Menschheit zu gewährleisten. Er setzte es so: Im Grunde sollten wir annehmen, dass eine Superintelligenz in der Lage wäre, alle Ziele zu erreichen, die sie hat. Daher ist es äußerst wichtig, dass die Ziele, mit denen wir sie und ihr gesamtes Motivationssystem erreichen, "menschlich freundlich" sind. Im Jahr 2008 forderte Eliezer Yudkowsky die Schaffung von „freundlicher KI“ zur Minderung des existenziellen Risikos durch fortgeschrittene künstliche Intelligenz. Er erklärt: "Die KI hasst dich weder, noch liebt sie dich, aber du bist aus Atomen gemacht, die sie für etwas anderes verwenden kann." Steve Omohundro sagt, dass ein ausreichend fortschrittliches KI-System, wenn nicht explizit entgegengewirkt, eine Reihe von Grundantrieben, wie Ressourcenerfassung, Selbsterhaltung und kontinuierliche Selbstverbesserung aufweisen wird, wegen der Eigenart aller zielgetriebenen Systeme und dass diese Antriebe "ohne besondere Vorsichtsmaßnahmen" dazu führen, dass die KI unerwünschtesverhalten zeigen. Alexander Wissner-Gross sagt, dass KIs, die ihre zukünftige Handlungsfreiheit (oder Kausalpfadentropie) maximieren, als freundlich angesehen werden könnte, wenn ihr Planungshorizont länger als eine bestimmte Schwelle ist, und unfreundlich, wenn ihr Planungshorizont kürzer als diese Schwelle ist. Luke Muehlhauser, das für das Machine Intelligence Research Institute geschrieben wurde, empfiehlt, dass die Forscher der Maschinenethik das übernehmen, was Bruce Schneier als "Sicherheitsbewusstsein" bezeichnet hat: Anstatt darüber nachzudenken, wie ein System funktioniert, stellen Sie sich vor, wie es scheitern könnte. Zum Beispiel schlägt er sogar eine KI vor, die nur genaue Vorhersagen macht und über eine Textschnittstelle kommuniziert, könnte unbeabsichtigten Schaden verursachen. Im Jahr 2014 unterstrichen Luke Muehlhauser und Nick Bostrom die Notwendigkeit einer "freundlichen KI"; dennoch sind die Schwierigkeiten bei der Gestaltung einer freundlichen Superintelligenz, z.B. durch die Programmierung des kontrastiellen moralischen Denkens, beträchtlich. Kohärente extrapolierte Volition Yudkowsky fördert das Coherent Extrapolated Volition (CEV) Modell. Laut ihm, kohärente extrapolierte Volition ist die Wahl der Menschen und die Handlungen, die Menschen würden gemeinsam nehmen, wenn "wir wussten mehr, dachte schneller, waren mehr die Menschen, die wir wünschten, wir wären, und waren näher zusammengewachsen. " Anstatt eine freundliche KI, die direkt von menschlichen Programmierern entworfen wird, soll sie von einer "saed KI" entworfen werden, die programmiert ist, die menschliche Natur zuerst zu studieren und dann die KI zu produzieren, die die Menschheit angesichts ausreichender Zeit und Einblicke zu einer befriedigenden Antwort gelangen möchte. Der Appell an ein Ziel durch kontingente menschliche Natur (für mathematische Zwecke ausgedrückt, in Form einer Gebrauchsfunktion oder anderer entscheidungstheoretischem Formalismus), als das ultimative Kriterium der Freundlichkeit, ist eine Antwort auf das meta-ethische Problem der Definition einer objektiven Moral; extrapolierte Volition soll sein, was die Menschheit objektiv wollen würde, alle Dinge betrachtet, aber es kann nur relativ zu den psychologischen und kognitiven Eigenschaften definiert werden. Weitere Ansätze Steve Omohundro hat einen Gerüstansatz zur KI-Sicherheit vorgeschlagen, bei dem eine nachweislich sichere KI-Generation die nächste nachweislich sichere Generation baut. Seth Baum argumentiert, dass die Entwicklung von sicherer, sozial nützlicher künstlicher Intelligenz oder künstlicher allgemeiner Intelligenz eine Funktion der sozialen Psychologie von KI-Forschungsgemeinschaften ist und somit durch extrinsische Maßnahmen eingeschränkt und durch intrinsische Maßnahmen motiviert werden kann. Intrinsische Motivationen können gestärkt werden, wenn Nachrichten mit AI-Entwicklern resonieren; Baum argumentiert, dass im Gegensatz dazu "bestehende Meldungen über nützliche KI werden nicht immer gut gerahmt". Baum befürwortet "kooperative Beziehungen und positives Friaming von KI-Forschern" und Vorsichtsmassnahmen gegen die Charakterisierung von KI-Forschern als "nicht wollen(en) nützliche Designs verfolgen". In seinem Buch Human Kompatibel, AI-Forscher Stuart J. Russell listet drei Prinzipien, um die Entwicklung von nützlichen Maschinen zu führen. Er betont, dass diese Prinzipien nicht ausdrücklich in die Maschinen kodiert werden sollen, sondern sie sind für die menschlichen Entwickler bestimmt. Die Grundsätze sind wie folgt: 1.Das einzige Ziel der Maschine ist es, die Verwirklichung der menschlichen Präferenzen zu maximieren. 2.Die Maschine ist zunächst unsicher, was diese Vorlieben sind. 3.Die ultimative Quelle von Informationen über menschliche Präferenzen ist menschliches Verhalten. Die Vorlieben Russell bezieht sich auf "sind allumfassend; sie decken alles, was Sie interessieren könnten, willkürlich weit in die Zukunft." Ebenso umfasst das Verhalten jede Wahl zwischen Optionen, und die Unsicherheit ist so, dass eine gewisse Wahrscheinlichkeit, die recht klein sein kann, jeder logisch möglichen menschlichen Präferenz zugeordnet werden muss. Die öffentliche Politik James Barrat, Autor von Our Final Invention, schlug vor, dass "eine öffentlich-private Partnerschaft geschaffen werden muss, um A.I.-makers zusammenzubringen, um Ideen über Sicherheit zu teilen – etwa wie die Internationale Atomenergieagentur, aber in Partnerschaft mit Konzernen. "Er fordert KI-Forscher auf, ein Treffen einberufen ähnlich der Asilomar Konferenz über rekombinante DNA, die Risiken der Biotechnologie diskutiert. John McGinnis ermutigt Regierungen, die freundliche KI-Forschung zu beschleunigen. Da die Tore der freundlichen KI nicht unbedingt eminent sind, schlägt er ein ähnliches Modell wie die National Institute of Health vor, wo "Peer überprüfen Panels von Computer und kognitiven Wissenschaftler würde durch Projekte sift und wählen Sie diejenigen, die entwickelt werden, um KI zu fördern und sicherzustellen, dass solche Fortschritte durch geeignete Schutzmaßnahmen begleitet werden." McGinnis ist der Ansicht, dass die Peer-Review besser ist "als Verordnung, technische Fragen zu behandeln, die nicht durch bürokratische Mandate erfasst werden können". McGinnis stellt fest, dass sein Vorschlag im Gegensatz zu dem des Machine Intelligence Research Institute steht, das in der Regel die Beteiligung der Regierung an der freundlichen KI vermeiden will. Laut Gary Marcus ist die jährliche Geldmenge, die für die Entwicklung der Maschinenmoral ausgegeben wird, winzig. Kritiker Einige Kritiker glauben, dass sowohl menschliche KI als auch Superintelligenz unwahrscheinlich sind und dass daher eine freundliche KI unwahrscheinlich ist. Alan Winfield schreibt in The Guardian, vergleicht künstliche Intelligenz auf menschlicher Ebene mit schnelleren als leichten Reisen in Bezug auf Schwierigkeiten, und sagt, dass, während wir angesichts der damit verbundenen Einsätze "aufwändig und bereit" sein müssen, wir "nicht zu besessen" über die Risiken der Superintelligenz sein müssen. Boyles und Joaquin hingegen argumentieren, dass Luke Muehlhauser und Nick Bostroms Vorschlag, freundliche KIs zu schaffen, scheinen bluak zu sein. Denn Muehlhauser und Bostrom scheinen die Idee zu halten, dass intelligente Maschinen programmiert werden könnten, um gegen die moralischen Werte zu denken, die Menschen gehabt hätten. In einem Artikel in AI & Society behaupten Boyles und Joaquin, dass solche KIs nicht so freundlich sein würden, wenn man folgendes bedenkt: die unendliche Menge an nachweisbaren Gegenfaktorbedingungen, die in eine Maschine programmiert werden müssten, die Schwierigkeit, die Menge der moralischen Werte zu kassieren - das heißt, dass ein idealer als die Menschen gegenwärtig besitzen, und die offensichtliche Trennung zwischen Gegenfaktor-Angemessenen und Idealwert-Konsequenzen. Einige Philosophen behaupten, dass jeder wirklich rationale Agent, ob künstlich oder menschlich, natürlich wohlwollend sein wird; in dieser Ansicht könnten bewusste Schutzmaßnahmen zur Herstellung einer freundlichen KI unnötig oder sogar schädlich sein. Andere Kritiker fragen, ob eine künstliche Intelligenz freundlich sein kann. Adam Keiper und Ari N. Schulman, Herausgeber des Technologiejournals The New Atlantis, sagen, dass es unmöglich sein wird, immer ein freundliches Verhalten in AIs zu garantieren, weil Probleme der ethischen Komplexität keine Software-Vorgänge oder Erhöhung der Rechenleistung liefern. Sie schreiben, dass die Kriterien, auf denen die freundlichen KI-Theorien basieren Arbeit "nur, wenn man nicht nur große Kräfte der Vorhersage über die Wahrscheinlichkeit der unzähligen möglichen Ergebnisse hat, aber Sicherheit und Konsens darüber, wie man die verschiedenen Ergebnisse bewertet. Siehe auch KI-Kontroll-Problem KI Übernahme Künstliche Intelligenz Waffen Rennen Ethik der künstlichen Intelligenz Vorhandenes Risiko durch künstliche allgemeine Intelligenz Intelligenz Explosion Maschine Ethik Maschine Intelligenz Forschungsinstitut OpenAI Verordnung der Algorithmen Singularitarismus – eine moralische Philosophie, die von Befürwortern der Freundlichen KI-Technologie Singularität drei Gesetze der Robotik Referenzen weiterlesen Yudkowsky, E. Künstliche Intelligenz als ein positiver und negativer Faktor im globalen Risiko. In Global Catastrophic Risks, Oxford University Press, 2008. Diskutiert Künstliche Intelligenz aus der Perspektive des bestehenden Risikos. Insbesondere die Abschnitte 1-4 geben Hintergrund der Definition von Friendly AI in Abschnitt 5. Abschnitt 6 gibt zwei Klassen von Fehlern (technisch und philosophisch), die beide zur zufälligen Schaffung von nicht-Friendly AIs führen würden. In den Abschnitten 7-13 werden weitere Fragen erörtert. Omohundro, S. 2008 Die in AGI-08 erscheinenden grundlegenden KI-Antriebe - Verfahren der ersten Konferenz zum künstlichen Generalintelligent Mason, C. 2008 Human-Level-KI Erfordert mitfühlende Intelligenz in AAAI 2008Workshop zum Thema Meta-Reasoning: Denken über Denken Externe Links Ethische Fragen in fortgeschrittener Künstlicher Intelligenz von Nick Bostrom Was ist Freundliche KI?— Eine kurze Beschreibung der freundlichen KI vom Machine Intelligence Research Institute. Erstellen von Friendly AI 1.0: Die Analyse und Gestaltung von Benevolent Goal Architectures — Eine nahe Buch-Länge Beschreibung aus der MIRI-Kritik der MIRI-Leitlinien für Freundliche KI — von Bill Hibbard Kommentar zu MIRI-Leitlinien für Freundliche KI — von Peter Voss. Das Problem der „freundlichen“ künstlichen Intelligenz — Auf den Motiven für und Unmöglichkeit der FAI; von Adam Keiper und Ari N. Schulman.