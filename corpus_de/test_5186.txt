Die Berechnungskomplexitätstheorie konzentriert sich auf die Klassifizierung von Rechenproblemen nach ihrer Ressourcennutzung und die Zuordnung dieser Klassen zueinander. Ein Rechenproblem ist eine Aufgabe, die von einem Computer gelöst wird. Ein Rechenproblem ist durch mechanische Anwendung von mathematischen Schritten, wie einem Algorithmus, auflösbar. Ein Problem wird als inhärent schwierig angesehen, wenn seine Lösung erhebliche Ressourcen benötigt, was auch immer der verwendete Algorithmus verwendet. Die Theorie formalisiert diese Intuition, indem sie mathematische Modelle der Berechnung einführt, um diese Probleme zu untersuchen und ihre rechnerische Komplexität zu quantifizieren, d.h. die Menge der Ressourcen, die benötigt werden, um sie zu lösen, wie Zeit und Lagerung. Es werden auch andere Komplexitätsmaße verwendet, wie z.B. die Menge der Kommunikation (in der Kommunikationskomplexität verwendet), die Anzahl der Gates in einer Schaltung (in der Schaltungskomplexität verwendet) und die Anzahl der Prozessoren (im Parallelrechner verwendet). Eine der Aufgaben der Berechnungskomplexitätstheorie ist es, die praktischen Grenzen zu bestimmen, was Computer können und können. Das P versus NP-Problem, eines der sieben Millennium-Preis-Probleme, widmet sich dem Bereich der rechnerischen Komplexität. Eng verwandte Felder in der theoretischen Informatik sind Analyse von Algorithmen und Rechenschaftstheorie. Eine wesentliche Unterscheidung zwischen der Analyse von Algorithmen und der Berechnungskomplexitätstheorie besteht darin, dass der frühere sich der Analyse der von einem bestimmten Algorithmus benötigten Ressourcen zur Lösung eines Problems widmet, während dieser eine allgemeinere Frage zu allen möglichen Algorithmen stellt, die zur Lösung desselben Problems verwendet werden könnten. Genauer gesagt versucht die Rechenkomplexitätstheorie, Probleme zu klassifizieren, die mit entsprechend eingeschränkten Ressourcen gelöst werden können oder nicht. Die Einführung von Einschränkungen auf die verfügbaren Ressourcen ist wiederum das, was die Rechenkomplexität von der Rechenschaftstheorie unterscheidet: Letztere Theorie fragt, welche Probleme grundsätzlich algorithmisch gelöst werden können. Computerprobleme Probleminstanzen Ein rechnerisches Problem kann als unendliche Sammlung von Instanzen zusammen mit einem Satz (möglicherweise leer) von Lösungen für jeden Fall betrachtet werden. Die Eingabekette für ein Rechenproblem wird als Probleminstanz bezeichnet und sollte nicht mit dem Problem selbst verwechselt werden. In der rechnerischen Komplexitätstheorie bezieht sich ein Problem auf die zu lösende abstrakte Frage. Ein Beispiel dieses Problems ist dagegen eine eher konkrete Äußerung, die als Eingabe für ein Entscheidungsproblem dienen kann. Betrachten Sie beispielsweise das Problem der Primalitätsprüfung. Die Instanz ist eine Zahl (z.B. 15) und die Lösung ist ja, wenn die Zahl Prime ist und nicht anders (in diesem Fall ist 15 nicht Prime und die Antwort ist Nein). In einer anderen Weise ist die Instanz ein bestimmter Eingang zum Problem, und die Lösung ist die Ausgabe entsprechend dem gegebenen Eingang. Um den Unterschied zwischen einem Problem und einer Instanz weiter hervorzuheben, beachten Sie folgende Instanz der Entscheidungsversion des reisenden Verkäuferproblems: Gibt es eine Strecke von maximal 2000 Kilometern durch alle 15 größten Städte Deutschlands? Die quantitative Antwort auf diese spezielle Probleminstanz ist für die Lösung anderer Fälle des Problems von geringer Bedeutung, wie zum Beispiel eine Rundreise durch alle Standorte in Mailand, deren Gesamtlänge höchstens 10 km beträgt. Aus diesem Grund thematisiert die Komplexitätstheorie Rechenprobleme und nicht bestimmte Problemfälle. Darstellung von Problemfällen Bei der Betrachtung von Rechenproblemen ist eine Probleminstanz ein String über ein Alphabet. Üblicherweise wird das Alphabet als binäres Alphabet (d.h. der Satz {0,1}) genommen und somit sind die Strings Bitstrings. Wie bei einem realen Computer müssen andere mathematische Objekte als Bitstrings geeignet kodiert werden. Beispielsweise können ganze Zahlen in binärer Notation dargestellt werden, und Graphen können direkt über ihre Adjacency-Matrizen kodiert werden, oder indem sie ihre Adjacency-Listen in binär kodieren. Obwohl einige Nachweise der Komplexitätstheoretischen Theoretiker regelmäßig eine konkrete Wahl der Eingabecodierung annehmen, versucht man, die Diskussion abstrakt genug zu halten, um unabhängig von der Wahl der Kodierung zu sein. Dies kann dadurch erreicht werden, dass unterschiedliche Darstellungen effizient ineinander umgewandelt werden können. Entscheidungsprobleme als formale Sprachen Entscheidungsprobleme sind eines der zentralen Aufgaben des Studiums in der rechnerischen Komplexitätstheorie. Ein Entscheidungsproblem ist eine spezielle Art von Rechenproblemen, deren Antwort entweder ja oder nein ist, oder abwechselnd entweder 1 oder 0.Ein Entscheidungsproblem kann als formale Sprache angesehen werden, wobei die Mitglieder der Sprache Instanzen sind, deren Ausgabe ja ist, und die Nichtmitglieder sind solche Instanzen, deren Ausgabe nicht ist. Ziel ist es, mit Hilfe eines Algorithmus zu entscheiden, ob ein bestimmter Eingabestring ein Mitglied der betrachteten formalen Sprache ist. Wenn der Algorithmus, der dieses Problem entscheidet, die Antwort Ja zurückgibt, wird der Algorithmus gesagt, die Eingabekette zu akzeptieren, andernfalls wird gesagt, die Eingabe abzulehnen. Ein Beispiel für ein Entscheidungsproblem ist das folgende. Der Eingang ist ein beliebiges Diagramm. Das Problem besteht darin, zu entscheiden, ob der angegebene Graph verbunden ist oder nicht. Die formale Sprache, die mit diesem Entscheidungsproblem verbunden ist, ist dann der Satz aller verbundenen Graphen — um eine genaue Definition dieser Sprache zu erhalten, muss man entscheiden, wie Graphen als binäre Strings kodiert werden. Funktionsprobleme Ein Funktionsproblem ist ein Rechenproblem, bei dem für jeden Eingang ein einziger Ausgang (von einer Gesamtfunktion) erwartet wird, aber der Ausgang ist komplexer als der eines Entscheidungsproblems, d.h. der Ausgang ist nicht nur ja oder nein. Bemerkenswerte Beispiele sind das reisende Verkäuferproblem und das Problem der Integerfaktorisierung. Es ist verlockend zu denken, dass die Vorstellung von Funktionsproblemen viel reicher ist als die Vorstellung von Entscheidungsproblemen. Dies ist jedoch nicht wirklich der Fall, da Funktionsprobleme als Entscheidungsprobleme neu formuliert werden können. Beispielsweise kann die Multiplikation zweier ganze Zahlen als Dreifachsatz (a, b, c) ausgedrückt werden, so dass die Relation a × b = c hält. Entscheidend, ob ein vorgegebenes Tripel ein Mitglied dieses Satzes ist, entspricht der Lösung des Problems der Multiplikation zweier Zahlen. Messung der Größe einer Instanz Um die Schwierigkeit der Lösung eines Rechenproblems zu messen, kann man sehen, wie viel Zeit der beste Algorithmus benötigt, um das Problem zu lösen. Die Laufzeit kann jedoch im allgemeinen von der Instanz abhängen. Insbesondere werden größere Instanzen mehr Zeit benötigen, um zu lösen. So wird die Zeit, die erforderlich ist, um ein Problem zu lösen (oder der benötigte Raum oder irgendein Maß an Komplexität) in Abhängigkeit von der Größe der Instanz berechnet. Dies wird üblicherweise als Größe der Eingabe in Bits genommen. Komplexitätstheorie ist daran interessiert, wie Algorithmen mit einer Erhöhung der Eingangsgröße skaliert. Zum Beispiel, im Problem der Feststellung, ob ein Graph verbunden ist, wie viel mehr Zeit braucht es, um ein Problem für einen Graph mit 2n Vertices im Vergleich zu der Zeit für einen Graph mit n Vertices zu lösen? Ist die Eingangsgröße n, so kann die aufgenommene Zeit in Abhängigkeit von n ausgedrückt werden. Da die auf unterschiedliche Eingänge gleicher Größe genommene Zeit unterschiedlich sein kann, wird die schlimmste Zeitkomplexität T(n) als die maximale Zeit definiert, die über alle Eingänge der Größe n genommen wird.Wenn T(n) ein Polynom in n ist, so wird der Algorithmus als Polynomzeitalgorithmus bezeichnet. Cobhams Dissertation argumentiert, dass ein Problem mit einer machbaren Menge an Ressourcen gelöst werden kann, wenn es einen Polynomzeitalgorithmus zugibt. Maschinenmodelle und Komplexitätsmaßnahmen Turing Maschine Eine Turing-Maschine ist ein mathematisches Modell einer allgemeinen Rechenmaschine. Es ist ein theoretisches Gerät, das auf einem Bandstreifen enthaltene Symbole manipuliert. Turingmaschinen sind nicht als praktische Rechentechnik gedacht, sondern als allgemeines Modell einer Rechenmaschine – von einem fortgeschrittenen Supercomputer bis hin zu einem Mathematiker mit Bleistift und Papier. Es wird angenommen, dass, wenn ein Problem durch einen Algorithmus gelöst werden kann, gibt es eine Turing-Maschine, die das Problem löst. In der Tat ist dies die Aussage der Kirche-Turing-Thesis. Darüber hinaus ist bekannt, dass alles, was auf anderen, uns heute bekannten Rechenmodellen berechnet werden kann, wie zum Beispiel eine RAM-Maschine, Conway's Game of Life, zellular automata oder jede Programmiersprache auf einer Turing-Maschine berechnet werden kann. Da Turing-Maschinen einfach rechnerisch zu analysieren sind und als leistungsstark wie jedes andere Modell der Berechnung angenommen werden, ist die Turing-Maschine das am häufigsten verwendete Modell in der Komplexitätstheorie. Viele Arten von Turing-Maschinen werden verwendet, um Komplexitätsklassen zu definieren, wie deterministische Turing-Maschinen, probabilistische Turing-Maschinen, nicht-deterministische Turing-Maschinen, Quanten Turing-Maschinen, symmetrische Turing-Maschinen und alternierende Turing-Maschinen. Sie sind im Prinzip alle gleich mächtig, aber wenn Ressourcen (wie Zeit oder Raum) gebunden sind, können einige davon stärker sein als andere. Eine deterministische Turing-Maschine ist die grundlegendste Turing-Maschine, die eine feste Reihe von Regeln verwendet, um ihre zukünftigen Aktionen zu bestimmen. Eine probabilistische Turing-Maschine ist eine deterministische Turing-Maschine mit einer zusätzlichen Zufuhr von zufälligen Bits. Die Fähigkeit, probabilistische Entscheidungen zu treffen, hilft Algorithmen, Probleme effizienter zu lösen. Algorithmen, die zufällige Bits verwenden, werden randomisierte Algorithmen genannt. Eine nicht-deterministische Turing-Maschine ist eine deterministische Turing-Maschine mit einem zusätzlichen Merkmal des Nicht-Determinismus, die es einer Turing-Maschine ermöglicht, mehrere mögliche zukünftige Aktionen aus einem bestimmten Zustand zu haben. Eine Möglichkeit, den Nichtdeterminismus zu sehen, ist, dass die Turing-Maschine an jedem Schritt in viele mögliche Rechenpfade verzweigt, und wenn sie das Problem in einem dieser Zweige löst, soll sie das Problem gelöst haben. Natürlich ist dieses Modell nicht als physikalisch realisierbares Modell gedacht, es ist nur eine theoretisch interessante abstrakte Maschine, die besonders interessante Komplexitätsklassen hervorruft. Für Beispiele siehe nicht-deterministischer Algorithmus. Andere Maschinenmodelle Viele von den Standard-Multi-Tape-Turniermaschinen unterschiedliche Maschinenmodelle wurden in der Literatur vorgeschlagen, beispielsweise Zutrittsmaschinen. Überraschend kann jede dieser Modelle ohne zusätzliche Rechenleistung in eine andere umgewandelt werden. Der Zeit- und Speicherverbrauch dieser Wechselmodelle kann variieren. All diese Modelle haben gemeinsam, dass die Maschinen deterministisch arbeiten. Einige rechnerische Probleme lassen sich jedoch in Bezug auf ungewöhnlichere Ressourcen einfacher analysieren. Eine nicht-deterministische Turing-Maschine ist beispielsweise ein rechnerisches Modell, das zur Überprüfung vieler unterschiedlicher Möglichkeiten auf einmal ausgeschaltet werden kann. Die nicht-deterministische Turing-Maschine hat sehr wenig zu tun, wie wir physisch Algorithmen berechnen wollen, aber ihre Verzweigung erfasst genau viele der mathematischen Modelle, die wir analysieren möchten, so dass nicht-deterministische Zeit eine sehr wichtige Ressource bei der Analyse von Rechenproblemen ist. Komplexitätsmaßnahmen Für eine genaue Definition dessen, was es bedeutet, ein Problem mit einer bestimmten Zeit und Raum zu lösen, wird ein Rechenmodell wie die deterministische Turing-Maschine verwendet. Die von einer deterministischen Turing-Maschine M am Eingang x benötigte Zeit ist die Gesamtzahl der Zustandsübergänge oder -schritte, die Maschine macht, bevor sie die Antwort stoppt und ausgibt (ja oder nein). A Die Turingmaschine M soll innerhalb der Zeit f(n) arbeiten, wenn die von M an jedem Eingang der Länge n benötigte Zeit höchstens f(n) beträgt. Ein Entscheidungsproblem A kann in der Zeit f(n) gelöst werden, wenn eine in der Zeit f(n) arbeitende Turingmaschine vorhanden ist, die das Problem löst. Da die Komplexitätstheorie daran interessiert ist, Probleme basierend auf ihrer Schwierigkeit zu klassifizieren, definiert man Probleme auf der Grundlage einiger Kriterien. So wird beispielsweise der innerhalb der Zeit f(n) auf einer deterministischen Turingmaschine auflösbare Fehlersatz durch DTIME(f(n)) bezeichnet. Analoge Definitionen können für Raumanforderungen gemacht werden. Obwohl Zeit und Raum die bekanntesten Komplexitätsressourcen sind, kann jede Komplexitätsmaßnahme als Rechenressource betrachtet werden. Die Komplexitätsmassnahmen sind sehr allgemein durch die Blum-Komplexitäts-Axiome definiert. Andere Komplexitäts-Maßnahmen, die in der Komplexitätstheorie verwendet werden, umfassen Kommunikationskomplexität, Schaltungskomplexität und Entscheidungsstrukturkomplexität. Die Komplexität eines Algorithmus wird oft mit großer O-Notation ausgedrückt. Beste, schlechteste und durchschnittliche Komplexität Die beste, schlimmste und durchschnittliche Fallkomplexität bezieht sich auf drei verschiedene Arten der Messung der Zeitkomplexität (oder jede andere Komplexitätsmaßnahme) von verschiedenen Eingängen gleicher Größe. Da einige Eingaben der Größe n schneller zu lösen sind als andere, definieren wir die folgenden Komplexitäten: Beste Komplexität: Dies ist die Komplexität der Lösung des Problems für den besten Eingang der Größe n. Durchschnittliche Komplexität: Dies ist die Komplexität der Lösung des Problems im Durchschnitt. Diese Komplexität ist nur bezüglich einer Wahrscheinlichkeitsverteilung über die Eingänge definiert. Wenn beispielsweise angenommen wird, dass alle Eingänge gleicher Größe gleich wahrscheinlich erscheinen, kann die durchschnittliche Fallkomplexität bezüglich der gleichmäßigen Verteilung über alle Eingänge der Größe n definiert werden. Amortisierte Analyse: Die amortisierte Analyse betrachtet sowohl die kostspieligen als auch weniger kostspieligen Operationen zusammen über die gesamte Serie von Operationen des Algorithmus. Worst-case-Komplexität: Dies ist die Komplexität der Lösung des Problems für den schlimmsten Eingang der Größe n. Der Auftrag von billig zu kostspielig ist: Best, durchschnittlich (von diskreter gleichmäßiger Verteilung), amortisiert, schlimmsten. Betrachten Sie beispielsweise den deterministischen Sortieralgorithmus Quicksort. Dies löst das Problem der Sortierung einer Liste von Ganzzahlen, die als Eingabe angegeben wird. Der schlimmste Fall ist, wenn der Drehpunkt immer der größte oder kleinste Wert in der Liste ist (so ist die Liste nie geteilt). In diesem Fall nimmt der Algorithmus Zeit O(n2). Wenn wir annehmen, dass alle möglichen Permutationen der Eingabeliste gleichwahrscheinlich sind, ist die für die Sortierung benötigte Durchschnittszeit O(n log n). Der beste Fall tritt auf, wenn jedes Verschwenken die Liste in der Hälfte teilt, auch O(n log n) Zeit benötigt. Ober- und Untergrenze für die Komplexität der Probleme Um die Rechenzeit (oder ähnliche Ressourcen, wie Raumverbrauch) einzustufen, ist es hilfreich, obere und untere Begrenzungen auf den maximalen Zeitaufwand zu demonstrieren, den der effizienteste Algorithmus zur Lösung eines bestimmten Problems benötigt. Die Komplexität eines Algorithmus wird in der Regel genommen, um seine schlimmste Komplexität zu sein, sofern nicht anders angegeben. Eine Analyse eines bestimmten Algorithmus fällt unter das Feld der Analyse von Algorithmen. Um eine obere gebundene T(n) auf der Zeitkomplexität eines Problems zu zeigen, muss man nur zeigen, dass es einen bestimmten Algorithmus mit Laufzeit höchstens T(n) gibt. Allerdings ist der Nachweis niedrigerer Grenzen viel schwieriger, da niedrigere Grenzen eine Aussage über alle möglichen Algorithmen machen, die ein gegebenes Problem lösen. Der Begriff "alle möglichen Algorithmen" umfasst nicht nur die heute bekannten Algorithmen, sondern jeden Algorithmus, der in Zukunft entdeckt werden könnte. Um eine geringere Grenze von T(n) für ein Problem zu zeigen, muss gezeigt werden, dass kein Algorithmus Zeitaufwand kleiner als T(n) haben kann. Ober- und Untergrenzen werden in der Regel mit der großen O-Notation angegeben, die konstante Faktoren und kleinere Begriffe verbirgt. Dies macht die Grenzen unabhängig von den spezifischen Details des verwendeten Rechenmodells. Wenn z.B. T(n) = 7n2 + 15n + 40, würde man in großer O-Notation T(n) = O(n2) schreiben. Komplexitätsklassen definieren Komplexitätsklassen Eine Komplexitätsklasse ist eine Reihe von Problemen der damit verbundenen Komplexität. Einfachere Komplexitätsklassen werden durch folgende Faktoren definiert: Die Art des Rechenproblems: Die am häufigsten verwendeten Probleme sind Entscheidungsprobleme. Jedoch können Komplexitätsklassen basierend auf Funktionsproblemen, Zählproblemen, Optimierungsproblemen, Versprechensproblemen usw. definiert werden. Das Modell der Berechnung: Das häufigste Modell der Berechnung ist die deterministische Turing-Maschine, aber viele Komplexitätsklassen basieren auf nicht-deterministischen Turing-Maschinen, Boolean-Schaltungen, Quanten Turing-Maschinen, Monoton-Schaltungen, etc. Die zu begrenzende Ressource (oder Ressourcen) und die gebundene: Diese beiden Eigenschaften werden üblicherweise gemeinsam angegeben, wie "Polynomzeit", "Logarithmischer Raum", "Konstante Tiefe", usw. Einige Komplexitätsklassen haben komplizierte Definitionen, die nicht in diesen Rahmen passen. So hat eine typische Komplexitätsklasse eine Definition wie folgt: Die von einer deterministischen Turingmaschine innerhalb der Zeit f(n) auflösbare Entscheidungsprobleme. ( Diese Komplexitätsklasse wird als DTIME(f(n)) bezeichnet. Aber die oben durch eine konkrete Funktion f(n) vorgegebene Rechenzeit ergibt oft Komplexitätsklassen, die vom gewählten Maschinenmodell abhängen. Beispielsweise kann die Sprache {xx | x eine binäre Saite} in linearer Zeit auf einer Multi-Tape-Turniermaschine gelöst werden, erfordert aber notwendigerweise quadratische Zeit im Modell von Einband-Turniermaschinen. Wenn wir polynome Schwankungen der Laufzeit zulassen, sagt Cobham-Edmonds These, dass "die Zeitkomplexitäten in zwei vernünftigen und allgemeinen Berechnungsmodellen polynom verwandt sind" (Goldreich 2008, Kapitel 1.2). Dies bildet die Basis für die Komplexitätsklasse P, die die von einer deterministischen Turingmaschine in der Polynomzeit auflösbare Entscheidungsprobleme darstellt. Der entsprechende Satz von Funktionsproblemen ist FP. Wichtige Komplexitätsklassen Viele wichtige Komplexitätsklassen können durch die Begrenzung der Zeit oder des vom Algorithmus verwendeten Raums definiert werden. Einige wichtige Komplexitätsklassen von auf diese Weise definierten Entscheidungsproblemen sind die folgenden: Die logarithmischen Raumklassen (erforderlich) berücksichtigen nicht den Raum, der benötigt wird, um das Problem darzustellen. Es stellt sich heraus, dass PSPACE = NPSPACE und EXPSPACE = NEXPSPACE von Savitchs Theorem. Weitere wichtige Komplexitätsklassen umfassen BPP, ZPP und RP, die mit probabilistischen Turing-Maschinen definiert sind; AC und NC, die mit Boolean-Schaltungen definiert sind; und BQP und QMA, die mit Quanten-Turniermaschinen definiert sind.#P ist eine wichtige Komplexitätsklasse von Zählproblemen (nicht Entscheidungsprobleme). Klassen wie IP und AM werden mit Interactive Proof-Systemen definiert. ALL ist die Klasse aller Entscheidungsprobleme. Hierarchie theorems Für die auf diese Weise definierten Komplexitätsklassen ist es wünschenswert, zu beweisen, dass die Erholung der Anforderungen an die Berechnungszeit in der Tat eine größere Menge von Problemen definiert. Insbesondere, obwohl DTIME(n) in DTIME(n2) enthalten ist, wäre es interessant zu wissen, ob die Aufnahme streng ist. Für Zeit- und Raumbedarf wird die Antwort auf solche Fragen durch die Zeit- und Raumhierarchietheorems gegeben. Sie werden Hierarchietheorems genannt, weil sie eine richtige Hierarchie auf den Klassen induzieren, die durch die Konkursierung der jeweiligen Ressourcen definiert sind. So gibt es Paare von Komplexitätsklassen, so dass man richtig in den anderen integriert ist. Nachdem wir solche richtigen Set-Inklusions abgebaut haben, können wir quantitative Aussagen darüber machen, wie viel mehr Zeit oder Raum benötigt wird, um die Anzahl der Probleme zu erhöhen, die gelöst werden können. Genauer gesagt, die Zeithierarchie Theorem gibt an, dass D T I M E (f (n ) ⊊ D T I M E (f (n ) ∙ log 2Eine C E (f ( n ) ⋅ log ‡ (f ( n ) ) ) {\displaystyle {\mathsf DSPACE}{\big }f(n){\big \})subsetneq {\mathsf DSPACE}}}{\big }f(n)\cdot log(f(n){\bige ){\bige )}}}}}} Die Zeithierarchie Theorem sagt uns zum Beispiel, dass P streng in EXPTIME enthalten ist, und die Raumhierarchie Theorem sagt uns, dass L streng in PSPACE enthalten ist. Reduktion Viele Komplexitätsklassen werden mit dem Konzept einer Reduktion definiert. Eine Reduktion ist eine Transformation eines Problems in ein anderes Problem. Es erfasst die informelle Vorstellung eines Problems, das so schwierig wie ein anderes Problem ist. Wenn beispielsweise ein Problem X mit einem Algorithmus für Y gelöst werden kann, ist X nicht schwieriger als Y, und wir sagen, dass X auf Y reduziert. Es gibt viele verschiedene Arten von Reduktionen, basierend auf der Methode der Reduktion, wie Cook-Reduktionen, Karp-Reduktionen und Levin-Reduktionen, und die gebunden auf die Komplexität von Reduktionen, wie Polynom-Zeit-Reduktionen oder Log-Raum-Reduktionen. Die am häufigsten verwendete Reduktion ist eine Polynom-Zeit-Reduktion. Das bedeutet, dass der Reduktionsprozess Polynomzeit einnimmt. Beispielsweise kann das Problem der Squringierung einer Ganzzahl auf das Problem der Multiplizierung zweier Ganzzahlen reduziert werden. Dies bedeutet, dass ein Algorithmus zur Multiplikation zweier Ganzzahlen verwendet werden kann, um eine ganze Zahl zu ordnen. In der Tat kann dies geschehen, indem beide Eingänge des Multiplikationsalgorithmus den gleichen Eingang erhalten. So sehen wir, dass der Squaring nicht schwieriger ist als die Multiplikation, da Squaring auf Multiplikation reduziert werden kann. Dies motiviert das Konzept eines Problems, das für eine Komplexitätsklasse schwierig ist. Ein Problem X ist schwer für eine Klasse von Problemen Cif jedes Problem in C kann auf X reduziert werden.Thus kein Problem in C ist härter als X, da ein Algorithmus für X uns erlaubt, jedes Problem in C zu lösen. Für Komplexitätsklassen größer als P werden häufig Polynom-Zeit-Reduktionen verwendet. Insbesondere ist die Reihe von Problemen, die für NP schwer sind, das Set von NP-harten Problemen. Ist ein Problem X in C und hart für C, dann wird X für C.This bedeutet, dass X das schwierigste Problem in C ist. (Da viele Probleme ebenso hart sein könnten, könnte man sagen, dass X eines der schwierigsten Probleme in C.)Dann die Klasse der NP-komplete Probleme enthält die schwierigsten Probleme in NP, in dem Sinne, dass sie die sind, die wahrscheinlich nicht in P sein. Da das Problem P = NP nicht gelöst ist und ein bekanntes NP-komplettes Problem, D2, auf ein anderes Problem reduzieren kann, würde D1 angeben, dass es für D1 keine bekannte Polynom-Zeit-Lösung gibt. Dies liegt daran, dass eine Polynom-Zeit-Lösung zu D1 eine Polynom-Zeit-Lösung zu D2 liefern würde. Ebenso, weil alle NP-Probleme auf das Set reduziert werden können, würde das Finden eines NP-kompletten Problems, das in der Polynomzeit gelöst werden kann, bedeuten, dass P = NP. Wichtige offene Probleme P gegen NP Problem Die Komplexitätsklasse P wird oft als mathematische Abstraktion betrachtet, die diese Rechenaufgaben modelliert, die einen effizienten Algorithmus zugeben. Diese Hypothese wird die Cobham-Edmonds-Thesis genannt. Die Komplexitätsklasse NP hingegen enthält viele Probleme, die Menschen effizient lösen möchten, für die jedoch kein effizienter Algorithmus bekannt ist, wie das Boolesche Problem der Befriedigung, das Hamiltonische Pfadproblem und das Vertex-Deckelproblem. Seit deterministisch Turing-Maschinen sind spezielle nicht-deterministische Turing-Maschinen, es ist leicht zu beobachten, dass jedes Problem in P auch Mitglied der Klasse NP ist. Die Frage, ob P gleich NP eine der wichtigsten offenen Fragen in der theoretischen Informatik aufgrund der großen Auswirkungen einer Lösung ist. Wenn die Antwort ja ist, können viele wichtige Probleme gezeigt werden, um effizientere Lösungen zu haben. Dazu gehören verschiedene Arten von ganzzahligen Programmierprobleme in der Betriebsforschung, viele Probleme in der Logistik, Proteinstruktur Vorhersage in der Biologie, und die Fähigkeit, formale Beweise der reinen Mathematik Theorems zu finden. Das P gegen NP-Problem ist eines der Millennium-Preis-Probleme, die vom Clay Mathematics Institute vorgeschlagen werden. Es gibt einen US$ 1.000.000 Preis für die Lösung des Problems. Probleme in NP nicht bekannt, in P oder NP-vollständig zu sein Es wurde von Ladner gezeigt, dass wenn P ≠ NP dann Probleme in NP existieren, die weder in P noch NP-vollständig sind. Solche Probleme werden als NP-Intermediate-Probleme bezeichnet. Das graphische Isomorphismusproblem, das diskrete Logarithm-Problem und das Ganzzahl-Faktorisierungsproblem sind Beispiele für Probleme, die als NP-Zwischenmittel angenommen werden. Sie sind einige der wenigen NP-Probleme, die nicht bekannt sind, in P zu sein oder NP-vollständig zu sein. Das graphische Isomorphismusproblem ist das rechnerische Problem, ob zwei endliche Graphen isomorph sind. Ein wichtiges ungelöstes Problem in der Komplexitätstheorie ist, ob das Graph-Isomorphismus-Problem in P, NP-komplete oder NP-Intermediate ist. Die Antwort ist nicht bekannt, aber es wird angenommen, dass das Problem zumindest nicht NP-vollständig ist. Ist Graphisomorphismus NP-vollständig, kollabiert die Polynomzeithierarchie auf ihre zweite Ebene. Da es weithin angenommen wird, dass die polynomische Hierarchie nicht auf eine endliche Ebene zusammenbricht, wird angenommen, dass Graphisomorphismus nicht NP-vollständig ist. Der beste Algorithmus für dieses Problem, aufgrund von László Babai und Eugene Luks hat Laufzeit O ( 2 n log ‡ n) {\displaystyle O(2^{\sqrt {n\log n}) für Graphen mit n vertices, obwohl einige jüngste Arbeit von Babai bietet einige potentiell neue Perspektiven auf diesem Gebiet. Das Ganzzahl-Faktorisierungsproblem ist das rechnerische Problem, die Prime Factorisierung einer bestimmten Ganzzahl zu bestimmen. Als Entscheidungsproblem formuliert, ist es das Problem zu entscheiden, ob die Eingabe einen Grundfaktor kleiner als k hat. Es ist kein effizienter Integerfaktorisierungsalgorithmus bekannt, der die Grundlage mehrerer moderner kryptographischer Systeme, wie z.B. des RSA-Algorithmus, bildet. Das Ganzzahl-Faktorisierungsproblem ist in NP und in Co-NP (und sogar in UP und Co-UP). Wenn das Problem NP-vollständig ist, wird die Polynomzeithierarchie auf ihre erste Ebene zusammenbrechen (d.h. NP wird gleich Co-NP sein). Der bekannteste Algorithmus für die Integerfaktorisierung ist das allgemeine Zahlenfeldsieb, das die Zeit O (e (64 9 3 ) ( log ‡ n) 3 ( log ♦ log ‡ n ) 2 3 ) {\displaystyle O(e^{\left({\s\qrt[{}]{\frac 64}{9}}}{\right){\sqrt[{3}}}]{ Allerdings läuft der bekannteste Quantenalgorithmus für dieses Problem, Shors Algorithmus, in Polynomzeit. Leider sagt diese Tatsache nicht viel darüber, wo das Problem in Bezug auf Nicht-Quanten-Komplexitätsklassen liegt. Trennungen zwischen anderen Komplexitätsklassen Viele bekannte Komplexitätsklassen werden vermutet, ungleich zu sein, aber dies wurde nicht nachgewiesen. So ist es z.B. P ‡ NP ‡PP konform PSPACE, aber es ist möglich, dass P = PSPACE. Ist P nicht gleich NP, so ist P auch nicht gleich PSPACE. Da es viele bekannte Komplexitätsklassen zwischen P und PSPACE gibt, wie RP, BPP, PP, BQP, MA, PH, etc. es ist möglich, dass all diese Komplexitätsklassen zu einer Klasse zusammenbrechen. Als Beweis dafür, dass jede dieser Klassen ungleich ist, wäre ein großer Durchbruch in der Komplexitätstheorie. In den gleichen Zeilen ist das Co-NP die Klasse, die die Komplement-Probleme (d.h. die Probleme mit den ja/no Antworten umgekehrt) von NP-Problemen enthält. Es wird angenommen, dass NP nicht gleich dem Co-NP ist; es wurde jedoch noch nicht nachgewiesen. Es ist klar, dass wenn diese beiden Komplexitätsklassen nicht gleich sind, P nicht gleich NP ist, da P = Co-P. Wenn also P=NP hätten wir co-P=co-NP, wennce NP=P=co-P=co-NP. Ebenso ist es nicht bekannt, wenn L (die Menge aller Probleme, die im logarithmischen Raum gelöst werden können) streng in P oder gleich P enthalten ist. Es gibt wiederum viele Komplexitätsklassen zwischen den beiden, wie NL und NC, und es ist nicht bekannt, wenn sie unterschiedliche oder gleiche Klassen sind. Es wird vermutet, dass P und BPP gleich sind. Es ist jedoch aktuell offen, wenn BPP = NEXP. Behinderung Ein Problem, das in der Theorie gelöst werden kann (z.B. bei großen, aber endlichen Ressourcen, vor allem Zeit), für das in der Praxis jede Lösung zu viele Ressourcen benötigt, um nützlich zu sein, ist als ein schwieriges Problem bekannt. Umgekehrt nennt man ein Problem, das in der Praxis gelöst werden kann, ein lenkbares Problem, buchstäblich "ein Problem, das man bewältigen kann". Der Begriff unfehlbar (weniger "kann nicht gemacht werden") wird manchmal austauschbar mit unlösbar verwendet, obwohl diese Verwirrung mit einer durchführbaren Lösung in der mathematischen Optimierung gefährdet. Traktierbare Probleme werden häufig mit Problemen identifiziert, die Polynom-Zeit-Lösungen (P, PTIME;) dies ist als Cobham-Edmonds-Thesis bekannt. Probleme, die in diesem Sinne bekannt sind, sind EXPTIME-hart. Ist NP nicht dasselbe wie P, so sind auch in diesem Sinne NP-harte Probleme anlenkbar. Diese Identifikation ist jedoch ungenau: Eine Polynom-Zeit-Lösung mit großem oder großem Spitzenkoeffizienten wächst schnell und kann für praktische Größenprobleme unpraktisch sein; umgekehrt kann eine langsam wachsende exponentielle Zeit-Lösung auf realistischem Eingang praktisch sein, oder eine im schlimmsten Fall lange Lösung kann in den meisten Fällen oder im mittleren Fall eine kurze Zeit dauern und damit noch praktisch sein. Wenn man sagt, dass ein Problem nicht in P liegt, bedeutet das nicht, dass alle großen Fälle des Problems hart sind oder sogar die meisten von ihnen sind. Zum Beispiel hat sich das Entscheidungsproblem in Presburger arithmetic nicht in P gezeigt, aber Algorithmen wurden geschrieben, die das Problem in vernünftigen Zeiten in den meisten Fällen lösen. Ebenso können Algorithmen das NP-komplete Knapsack-Problem über eine Vielzahl von Größen in weniger als quadratischer Zeit lösen und SAT-Löser behandeln routinemäßig große Instanzen des NP-komplete Boolean Zufriedenheit Problem. Um zu sehen, warum exponentielle Zeitalgorithmen in der Praxis in der Regel unbrauchbar sind, betrachten Sie ein Programm, das 2n Operationen vor dem Stoppen macht. Für kleine n, sagen 100, und vorausgesetzt, zum Beispiel, dass der Computer 1012 Operationen jede Sekunde, würde das Programm für etwa 4 × 1010 Jahre laufen, die die gleiche Größenordnung wie das Zeitalter des Universums ist. Auch bei einem viel schnelleren Computer wäre das Programm nur für sehr kleine Fälle nützlich, und in diesem Sinne ist die Anziehungskraft eines Problems etwas unabhängig vom technologischen Fortschritt. Ein exponentieller Zeitalgorithmus, der 1.0001n Operationen benötigt, ist jedoch praktisch, bis n relativ groß wird. Ebenso ist ein Polynomzeitalgorithmus nicht immer praktisch. Wenn seine Laufzeit n15 ist, ist es unangemessen, sie effizient zu betrachten und es ist immer noch nutzlos bis auf kleine Fälle. Tatsächlich sind in der Praxis sogar n3 oder n2 Algorithmen oft unpraktisch auf realistische Größen von Problemen. Kontinuierliche Komplexitätstheorie Kontinuierliche Komplexitätstheorie kann sich auf Komplexitätstheorie von Problemen beziehen, die kontinuierliche Funktionen beinhalten, die durch Diskretierungen angenähert werden, wie in der numerischen Analyse untersucht. Ein Ansatz zur Komplexitätstheorie der numerischen Analyse ist die Komplexität der Informationen. Kontinuierliche Komplexitätstheorie kann sich auch auf die Komplexitätstheorie der Verwendung von analoger Berechnung beziehen, die kontinuierliche dynamische Systeme und Differentialgleichungen verwendet. Die Steuerungstheorie kann als eine Form von Berechnungs- und Differenzgleichungen bei der Modellierung von Dauer- und Hybrid-Direkt-Continuous-Time-Systemen verwendet werden. Geschichte Ein frühes Beispiel für die Algorithmus-Komplexitätsanalyse ist die Laufzeitanalyse des Euclidean-Algorithmus von Gabriel Lamé im Jahre 1844. Bevor die eigentliche, der Komplexität der algorithmischen Probleme explizit gewidmete Forschung begann, wurden zahlreiche Grundlagen von verschiedenen Forschern geschaffen. Einflussreich war dabei die Definition von Turing-Maschinen von Alan Turing im Jahr 1936, die sich als eine sehr robuste und flexible Vereinfachung eines Computers erwiesen. Der Beginn systematischer Studien in der rechnerischen Komplexität wird auf das Halbnal 1965 "On the Computational Complexity of Algorithms" von Juris Hartmanis und Richard E. Stearns zurückgeführt, das die Definitionen von Zeitkomplexität und Raumkomplexität herausstellte und die Hierarchietheorems bewiesen. Darüber hinaus schlug Edmonds 1965 vor, einen guten Algorithmus als einen mit Laufzeit gebunden durch ein Polynom der Eingangsgröße zu betrachten. Frühere Arbeiten, die Probleme untersuchen, die Turing-Maschinen mit spezifischen gebundenen Ressourcen lösen können, umfassen John Myhills Definition von linear gebundenen Automaten (Myhill 1960,) Raymond Smullyans Untersuchung von rudimentären Sätzen (1961,) sowie Hisao Yamadas Papier auf Echtzeit-Rechnungen (1962). Etwas früher untersuchte Boris Trakhtenbrot (1956), ein Pionier auf dem Gebiet der UdSSR, eine weitere spezifische Komplexitätsmaßnahme. Wie er sich erinnert: [meine] anfängliche Interesse [in automata Theorie] wurde jedoch zunehmend zugunsten der rechnerischen Komplexität, einer spannenden Fusion von kombinatorischen Methoden, geerbt von der Schalttheorie, mit dem konzeptuellen Arsenal der Theorie der Algorithmen. Diese Ideen waren mir bereits 1955 aufgetreten, als ich den Begriff "Signalisierungsfunktion" geprägt habe, der heute als "Komplexitätsmaßnahme" bekannt ist. Im Jahr 1967 formulierte Manuel Blum eine Reihe von Axiomen (jetzt bekannt als Blum Axioms), die wünschenswerte Eigenschaften von Komplexitätsmassnahmen auf dem Satz von Rechenfunktionen vorlegte und ein wichtiges Ergebnis, das sogenannte Speed-up Theorem, erwies. Das Feld begann 1971 zu florieren, als Stephen Cook und Leonid Levin die Existenz praktisch relevanter Probleme bewiesen, die NP-vollständig sind. Im Jahr 1972 nahm Richard Karp diese Idee einen Sprung nach vorne mit seinem Wahrzeichen-Papier, "Reducibility Unter Combinatorial Problems", in dem er zeigte, dass 21 verschiedene kombinatorische und graphische theoretische Probleme, jedes berüchtigt für seine rechnerische Intraktivität, sind NP-komplete. Siehe auch Werke über Komplexität Wuppuluri, Shyam; Doria, Francisco A,. eds.(2020,) Unravelling Complexity: The Life and Work of Gregory Chaitin, World Scientific, doi:10.1142/11270, ISBN 978-981-12-0006-981 Referenzen Zitate Literaturbücher Arora, Sanjeev; Barak, Boaz (2009,) Computational Complexity Downey, Rod; Fellows, Michael (1999,) Parameterisierte Komplexität, Monographien in Informatik, Berlin, New York: Springer-Verlag, ISBN 9780387948836 Du. Ding-Zhu; Ko, Ker-I (2000,) Theory of Computational Complexity, John Wiley & Sons, ISBN 978-0-471-34506-0 Garey, Michael R.; Johnson, David S. (1979,) Computer und Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman, ISBN 0-7167-1045-5 Goldreich, Odedhanded 1990 Computational Complex MITTEL Presse, ISBN 978-0-444-88071-0 Papadimitriou, Christos (1994), Computational Complexity (1st ed,.) Addison Wesley, ISBN 978-0-201-53082-7 Sipser, Michael (2006), Einführung in die Computationstheorie (2. ed,.) USA: Thomson Course Technology, ISBN 978-0-534-95097-2 Umfragen Khalil, Hatem; Ulery, Dana (1976,) "A review of Current Studies on Complexity of Algorithms for Partial Differential Equations", Proceedings of the Annual Conference on - ACM 76, ACM '76: 197–201, doi:10.1145/80073491.C8055 Cook, Stephen (1983,) "Ein Überblick über die rechnerische Komplexität", Commun.ACM, 26 (6): 400–408, doi:10.1145/358141.358144, ISSN 0001-0782, S2CID 14323396 Fortnow, Lance; Homer, Steven (2003,) "A Short History of Computational Complexity" (PDF,) Bulletin of the EATCS, 80: 95–133 Mertens, Stephan (2002,) "Computational Complexity for Physicists", Computing in Science and Eng,. 4 (3): 31–47, arXiv:cond-mat/0012185, Bibcode:2002C. Externe Links Der Complexity Zoo "Computational Komplexität classs", Encyclopedia of Mathematics, EMS Press, 2001 [1994]Was sind die wichtigsten Ergebnisse (und Papiere) in der Komplexitätstheorie, die jeder wissen sollte? Scott Aaronson: Warum Philosophen sollten sich über Computational Complexity kümmern Die Alliance to Rescue Civilization ist eine Organisation, die sich der Einrichtung einer Off-Earth-Backupage der menschlichen Zivilisation widmet. Diese Einrichtung oder eine Gruppe von Einrichtungen würde dazu dienen, die Erde nach einer weltweiten Katastrophe oder Krieg wieder zu bevölkern und so viel wie möglich sowohl der Wissenschaften als auch der Kunst zu bewahren. Die Organisation ruft zurzeit dazu auf, dass eine solche Sicherungsanlage auf dem Mond gebaut wird, anstatt dass die NASA bis 2024 dorthin zurückkehren will. Er wurde vom Autor und Journalist William E. Burrows und dem Biochemiker Robert Shapiro gegründet. Referenzen Externe Links Die Allianz zur Rettung Zivilisation - Ein Organisationsrahmen - Internet Archive ARC Website Eine Allianz zur Rettung Zivilisation, Ad Astra, 1999 Morgan, Richard (August 1, 2006). " Leben nach der Erde: Imagining Survival Beyond This Terra Firma".New York Times. Retrieved April 23, 2010.