Theoretische Informatik (TCS) ist eine Untergruppe der allgemeinen Informatik und Mathematik, die sich auf mathematische Aspekte der Informatik wie die Theorie der Berechnung, Lambda-Calculus und Typ-Theorie konzentriert. Es ist schwierig, die theoretischen Bereiche genau zu umgehen. Die ACM Special Interest Group on Algorithms and Computation Theory (SIGACT) bietet folgende Beschreibung: TCS umfasst eine Vielzahl von Themen wie Algorithmen, Datenstrukturen, rechnerische Komplexität, parallele und verteilte Berechnung, probabilistische Berechnung, Quantenrechnung, Automatisierungstheorie, Informationstheorie, Kryptographie, Programm semantik und Verifikation, maschinelles Lernen, Rechenbiologie, Rechenökonomie, Rechengeometrie, Rechen- und Rechennummerntheorie und Algebra. Die Arbeit in diesem Bereich wird oft durch seine Betonung auf mathematische Technik und Strenge unterschieden. Geschichte Während zuvor logische Inferenz und mathematische Beweise existierten, bewies Kurt Gödel 1931 mit seinem Unvollkommenheitstheorem, dass es grundlegende Einschränkungen darüber gibt, welche Aussagen bewiesen oder nicht bewiesen werden könnten. Diese Entwicklungen haben zur modernen Untersuchung der Logik und Rechenschaftspflicht geführt, und zwar dem Bereich der theoretischen Informatik insgesamt. Die Informationstheorie wurde dem Feld mit einer mathematischen Theorie der Kommunikation von Claude Shannon 1948 hinzugefügt. Im gleichen Jahrzehnt stellte Donald Hebb ein mathematisches Modell des Lernens im Gehirn vor. Bei der Montage von biologischen Daten, die diese Hypothese mit einer Modifikation unterstützen, wurden die Felder der neuronalen Netze und der parallel verteilten Verarbeitung festgelegt. 1971 bewiesen Stephen Cook und, unabhängig arbeitend, Leonid Levin, dass es praktisch relevante Probleme gibt, die NP-komplete sind – ein Meilenstein in der Berechnungskomplexitätstheorie. Mit der Entwicklung der Quantenmechanik Anfang des 20. Jahrhunderts kam das Konzept, dass mathematische Operationen auf einer ganzen Teilchenwellenfunktion durchgeführt werden könnten. Mit anderen Worten könnte man gleichzeitig Funktionen auf mehreren Zuständen berechnen. Dies führte zu dem Konzept eines Quantencomputers in der letzten Hälfte des 20. Jahrhunderts, der in den 1990er Jahren ausbrachte, als Peter Shor zeigte, dass solche Methoden verwendet werden könnten, um große Zahlen in der Polynomzeit zu berücksichtigen, die, wenn implementiert, einige moderne öffentliche Schlüssel-Kryptographie-Algorithmen wie RSA_(cryptosystem) unsicher machen würde. Moderne theoretische Informatikforschung basiert auf diesen grundlegenden Entwicklungen, umfasst aber viele andere mathematische und interdisziplinäre Probleme, die gestellt wurden, wie unten gezeigt: Themen Algorithmen Ein Algorithmus ist ein Schritt für Schritt Verfahren für Berechnungen. Algorithmen werden zur Berechnung, Datenverarbeitung und automatisierten Argumentation verwendet. Ein Algorithmus ist eine effektive Methode, die als eine endliche Liste von gut definierten Anweisungen zur Berechnung einer Funktion ausgedrückt wird. Ausgehend von einem Anfangszustand und einem Anfangseingang (perhaps leer) beschreiben die Instruktionen eine Berechnung, die bei Ausführung durch eine endliche Anzahl von gut definierten aufeinanderfolgenden Zuständen verläuft, schließlich die Ausgabe erzeugt und an einem Endendzustand endet. Der Übergang von einem Zustand zum nächsten ist nicht unbedingt deterministisch; einige Algorithmen, die als randomisierte Algorithmen bekannt sind, enthalten zufällige Eingaben. Automata Theorie Automata Theorie ist die Studie von abstrakten Maschinen und automata, sowie die rechnerischen Probleme, die mit ihnen gelöst werden können. Es ist eine Theorie in der theoretischen Informatik, unter diskreten Mathematik (ein Teil der Mathematik und auch der Informatik). Automata stammt aus dem griechischen Wort αϊτόματα, was Selbstwirken bedeutet." Automata Die Theorie ist die Studie von selbst arbeitenden virtuellen Maschinen, um beim logischen Verständnis von Eingabe- und Ausgabeprozess zu helfen, ohne oder mit Zwischenstufen der Berechnung (oder jede Funktion/Prozess). Coding Theorie Coding Theorie ist die Untersuchung der Eigenschaften von Codes und ihre Eignung für eine bestimmte Anwendung. Codes werden für die Datenkompression, Kryptographie, Fehlerkorrektur und vor kurzem auch für die Netzwerkcodierung verwendet. Kodizes werden von verschiedenen wissenschaftlichen Disziplinen - wie Informationstheorie, Elektrotechnik, Mathematik und Informatik - untersucht, um effiziente und zuverlässige Datenübertragungsmethoden zu entwickeln. Hierbei handelt es sich typischerweise um die Entfernung von Redundanz und die Korrektur (oder Detektion) von Fehlern in den übertragenen Daten. Computational biology Computational biology beinhaltet die Entwicklung und Anwendung von datenanalytischen und theoretischen Methoden, mathematische Modellierung und rechnerische Simulationstechniken zur Untersuchung biologischer, verhaltensbezogener und sozialer Systeme. Das Feld ist breit definiert und umfasst Grundlagen in der Informatik, angewandte Mathematik, Animation, Statistiken, Biochemie, Chemie, Biophysik, Molekularbiologie, Genetik, Genomik, Ökologie, Evolution, Anatomie, Neurowissenschaften und Visualisierung. Die rechnerische Biologie unterscheidet sich von der biologischen Berechnung, die ein Unterfeld der Informatik und Informatik mit Bioengineering und Biologie zum Aufbau von Computern ist, ist aber ähnlich wie Bioinformatik, die eine interdisziplinäre Wissenschaft mit Computern ist, um biologische Daten zu speichern und zu verarbeiten. Rechenkomplexitätstheorie Rechenkomplexitätstheorie ist ein Zweig der Berechnungstheorie, der sich auf die Klassifizierung von Rechenproblemen nach ihrer inhärenten Schwierigkeit konzentriert und diese Klassen miteinander in Beziehung setzt. Unter einem rechnerischen Problem wird eine Aufgabe verstanden, die grundsätzlich durch einen Computer gelöst werden kann, was der Aussage entspricht, dass das Problem durch mechanische Anwendung von mathematischen Schritten, wie einem Algorithmus, gelöst werden kann. Ein Problem wird als inhärent schwierig angesehen, wenn seine Lösung erhebliche Ressourcen benötigt, was auch immer der verwendete Algorithmus verwendet. Die Theorie formalisiert diese Intuition, indem sie mathematische Modelle der Berechnung, um diese Probleme zu studieren und die Menge der Ressourcen zu quantifizieren, die benötigt werden, um sie zu lösen, wie Zeit und Lagerung. Es werden auch andere Komplexitätsmassnahmen verwendet, wie z.B. die in der Kommunikationskomplexität verwendete Menge der Kommunikation, die Anzahl der Gates in einer Schaltung (in der Schaltungskomplexität verwendet) und die Anzahl der Prozessoren (im Parallelrechner verwendet). Eine der Aufgaben der rechnerischen Komplexitätstheorie ist es, die praktischen Grenzen zu bestimmen, was Computer können und können. Computational Geometrie Computational Geometrie ist ein Zweig der Informatik gewidmet der Studie von Algorithmen, die in Bezug auf Geometrie angegeben werden können. Einige rein geometrische Probleme ergeben sich aus der Untersuchung von rechnerischen geometrischen Algorithmen, und solche Probleme werden auch als Teil der rechnerischen Geometrie betrachtet. Während moderne Rechengeometrie eine jüngste Entwicklung ist, ist es eine der ältesten Felder des Computing mit Geschichte zurück zu Antike. Ein alter Vorläufer ist die Sanskrit-Verhandlung Shulba Sutras oder "Rules of the Chord", das ist ein Buch von Algorithmen geschrieben in 800 BCE. Das Buch schreibt Schritt für Schritt Verfahren zum Konstruieren von geometrischen Objekten wie Altar mit einem Zapfen und Akkord vor. Der Hauptimpuls für die Entwicklung der Rechengeometrie als Disziplin war Fortschritt in der Computergrafik und computergestützten Konstruktion und Fertigung (CAD/CAM), aber viele Probleme in der Rechengeometrie sind klassisch in der Natur und kann von der mathematischen Visualisierung kommen. Weitere wichtige Anwendungen der Rechengeometrie sind Robotik (Bewegungsplanung und Sichtbarkeitsprobleme), geographische Informationssysteme (GIS) (geometrische Lage und Suche, Routenplanung), integriertes Schaltkreisdesign (IC Geometrie Design und Verifikation), computergestütztes Engineering (CAE) (mesh generation,) Computer Vision (3D Rekonstruktion). Die theoretischen Ergebnisse des maschinellen Lernens befassen sich hauptsächlich mit einer Art induktiven Lernens, genannt beaufsichtigtes Lernen. Im beaufsichtigten Lernen wird ein Algorithmus Proben gegeben, die auf sinnvolle Weise gekennzeichnet werden. Beispielsweise können die Proben Beschreibungen von Pilzen sein, und die Etiketten könnten sein, ob die Pilze essbar sind oder nicht. Der Algorithmus nimmt diese zuvor markierten Proben und verwendet sie, um einen Klassifikator zu induzieren. Dieser Klassifikator ist eine Funktion, die Etiketten den Proben zuordnet, einschließlich der Proben, die noch nie vom Algorithmus gesehen wurden. Ziel des überwachten Lernalgorithmus ist es, einige Maß an Leistung zu optimieren, wie die Anzahl der Fehler, die an neuen Proben gemacht wurden, zu minimieren. Die Computational Number Theorie, auch als algorithmische Zahlentheorie bekannt, ist die Studie von Algorithmen zur Durchführung von Zahlentheoretischen Berechnungen. Das bekannteste Problem im Feld ist die Integerfaktorisierung. Cryptography Cryptography ist die Praxis und Untersuchung von Techniken für sichere Kommunikation in Gegenwart von Dritten (genannte Gegner). Im Allgemeinen geht es darum, Protokolle zu konstruieren und zu analysieren, die den Einfluss von Gegnern überwinden und mit verschiedenen Aspekten der Informationssicherheit wie Datengeheimnis, Datenintegrität, Authentifizierung und Nicht-Repudiation in Zusammenhang stehen. Moderne Kryptographie schneidet die Disziplinen der Mathematik, Informatik und Elektrotechnik. Anwendungen der Kryptographie umfassen ATM-Karten, Computer-Passwörter und elektronischen Handel. Moderne Kryptographie basiert stark auf mathematischer Theorie und Informatik Praxis; kryptographische Algorithmen sind um rechnerische Härte Annahmen entworfen, so dass solche Algorithmen schwer in der Praxis durch jeden Gegner zu brechen. Es ist theoretisch möglich, ein solches System zu brechen, aber es ist unfehlbar, dies mit allen bekannten praktischen Mitteln zu tun. Diese Systeme werden daher als rechnerisch sicher bezeichnet; theoretische Fortschritte, z.B. Verbesserungen in Ganzzahl-Faktorisierungsalgorithmen und schnellere Rechentechnik erfordern, dass diese Lösungen kontinuierlich angepasst werden. Es gibt informationstheoretisch sichere Systeme, die auch mit unbegrenzter Rechenleistung nachweislich nicht durchbrochen werden können – ein Beispiel ist das einmalige Pad –, aber diese Systeme sind schwieriger zu implementieren als die besten theoretisch bruchfähigen, aber rechnerisch gesicherten Mechanismen. Datenstrukturen Eine Datenstruktur ist eine besondere Möglichkeit, Daten in einem Computer zu organisieren, so dass sie effizient genutzt werden kann. Verschiedene Arten von Datenstrukturen sind für verschiedene Arten von Anwendungen geeignet, und einige sind hoch spezialisiert auf bestimmte Aufgaben. Beispielsweise verwenden Datenbanken B-Tree-Indizes für kleine Prozentangaben von Datenabrufen und Compilern und Datenbanken dynamische Hash-Tabellen als Look-up-Tabellen. Datenstrukturen bieten eine Möglichkeit, große Datenmengen effizient für Anwendungen wie große Datenbanken und Internet-Indexing-Dienste zu verwalten. In der Regel sind effiziente Datenstrukturen der Schlüssel, um effiziente Algorithmen zu entwerfen. Einige formale Gestaltungsmethoden und Programmiersprachen betonen Datenstrukturen, anstatt Algorithmen, als Schlüsselorganisierungsfaktor in Software-Design. Das Speichern und Abrufen kann auf Daten erfolgen, die sowohl im Hauptspeicher als auch im Sekundärspeicher gespeichert sind. Verteilte Berechnung Verteilte Berechnungsstudien verteilte Systeme. Ein verteiltes System ist ein Softwaresystem, in dem auf vernetzten Computern befindliche Komponenten ihre Aktionen durch Nachrichtenübermittlung kommunizieren und koordinieren. Die Komponenten interagieren miteinander, um ein gemeinsames Ziel zu erreichen. Drei wesentliche Merkmale verteilter Systeme sind: Konkurrenz von Komponenten, Mangel an globaler Uhr und unabhängiger Ausfall von Komponenten. Beispiele für verteilte Systeme variieren von SOA-basierten Systemen bis hin zu massiven Multiplayer-Onlinespielen bis hin zu Peer-to-Peer-Anwendungen und Blockchain-Netzwerken wie Bitcoin. Ein Computerprogramm, das in einem verteilten System läuft, wird als verteiltes Programm bezeichnet, und verteilte Programmierung ist der Prozess des Schreibens solcher Programme. Es gibt viele Alternativen für den Nachrichtenübermittlungsmechanismus, einschließlich RPC-ähnliche Steckverbinder und Nachrichtenwarten. Ein wichtiges Ziel und die Herausforderung verteilter Systeme ist die Standorttransparenz. Informationsbasierte Komplexität Informationsbasierte Komplexität (IBC) untersucht optimale Algorithmen und rechnerische Komplexität für kontinuierliche Probleme. IBC hat kontinuierliche Probleme als Pfadintegration, partielle Differentialgleichungen, Systeme von gewöhnlichen Differentialgleichungen, nichtlineare Gleichungen, integrale Gleichungen, Fixpunkte und sehr hochdimensionale Integration untersucht. Formale Methoden Formale Methoden sind eine bestimmte Art von Mathematik-basierten Techniken für die Spezifikation, Entwicklung und Überprüfung von Software- und Hardwaresystemen. Die Verwendung formaler Methoden für Software- und Hardware-Design wird durch die Erwartung motiviert, dass, wie in anderen Ingenieursdisziplinen, eine entsprechende mathematische Analyse zur Zuverlässigkeit und Robustheit eines Designs beitragen kann. Formale Methoden werden am besten als Anwendung einer ziemlich breiten Vielfalt von theoretischen Informatik-Grundlagen, insbesondere Logik-Kalkuli, formale Sprachen, Automata-Theorie und Programm-Semantik beschrieben, aber auch Typ-Systeme und algebraische Datentypen zu Problemen in der Software und Hardware-Spezifikation und Verifikation. Informationstheorie Informationstheorie ist ein Zweig der angewandten Mathematik, Elektrotechnik und Informatik mit der Quantifizierung von Informationen. Die Informationstheorie wurde von Claude E. Shannon entwickelt, um grundlegende Grenzen für Signalverarbeitungsvorgänge zu finden, wie z.B. Komprimierung von Daten und zuverlässige Speicherung und Übermittlung von Daten. Seit seiner Gründung hat es sich erweitert, Anwendungen in vielen anderen Bereichen zu finden, darunter statistische Inferenz, natürliche Sprachverarbeitung, Kryptographie, Neurobiologie, die Evolution und Funktion von molekularen Codes, Modellauswahl in Statistiken, thermische Physik, Quanten-Computing, Linguistik, Plagiarismus-Erkennung, Mustererkennung, Anomalie-Erkennung und andere Formen der Datenanalyse. Anwendungen grundlegender Themen der Informationstheorie umfassen verlustfreie Datenkompression (z.B. ZIP-Dateien), verlustige Datenkompression (z.B. MP3s und JPEGs) und Kanalcodierung (z.B. für Digital Subscriber Line (DSL). Das Feld ist an der Schnittstelle von Mathematik, Statistik, Informatik, Physik, Neurobiologie und Elektrotechnik. Seine Auswirkungen waren entscheidend für den Erfolg der Voyager-Missionen in den tiefen Raum, die Erfindung der kompakten Scheibe, die Machbarkeit von Mobiltelefonen, die Entwicklung des Internets, die Studie der Linguistik und der menschlichen Wahrnehmung, das Verständnis von schwarzen Löchern und zahlreiche andere Felder. Wichtige Teilfelder der Informationstheorie sind Quellcodierung, Kanalcodierung, algorithmische Komplexitätstheorie, algorithmische Informationstheorie, informationstheoretische Sicherheit und Informationsmaße. Machine Learning Machine Learning ist eine wissenschaftliche Disziplin, die sich mit der Konstruktion und Studie von Algorithmen befasst, die aus Daten lernen können. Solche Algorithmen arbeiten durch den Aufbau eines Modells, das auf Eingaben basiert und das verwendet, um Vorhersagen oder Entscheidungen zu treffen, anstatt nur explizit programmierte Anweisungen zu folgen. Machine Learning kann als Unterfeld der Informatik und Statistik betrachtet werden. Es hat starke Verbindungen zu künstlicher Intelligenz und Optimierung, die Methoden, Theorie und Anwendung Domains auf das Feld liefern. Das maschinelle Lernen wird in einer Reihe von Rechenaufgaben eingesetzt, bei denen explizite, regelbasierte Algorithmen entworfen und programmiert werden können. Beispiele sind Spamfilterung, optische Charaktererkennung (OCR), Suchmaschinen und Computer Vision. Das maschinelle Lernen ist manchmal mit dem Datenbergbau beschränkt, obwohl das mehr auf die explorative Datenanalyse konzentriert. maschinelles Lernen und Mustererkennung "kann als zwei Facetten des gleichen Feldes angesehen werden". Parallele Berechnung Parallel Computing ist eine Berechnungsform, in der viele Berechnungen gleichzeitig durchgeführt werden, wobei grundsätzlich große Probleme oft in kleinere aufgeteilt werden können, die dann "parallel" gelöst werden. Es gibt mehrere verschiedene Formen des Parallel Computing: Bit-Level, Befehlsebene, Daten und Aufgabenparallelismus. Parallelismus ist seit vielen Jahren, vor allem im Hochleistungs-Computing, eingesetzt worden, aber das Interesse daran ist in letzter Zeit aufgrund der physischen Zwänge gewachsen, die Frequenzskalierung verhindern. Da der Stromverbrauch (und damit die Wärmeerzeugung) von Computern in den letzten Jahren zu einem Anliegen geworden ist, ist Parallel Computing zum dominanten Paradigma in der Computerarchitektur geworden, vor allem in Form von Multi-Core-Prozessoren. Parallele Computerprogramme sind schwieriger zu schreiben als sequentielle, weil die Konkurrenz mehrere neue Klassen potenzieller Software-Bugs einführt, von denen die Race-Bedingungen am häufigsten sind. Kommunikation und Synchronisation zwischen den verschiedenen Subtasks sind in der Regel einige der größten Hindernisse, um gute parallele Programmleistung zu erhalten. Die maximal mögliche Beschleunigung eines einzelnen Programms durch Parallelisierung ist als Amdahls Gesetz bekannt. Programm semantik In der Programmiersprache Theorie, Semantik ist das Feld mit der strengen mathematischen Studie der Bedeutung der Programmiersprachen beschäftigt. Dies geschieht durch die Bewertung der Bedeutung syntaktischer rechtlicher Strings, die durch eine bestimmte Programmiersprache definiert sind und die Berechnung zeigt. In einem solchen Fall, dass die Bewertung von syntaktisch illegalen Strings sein würde, wäre das Ergebnis nicht komputiert. Semantik beschreibt die Prozesse, die ein Computer bei der Durchführung eines Programms in dieser spezifischen Sprache folgt. Dies kann gezeigt werden, indem die Beziehung zwischen Eingabe und Ausgabe eines Programms beschrieben wird, oder eine Erklärung, wie das Programm auf einer bestimmten Plattform ausgeführt wird, wodurch ein Modell der Berechnung erstellt wird. Quantenrechner Ein Quantenrechner ist ein Rechensystem, das den direkten Einsatz von quantenmechanischen Phänomenen, wie Überlagerung und Verschränkung, zur Durchführung von Datenoperationen macht. Quantenrechner unterscheiden sich von digitalen Computern auf Basis von Transistoren. Während digitale Rechner verlangen, dass Daten in binäre Ziffern (Bits) kodiert werden, von denen jeder immer in einem von zwei bestimmten Zuständen (0 oder 1) ist, verwendet Quantenberechnung Qubits (Quantenbits), die in Überlagerungen von Zuständen sein können. Ein theoretisches Modell ist die Quanten-Turniermaschine, auch als universeller Quantenrechner bekannt. Quantum-Computer teilen theoretische Ähnlichkeiten mit nicht-deterministischen und probabilistischen Computern; ein Beispiel ist die Fähigkeit, in mehr als einem Zustand gleichzeitig zu sein. Der Bereich Quantenrechner wurde 1980 von Yuri Manin und 1982 von Richard Feynman eingeführt. Ein Quantencomputer mit Spins als Quantenbits wurde auch für die Verwendung als Quantenraum-Zeit 1968 formuliert. Seit 2014 befindet sich das Quanten-Computing noch in seiner Kindheit, aber Experimente wurden durchgeführt, in denen Quanten-Computational-Operationen auf einer sehr geringen Anzahl von Qubits durchgeführt wurden. Sowohl die praktische als auch die theoretische Forschung geht weiter, und viele nationale Regierungen und militärische Förderorganisationen unterstützen die Quanten-Computing-Forschung, um Quantencomputer für zivile und nationale Sicherheitszwecke wie die Kryptanalyse zu entwickeln. Symbolische Berechnung Computer Algebra, auch als symbolische Berechnung oder algebraische Berechnung bezeichnet, ist ein wissenschaftlicher Bereich, der sich auf die Studie und Entwicklung von Algorithmen und Software zur Manipulation mathematischer Ausdrücke und anderer mathematischer Objekte bezieht. Obwohl, richtig gesprochen, Computer-Algebra sollte ein Unterfeld der wissenschaftlichen Berechnung sein, werden sie in der Regel als verschiedene Felder betrachtet, weil wissenschaftliche Berechnung in der Regel auf der numerischen Berechnung mit ungefähren Floating-Point-Nummern basiert, während symbolische Berechnung betont genaue Berechnung mit Expressionen, die Variablen, die keinen bestimmten Wert haben und somit als Symbole manipuliert werden (daher der Name der symbolischen Berechnung). Software-Anwendungen, die symbolische Berechnungen durchführen, werden Computer-Algebra-Systeme genannt, mit dem Begriff System, das auf die Komplexität der Haupt-Anwendungen, die mindestens ein Verfahren zur Darstellung mathematischer Daten in einem Computer, eine Benutzer-Programmiersprache (in der Regel anders als die für die Implementierung verwendete Sprache), einen dedizierten Speichermanager, eine Benutzer-Schnittstelle für die Eingabe/Ausgabe mathematischer Ausdrücke, eine große Reihe von Routinen zur Durchführung üblicher Operationen, wie Vereinfachung von Ausdrücken,Sehr große Integration Die sehr große Integration (VLSI) ist der Prozess, eine integrierte Schaltung (IC) zu schaffen, indem Tausende von Transistoren in einen einzigen Chip zusammengefasst werden. VLSI begann in den 1970er Jahren, als komplexe Halbleiter- und Kommunikationstechnologien entwickelt wurden. Der Mikroprozessor ist ein VLSI-Gerät. Vor der Einführung der VLSI-Technologie hatten die meisten ICs eine begrenzte Anzahl von Funktionen, die sie durchführen konnten. Eine elektronische Schaltung könnte aus einer CPU, ROM, RAM und anderen Klebelogik bestehen. Mit VLSI können IC-Hersteller alle diese Schaltungen in einen Chip einfügen. Organisationen European Association for Theoretical Computer Science SIGACT Simons Institute for the Theory of Computing Journals and newsletters “Discrete Mathematics and Theoretical Computer Science” Information and Computation Theory of Computing (offenFormal Aspects of Computing Journal of the ACM SIAM Journal on Computing (SICOMP)SIGACT Theoretische Informatik-Theorie der Computing Systems International Journal of Foundations of Computer Science Chicago Journal of Theoretical Computer Science (Open Access Journal) Foundations and Trends in Theoretical Computer Science Journal of Automata, Languages and Combinatorics Acta Informatica Fundamenta Informatica ACM Transactions on Computation Theory Computational Complexity Journal of Complexity ACM Weitere Informationen: Martin Davis, Ron Sigal, Elaine J. Weyuker, Rechenschaftspflicht, Komplexität und Sprachen: Grundlagen der theoretischen Informatik, 2. ed. Academic Press, 1994, ISBN 0-12-206382-1.Covers Theorie der Berechnung, aber auch Programm semantik und Quantifizierung Theorie. Ziel bei Absolventen Studenten. Externe Links SIGACT-Verzeichnis zusätzlicher Theorie-Links Theorie Materie Wiki Theoretische Informatik (TCS)Advocacy Wiki Liste der akademischen Konferenzen im Bereich der theoretischen Informatik bei confsearch Theoretical Computer Science - StackExchange, eine Frage- und Antwortstelle für Forscher in der theoretischen Informatik Informatik Animated http://theory.csail.mit.edu @Massachusetts Institut für Technologie