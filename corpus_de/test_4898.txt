Algorithmische Vorurteile beschreiben systematische und wiederholbare Fehler in einem Computersystem, das unlautere Ergebnisse hervorruft, wie z.B. eine beliebige Gruppe von Benutzern über andere. Bias kann durch viele Faktoren entstehen, einschließlich, aber nicht beschränkt auf die Gestaltung des Algorithmus oder die unbeabsichtigte oder unvorhergesehene Nutzung oder Entscheidungen über die Art, wie Daten kodiert, gesammelt, ausgewählt oder verwendet werden, um den Algorithmus zu trainieren. Algorithmische Vorurteile finden sich auf verschiedenen Plattformen, einschließlich, aber nicht beschränkt auf Suchmaschinen-Ergebnisse und Social-Media-Plattformen, und können Auswirkungen haben, von unbeabsichtigten Datenschutzverletzungen bis zur Stärkung sozialer Vorurteile von Rasse, Geschlecht, Sexualität und Ethnizität. Die Studie über algorithmische Vorurteile beschäftigt sich am meisten mit Algorithmen, die "systematische und unfaire" Diskriminierung widerspiegeln. Diese Voreingenommenheit wurde erst kürzlich in rechtlichen Rahmenbedingungen, wie der Datenschutzgrundverordnung der Europäischen Union 2018, behandelt. Da Algorithmen ihre Fähigkeit erweitern, Gesellschaft, Politik, Institutionen und Verhalten zu organisieren, haben sich Soziologen mit der Art beschäftigt, wie unvorhergesehene Ausgabe und Manipulation von Daten die physische Welt beeinflussen können. Da Algorithmen oft als neutral und unvoreingenommen betrachtet werden, können sie ungenau größere Autorität als menschliches Know-how projizieren, und in einigen Fällen kann die Abhängigkeit von Algorithmen die menschliche Verantwortung für ihre Ergebnisse verschieben. Bias kann aufgrund vorbestehender kultureller, sozialer oder institutioneller Erwartungen in algorithmische Systeme eintreten; wegen technischer Einschränkungen ihres Designs oder durch die Verwendung in unvorhergesehenen Kontexten oder durch Publikum, die nicht im ursprünglichen Design der Software betrachtet werden. Algorithmische Vorurteile wurden in Fällen zitiert, die von den Wahlergebnissen bis zur Verbreitung der Online-Haßrede reichen. Es ist auch in Strafjustiz, Gesundheit und Einstellung entstanden, die Verbindung bestehender Rassen-, Wirtschafts- und Geschlechtsvorstellungen. Die relative Unfähigkeit der Gesichtserkennungstechnologie, dunklere Gesichter genau zu identifizieren, ist mit mehreren falschen Verhaftungen von schwarzen Männern verbunden, ein Problem, das aus unausgeglichenen Datensätzen stammt. Probleme des Verständnisses, der Forschung und der Entdeckung algorithmischer Vorurteile bestehen weiterhin aufgrund der Eigenart von Algorithmen, die typischerweise als Handelsgeheimnisse behandelt werden. Auch wenn die volle Transparenz gegeben ist, stellt die Komplexität bestimmter Algorithmen eine Barriere für das Verständnis ihrer Funktionsweise dar. Darüber hinaus können Algorithmen auf Eingabe oder Ausgabe in einer für die Analyse nicht zu erwartenden oder leicht reproduzierbaren Weise ändern oder reagieren. In vielen Fällen, auch innerhalb einer einzigen Website oder Anwendung, gibt es keinen einzigen Algorithmus zu untersuchen, sondern ein Netzwerk von vielen zusammenhängenden Programmen und Dateneingängen, auch zwischen Benutzern desselben Dienstes. Begriffsbestimmungen Algorithmen sind schwer zu definieren, können aber allgemein als Verzeichnisse von Anweisungen verstanden werden, die bestimmen, wie Programme lesen, sammeln, bearbeiten und analysieren Daten, um Ausgabe zu erzeugen. Für eine strenge technische Einführung siehe Algorithmen. Fortschritte in der Computer-Hardware haben zu einer erhöhten Fähigkeit, Daten zu verarbeiten, zu speichern und zu übertragen. Dies hat wiederum die Gestaltung und Einführung von Technologien wie maschinelles Lernen und künstliche Intelligenz verstärkt. Durch die Analyse und Verarbeitung von Daten sind Algorithmen das Rückgrat von Suchmaschinen, Social Media-Websites, Empfehlungs-Engines, Online-Handel, Online-Werbung und mehr. Zeitgenössische Sozialwissenschaftler beschäftigen sich mit algorithmischen Prozessen, die aufgrund ihrer politischen und sozialen Auswirkungen in Hardware- und Softwareanwendungen eingebettet sind, und fragen die zugrunde liegenden Annahmen der Neutralität eines Algorithmus. Der Begriff algorithmische Bias beschreibt systematische und wiederholbare Fehler, die unlautere Ergebnisse erzeugen, wie z.B. eine willkürliche Gruppe von Benutzern über andere zu privieren. Beispielsweise kann ein Kredit-Score-Algorithmus ein Darlehen leugnen, ohne ungerecht zu sein, wenn es konsequent relevante Finanzkriterien wägt. Wenn der Algorithmus Kredite an eine Gruppe von Benutzern empfiehlt, aber Kredite an einen anderen Satz von fast identischen Benutzern auf Basis von unbezogenen Kriterien ablehnt, und wenn dieses Verhalten über mehrere Ereignisse wiederholt werden kann, kann ein Algorithmus als vorgespannt bezeichnet werden. Diese Vorspannung kann absichtlich oder unbeabsichtigt sein (z.B. kann es aus voreingenommenen Daten eines Arbeiters kommen, der zuvor den Job gemacht hat, den der Algorithmus von nun an tun wird). Methoden Bias kann auf verschiedene Weise einem Algorithmus eingeführt werden. Beim Zusammenbau eines Datensatzes können Daten erhoben, digitalisiert, angepasst und in eine Datenbank nach humanen Gestaltungskriterien eingegeben werden.Als nächstes vergeben Programmierer Prioritäten oder Hierarchien, wie ein Programm diese Daten bewertet und sortiert. Dies erfordert menschliche Entscheidungen darüber, wie Daten kategorisiert werden und welche Daten enthalten oder verworfen werden. Einige Algorithmen sammeln ihre eigenen Daten basierend auf human ausgewählten Kriterien, die auch die Vorspannung der menschlichen Designer widerspiegeln können. Andere Algorithmen können Stereotypen und Präferenzen verstärken, da sie relevante Daten für menschliche Benutzer verarbeiten und anzeigen, beispielsweise indem sie Informationen basierend auf früheren Entscheidungen eines ähnlichen Benutzers oder einer Gruppe von Nutzern auswählen. Über die Montage und Verarbeitung von Daten hinaus kann sich aufgrund des Designs Vorspannung ergeben. Beispielsweise können Algorithmen, die die Zuweisung von Ressourcen oder Überprüfungen bestimmen (z.B. die Bestimmung von Schulplatzierungen) bei der Ermittlung von Risiko basierend auf ähnlichen Benutzern (wie bei Kreditpunkten) versehentlich gegen eine Kategorie diskriminieren. In der Zwischenzeit könnten sich Empfehlungsmotoren, die durch die Verknüpfung von Nutzern mit ähnlichen Nutzern oder die Verwendung von abgeleiteten Marketingeigenschaften arbeiten, auf ungenaue Assoziationen verlassen, die breite ethnische, geschlechtsspezifische, sozioökonomische oder rassische Stereotypen widerspiegeln. Ein weiteres Beispiel ist die Festlegung von Kriterien für das, was enthalten und von den Ergebnissen ausgeschlossen ist. Diese Kriterien könnten unvorhergesehene Ergebnisse für Suchergebnisse darstellen, wie z.B. mit Flug-Recommendation-Software, die Flüge versäumt, die nicht den Flugwegen der Sponsoring Airline folgen. Algorithmen können auch eine Ungewissheitsvoreinschätzung anzeigen und bieten zuversichtlichere Bewertungen, wenn größere Datensätze verfügbar sind. Dies kann algorithmische Prozesse zu Ergebnissen skewieren, die enger mit größeren Proben entsprechen, die Daten von unterrepräsentierten Populationen missachten können. Geschichte Frühe Kritiken Die frühesten Computerprogramme wurden entwickelt, um menschliche Vernunft und Abzüge nachzuahmen und wurden als funktionierend angesehen, wenn sie diese menschliche Logik erfolgreich und konsequent reproduzierten. In seinem 1976 Buch Computer Power and Human Reason, Artificial Intelligence Pionier Joseph Weizenbaum schlug vor, dass Bias sowohl aus den Daten, die in einem Programm verwendet werden, als auch aus der Art, wie ein Programm kodiert. Weizenbaum schrieb, dass Programme eine Reihe von Regeln sind, die von Menschen für einen Computer zu folgen erstellt. Durch konsequente Verfolgung dieser Regeln, solche Programme "embody law", d.h., durchsetzen einen spezifischen Weg, um Probleme zu lösen. Die Regeln, die ein Computer folgt, basieren auf den Annahmen eines Computerprogrammierers, wie diese Probleme gelöst werden können. Das heißt, der Code könnte die Phantasie des Programmierers einbeziehen, wie die Welt funktioniert, einschließlich seiner Vorurteile und Erwartungen. Während ein Computerprogramm auf diese Weise Bias einfügen kann, merkte Weizenbaum auch an, dass alle Daten, die einer Maschine zugeführt werden, zusätzlich "menschliche Entscheidungsprozesse" widerspiegeln, da Daten ausgewählt werden. Schließlich stellte er fest, dass Maschinen auch gute Informationen mit unbeabsichtigten Konsequenzen übertragen könnten, wenn Benutzer unklar sind, wie die Ergebnisse zu interpretieren. Weizenbaum warnte vor vertrauensvollen Entscheidungen, die von Computerprogrammen getroffen wurden, die ein Nutzer nicht versteht, und vergleichte diesen Glauben mit einem Touristen, der seinen Weg zu einem Hotelzimmer ausschließlich durch Links- oder Rechtsdrehen auf einem Münzton finden kann. Crucially, der Tourist hat keine Grundlage zu verstehen, wie oder warum er an seinem Ziel angekommen ist, und eine erfolgreiche Ankunft bedeutet nicht, dass der Prozess genau oder zuverlässig ist. Ein frühes Beispiel für algorithmische Vorurteile führte dazu, dass von 1982 bis 1986 insgesamt 60 Frauen und ethnische Minderheiten den Eintritt in die St. George's Hospital Medical School verweigerten, basierend auf der Einführung eines neuen Computer-Leitungen-Bewertungssystems, das den Eintritt von Frauen und Männern mit "fremden Namen" auf der Grundlage historischer Tendenzen in den Zulassungen verweigerte. Während viele Schulen damals ähnliche Vorurteile in ihrem Auswahlprozess beschäftigten, war St. George am bemerkenswertesten, um diese Vorurteile durch den Einsatz eines Algorithmus zu automatisieren und so die Aufmerksamkeit der Menschen auf viel breiterem Maßstab zu gewinnen. In den letzten Jahren, wenn mehr Algorithmen begannen, maschinelle Lernmethoden auf realen Weltdaten zu verwenden, kann algorithmische Vorspannung häufig aufgrund der Vorspannung in den Daten gefunden werden. Zeitgenössische Kritiken und Antworten Obwohl gut gestaltete Algorithmen häufig Ergebnisse bestimmen, die gleich (oder mehr) äquitable sind als die Entscheidungen des Menschen, Fälle von Vorurteilen treten immer noch auf und sind schwer vorherzusagen und zu analysieren. Die Komplexität der Analyse algorithmischer Vorurteile ist neben der Komplexität von Programmen und ihrem Design gewachsen. Entscheidungen eines Designers oder eines Teams von Designern können unter den vielen Codes, die für ein einzelnes Programm erstellt wurden, verdeckt werden; im Laufe der Zeit können diese Entscheidungen und ihre kollektiven Auswirkungen auf die Leistung des Programms vergessen werden.In der Theorie können diese Bias neue Verhaltensmuster oder Skripte in Bezug auf spezifische Technologien schaffen, da der Code mit anderen Elementen der Gesellschaft interagiert. Biasen kann auch beeinflussen, wie sich die Gesellschaft um die Datenpunkte, die Algorithmen erfordern, formt. Wenn beispielsweise Daten eine hohe Anzahl von Verhaftungen in einem bestimmten Bereich zeigen, kann ein Algorithmus mehr Polizeipatrouillen zu diesem Bereich zuordnen, was zu mehr Verhaftungen führen könnte. Die Entscheidungen der algorithmischen Programme können als maßgeblicher angesehen werden als die Entscheidungen der Menschen, die sie unterstützen sollen, ein Prozess, der von der Autorin Clay Shirky als "algorithmische Autorität" beschrieben wird. Shirky verwendet den Begriff, um "die Entscheidung, als autoritativen ein unmanaged Prozess der Gewinnung von Wert aus verschiedenen, nicht vertrauenswürdigen Quellen zu betrachten", wie Suchergebnisse zu beschreiben. Diese Neutralität kann auch von der Sprache, die von Experten und den Medien verwendet wird, falsch dargestellt werden, wenn die Ergebnisse der Öffentlichkeit vorgestellt werden. So kann beispielsweise eine Liste von News-Elementen, die als Trending oder Popular präsentiert werden, basierend auf deutlich breiteren Kriterien erstellt werden als nur ihre Popularität. Wegen ihrer Bequemlichkeit und Autorität werden Algorithmen als Mittel zur Übertragung von Verantwortung von Menschen theorisiert. Dies kann dazu führen, alternative Optionen, Kompromisse oder Flexibilität zu reduzieren. Der Soziologe Scott Lash hat Algorithmen als eine neue Form der "generativen Macht", indem sie ein virtuelles Mittel zur Generierung von tatsächlichen Enden sind. Wo zuvor menschliches Verhalten Daten erzeugte, die gesammelt und untersucht werden, könnten leistungsfähige Algorithmen zunehmend menschliche Verhaltensweisen formen und definieren. Die Auswirkungen von Algorithmen auf die Gesellschaft haben zur Schaffung von Arbeitsgruppen in Organisationen wie Google und Microsoft geführt, die eine Arbeitsgruppe namens Fairness, Rechenschaftspflicht und Transparenz im maschinellen Lernen mitgestaltet haben. Ideen von Google haben Community-Gruppen enthalten, die die Ergebnisse von Algorithmen patrouillieren und wählen, um die Ausgänge zu kontrollieren oder zu begrenzen, die sie für negative Folgen halten. In den letzten Jahren ist die Studie der Fairness, Accountability und Transparency (FAT) von Algorithmen als eigenen interdisziplinären Forschungsraum mit einer jährlichen Konferenz namens FAT* entstanden. Kritiker haben vorgeschlagen, dass FAT-Initiativen nicht effektiv als unabhängige Watchdogs dienen können, wenn viele von Unternehmen finanziert werden, die die untersuchten Systeme aufbauen. Vorbestehende Typen Vorbeugung in einem Algorithmus ist eine Folge der zugrunde liegenden sozialen und institutionellen Ideologien. Solche Ideen können persönliche Vorurteile innerhalb einzelner Designer oder Programmierer beeinflussen oder schaffen. Solche Vorurteile können explizit und bewusst oder implizit und unbewusst sein. Schlecht ausgewählte Eingabedaten oder einfach Daten aus einer vorgespannten Quelle werden die Ergebnisse beeinflussen, die von Maschinen erstellt werden. Die Kodierung vorbestehender Vorurteile in Software kann soziale und institutionelle Vorurteile bewahren und ohne Korrektur in allen zukünftigen Anwendungen dieses Algorithmus repliziert werden. Ein Beispiel für diese Art von Vorurteilen ist das British Nationality Act Programm, das darauf abzielt, die Bewertung neuer britischer Bürger nach dem britischen Nationality Act von 1981 zu automatisieren. Das Programm spiegelte genau die Grundsätze des Gesetzes wider, das besagte, dass "ein Mann nur der Vater seiner legitimen Kinder ist, während eine Frau die Mutter aller ihrer Kinder ist, legitim oder nicht. " In seinem Versuch, eine bestimmte Logik in einen algorithmischen Prozess zu übertragen, beschrieb der BNAP die Logik des britischen Nationalitätsgesetzes in seinen Algorithmus, der ihn selbst dann fortsetzte, wenn der Akt schließlich aufgehoben wurde. Technische technische Vorspannung entsteht durch Einschränkungen eines Programms, Rechenleistung, sein Design oder andere Einschränkungen auf dem System. Eine solche Vorspannung kann auch eine Rückhaltung des Designs sein, beispielsweise eine Suchmaschine, die drei Ergebnisse pro Bildschirm zeigt, kann verstanden werden, die oberen drei Ergebnisse etwas mehr als die nächsten drei zu bevorzugen, wie bei einer Airline-Preisanzeige. Ein weiterer Fall ist die Software, die auf Zufall für faire Verteilungen von Ergebnissen basiert. Wenn der Zufallszahlenerzeugungsmechanismus nicht wirklich zufällig ist, kann er z.B. Bias einführen, indem er am Ende oder am Anfang einer Liste Auswahlen zu Objekten hin skewingt. Ein dekontextualisierter Algorithmus verwendet unverwandte Informationen, um Ergebnisse zu sortieren, z.B. einen flugführenden Algorithmus, der Ergebnisse nach alphabetischer Reihenfolge sortiert, würde zugunsten von American Airlines über United Airlines vorgespannt werden. Das Gegenteil kann auch gelten, wobei die Ergebnisse in Kontexten ausgewertet werden, von denen sie gesammelt werden. Daten können ohne entscheidenden externen Kontext erhoben werden: zum Beispiel, wenn die Gesichtserkennungssoftware von Überwachungskameras verwendet wird, aber von Remote-Mitarbeitern in einem anderen Land oder in einer anderen Region ausgewertet wird, oder von nicht-menschlichen Algorithmen ausgewertet werden, kein Bewusstsein dafür, was über das Sichtfeld der Kamera hinaus geschieht.Dies könnte ein unvollständiges Verständnis für einen Tatort schaffen, zum Beispiel potenziell missbräuchliche Zuschauer für diejenigen, die das Verbrechen begehen. Schließlich kann technische Vorurteile geschaffen werden, indem versucht wird, Entscheidungen in konkrete Schritte zu formieren, unter der Annahme, dass das menschliche Verhalten in gleicher Weise funktioniert. So wiegt Software beispielsweise Datenpunkte, um festzustellen, ob ein Angeklagter ein Plädoyer zu akzeptieren, während er die Auswirkungen der Emotion auf eine Jury ignoriert. Ein weiteres unbeabsichtigtes Ergebnis dieser Art von Voreingenommenheit wurde in der Plagiat-Detektions-Software Turnitin gefunden, die studentisch geschriebene Texte mit Informationen vergleicht, die online gefunden werden und eine Wahrscheinlichkeitsanzeige zurückgibt, dass die Arbeit des Schülers kopiert wird. Da die Software lange Textstränge vergleicht, ist es wahrscheinlicher, nicht-native Lautsprecher von Englisch als Muttersprachler zu identifizieren, da letztere Gruppe besser in der Lage sein kann, einzelne Wörter zu ändern, Strings von plagiarisierten Text zu zerbrechen, oder obskur kopierte Passagen durch Synonyme. Weil es für Muttersprachlern einfacher ist, die Erkennung aufgrund der technischen Zwänge der Software zu vermeiden, schafft dies ein Szenario, in dem Turnitin Fremdsprecher von Englisch für Plagiat identifiziert, während es mehr Muttersprachler ermöglicht, die Erkennung zu umgehen. Emergent Emergent-Bias ist das Ergebnis der Verwendung und der Abhängigkeit von Algorithmen in neuen oder unvorhergesehenen Kontexten. Algorithmen sind möglicherweise nicht angepasst worden, um neue Formen von Wissen zu betrachten, wie neue Drogen oder medizinische Durchbrüche, neue Gesetze, Geschäftsmodelle oder Verschiebung kultureller Normen. Dies kann Gruppen durch Technologie ausschließen, ohne klare Umrisse zu verstehen, wer für ihre Ausgrenzung verantwortlich ist. Ebenso können Probleme auftreten, wenn die Trainingsdaten (die Proben, die einer Maschine zugeführt werden, mit denen sie bestimmte Schlussfolgerungen modelliert) nicht mit Kontexten, die ein Algorithmus in der realen Welt trifft, übereinstimmen. 1990 wurde in der Software ein Beispiel für Notvoreingenommenheit identifiziert, mit der die US-amerikanischen Medizinstudenten in Residenzen, das National Residency Match Program (NRMP) gebracht werden. Der Algorithmus wurde zu einer Zeit entworfen, als wenige verheiratete Paare zusammen Wohnungen suchen würden. Da mehr Frauen in medizinische Schulen eintraten, waren mehr Studenten wahrscheinlich neben ihren Partnern eine Residenz anfordern. Das Verfahren forderte jeden Antragsteller auf, eine Liste der Präferenzen für die Platzierung in den USA bereitzustellen, die dann sortiert und zugewiesen wurde, wenn ein Krankenhaus und ein Antragsteller beide vereinbarten eine Übereinstimmung. Bei verheirateten Paaren, in denen beide Residenzen suchten, wiegte der Algorithmus zuerst die Standortwahlen des höher bewerteten Partners. Das Ergebnis war eine häufige Zuweisung von hoch bevorzugten Schulen an den ersten Partner und den unteren Schulen an den zweiten Partner, anstatt nach Kompromissen in der Vermittlungspräferenz zu sortieren. Zu den zusätzlichen Vorspannungen gehören: Korrelationen Unvorhersehbare Korrelationen können entstehen, wenn große Datensätze miteinander verglichen werden. Beispielsweise können die über Web-Browsing-Muster erhobenen Daten mit Signalen ausrichten, die sensible Daten (wie Rasse oder sexuelle Orientierung) markieren. Durch die Auswahl nach bestimmten Verhaltens- oder Surfmustern wäre der Endeffekt nahezu identisch mit Diskriminierung durch die Verwendung von direkten Rassen- oder sexuellen Orientierungsdaten. Dies könnte zu Erfahrungen der algorithmischen Dissonanz führen. In anderen Fällen zieht der Algorithmus Rückschlüsse auf Korrelationen, ohne diese Zusammenhänge verstehen zu können. Zum Beispiel, ein Triage-Programm gab niedrigere Priorität für Asthmatiker, die Lungenentzündung hatte als Asthmatiker, die keine Lungenentzündung hatten. Der Programmalgorithmus hat dies getan, weil er einfach die Überlebensraten verglichen: Asthmatik mit Lungenentzündung sind auf höchstem Risiko. Historisch, aus diesem gleichen Grund, geben Krankenhäuser in der Regel solche Asthmatiken die beste und unmittelbare Pflege. Unvorhergesehene Verwendungen Emergent-Bias können auftreten, wenn ein Algorithmus von unvorhergesehenen Publikum verwendet wird. Beispielsweise können Maschinen verlangen, dass Benutzer Zahlen lesen, schreiben oder verstehen können, oder sich auf eine Schnittstelle mit Metaphern beziehen, die sie nicht verstehen. Diese Ausschlüsse können miteinander verbunden werden, da voreingenommene oder ausschlussreiche Technologien tiefer in die Gesellschaft integriert sind. Abgesehen von Ausschluss, können unvorhergesehene Verwendungen aus dem Endbenutzer entstehen, die sich auf die Software und nicht auf ihr eigenes Wissen verlassen. In einem Beispiel führte eine unvorhergesehene Nutzergruppe zu algorithmischen Vorurteilen in Großbritannien, als das British National Act Program als Nachweis von Computerwissenschaftlern und Immigrationsanwälten erstellt wurde, um die Eignung für die britische Staatsbürgerschaft zu bewerten. Die Designer hatten Zugang zu Rechtsexpertise über die Endnutzer in Einwanderungsbüros, deren Verständnis von Software- und Einwanderungsrechten wahrscheinlich unophistisch gewesen wäre.Die Verwalten der Fragen, die vollständig auf die Software zurückgelegt wurden, die alternative Wege zur Staatsbürgerschaft ausschloss und die Software auch nach neuen Rechtsprechungen und rechtlichen Interpretationen verwendet, führte den Algorithmus zu veraltet. Infolge der Gestaltung eines Algorithmus für Nutzer, die als rechtlich savvy auf Einwanderungsrecht angenommen werden, führte der Algorithmus der Software indirekt zu Vorurteilen zugunsten von Bewerbern, die eine sehr enge Reihe von rechtlichen Kriterien des Algorithmus passen, anstatt durch die breiteren Kriterien des britischen Einwanderungsrechts. Feedbackschleifen Emergent-Bias kann auch eine Rückkopplungsschleife oder Rekursion erstellen, wenn Daten, die für einen Algorithmus erhoben werden, zu real-world Antworten führen, die in den Algorithmus zurückgespeist werden. So schlugen beispielsweise Simulationen der prädiktiven Polizeisoftware (PredPol), die in Oakland (Kalifornien) eingesetzt wurde, eine erhöhte Polizeipräsenz in schwarzen Vierteln vor, die auf von der Öffentlichkeit gemeldeten Kriminalitätsdaten basiert. Die Simulation zeigte, dass das öffentliche Verbrechen auf der Grundlage von Polizeiautos gemeldet wurde, unabhängig davon, was die Polizei tat. Die Simulation interpretierte polizeiliche Sichtungen bei der Modellierung ihrer Verbrechensvorhersagen und würde wiederum eine noch größere Zunahme der polizeilichen Präsenz in diesen Gegenden zuordnen. Die Gruppe Human Rights Data Analysis, die die Simulation durchführte, warnte, dass an Orten, an denen die Rassendiskriminierung ein Faktor bei Verhaftungen ist, solche Rückkopplungsschleifen die Rassendiskriminierung bei der Polizei verstärken und fortführen könnten. Ein weiteres bekanntes Beispiel eines solchen Algorithmus, der ein solches Verhalten zeigt, ist COMPAS, eine Software, die die Wahrscheinlichkeit eines Verbrechers bestimmt. Die Software wird oft kritisiert, dass Black Individuen als Kriminelle viel wahrscheinlicher als andere gekennzeichnet werden, und dann die Daten wieder in sich selbst, wenn Indivuals registrierte Kriminelle werden, um die Voreingenommenheit, die durch den Datensatz erzeugt, auf den der Algorithmus wirkt. Weiterempfehler-Systeme, wie sie verwendet werden, um Online-Videos oder News-Artikel zu empfehlen, können Feedbackschleifen erstellen. Wenn Benutzer auf Inhalte klicken, die von Algorithmen vorgeschlagen werden, beeinflusst es den nächsten Satz von Vorschlägen. Dies kann im Laufe der Zeit dazu führen, dass Benutzer einen Filter Bubble eingeben und sich nicht auf wichtige oder nützliche Inhalte verlassen. Auswirkungen kommerzieller Einflüsse Unternehmensalgorithmen könnten versäumt werden, finanzielle Vereinbarungen oder Vereinbarungen zwischen Unternehmen, ohne die Kenntnis eines Benutzers, der den Algorithmus als unparteiisch verwechseln kann. Zum Beispiel hat American Airlines in den 1980er Jahren einen Flugsuchealgorithmus erstellt. Die Software präsentierte eine Reihe von Flügen von verschiedenen Fluggesellschaften zu Kunden, aber gewogene Faktoren, die ihre eigenen Flüge, unabhängig von Preis oder Bequemlichkeit gesteigert. Der Präsident der Fluggesellschaft erklärte in Aussage des Kongresses der Vereinigten Staaten, dass das System mit der Absicht geschaffen wurde, Wettbewerbsvorteile durch Vorzugsbehandlung zu gewinnen. In einem 1998 erschienenen Papier, in dem Google beschrieben wurde, hatten die Gründer des Unternehmens eine Transparenzpolitik in den Suchergebnissen bezüglich der bezahlten Platzierung verabschiedet, in der argumentiert wurde, dass "advertising-finanzierte Suchmaschinen inhärent gegenüber den Werbetreibenden und von den Bedürfnissen der Verbraucher abgelenkt werden". Diese Vorspannung wäre eine unsichtbare Manipulation des Benutzers. Abstimmungsverhalten Eine Reihe von Studien über unentschlossene Wähler in den USA und in Indien ergab, dass Suchmaschinenergebnisse in der Lage waren, die Abstimmungsergebnisse um etwa 20% zu verschieben. Die Forscher kamen zu dem Schluss, dass Kandidaten "kein Mittel zum Wettbewerb" haben, wenn ein Algorithmus, mit oder ohne Absicht, verbesserte Seitenauflistungen für einen Rivalen-Kandidat. Facebook-Nutzer, die Nachrichten im Zusammenhang mit der Abstimmung sahen, waren wahrscheinlicher zu wählen. Eine 2010 randomisierte Studie von Facebook-Nutzern zeigte eine Erhöhung um 20% (340.000 Stimmen) unter Benutzern, die Nachrichten sah, die das Voting ermutigen, sowie Bilder ihrer Freunde, die gestimmt hatten. Juristische Gelehrte Jonathan Zittrain hat gewarnt, dass dies einen "digitalen Gerrymandering" Effekt bei Wahlen schaffen könnte, "die selektive Präsentation von Informationen durch einen Vermittler, um seine Agenda zu erfüllen, anstatt seinen Nutzern zu dienen", wenn absichtlich manipuliert. Geschlechtsdiskriminierung Im Jahr 2016 wurde die professionelle Netzwerk-Website LinkedIn entdeckt, um männliche Variationen von Frauen Namen in Reaktion auf Suchanfragen zu empfehlen. Die Website gab keine ähnlichen Empfehlungen in der Suche nach männlichen Namen. Zum Beispiel würde Andrea eine Aufforderung einholen, um zu fragen, ob Benutzer Andrew gemeint, aber Fragen nach Andrew nicht fragen, ob Benutzer Andrea finden sollen". Das Unternehmen sagte, dies sei das Ergebnis einer Analyse der Interaktionen der Nutzer mit der Website.Im Jahr 2012 wurde das Kaufhaus Franchise Target zitiert, um Datenpunkte zu sammeln, um zu mindern, wenn Frauen Kunden schwanger waren, auch wenn sie es nicht angekündigt hatten, und dann teilen Sie diese Informationen mit Marketing-Partnern. Da die Daten vorhergesagt wurden, anstatt direkt beobachtet oder gemeldet, hatte das Unternehmen keine gesetzliche Verpflichtung, die Privatsphäre dieser Kunden zu schützen. Web-Suchalgorithmen wurden auch von Vorurteilen beschuldigt. Googles Ergebnisse können pornografische Inhalte in Suchbegriffen bezüglich Sexualität priorisieren, zum Beispiel lesbisch". Diese Vorspannung erstreckt sich auf die Suchmaschine, die beliebte, aber sexualisierte Inhalte in neutralen Suchvorgängen zeigt. Zum Beispiel "Top 25 Sexiest Women Athletes" Artikel als erste Ergebnisse in der Suche nach "Frauen Athleten". Im Jahr 2017 hat Google diese Ergebnisse zusammen mit anderen, die Hassgruppen, rassistische Ansichten, Kindermissbrauch und Pornografie, und andere drängende und offensive Inhalte. Weitere Beispiele sind die Darstellung von höher bezahlten Jobs an männliche Bewerber auf Stellensuche-Websites. Forscher haben auch festgestellt, dass die maschinelle Übersetzung eine starke Tendenz zu männlichen Versagen zeigt. Insbesondere wird dies in Bereichen beobachtet, die mit der unsymmetrischen Geschlechterverteilung, einschließlich der STEM-Berufe, verbunden sind. In der Tat versagen die aktuellen maschinellen Übersetzungssysteme die reale Weltverteilung der weiblichen Arbeiter. Im Jahr 2015, Amazon.com schaltete ein KI-System es entwickelt, um Job-Anwendungen zu überwachen, als sie erkannte, dass es vor Frauen. Das Rekrutierungstool schließt Bewerber aus, die an allen Colleges und Lebensläufen der Frau teilgenommen haben, die das Wort Frauen enthalten." Während in den Musik-Streaming-Diensten, die ähnliche Dinge geschah. Im Jahr 2019 wurde Spotify entdeckt, dass sein Empfehlungssystemalgorithmus gegen Künstlerinnen voreingenommen wurde. Spotifys Song-Empfehlungen schlugen mehr männliche Künstler über Frauen Künstler. Rassen- und ethnische Diskriminierung Algorithmen wurden als Methode kritisiert, rassische Vorurteile in der Entscheidungsfindung zu verhüten. Aufgrund der Art, wie bestimmte Rassen und ethnische Gruppen in der Vergangenheit behandelt wurden, können Daten oft versteckte Vorurteile enthalten. Zum Beispiel werden Schwarze wahrscheinlich längere Sätze erhalten als Weiße, die dasselbe Verbrechen begangen haben. Dies könnte möglicherweise bedeuten, dass ein System die ursprünglichen Bias in den Daten verstärkt. Im Jahr 2015 entschuldigte sich Google, als schwarze Nutzer beschwerten, dass ein Bildverarbeitungsalgorithmus in seiner Foto-Anwendung identifizierte sie als Gorillas. Im Jahr 2010 wurden Nikon-Kameras kritisiert, als Bilderkennungsalgorithmen asiatische Nutzer konsequent gefragten, ob sie blinkten. Solche Beispiele sind das Produkt von Bias in biometrischen Datensätzen. Aus den Aspekten des Körpers werden biometrische Daten gezogen, einschließlich rassistischer Merkmale, die dann in Datenpunkte übertragen werden können. Die Spracherkennungstechnologie kann je nach Akzent unterschiedliche Akzente setzen. Dies kann durch das Fehlen von Trainingsdaten für Lautsprecher dieses Akzents verursacht werden. Biometrische Daten über Rasse können auch abgeleitet werden, anstatt zu beobachten. Eine 2012 Studie zeigte beispielsweise, dass Namen, die häufig mit Schwarzen verbunden waren, eher Suchergebnisse liefern konnten, die Verhaftungsaufzeichnungen bedeuten, unabhängig davon, ob es irgendwelche Polizeiaufzeichnungen über den Namen dieses Individuums gibt. Eine 2015 Studie ergab auch, dass Schwarze und asiatische Menschen angenommen werden, weniger funktionierende Lungen aufgrund von Rassen- und Berufsexpositionsdaten nicht in das Modell der Lungenfunktion des Vorhersagealgorithmus aufgenommen werden. Im Jahr 2019 zeigte eine Studie, dass ein von Optum verkaufter Gesundheitsalgorithmus weiße Patienten über krankere schwarze Patienten begünstigte. Der Algorithmus prognostiziert, wie viel Patienten das Gesundheitssystem in Zukunft kosten würden. Die Kosten sind jedoch nicht raceneutral, da schwarze Patienten etwa $1.800 weniger an medizinischen Kosten pro Jahr anfielen als weiße Patienten mit der gleichen Anzahl von chronischen Erkrankungen, die dazu führten, dass der Algorithmus weiße Patienten genauso skalierte wie bei zukünftigen gesundheitlichen Problemen, wie schwarze Patienten, die deutlich mehr Krankheiten erlitten. Eine Studie von Forschern der UC Berkeley im November 2019 ergab, dass Hypothekenalgorithmen diskriminierend gegenüber lateinischen und afrikanischen Amerikanern sind, die gegen Minderheiten auf Basis von Kreditwürdigkeit diskriminiert, die in den US-Gesetzen verankert ist, die es den Kreditgebern ermöglichen, Maßnahmen zur Identifizierung zu verwenden, um festzustellen, ob ein Individuum Kredite würdig ist. Diese speziellen Algorithmen waren in FinTech-Unternehmen vorhanden und wurden gezeigt, um gegen Minderheiten zu diskriminieren. Strafverfolgung und GerichtsverfahrenAlgorithmen haben bereits zahlreiche Anwendungen in Rechtssystemen. Ein Beispiel dafür ist COMPAS, ein kommerzielles Programm, das von US-Gerichtshöfen weit verbreitet wird, um die Wahrscheinlichkeit eines Angeklagten zu bewerten, der zu einem Rezidivisten wird. ProPublica behauptet, dass der durchschnittliche COMPAS zugewiesene Forderungsrisikopegel von schwarzen Angeklagten deutlich höher ist als der durchschnittliche COMPAS zugewiesene Risikoniveau von weißen Angeklagten, und dass schwarze Angeklagte doppelt so wahrscheinlich falsch dem Label High-Risiko als weiße Angeklagte zugewiesen werden. Ein Beispiel ist die Verwendung von Risikobewertungen bei kriminellen Verurteilungen in den Vereinigten Staaten und Bewährungs Anhörungen, Richter wurden mit einer algorithmisch generierten Punktzahl präsentiert, um das Risiko zu reflektieren, dass ein Gefangener ein Verbrechen wiederholen wird. Für den Zeitraum, der 1920 beginnt und 1970 endete, war die Nationalität des Vaters eines Verbrechers eine Überlegung in diesen Risikobewertungspunkten. Heute werden diese Punkte mit Richtern in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington und Wisconsin geteilt. Eine unabhängige Untersuchung von ProPublica ergab, dass die Ergebnisse 80% der Zeit ungenau waren, und unverhältnismäßig gesunken, um Schwarze zu vermuten, dass es zu einem Rückfallrisiko kam, 77% häufiger als Weiße. Eine Studie, die auf die Prüfung “Risk, Rennen, & Recidivism: Predictive Bias and Disparate Impact” setzt eine zweifache (45 Prozent vs. 23 Prozent) negative Wahrscheinlichkeit für schwarz vs. Die kaukasischen Angeklagten sollten als ein höheres Risiko eingestuft werden, obwohl sie objektiv ohne dokumentierten Rezidivismus über einen zweijährigen Beobachtungszeitraum blieben. Online HassredeIn 2017 wurde ein Facebook-Algorithmus entwickelt, um Online Hassrede zu entfernen, gefunden, um Weiße über schwarze Kinder bei der Beurteilung von unerwünschten Inhalten, nach internen Facebook-Dokumenten zu nutzen. Der Algorithmus, der eine Kombination von Computerprogrammen und menschlichen Content-Reviewern ist, wurde erstellt, um breite Kategorien zu schützen, anstatt bestimmte Untergruppen von Kategorien. Zum Beispiel würden Beiträge, die Muslime bestreiten, blockiert, während Posts, die "Radische Muslime" bestreiten, erlaubt würden. Ein unvorhergesehenes Ergebnis des Algorithmus ist es, Hassrede gegen schwarze Kinder zu ermöglichen, weil sie die Kinder Untermenge von Schwarzen ankündigen, anstatt "alle Schwarzen", während "alle weißen Männer" einen Block auslösen würden, weil Weiße und Männchen nicht als Untergruppen betrachtet werden. Facebook wurde auch gefunden, dass Ad-Käufer "Jew-Hüter" als Kategorie von Benutzern ansprechen, die das Unternehmen sagte, war ein unbeabsichtigtes Ergebnis von Algorithmen bei der Bewertung und Kategorisierung von Daten verwendet. Das Design des Unternehmens erlaubte auch Anzeigenkäufer zu blockieren Afro-Amerikaner von Gehäuse Anzeigen. Während Algorithmen verwendet werden, um Hassrede zu verfolgen und zu blockieren, wurden einige gefunden, um 1,5 mal wahrscheinlicher zu Flag-Informationen von Black-Nutzern und 2,2 mal wahrscheinlich, um Informationen als Hassrede zu markieren, wenn in Ebonics geschrieben. Ohne Kontext für Slums und Epithets, auch wenn sie von Gemeinschaften verwendet werden, die sie wieder angeeignet haben, wurden markiert. Surveillance Surveillance Kamera-Software kann als inhärent politisch betrachtet werden, weil es Algorithmen erfordert, normal von anormalen Verhaltensweisen zu unterscheiden und zu bestimmten Zeiten zu bestimmen, wer an bestimmten Orten gehört. Die Fähigkeit solcher Algorithmen, Gesichter über ein Rassenspektrum zu erkennen, hat sich gezeigt, dass sie durch die rassische Vielfalt der Bilder in ihrer Trainingsdatenbank eingeschränkt ist; wenn die meisten Fotos zu einem Rennen oder Geschlecht gehören, ist die Software besser, andere Mitglieder dieses Rennens oder Geschlechts zu erkennen. Allerdings sind auch Audits dieser Bilderkennungssysteme ethisch erschüttert, und einige Wissenschaftler haben vorgeschlagen, dass der Kontext der Technologie immer einen unverhältnismäßigen Einfluss auf Gemeinschaften haben wird, deren Handlungen über-surveilled sind. Zum Beispiel fand eine 2002-Analyse von Software zur Identifizierung von Individuen in CCTV-Bildern mehrere Beispiele von Vorurteilen, wenn sie gegen kriminelle Datenbanken laufen. Die Software wurde als Identifizierung von Männern häufiger als Frauen, ältere Menschen häufiger als die jungen, und identifizierte Asiaten, Afroamerikaner und andere Rassen häufiger als Weiße. Zusätzliche Studien der Gesichtserkennungssoftware haben das Gegenteil gefunden, um wahr zu sein, wenn auf nicht-kriminellen Datenbanken trainiert, wobei die Software die geringste Genauigkeit bei der Identifizierung dunklerhäutiger weiblicher. sexuelle Diskriminierung Im Jahr 2011, Benutzer der Homosexuell-Hookup-Anwendung Grindr berichtet, dass der Android-Shop Empfehlungsalgorithmus verknüpft Grindr mit Anwendungen entwickelt, um Sex-Tender zu finden, die Kritiker sagten ungenau verwandte Homosexualität mit Pädophilie.Der Schriftsteller Mike Ananny kritisierte diesen Verein im Atlantik und argumentierte, dass solche Verbände weitere stigmatisierte Homosexuelle Männer. Im Jahr 2009 hat der Online-Händler Amazon de-notierte 57.000 Bücher nach einer algorithmischen Änderung seine "Erwachsene Inhalte" Blacklist erweitert, um jedes Buch, das sich mit Sexualität oder schwulen Themen befasst, wie zum Beispiel den kritisch gefeierten Roman Brokeback Mountain. Im Jahr 2019 wurde gefunden, dass auf Facebook nach "Fotos von meinen weiblichen Freunden" Vorschläge wie "in Bikini" oder "am Strand" gab. Die Suche nach "Fotos meiner männlichen Freunde" ergab dagegen keine Ergebnisse. Die Gesichtserkennungstechnologie wurde gesehen, um Probleme für Transgender zu verursachen. Im Jahr 2018 gab es Berichte von uber Fahrern, die transgender oder übergangen waren, mit der Gesichtserkennungssoftware, die Uber als integrierte Sicherheitsmaßnahme implementiert. Dies hat zur Folge, dass einige der Konten von trans uber-Treibern ausgesetzt wurden, die sie kosten und sie möglicherweise einen Job kosten, alle aufgrund der Gesichtserkennungssoftware Schwierigkeiten mit dem Erkennen des Gesichts eines trans-Treibers, derstieg. Obwohl die Lösung dieses Problems scheint, die trans-Individuen in Ausbildungseinheiten für maschinelle Lernmodelle einzubeziehen, ein Beispiel für trans-Individuen YouTube-Videos, die gesammelt wurden, um in Trainingsdaten verwendet werden, erhielten keine Zustimmung von den in den Videos enthaltenen Trans-Individuen, die eine Frage der Verletzung der Privatsphäre erstellten. Es gab auch eine Studie, die an der Stanford University im Jahr 2017 durchgeführt wurde, dass getestet Algorithmen in einem maschinellen Lernsystem, die gesagt wurde, um eine Person sexuelle Orientierung auf der Grundlage ihrer Gesichtsbilder zu erkennen. Das Modell in der Studie vorhergesagt eine richtige Unterscheidung zwischen Homosexuellen und geraden Männern 81% der Zeit, und eine richtige Unterscheidung zwischen Homosexuellen und gerade Frauen 74% der Zeit. Diese Studie führte zu einem Rückschlag der LGBTQIA-Gemeinschaft, die Angst vor den möglichen negativen Auswirkungen hatten, die dieses KI-System auf Einzelpersonen der LGBTQIA-Gemeinschaft haben könnte, indem Menschen das Risiko haben, gegen ihren Willen hinauszugehen. Google SearchWhile Benutzer generieren Ergebnisse, die automatisch abgeschlossen werden, Google hat es versäumt, sexist und rassist autocompletion Text zu entfernen. Zum Beispiel, Algorithmen der Unterdrückung: Wie Search Engines Racism Safiya verstärken Noble stellt ein Beispiel für die Suche nach "schwarzen Mädchen", die berichtet wurde, um in pornografische Bilder zu führen. Google behauptete, es sei nicht in der Lage, diese Seiten zu löschen, es sei denn, sie seien rechtswidrig. Hindernisse für die Forschung Mehrere Probleme behindern die Studie von groß angelegten algorithmischen Vorurteilen, die die Anwendung von wissenschaftlich strengen Studien und öffentlichem Verständnis behindern. Die Definition von Fairness Literatur über algorithmische Vorurteile hat sich auf das Mittel der Fairness konzentriert, aber Definitionen von Fairness sind oft unvereinbar miteinander und die Realitäten der maschinellen Lernoptimierung. So kann beispielsweise die Definition von Fairness als "Equality of Results" auf ein System, das für alle Menschen das gleiche Ergebnis produziert, verweisen, während Fairness definiert als "Equality of Treatment" explizit Unterschiede zwischen Individuen berücksichtigen könnte. Infolgedessen wird die Fairness manchmal als in Konflikt mit der Genauigkeit eines Modells beschrieben, was darauf hindeutet, dass Spannungen zwischen den Prioritäten des sozialen Wohlergehens und den Prioritäten der Anbieter, die diese Systeme bezeichnen, angeboren werden. Als Reaktion auf diese Spannung haben Forscher mehr Sorgfalt auf die Gestaltung und Nutzung von Systemen vorgeschlagen, die auf potenziell vorgespannte Algorithmen zeichnen, mit Fairness für spezifische Anwendungen und Kontexte definiert. Komplexität Algorithmische Prozesse sind komplex, oft über das Verständnis der Menschen, die sie verwenden. Große Operationen können auch nicht von denen verstanden werden, die an ihrer Erstellung beteiligt sind. Die Methoden und Prozesse zeitgenössischer Programme werden oft durch die Unfähigkeit verdeckt, jede Permutation der Eingabe oder Ausgabe eines Codes zu kennen. Sozialwissenschaftler Bruno Latour hat diesen Prozess als Blackboxing identifiziert, ein Prozess, in dem "wissenschaftliche und technische Arbeit durch seinen eigenen Erfolg unsichtbar gemacht wird. Wenn eine Maschine effizient läuft, wenn eine Frage der Tatsache geklärt ist, muss man sich nur auf ihre Ein- und Ausgänge und nicht auf ihre interne Komplexität konzentrieren. So gelingt paradoxerweise, je mehr Wissenschaft und Technologie gelingt, desto undurchsichtiger und unsicherer werden sie." Andere haben die schwarze Box Metapher kritisiert, was darauf hindeutet, dass aktuelle Algorithmen keine schwarze Box sind, sondern ein Netzwerk von miteinander verbundenen. Ein Beispiel für diese Komplexität ist im Bereich der Eingaben in das individuelle Feedback zu finden.Die Social Media Website Facebook faktorierte in mindestens 100.000 Datenpunkten, um das Layout des Social Media Feeds eines Nutzers im Jahr 2013 zu bestimmen. Darüber hinaus können große Teams von Programmierern relativ isoliert voneinander arbeiten und sich nicht über die kumulativen Auswirkungen kleiner Entscheidungen in vernetzten, aufwändigen Algorithmen bewusst sein. Nicht alle Codes sind originell und können von anderen Bibliotheken ausgeliehen werden, wodurch eine komplizierte Beziehung zwischen Datenverarbeitung und Dateneingabesystemen entsteht. Zusätzliche Komplexität tritt durch maschinelles Lernen und die Personalisierung von Algorithmen auf Basis von Benutzerinteraktionen wie Klicks, Zeit vor Ort und anderen Metriken auf. Diese persönlichen Anpassungen können allgemeine Versuche verwirren, Algorithmen zu verstehen. Ein nicht identifizierter Streaming-Radio-Service berichtete, dass es fünf einzigartige Musik-Auswahl-Algorithmen verwendet, die es für seine Benutzer ausgewählt, basierend auf ihrem Verhalten. Dies schafft verschiedene Erfahrungen der gleichen Streaming-Dienste zwischen verschiedenen Benutzern, so dass es schwieriger zu verstehen, was diese Algorithmen tun. Unternehmen führen auch häufige A/B-Tests auf Feinabstimmungsalgorithmen basierend auf der Nutzerantwort. Zum Beispiel kann die Suchmaschine Bing bis zu zehn Millionen subtile Variationen seines Dienstes pro Tag ausführen und verschiedene Erfahrungen des Dienstes zwischen jeder Nutzung und/oder Benutzer erstellen. Mangel an Transparenz Handelsalgorithmen sind proprietär und können als Handelsgeheimnisse behandelt werden. Algorithmen wie Handelsgeheimnisse zu behandeln schützt Unternehmen, wie Suchmaschinen, wo ein transparenter Algorithmus könnte Taktiken zeigen, um Suchmaschinen-Rankings zu manipulieren. Damit ist es für Forscher schwierig, Interviews oder Analysen durchzuführen, um herauszufinden, wie Algorithmen funktionieren. Kritiker legen nahe, dass eine solche Geheimhaltung auch mögliche unethische Methoden, die bei der Produktion oder Verarbeitung algorithmischer Ausgabe verwendet werden, belasten kann. Andere Kritiker, wie Rechtsanwalt und Aktivist Katarzyna Szymielewicz, haben vorgeschlagen, dass die mangelnde Transparenz oft durch algorithmische Komplexität verschleiert wird, die Unternehmen davor abschirmt, eigene algorithmische Prozesse aufzudecken oder zu untersuchen. Fehlen von Daten über sensible Kategorien Eine wesentliche Barriere für das Verständnis der Bekämpfung von Vorurteilen in der Praxis ist, dass Kategorien, wie demografische Personen, die durch Antidiskriminierungsgesetz geschützt sind, oft nicht explizit bei der Erhebung und Verarbeitung von Daten berücksichtigt werden. In einigen Fällen gibt es wenig Gelegenheit, diese Daten explizit zu sammeln, wie zum Beispiel beim Gerät Fingerabdruck, beim Ubiquitous Computing und im Internet der Dinge. In anderen Fällen kann der für die Verarbeitung Verantwortliche diese Daten aus ruflichen Gründen nicht erheben oder weil er ein erhöhtes Haftungs- und Sicherheitsrisiko darstellt. Es kann auch der Fall sein, dass diese Daten zumindest im Zusammenhang mit der Datenschutz-Grundverordnung der Europäischen Union unter die „Sonderkategorie"-Vorschriften fallen (Artikel 9), und daher mehr Einschränkungen bei der möglichen Erhebung und Verarbeitung aufweisen. Einige Praktizierende haben versucht, diese fehlenden sensiblen Kategorisierungen abzuschätzen und zu bestreiten, um eine Vorurteilung zu ermöglichen, z.B. Bausysteme, um die Ethnizität von Namen zu mindern, aber dies kann andere Formen von Vorurteilen einführen, wenn sie nicht mit Sorgfalt durchgeführt werden. Machine Learning-Forscher haben auf kryptographische Datenschutz-Enhancing-Technologien wie sichere Multi-Party-Rechnung gezogen, um Methoden vorzuschlagen, mit denen algorithmische Vorurteile beurteilt oder gemildert werden können, ohne dass diese Daten jemals in Klartext für Modellierer zur Verfügung stehen. Algorithmische Vorurteile umfassen nicht nur geschützte Kategorien, sondern können auch Merkmale betreffen, die weniger leicht beobachtbar oder kodierbar sind, wie politische Standpunkte. In diesen Fällen gibt es selten eine leicht zugängliche oder nicht kontroverse Bodenwahrheit, und das Entfernen der Vorspannung von einem solchen System ist schwieriger. Darüber hinaus können falsche und versehentliche Korrelationen aus einem mangelnden Verständnis der geschützten Kategorien entstehen, z.B. Versicherungen auf der Grundlage historischer Daten von Autounfällen, die sich mit Wohn-Clustern ethnischer Minderheiten streng zufällig überschneiden können. Lösungen Eine Studie von 84 Richtlinien zur ethischen KI ergab, dass Fairness und "Bestätigung unerwünschter Vorurteilung" ein gemeinsames Anliegen war und durch eine Mischung von technischen Lösungen, Transparenz und Überwachung, Recht auf Abhilfe und erhöhte Aufsicht sowie Diversität und Integrationsbemühungen behandelt wurden. Technische Daten Es gab mehrere Versuche, Methoden und Werkzeuge zu schaffen, die Bias innerhalb eines Algorithmus erkennen und beobachten können. Diese entstehenden Felder konzentrieren sich auf Werkzeuge, die typischerweise auf die (Training-)Daten angewendet werden, die vom Programm verwendet werden, anstatt auf die internen Prozesse des Algorithmus.Diese Methoden können auch die Ausgabe eines Programms und dessen Nützlichkeit analysieren und somit die Analyse seiner Verwirrungsmatrix (oder Verwirrungstabelle) beinhalten. Erklärbare KI zum Erkennen von Algorithmus Bias ist eine vorgeschlagene Möglichkeit, die Existenz von Bias in einem Algorithmus oder Lernmodell zu erkennen. Die Verwendung von maschinellem Lernen zur Erkennung von Vorurteilen wird als "leiten eines AI-Audits" bezeichnet, wo der Auditor ein Algorithmus ist, der durch das AI-Modell und die Trainingsdaten geht, um Vorurteile zu identifizieren. Derzeit wird ein neuer IEEE-Standard erstellt, der darauf abzielt, Methoden festzulegen, die den Erstellern von Algorithmen helfen, Probleme der Vorspannung zu beseitigen und Transparenz (d.h. Behörden oder Endbenutzer) über die Funktion und mögliche Auswirkungen ihrer Algorithmen zu artikulieren. Das Projekt wurde im Februar 2017 genehmigt und wird vom Software & Systems Engineering Standards Committee, einem von der IEEE Computer Society gecharterten Ausschuss, gesponsert. Ein Entwurf der Norm wird voraussichtlich im Juni 2019 zur Abstimmung vorgelegt werden. Transparenz und Überwachung Ethik-Richtlinien zur KI weisen auf die Notwendigkeit der Rechenschaftspflicht hin, was empfiehlt, Schritte zur Verbesserung der Interpretationsfähigkeit der Ergebnisse zu ergreifen. Solche Lösungen umfassen die Überlegung des "Rechts auf Verständnis" in maschinellen Lernalgorithmen und den Einsatz von maschinellem Lernen in Situationen zu widerstehen, in denen die Entscheidungen nicht erläutert oder überprüft werden konnten. Zu diesem Zweck ist eine Bewegung für "erklärbare KI" bereits innerhalb von Organisationen wie DARPA, aus Gründen, die über das Mittel der Bias hinausgehen. Price Waterhouse Coopers schlägt z.B. auch vor, dass die Überwachung der Ausgabesysteme so gestaltet werden kann, dass Einzelkomponenten des Systems isoliert und geschlossen werden können, wenn sie Ergebnisse erzielen. Ein anfänglicher Ansatz zur Transparenz beinhaltete die Open-Source von Algorithmen. Software-Code kann eingesehen werden und Verbesserungen können über Quellcode-Hosting-Einrichtungen vorgeschlagen werden. Dieser Ansatz erzeugt jedoch nicht unbedingt die beabsichtigten Effekte. Unternehmen und Organisationen können alle möglichen Dokumentationen und Code teilen, aber dies schafft keine Transparenz, wenn das Publikum die gegebenen Informationen nicht versteht. Daher lohnt sich die Rolle eines interessierten kritischen Publikums in Bezug auf Transparenz. Algorithmen können ohne kritisches Publikum nicht rechenschaftspflichtig gehalten werden. Recht auf Abhilfe Aus regulatorischer Sicht fordert die Toronto-Deklaration die Anwendung eines Menschenrechtsrahmens auf Schäden, die durch algorithmische Bias verursacht werden. Dazu gehören gesetzliche Erwartungen an Due Diligence im Auftrag von Designern dieser Algorithmen und die Rechenschaftspflicht, wenn private Akteure das öffentliche Interesse nicht schützen, wobei darauf hingewiesen wird, dass diese Rechte durch die Komplexität der Ermittlung der Verantwortung innerhalb eines Netzes komplexer, verflechtender Prozesse beeinträchtigt werden können. Andere schlagen die Notwendigkeit klarer Haftpflichtversicherungsmechanismen vor. Diversity und Inklusion Amid ist besorgt, dass das Design von KI-Systemen in erster Linie die Domäne von weißen, männlichen Ingenieuren ist, eine Reihe von Wissenschaftlern haben vorgeschlagen, dass algorithmische Voreingenommenheit durch die Erweiterung der Inklusion in den Reihen von denen, die KI-Systeme zu minimieren. Zum Beispiel sind nur 12% der Maschinenbauer Frauen, mit schwarzen KI-Führern, die auf eine "Diversitätskrise" in diesem Bereich hinweisen. Gruppen wie Black in KI und Queer in KI versuchen, in der KI-Gemeinschaft mehr integrative Räume zu schaffen und gegen die oft schädlichen Wünsche von Unternehmen zu arbeiten, die die Tragödie der KI-Forschung kontrollieren. Kritiken einfacher Inklusivitätsbemühungen legen nahe, dass Diversity-Programme keine überlappenden Formen von Ungleichheit ansprechen können, und haben aufgerufen, ein bewussteres Objektiv der Schnittlichkeit auf das Design von Algorithmen anzuwenden. Forscher der Universität Cambridge haben argumentiert, dass die Ansprache der Rassenvielfalt durch die Weiße der Kultur der KI behindert wird. Verordnung Europa Die Datenschutz-Grundverordnung (DSGVO) des 2018 umgesetzten überarbeiteten Datenschutzregimes der Europäischen Union befasst sich mit der "Automatisierten individuellen Entscheidungsfindung einschließlich Profiling" in Artikel 22. Diese Regeln verbieten ausschließlich automatisierte Entscheidungen, die eine signifikante oder rechtliche Wirkung auf eine Person haben, es sei denn, sie sind ausdrücklich durch Zustimmung, Vertrag oder mitgliedstaatliches Recht zugelassen. Wenn sie zugelassen sind, müssen vor Ort Schutzmaßnahmen getroffen werden, wie zum Beispiel ein Recht auf ein Mensch-in-the-Loop und ein unverbindliches Recht auf eine Erklärung der getroffenen Entscheidungen. Obwohl diese Vorschriften allgemein als neu angesehen werden, existieren seit 1995 in ganz Europa fast identische Bestimmungen, in Artikel 15 der Datenschutzrichtlinie.Die in französischem Recht seit Ende der 1970er-Jahre festgestellten automatisierten Entscheidungsregeln und -sicherungen. Die DSGVO befasst sich mit algorithmischen Vorurteilen in Profiling-Systemen sowie den statistischen Ansätzen, die möglich sind, um sie direkt unter Randnummer 71 zu reinigen, wobei ... der für die Verarbeitung Verantwortliche geeignete mathematische oder statistische Verfahren für die Profilierung, technische und organisatorische Maßnahmen anwenden sollte, die u.a. diskriminierende Auswirkungen auf natürliche Personen auf der Grundlage rassischer oder ethnischer Herkunft, politischer Meinung, Religion oder Überzeugungen, Gewerkschaftsmitgliedschaft, genetischer oder Gesundheitsstatus oder sexueller oder sexueller Ausrichtung oder sexueller Ausrichtung verhindern. Wie das unverbindliche Recht auf eine Erklärung unter Randnummer 71 ist das Problem die unverbindliche Art der Erwägungen. Während sie von der Arbeitsgruppe Artikel 29 als Anforderung behandelt wurde, die über die Umsetzung des Datenschutzrechts beraten hat, sind ihre praktischen Abmessungen unklar. Es wurde argumentiert, dass die Datenschutzfolgenabschätzungen für das Profiling von Hochrisikodaten (zusammen mit anderen vorbeugenden Maßnahmen im Datenschutz) ein besserer Weg sein können, um Probleme der algorithmischen Diskriminierung anzupacken, da sie die Handlungen von Algorithmen einschränkt, anstatt den Verbrauchern eine Beschwerde einzureichen oder Änderungen anzufordern. Vereinigte Staaten Die Vereinigten Staaten haben keine allgemeinen Rechtsvorschriften, die algorithmische Vorurteile kontrollieren, und nähern sich dem Problem durch verschiedene Staats- und Bundesgesetze, die von Industrie, Sektor und wie ein Algorithmus verwendet werden kann. Viele Politiken werden von der Bundeshandelskommission selbst gestärkt oder kontrolliert. 2016 veröffentlichte die Obama-Administration den Nationalen Künstlichen Intelligenz-Forschungs- und Entwicklungs-Strategischen Plan, der politische Entscheidungsträger auf eine kritische Beurteilung von Algorithmen zu lenken beabsichtigte. Es empfahl Forschern, "diese Systeme so zu gestalten, dass ihre Handlungen und Entscheidungsfindungen durch den Menschen transparent und leicht interpretierbar sind und somit auf jede Bias, die sie enthalten können, geprüft werden können, anstatt nur diese Bias zu lernen und zu wiederholen". Der Bericht hat nur als Orientierungshilfe keinen rechtlichen Präzedenzfall geschaffen. Im Jahr 2017 leitete New York City die erste algorithmische Rechenschaftsrechnung in den USA. Der Gesetzentwurf, der am 1. Januar 2018 in Kraft getreten ist, forderte "die Schaffung einer Task Force, die Empfehlungen gibt, wie Informationen über automatisierte Entscheidungssysteme der Agentur mit der Öffentlichkeit geteilt werden können, und wie Agenturen Instanzen ansprechen können, in denen Menschen durch automatisierte Entscheidungssysteme der Agentur verletzt werden." Die Task Force muss im Jahr 2019 Erkenntnisse und Empfehlungen für weitere regulatorische Maßnahmen präsentieren. IndienAm 31. Juli 2018 wurde ein Entwurf des Personal Data Bill vorgelegt. Der Entwurf schlägt Normen für die Speicherung, Verarbeitung und Übermittlung von Daten vor. Während es den Begriff Algorithmus nicht verwendet, macht es Bestimmungen für ."harm aus jeder Verarbeitung oder jede Art von Verarbeitung, die von der Pflicht durchgeführt". Es definiert "jedes Verleugnen oder Entziehen eines Dienstes, Nutzens oder Gutes, das sich aus einer evaluativen Entscheidung über den Datenvorstand ergibt" oder "eine diskriminierende Behandlung" als eine Quelle von Schäden, die durch unsachgemäße Nutzung von Daten entstehen könnten. Es gibt auch Sonderbestimmungen für Menschen mit "Intersex-Status". Siehe auch Ethik der künstlichen Intelligenz weiterlesen Baer, Tobias (2019). Verstehen, verwalten und verhindern Algorithmik Bias: Ein Leitfaden für Unternehmer und Data Scientists. New York: Apress.ISBN 9781484248843. Noble, Safiya Umoja (2018). Algorithmen der Unterdrückung: Wie Suchmaschinen Rassismus stärken. New York: New York University Press.ISBN 9781479837243. Fairness (Maschinenlernen) == Referenzen ==