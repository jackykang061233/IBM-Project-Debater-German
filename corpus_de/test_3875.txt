Gradientenabstieg ist ein iterativer Optimierungsalgorithmus erster Ordnung, um ein lokales Minimum einer differenzierbaren Funktion zu finden. Der Gedanke besteht darin, wiederholte Schritte in die entgegengesetzte Richtung des Gradienten (oder ungefÃ¤hrer Gradienten) der Funktion am aktuellen Punkt zu unternehmen, da dies die Richtung steilster Abstieg ist. Umgekehrt fÃ¼hrt das Schritten in Richtung des Gradienten zu einem lokalen Maximum dieser Funktion; das Verfahren wird dann als Gradientenanhebung bezeichnet. Gradient Abstieg wird in der Regel Cauchy zugeschrieben, die es erst in 1847 vorgeschlagen. Hadamard hat 1907 selbst eine Ã¤hnliche Methode vorgeschlagen. Seine Konvergenzeigenschaften fÃ¼r nichtlineare Optimierungsprobleme wurden von Haskell Curry im Jahr 1944 zunÃ¤chst untersucht, wobei die Methode in den folgenden Jahrzehnten zunehmend gut untersucht und genutzt wurde, oft auch steilster Abstieg genannt. Beschreibung Gradient Abstieg basiert auf der Beobachtung, dass, wenn die multivariable Funktion F ( x ) {\displaystyle F(\mathbf {x} )} in einer Nachbarschaft eines Punktes definiert und differenzierbar ist ein {\displaystyle \mathbf {a}, dann F ( x ) Daraus folgt, dass, wenn eine n + 1 = n - Î³ MENT F (a n ) {\displaystyle \mathbf {a} {_n+1}=\mathbf {a} {_n}-\gamma \nabla F(\mathbf {a} {_n}) fÃ¼r ein Î³ Îµ R + {\displaystyle \gamma in\mathbb {R} _{+} klein genug, dann F (a n ) â‰¥ F (a n + 1 ) {\displaystyle F(\mathbf a_{n} )\geq F(\mathbf a_{n+1}}} .Mit anderen Worten ist der Begriff Î³ ğŸ˜‰ F (a) {\displaystyle \gamma \nabla F(\mathb} Bei dieser Beobachtung geht man mit einer Vermutung x 0 {\displaystyle \mathbf {x} {_0} fÃ¼r ein lokales Minimum von F {\displaystyle F} und betrachtet die Sequenz x 0, x 1 , x 2 , ... {\displaystyle \mathbf {x},\mathb {x} {x} {x}} - Nein. - Ja. Wir haben eine monotone Sequenz F ( x 0 ) â‰¥ F ( x 1 ) â‰¥ F ( x 2 ) â‰¥ â‹¯, {\displaystyle F(\mathbf {x} {_0})\geq F(\mathbf {x})\geq F(\mathbf {x} {_2})\geq \cdots sequenz,} so dass Beachten Sie, dass der Wert der SchrittgrÃ¶ÃŸe Î³ {\displaystyle \gamma } bei jeder Iteration geÃ¤ndert werden darf. Bei bestimmten Annahmen auf der Funktion F {\displaystyle F} (z.B. F {\displaystyle F} konvex und MENT F\displaystyle \nabla F} Lipschitz) und bestimmten Wahlen von Î³ {\displaystyle \gamma } (z.B. Ã¼ber eine Liniensuche, die die Wolfe-Bedingungen erfÃ¼llt, oder die Barzilai-Borwein-Methode wie folgt F(\mathbf {x} {_n})-\nabla F(\mathbf {x}_n-1})\right\|^{2 Konvergenz auf ein lokales Minimum kann gewÃ¤hrleistet werden. Wenn die Funktion F {\displaystyle F} konvex ist, sind alle lokalen Minima auch globale Minima, so kann in diesem Fall Gradientenabstieg zur globalen LÃ¶sung konvergieren. Dieser Vorgang ist im benachbarten Bild dargestellt. Dabei wird F {\displaystyle F} auf der Ebene definiert und sein Diagramm eine Schalenform aufweist. Die blauen Kurven sind die Konturlinien, d.h. die Bereiche, auf denen der Wert von F {\displaystyle F} konstant ist. Ein roter Pfeil, der an einer Stelle beginnt, zeigt die Richtung des negativen Gradienten an dieser Stelle. Beachten Sie, dass der (negative) Gradient an einem Punkt orthogonal zu der Konturlinie ist, die durch diesen Punkt geht. Wir sehen, dass Gradientenabstieg uns zum Boden der SchÃ¼ssel fÃ¼hrt, also zu dem Punkt, wo der Wert der Funktion F {\displaystyle F} minimal ist. Eine Analogie zum VerstÃ¤ndnis Gradientenabstieg Die grundlegende Intuition hinter Gradientenabstieg kann durch ein hypothetisches Szenario illustriert werden. Eine Person steckt in den Bergen fest und versucht, sich zu begeben (d.h. das globale Minimum zu finden). Es gibt einen schweren Nebel, so dass die Sichtbarkeit extrem gering ist. Daher ist der Weg hinunter den Berg nicht sichtbar, so dass sie lokale Informationen verwenden mÃ¼ssen, um das Minimum zu finden. Sie kÃ¶nnen die Methode der Gradientenabstieg verwenden, die die Steilheit des HÃ¼gels an ihrer aktuellen Position betrachtet, dann in Richtung mit der steilsten Abstieg (d.h. bergab). Wenn sie versuchen, die Spitze des Berges (d.h. das Maximum) zu finden, dann wÃ¼rden sie in Richtung steilsten Aufstieg (d.h. bergauf) gehen. Mit dieser Methode wÃ¼rden sie schlieÃŸlich ihren Weg hinunter den Berg finden oder mÃ¶glicherweise in einem Loch (d.h. lokales Minimum oder Sattelpunkt) wie ein Bergsee stecken. Nehmen Sie jedoch auch an, dass die Steilheit des HÃ¼gels nicht sofort mit einfacher Beobachtung offensichtlich ist, sondern es erfordert ein ausgeklÃ¼geltes Instrument zu messen, das die Person im Moment hat. Es braucht einige Zeit, um die Steilheit des HÃ¼gels mit dem Instrument zu messen, so sollten sie ihre Verwendung des Instruments minimieren, wenn sie den Berg vor Sonnenuntergang hinunter wollen. Die Schwierigkeit ist dann, die Frequenz zu wÃ¤hlen, mit der sie die Steilheit des HÃ¼gels messen sollten, um nicht von der Bahn zu gehen. In dieser Analogie stellt die Person den Algorithmus dar, und der Weg, der den Berg hinunter genommen wird, stellt die Reihenfolge der Parametereinstellungen dar, die der Algorithmus erforscht. Die Steilheit des HÃ¼gels stellt die Steigung der FehlerflÃ¤che an diesem Punkt dar. Das zur Steilheitsmessung verwendete Instrument ist eine Differenzierung (die Neigung der FehlerflÃ¤che kann durch Ableitung der quadratischen Fehlerfunktion an diesem Punkt berechnet werden). Die Richtung, in der sie sich entscheiden, in fluchtet mit dem Gradienten der FehlerflÃ¤che an dieser Stelle. Die Zeit, die sie vor einer weiteren Messung zurÃ¼cklegen, ist die StufengrÃ¶ÃŸe. Beispiele Gradientenabstieg hat Probleme mit pathologischen Funktionen wie der hier gezeigten Rosenbrock-Funktion. f ( x 1 , x 2 ) = ( 1 - x 1 ) 2 + 100 ( x 2 - x 1 2 ) 2 . Die Rosenbrock-Funktion hat ein schmales, gekrÃ¼mmtes Tal, das das Minimum enthÃ¤lt. Der Talboden ist sehr flach. Durch das geschwungene Flachtal wird die Optimierung langsam mit kleinen StufengrÃ¶ÃŸen auf das Minimum hin zickzackiert. Die zickzackierende Natur des Verfahrens ist weiter unten ersichtlich, wobei die Gradientenabstiegsmethode auf F ( x, y) = sin â€¡ ( 1 2 x 2 - 1 4 y 2 + 3 ) cos â‰  ( 2 x + 1 - e y ) angewendet wird. 1{4}y^{2}+3\right)\cos links(2x+1-e^{y}\right). Die Wahl der SchrittgrÃ¶ÃŸe und AbwÃ¤rtsrichtung Da die Verwendung einer zu kleinen SchrittgrÃ¶ÃŸe Î³ {\displaystyle \gamma } die Konvergenz langsamer wÃ¼rde, und eine zu groÃŸe Î³ {\displaystyle \gamma } wÃ¼rde zu Divergenz fÃ¼hren, eine gute Einstellung von Î³ {\displaystyle \gamma } zu finden ist ein wichtiges praktisches Problem. Philip Wolfe befÃ¼rwortete in der Praxis auch die Verwendung von "sauberen Wahlen der [absteigenden] Richtung". WÃ¤hrend mit einer Richtung, die von der steilsten Abstiegsrichtung abweicht, kontra-intuitiv erscheinen kann, ist die Idee, dass die kleinere Steigung durch eine viel lÃ¤ngere Strecke ausgeglichen werden kann. Um dies mathematisch zu begrÃ¼nden, verwenden wir eine Richtung p n {\displaystyle \mathbf {p} {_n} und SchrittgrÃ¶ÃŸe Î³ n {\displaystyle \gamma {_n} und betrachten das allgemeinere Update: a n + 1 = a n - Î³ n p n {\displaystyle \mathbf {a} {_n+1}=\mathbf {a} - Nein. {_n}\,\mathbf {p} {_n} . Gute Einstellungen von p n {\displaystyle \mathbf {p} {_n} und Î³ n {\displaystyle \gamma {_n} erfordern einen kleinen Gedanken. ZunÃ¤chst mÃ¶chten wir, dass die Update-Richtung bergab zeigt. Mathematisch bedeutet das Verlassen von Î¸ n {\displaystyle \theta {_n} den Winkel zwischen MENT F ( a n ) {\displaystyle \nabla F(\mathbf a_{n}} und p n {\displaystyle \mathbf {p} {_n} , dies erfordert, dass cos 0. {\displaystyle \cos \theta {_n}>0. Um mehr zu sagen, brauchen wir mehr Informationen Ã¼ber die Zielfunktion, die wir optimieren. Unter der ziemlich schwachen Annahme, dass F {\displaystyle F} kontinuierlich differenzierbar ist, kÃ¶nnen wir beweisen, dass: Diese Ungleichheit bedeutet, dass der Betrag, um den wir sicher sein kÃ¶nnen, dass die Funktion F {\displaystyle F} verringert wird, von einem Handel zwischen den beiden Begriffen in quadratischen Klammern abhÃ¤ngt. Der erste Begriff in quadratischen Klammern misst den Winkel zwischen der AbwÃ¤rtsrichtung und dem negativen Gradienten. Der zweite Begriff misst, wie schnell sich der Gradient entlang der AbwÃ¤rtsrichtung Ã¤ndert. GrundsÃ¤tzlich kÃ¶nnte Ungleichheit (1) Ã¼ber p n {\displaystyle \mathbf {p} {_n} und Î³ n {\displaystyle \gamma {_n} optimiert werden, um eine optimale SchrittgrÃ¶ÃŸe und -richtung zu wÃ¤hlen. Problematisch ist, dass die Auswertung des zweiten Begriffs in quadratischen Klammern die Auswertung von MENT F (a n - t Î³ n p n ) {\displaystyle \nabla F(\mathbf {a} {_n}-t\gamma {_n}\mathbf {p} {_n}) erfordert und zusÃ¤tzliche Gradientenauswertungen in der Regel teuer und unerwÃ¼nscht sind. Einige Wege um dieses Problem sind: Forgo die Vorteile einer cleveren Abstiegsrichtung durch Einstellung p n = MENT F ( a n ) {\displaystyle \mathbf {p} {_n}=\nabla F(\mathbf a_{n}} , und verwenden Sie die Zeilensuche, um eine geeignete Schritt-GrÃ¶ÃŸe Î³ n {\displaystyle \gamma_n} zu finden, wie eine, die die die die die die die die die Bedingungen erfÃ¼llt. Unter der Annahme, dass F {\displaystyle F} doppelt differenzierbar ist, verwenden Sie seine Hessian MENT 2 F {\displaystyle \nabla ^{2}F, um â€¡ ermittelt ğŸ˜‰ F (a n - t Î³ n p n ) - ğŸ˜‰ F (a n ) gebildet 2 â‰ˆ Îº t Ï‡ n ğŸ˜‰p n . {\displaystyle |\nabla F(\mathbf {a} - Nein. {_n}\mathbf {p} {_n}-\nabla F(\mathbf} n}\)_{2}\approx |t\gamma {_n}\nabla - Nein. WÃ¤hlen Sie dann p n {\displaystyle \mathbf {p} {_n} und Î³ n {\displaystyle \gamma {_n} durch Optimierung der Ungleichheit (1). Unter der Annahme, dass MENT F {\displaystyle \nabla F} Lipschitz ist, verwenden Sie seine Lipschitz-Konstante L {\displaystyle L} zu gebunden ğŸ˜‰ F ( a n - t Î³ n p n ) âˆ’ ğŸ˜‰ F ( a n ) â€¡ L t Î³ n geschichte n p n . . {\displaystyle |\nabla F(\mathbf} - Nein. {_n}\mathbf {p} {_n})-\nabla F(\mathbf {a} n}\|_{2}\leq Lt\gamma {_n}\|\\\mathbf {p} {_n|}\. WÃ¤hlen Sie dann p n {\displaystyle \mathbf {p} {_n} und Î³ n {\displaystyle \gamma {_n} durch Optimierung der Ungleichheit (1). Bauen Sie ein benutzerdefiniertes Modell von max t âˆˆ [0, 1 ] gebildet â™¦ F ( a n - t Î³ n p n ) âˆ’ â€¡ F ( a n ) â€¡ 2 Îº Îº F ( a n ) 2 {\displaystyle \max {_t\in [0,1]}\frac |{\nabla F(\mathbf {a} - Nein. {_n}\mathbf {p} {_n})-\nabla F(\mathbf {a} n}\|_{2}{\|\nabla F(\mathbf {a} n}\|\)_{2 fÃ¼r F {\displaystyle F} .Dann wÃ¤hlen Sie p n {\displaystyle \mathbf {p} {_n} und Î³ n {\displaystyle \gamma {_n} durch Optimierung der Ungleichheit (1). Unter stÃ¤rkeren Annahmen auf der Funktion F {\displaystyle F} wie KonvexitÃ¤t kÃ¶nnen fortgeschrittene Techniken mÃ¶glich sein. GewÃ¶hnlich kann durch Nachfolge eines der oben genannten Rezepte die Konvergenz zu einem lokalen Minimum gewÃ¤hrleistet werden. Wenn die Funktion F {\displaystyle F} konvex ist, sind alle lokalen Minima auch globale Minima, so kann in diesem Fall Gradientenabstieg zur globalen LÃ¶sung konvergieren. LÃ¶sung eines linearen Systems Gradient Abstieg kann verwendet werden, um ein System von linearen Gleichungen A x - b = 0 {\displaystyle - Ja. =0} als quadratisches Minimierungsproblem reformiert. Ist die Systemmatrix A {\displaystyle A} real symmetrisch und positiv-definit, ist die quadratische Funktion, um allgemein zu minimieren F (x) = x T A x - 2 x T b, {\displaystyle F(\mathbf {x} =)\mathbf {x} ^{T}A\mathbf {x} -2\mathbf {x} ^{T}\mathbf {b},} so dass â€¡ F (x) = 2 (A x âˆ’ b) . {\displaystyle \nabla F(\mathbf {x} =)2(A\mathbf {x} -mathbf} FÃ¼r eine allgemeine reale Matrix A {\displaystyle A} definieren lineare kleinste Quadrate F ( x ) = A x - b zusammengestellt 2 . In traditionellen linearen kleinsten Quadraten fÃ¼r reale A {\displaystyle A} und b {\displaystyle \mathbf {b} wird die Euclidean-Norm verwendet, in welchem Fall MENT F (x) = 2 A T (A x - b) . {\displaystyle \nabla F(\mathbf {x} =)2A^{T}(A\mathbf {x} -\mathbf {b}}}}} Die Liniensucheminimierung, die lokal optimale SchrittgrÃ¶ÃŸe Î³ {\displaystyle \gamma } bei jeder Iteration zu finden, kann analytisch fÃ¼r quadratische Funktionen durchgefÃ¼hrt werden, und es sind explizite Formeln fÃ¼r die lokal optimale Î³ {\displaystyle \gamma } bekannt. Beispielsweise kann fÃ¼r die reale symmetrische und positiv definierte Matrix A {\displaystyle A} ein einfacher Algorithmus wie folgt sein, in der Schleife wiederholen: r := b - A x Î³:= r T r / r T A r x := x + Î³ r wenn r T r ist ausreichend klein, dann Ausgang-Schleife-End-Repeat x als Ergebnis {\displaystyle begin{align}&{\text{repeat in der Schlaufe:}\\&\qquad \mathbf {r} \=:mathbf {b} - Ja. \{=:mathbf {r} ^{\mathsf {T}\mathbf {r} }{\/mathbf {r} ^{\mathf {T}\mathbf {Ar} }&\qquad \mathbf {x} \=mathbf {x} +\gamma \mathbf {r} &\qquad \hbox{if }\mathbf {r} ^{\mathsf {T}\mathbf {r} \text ist ausreichend klein, dann ausgangsschleife}\\&{\text{end wiederholschleife}\\\&{text{return }\mathbf {x} \text als Ergebnis}\end{ausgerichtet Um die Multiplikation mit A {\displaystyle A} zweimal pro Iteration zu vermeiden, beachten wir, dass x := x + Î³ r {\displaystyle \mathbf {x} \=:mathbf {x} +\gamma \mathbf {r} impliziert r A r {\displaystyle \mathbf {r} \=:mathbf {r} -\gamma \mathbf {Ar} , die den herkÃ¶mmlichen Algorithmus gibt, r:= b - A x Wiederholung in der Schleife: Î³ := r T r / r T A r x := x + Î³ r, wenn r T r ausreichend klein ist, dann Austrittsschleife r:= r - Î³ Ein r End-Repeatschleife zurÃ¼ck x als Ergebnis {\displaystyle begin{aligned}&\mathbf {r} \=:mathbf {b} -\mathbf {Ax} &\text{repeat in der Schleife:}\&\qquad \gamma \{=:mathbf r}^{\mathf {T}}\mathbf {r} }{\/mathbf {r} ^{\mathf {T}\mathbf {Ar} }&\qquad \mathbf {x} \=mathbf {x} +\gamma \mathbf {r} &\qquad \hbox{if}\mathbf {r} ^{\mathsf {T}\mathbf {r} \text ist ausreichend klein, dann ausgangsschleife}\\\&\qquad \mathbf {r} \=:mathbf {r} - Gamma \mathbf {Ar} &\text{end Repeatschleife}\&{\text{return}\mathbf {x} \text als Ergebnis}\end{ausgerichtet Das Verfahren wird selten zur LÃ¶sung von linearen Gleichungen verwendet, wobei die konjugierte Gradientenmethode eine der beliebtesten Alternativen ist. Die Anzahl der Gradientenabstiegs Iterationen ist Ã¼blicherweise proportional zur spektralen Zustandszahl Îº (A ) {\displaystyle \kappa (A}) der Systemmatrix A {\displaystyle A} (das VerhÃ¤ltnis der maximalen zu minimalen Eigenwerte von A T A {\displaystyle A^{T}A ), wÃ¤hrend die Konvergenz der konjugierten Gradientenmethode typischerweise durch eine quadratische Wurzel der Zustandszahl bestimmt wird, d. Beide Methoden kÃ¶nnen von der Vorkonditionierung profitieren, wobei die Gradientenabstieg mÃ¶glicherweise weniger Annahmen auf dem Vorkonditionierer erfordern. Eine LÃ¶sung eines nichtlinearen Systems Gradientenabstieg kann auch zur LÃ¶sung eines Systems nichtlinearer Gleichungen verwendet werden. Unten ist ein Beispiel, das zeigt, wie man den Gradientenabstieg fÃ¼r drei unbekannte Variablen, x1, x2, und x3 verwendet. Dieses Beispiel zeigt eine Iteration des Gradientenabstiegs. Betrachten Sie das nichtlineare System der Gleichungen { 3 x 1 âˆ’ cos â€¡ ( x 2 x 3 ) âˆ’ 3 2 = 0 4 x 1 2 âˆ’ 625 x 2 + 2 x 2 âˆ’ 1 = 0 exp â‰  ( âˆ’ x 1 x 2 ) + 20 x 3 + 10 Ï€ âˆ’ 3 = 0 {\displaystyle Anfang {cases}3x_{1}-\cos(x_{2}x_{3})-{\tfrac 3}{2}=0\4x_{1}{2}-625x_{2}{2}+2x_{2}-1=0\\\exp(-x_{1}x_{2}+20x_{3}+{\c\c\c\c\c\c(-x_{1}x_{2})+20x_{3}+{\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\ -3{3}===Legen Lassen Sie uns die zugehÃ¶rige Funktion G ( x ) = [ 3 x 1 âˆ’ cos Ä‹ ( x 2 x 3 ) âˆ’ 3 2 4 x 1 2 âˆ’ 625 x 2 2 2+ 2 x 2 - 1 exp ( âˆ’ x 1 x 2 ) + 20 x 3 + 10 Ï€ âˆ’ 3 3 ], {\displaystyle G(\mathbf {x} = begin{bmatrix}3x_{1}-\cos(x_{2}x_{3})-{\tfrac 3}{2}{2}\\4x_{1}{2}-625x_{2}{2}}{2}x_{2}-1\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}\\\\\d{bmatrix, wobei x = [ x 1 x 2 x 3 x 3] {\displaystyle \mathbf {x} =begin{bmatrix}x_{1}\x_{2}\x_{3}\\\end{bmatrix. Man kÃ¶nnte nun die objektive Funktion F ( x ) = 1 2 G T ( x ) G ( x ) = 1 2 [ ( 3 x 1 - cos Ä‹ ( x 2 x 3 ) - 3 2 ) 2 + ( 4 x 1 2 - 625 x 2 + 2 x 2 - 1 ) 2 + ( exp x 2 x 2 ) + 20 x 3 + 10 Ï€ - 3 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 1}{2}G^{\mathrm {T} (}\mathbf {x} )G(\mathbf {x} ){=\frac 1}{2}}\left(3x_{1}-\cos(x_{2}x_{3}-{\frac 3}{2}}{2}}{2}{2}}}}{2}}}}}}}}}}}}}}}}}}}}}}}\\\\\\\\\\\\\c\c\c\c\c\c\c}}}}}}}}}}}}\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c (4x_{1}^{2}-625x_{2}{2}}+2x_{2}-1\right)^{2}+\left(\exp(-x_{1}x_{2})+20x_{3}+{\frac {10\pi -3}}\right)^^{2}\right], die wir zu minimieren versuchen. Lassen Sie uns als erstes raten, x ( 0 ) = 0 = [ 0 0 0 ] . {\displaystyle \mathbf {x} ^{(0)}=\mathbf {0}=begin{bmatrix}0\\\0\\\\end{bmatrix. (x) (x_{2}x_{3}x_{2}\8x_{1}&-1250x_{2}+2&0\\-x_{2}\exp x_{1}x_{2})}&-x_{1}\exp(-x_{1}x_{2})&20\\end{bmatrix. Wir berechnen: J G ( 0 ) = [ 3 0 0 2 0 0 0 20], G ( 0 ) = [ - 2.5 - 1 10.472] . = 0 - Î³ 0 [ - 7.5 - 2 209.44 ], {\displaystyle \mathbf {x}{(1)}=\mathbf {0} -\gamma_0}{b\f\f\b\\2\209.44\d{bmatrix und F (0 ) = 0,5 (rechts - 2.5 ) Nun muss eine geeignete Î³ 0 {\displaystyle \gamma {_0} so gefunden werden, dass F ( x ( 1 ) ) â‰¤ F ( x ( 0 ) ) = F ( 0 ) . Dies kann mit einer Vielzahl von Zeilen-Suchalgorithmen geschehen. Man kÃ¶nnte auch einfach Î³ 0 = 0,001 , {\displaystyle \gamma {_0}=0.001 erraten, was x ( 1 ) = [ 0.0075 0.002 - 0.20944 ] gibt. Bewertung der Zielfunktion zu diesem Wert, Ausbeuten F ( x ( 1 ) = 0,5 ( ( - 2.48 ) 2 + ( - 1.00 ) 2 + ( 6.28 ) 2 ) = 23.306 {\displaystyle F\left(\mathbf {x} 1)}\right) = 0.5\left((-2.48)^{2}+(-1.00)^{2}+}} Die Abnahme von F ( 0 ) = 58.456 {\displaystyle F(\mathbf {0} =)58.456} auf den nÃ¤chsten Schrittwert von F ( x ( 1 ) ) = 23.306 {\displaystyle F\left(\mathbf {x} ^{(1)}\right)=23.306 ist eine sisierbare Abnahme der Objektivfunktion. Weitere Schritte wÃ¼rden den Wert weiter reduzieren, bis eine ungefÃ¤hre LÃ¶sung des Systems gefunden wurde. Kommentare Gradienten Abstieg arbeitet in RÃ¤umen jeder Anzahl von Dimensionen, auch in unendlichen Dimensionen. Im letzteren Fall ist der Suchraum typischerweise ein Funktionsraum, und man berechnet das FrÃ©chet-Derivat der zu minimierenden Funktion zur Bestimmung der AbwÃ¤rtsrichtung. Dieser Gradientenabstieg funktioniert in jeder Anzahl von Dimensionen (mindestens Anzahl) kann als Folge der Cauchy-Schwarz Ungleichheit gesehen werden. Dieser Artikel beweist, dass die GrÃ¶ÃŸe des inneren (dot) Produkts von zwei Vektoren jeder Dimension maximiert wird, wenn sie kolinear sind. Bei Gradientenabstieg, d.h. wenn der Vektor unabhÃ¤ngiger variabler Einstellungen proportional zum Gradientenvektor von Teilderivaten ist. Der Gradientenabstieg kann viele Iterationen nehmen, um ein lokales Minimum mit einer erforderlichen Genauigkeit zu berechnen, wenn die KrÃ¼mmung in verschiedenen Richtungen fÃ¼r die gegebene Funktion sehr unterschiedlich ist. FÃ¼r solche Funktionen hÃ¤rtet die Vorkonditionierung, die die Geometrie des Raumes Ã¤ndert, um die Funktionsebene wie konzentrische Kreise zu formen, die langsame Konvergenz. Der Aufbau und das Anlegen von Vorkonditionierungen kann jedoch rechnerisch teuer sein. Der Gradientenabstieg kann mit einer Liniensuche kombiniert werden, wobei die lokal optimale SchrittgrÃ¶ÃŸe Î³ {\displaystyle \gamma } bei jeder Iteration gefunden wird. Die DurchfÃ¼hrung der Zeilensuche kann zeitraubend sein. Umgekehrt kann mit einem festen kleinen Î³ {\displaystyle \gamma } eine schlechte Konvergenz erzielen. Methoden basierend auf Newtons Methode und Inversion des Hessischen mit konjugierten Gradiententechniken kÃ¶nnen bessere Alternativen sein. In der Regel konvergieren solche Verfahren in weniger Iterationen, aber die Kosten jeder Iteration ist hÃ¶her. Ein Beispiel ist die BFGS-Methode, die darin besteht, auf jedem Schritt eine Matrix zu berechnen, mit der der Gradientenvektor multipliziert wird, in eine bessere Richtung zu gehen, kombiniert mit einem ausgefeilteren Zeilensuchalgorithmus, um den besten Wert von Î³ zu finden. {\displaystyle \gamma . Bei extrem groÃŸen Problemen, bei denen die Computer-Memory-Probleme dominieren, sollte anstelle von BFGS oder dem steilsten Abstieg eine limitierte Methode wie L-BFGS verwendet werden. Der Gradientenabstieg kann als Anwendung von Eulers Methode zur LÃ¶sung von gewÃ¶hnlichen Differentialgleichungen x' (t ) = - MENT f ( x (t ) ) {\displaystyle x'(t)=-\nabla f(x(t}) auf einen Gradientenfluss angesehen werden. Diese Gleichung kann wiederum als optimaler Regler fÃ¼r das Steuerungssystem x' (t ) = u (t ) {\displaystyle x'(t)=u(t} mit u (t ) {\displaystyle u(t}) in RÃ¼ckkopplungsform u (t ) = - MENT f ( x (t )) ) abgeleitet werden.{\displaystyle u(t)=-\nabla f(x(t}) . Modifications Gradient Abstieg kann auf ein lokales Minimum konvergieren und in einer Nachbarschaft eines Sattelpunktes verlangsamen. Auch fÃ¼r unkonstrainierte quadratische Minimierung entwickelt sich der Gradientenabstieg ein Zick-Zack-Muster von nachfolgenden Iteraten, wenn es zu einer Verlangsamung kommt. Es wurden mehrere Ã„nderungen des Gradientenabstiegs vorgeschlagen, um diese MÃ¤ngel zu beheben. Schnelle Gradientenmethoden Yurii Nesterov hat eine einfache Modifikation vorgeschlagen, die eine schnellere Konvergenz bei konvexen Problemen ermÃ¶glicht und seitdem weiter verallgemeinert ist. Bei ungehinderten glatten Problemen wird das Verfahren als schnelle Gradientenmethode (FGM) oder als beschleunigte Gradientenmethode (AGM) bezeichnet. Wenn nÃ¤mlich die differenzierbare Funktion F {\displaystyle F} konvex ist und F {\displaystyle \nabla F} Lipschitz ist, und es wird nicht angenommen, dass F {\displaystyle F} stark konvex ist, dann wird der Fehler in dem bei jedem Schritt k\displaystyle k} durch die Gradientenabstiegsmethode erzeugten objektiven Wert durch O ( 1 k) Es ist bekannt, dass die Rate O (k - 2 ) {\displaystyle {\mathcal O}}\left({k^{-2}\right) fÃ¼r die Abnahme der Kostenfunktion optimal fÃ¼r Optimierungsmethoden erster Ordnung ist. Dennoch besteht die MÃ¶glichkeit, den Algorithmus zu verbessern, indem der konstante Faktor reduziert wird. Die optimierte Gradientenmethode (OGM) reduziert diese Konstante um einen Faktor von zwei und ist ein optimales Verfahren erster Ordnung fÃ¼r groÃŸtechnische Probleme. Bei eingeschrÃ¤nkten oder nicht-glÃ¤ttenden Problemen wird Nesterovs FGM als schnelle proximale Gradientenmethode (FPGM) eine Beschleunigung der proximalen Gradientenmethode bezeichnet. Momentum oder schwere Kugel-Methode Versuchen, das Zick-Zack-Muster der Gradientenabstieg zu brechen, verwendet die Dynamik oder schwere Kugel-Methode in Analogie zu einer schweren Kugel gleiten auf der OberflÃ¤che von Werten der Funktion minimiert wird, oder zu Massenbewegung in Newtonischer Dynamik durch ein viskoses Medium in einem konservativen Kraftfeld. Gradienter Abstieg mit Impuls erinnert an das LÃ¶sungsupdate bei jeder Iteration und bestimmt das nÃ¤chste Update als lineare Kombination des Gradienten und des vorherigen Updates. FÃ¼r eine unkonstrainierte quadratische Minimierung ist eine theoretische Konvergenzrate der Schwerkugelmethode asymptotisch gleich wie bei der optimalen konjugierten Gradientenmethode. Diese Technik wird in Stochastic Gradientenabstieg verwendet # Momentum und als Erweiterung auf die Backpropagation Algorithmen verwendet, um kÃ¼nstliche neuronale Netzwerke zu trainieren. Erweiterungen Gradient Abstieg kann erweitert werden, um ZwÃ¤nge zu handhaben, indem eine Projektion auf den Satz der ZwÃ¤nge. Dieses Verfahren ist nur dann mÃ¶glich, wenn die Projektion auf einem Computer effizient berechnet wird. Unter geeigneten Annahmen konvergiert diese Methode. Dieses Verfahren ist ein spezieller Fall des VorwÃ¤rts-RÃ¼ckwÃ¤rts-Algorithmus fÃ¼r Monoton-EinschlÃ¼sse (der konvexe Programmierung und variierende Ungleichheiten umfasst). Siehe auch Referenzen Weiter lesen Boyd, Stephen; Vandenberghe, Lieven (2004)." UnbeschrÃ¤nkte Minimierung" (PDF). Convex Optimierung. New York: Cambridge University Press.pp.457â€“520.ISBN 0-521-83378-7. Chong, Edwin K. P;. Å»ak, Stanislaw H. (2013)."Gradient Methods". Eine EinfÃ¼hrung in die Optimierung (Fourth ed.). Hoboken: Wiley.pp.131â€“160.ISBN 978-1-118-27901-4.Himmelblau, David M. (1972). "UnbeschrÃ¤nkte Minimierungsverfahren mit Derivaten". Angewandte nichtlineare Programmierung. New York: McGraw-Hill.pp.63â€“132.ISBN 0-07-028921-2. Externe Links Mit Gradientenabstieg in C,+ Boost, Ublas for linear regression Series of Khan Academy videos diskutieren Gradientenabstieg Online Buch Lehrgradientenabstieg in tiefen neuronalen Netzwerkkontext "Gradient Descent, How Neural Networks Learn".3Blue1Brown.October 16, 2017 â€“ via YouTube