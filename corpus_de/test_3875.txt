Gradientenabstieg ist ein iterativer Optimierungsalgorithmus erster Ordnung, um ein lokales Minimum einer differenzierbaren Funktion zu finden. Der Gedanke besteht darin, wiederholte Schritte in die entgegengesetzte Richtung des Gradienten (oder ungef√§hrer Gradienten) der Funktion am aktuellen Punkt zu unternehmen, da dies die Richtung steilster Abstieg ist. Umgekehrt f√ºhrt das Schritten in Richtung des Gradienten zu einem lokalen Maximum dieser Funktion; das Verfahren wird dann als Gradientenanhebung bezeichnet. Gradient Abstieg wird in der Regel Cauchy zugeschrieben, die es erst in 1847 vorgeschlagen. Hadamard hat 1907 selbst eine √§hnliche Methode vorgeschlagen. Seine Konvergenzeigenschaften f√ºr nichtlineare Optimierungsprobleme wurden von Haskell Curry im Jahr 1944 zun√§chst untersucht, wobei die Methode in den folgenden Jahrzehnten zunehmend gut untersucht und genutzt wurde, oft auch steilster Abstieg genannt. Beschreibung Gradient Abstieg basiert auf der Beobachtung, dass, wenn die multivariable Funktion F ( x ) {\displaystyle F(\mathbf {x} )} in einer Nachbarschaft eines Punktes definiert und differenzierbar ist ein {\displaystyle \mathbf {a}, dann F ( x ) Daraus folgt, dass, wenn eine n + 1 = n - Œ≥ MENT F (a n ) {\displaystyle \mathbf {a} {_n+1}=\mathbf {a} {_n}-\gamma \nabla F(\mathbf {a} {_n}) f√ºr ein Œ≥ Œµ R + {\displaystyle \gamma in\mathbb {R} _{+} klein genug, dann F (a n ) ‚â• F (a n + 1 ) {\displaystyle F(\mathbf a_{n} )\geq F(\mathbf a_{n+1}}} .Mit anderen Worten ist der Begriff Œ≥ üòâ F (a) {\displaystyle \gamma \nabla F(\mathb} Bei dieser Beobachtung geht man mit einer Vermutung x 0 {\displaystyle \mathbf {x} {_0} f√ºr ein lokales Minimum von F {\displaystyle F} und betrachtet die Sequenz x 0, x 1 , x 2 , ... {\displaystyle \mathbf {x},\mathb {x} {x} {x}} - Nein. - Ja. Wir haben eine monotone Sequenz F ( x 0 ) ‚â• F ( x 1 ) ‚â• F ( x 2 ) ‚â• ‚ãØ, {\displaystyle F(\mathbf {x} {_0})\geq F(\mathbf {x})\geq F(\mathbf {x} {_2})\geq \cdots sequenz,} so dass Beachten Sie, dass der Wert der Schrittgr√∂√üe Œ≥ {\displaystyle \gamma } bei jeder Iteration ge√§ndert werden darf. Bei bestimmten Annahmen auf der Funktion F {\displaystyle F} (z.B. F {\displaystyle F} konvex und MENT F\displaystyle \nabla F} Lipschitz) und bestimmten Wahlen von Œ≥ {\displaystyle \gamma } (z.B. √ºber eine Liniensuche, die die Wolfe-Bedingungen erf√ºllt, oder die Barzilai-Borwein-Methode wie folgt F(\mathbf {x} {_n})-\nabla F(\mathbf {x}_n-1})\right\|^{2 Konvergenz auf ein lokales Minimum kann gew√§hrleistet werden. Wenn die Funktion F {\displaystyle F} konvex ist, sind alle lokalen Minima auch globale Minima, so kann in diesem Fall Gradientenabstieg zur globalen L√∂sung konvergieren. Dieser Vorgang ist im benachbarten Bild dargestellt. Dabei wird F {\displaystyle F} auf der Ebene definiert und sein Diagramm eine Schalenform aufweist. Die blauen Kurven sind die Konturlinien, d.h. die Bereiche, auf denen der Wert von F {\displaystyle F} konstant ist. Ein roter Pfeil, der an einer Stelle beginnt, zeigt die Richtung des negativen Gradienten an dieser Stelle. Beachten Sie, dass der (negative) Gradient an einem Punkt orthogonal zu der Konturlinie ist, die durch diesen Punkt geht. Wir sehen, dass Gradientenabstieg uns zum Boden der Sch√ºssel f√ºhrt, also zu dem Punkt, wo der Wert der Funktion F {\displaystyle F} minimal ist. Eine Analogie zum Verst√§ndnis Gradientenabstieg Die grundlegende Intuition hinter Gradientenabstieg kann durch ein hypothetisches Szenario illustriert werden. Eine Person steckt in den Bergen fest und versucht, sich zu begeben (d.h. das globale Minimum zu finden). Es gibt einen schweren Nebel, so dass die Sichtbarkeit extrem gering ist. Daher ist der Weg hinunter den Berg nicht sichtbar, so dass sie lokale Informationen verwenden m√ºssen, um das Minimum zu finden. Sie k√∂nnen die Methode der Gradientenabstieg verwenden, die die Steilheit des H√ºgels an ihrer aktuellen Position betrachtet, dann in Richtung mit der steilsten Abstieg (d.h. bergab). Wenn sie versuchen, die Spitze des Berges (d.h. das Maximum) zu finden, dann w√ºrden sie in Richtung steilsten Aufstieg (d.h. bergauf) gehen. Mit dieser Methode w√ºrden sie schlie√ülich ihren Weg hinunter den Berg finden oder m√∂glicherweise in einem Loch (d.h. lokales Minimum oder Sattelpunkt) wie ein Bergsee stecken. Nehmen Sie jedoch auch an, dass die Steilheit des H√ºgels nicht sofort mit einfacher Beobachtung offensichtlich ist, sondern es erfordert ein ausgekl√ºgeltes Instrument zu messen, das die Person im Moment hat. Es braucht einige Zeit, um die Steilheit des H√ºgels mit dem Instrument zu messen, so sollten sie ihre Verwendung des Instruments minimieren, wenn sie den Berg vor Sonnenuntergang hinunter wollen. Die Schwierigkeit ist dann, die Frequenz zu w√§hlen, mit der sie die Steilheit des H√ºgels messen sollten, um nicht von der Bahn zu gehen. In dieser Analogie stellt die Person den Algorithmus dar, und der Weg, der den Berg hinunter genommen wird, stellt die Reihenfolge der Parametereinstellungen dar, die der Algorithmus erforscht. Die Steilheit des H√ºgels stellt die Steigung der Fehlerfl√§che an diesem Punkt dar. Das zur Steilheitsmessung verwendete Instrument ist eine Differenzierung (die Neigung der Fehlerfl√§che kann durch Ableitung der quadratischen Fehlerfunktion an diesem Punkt berechnet werden). Die Richtung, in der sie sich entscheiden, in fluchtet mit dem Gradienten der Fehlerfl√§che an dieser Stelle. Die Zeit, die sie vor einer weiteren Messung zur√ºcklegen, ist die Stufengr√∂√üe. Beispiele Gradientenabstieg hat Probleme mit pathologischen Funktionen wie der hier gezeigten Rosenbrock-Funktion. f ( x 1 , x 2 ) = ( 1 - x 1 ) 2 + 100 ( x 2 - x 1 2 ) 2 . Die Rosenbrock-Funktion hat ein schmales, gekr√ºmmtes Tal, das das Minimum enth√§lt. Der Talboden ist sehr flach. Durch das geschwungene Flachtal wird die Optimierung langsam mit kleinen Stufengr√∂√üen auf das Minimum hin zickzackiert. Die zickzackierende Natur des Verfahrens ist weiter unten ersichtlich, wobei die Gradientenabstiegsmethode auf F ( x, y) = sin ‚Ä° ( 1 2 x 2 - 1 4 y 2 + 3 ) cos ‚â† ( 2 x + 1 - e y ) angewendet wird. 1{4}y^{2}+3\right)\cos links(2x+1-e^{y}\right). Die Wahl der Schrittgr√∂√üe und Abw√§rtsrichtung Da die Verwendung einer zu kleinen Schrittgr√∂√üe Œ≥ {\displaystyle \gamma } die Konvergenz langsamer w√ºrde, und eine zu gro√üe Œ≥ {\displaystyle \gamma } w√ºrde zu Divergenz f√ºhren, eine gute Einstellung von Œ≥ {\displaystyle \gamma } zu finden ist ein wichtiges praktisches Problem. Philip Wolfe bef√ºrwortete in der Praxis auch die Verwendung von "sauberen Wahlen der [absteigenden] Richtung". W√§hrend mit einer Richtung, die von der steilsten Abstiegsrichtung abweicht, kontra-intuitiv erscheinen kann, ist die Idee, dass die kleinere Steigung durch eine viel l√§ngere Strecke ausgeglichen werden kann. Um dies mathematisch zu begr√ºnden, verwenden wir eine Richtung p n {\displaystyle \mathbf {p} {_n} und Schrittgr√∂√üe Œ≥ n {\displaystyle \gamma {_n} und betrachten das allgemeinere Update: a n + 1 = a n - Œ≥ n p n {\displaystyle \mathbf {a} {_n+1}=\mathbf {a} - Nein. {_n}\,\mathbf {p} {_n} . Gute Einstellungen von p n {\displaystyle \mathbf {p} {_n} und Œ≥ n {\displaystyle \gamma {_n} erfordern einen kleinen Gedanken. Zun√§chst m√∂chten wir, dass die Update-Richtung bergab zeigt. Mathematisch bedeutet das Verlassen von Œ∏ n {\displaystyle \theta {_n} den Winkel zwischen MENT F ( a n ) {\displaystyle \nabla F(\mathbf a_{n}} und p n {\displaystyle \mathbf {p} {_n} , dies erfordert, dass cos 0. {\displaystyle \cos \theta {_n}>0. Um mehr zu sagen, brauchen wir mehr Informationen √ºber die Zielfunktion, die wir optimieren. Unter der ziemlich schwachen Annahme, dass F {\displaystyle F} kontinuierlich differenzierbar ist, k√∂nnen wir beweisen, dass: Diese Ungleichheit bedeutet, dass der Betrag, um den wir sicher sein k√∂nnen, dass die Funktion F {\displaystyle F} verringert wird, von einem Handel zwischen den beiden Begriffen in quadratischen Klammern abh√§ngt. Der erste Begriff in quadratischen Klammern misst den Winkel zwischen der Abw√§rtsrichtung und dem negativen Gradienten. Der zweite Begriff misst, wie schnell sich der Gradient entlang der Abw√§rtsrichtung √§ndert. Grunds√§tzlich k√∂nnte Ungleichheit (1) √ºber p n {\displaystyle \mathbf {p} {_n} und Œ≥ n {\displaystyle \gamma {_n} optimiert werden, um eine optimale Schrittgr√∂√üe und -richtung zu w√§hlen. Problematisch ist, dass die Auswertung des zweiten Begriffs in quadratischen Klammern die Auswertung von MENT F (a n - t Œ≥ n p n ) {\displaystyle \nabla F(\mathbf {a} {_n}-t\gamma {_n}\mathbf {p} {_n}) erfordert und zus√§tzliche Gradientenauswertungen in der Regel teuer und unerw√ºnscht sind. Einige Wege um dieses Problem sind: Forgo die Vorteile einer cleveren Abstiegsrichtung durch Einstellung p n = MENT F ( a n ) {\displaystyle \mathbf {p} {_n}=\nabla F(\mathbf a_{n}} , und verwenden Sie die Zeilensuche, um eine geeignete Schritt-Gr√∂√üe Œ≥ n {\displaystyle \gamma_n} zu finden, wie eine, die die die die die die die die die Bedingungen erf√ºllt. Unter der Annahme, dass F {\displaystyle F} doppelt differenzierbar ist, verwenden Sie seine Hessian MENT 2 F {\displaystyle \nabla ^{2}F, um ‚Ä° ermittelt üòâ F (a n - t Œ≥ n p n ) - üòâ F (a n ) gebildet 2 ‚âà Œ∫ t œá n üòâp n . {\displaystyle |\nabla F(\mathbf {a} - Nein. {_n}\mathbf {p} {_n}-\nabla F(\mathbf} n}\)_{2}\approx |t\gamma {_n}\nabla - Nein. W√§hlen Sie dann p n {\displaystyle \mathbf {p} {_n} und Œ≥ n {\displaystyle \gamma {_n} durch Optimierung der Ungleichheit (1). Unter der Annahme, dass MENT F {\displaystyle \nabla F} Lipschitz ist, verwenden Sie seine Lipschitz-Konstante L {\displaystyle L} zu gebunden üòâ F ( a n - t Œ≥ n p n ) ‚àí üòâ F ( a n ) ‚Ä° L t Œ≥ n geschichte n p n . . {\displaystyle |\nabla F(\mathbf} - Nein. {_n}\mathbf {p} {_n})-\nabla F(\mathbf {a} n}\|_{2}\leq Lt\gamma {_n}\|\\\mathbf {p} {_n|}\. W√§hlen Sie dann p n {\displaystyle \mathbf {p} {_n} und Œ≥ n {\displaystyle \gamma {_n} durch Optimierung der Ungleichheit (1). Bauen Sie ein benutzerdefiniertes Modell von max t ‚àà [0, 1 ] gebildet ‚ô¶ F ( a n - t Œ≥ n p n ) ‚àí ‚Ä° F ( a n ) ‚Ä° 2 Œ∫ Œ∫ F ( a n ) 2 {\displaystyle \max {_t\in [0,1]}\frac |{\nabla F(\mathbf {a} - Nein. {_n}\mathbf {p} {_n})-\nabla F(\mathbf {a} n}\|_{2}{\|\nabla F(\mathbf {a} n}\|\)_{2 f√ºr F {\displaystyle F} .Dann w√§hlen Sie p n {\displaystyle \mathbf {p} {_n} und Œ≥ n {\displaystyle \gamma {_n} durch Optimierung der Ungleichheit (1). Unter st√§rkeren Annahmen auf der Funktion F {\displaystyle F} wie Konvexit√§t k√∂nnen fortgeschrittene Techniken m√∂glich sein. Gew√∂hnlich kann durch Nachfolge eines der oben genannten Rezepte die Konvergenz zu einem lokalen Minimum gew√§hrleistet werden. Wenn die Funktion F {\displaystyle F} konvex ist, sind alle lokalen Minima auch globale Minima, so kann in diesem Fall Gradientenabstieg zur globalen L√∂sung konvergieren. L√∂sung eines linearen Systems Gradient Abstieg kann verwendet werden, um ein System von linearen Gleichungen A x - b = 0 {\displaystyle - Ja. =0} als quadratisches Minimierungsproblem reformiert. Ist die Systemmatrix A {\displaystyle A} real symmetrisch und positiv-definit, ist die quadratische Funktion, um allgemein zu minimieren F (x) = x T A x - 2 x T b, {\displaystyle F(\mathbf {x} =)\mathbf {x} ^{T}A\mathbf {x} -2\mathbf {x} ^{T}\mathbf {b},} so dass ‚Ä° F (x) = 2 (A x ‚àí b) . {\displaystyle \nabla F(\mathbf {x} =)2(A\mathbf {x} -mathbf} F√ºr eine allgemeine reale Matrix A {\displaystyle A} definieren lineare kleinste Quadrate F ( x ) = A x - b zusammengestellt 2 . In traditionellen linearen kleinsten Quadraten f√ºr reale A {\displaystyle A} und b {\displaystyle \mathbf {b} wird die Euclidean-Norm verwendet, in welchem Fall MENT F (x) = 2 A T (A x - b) . {\displaystyle \nabla F(\mathbf {x} =)2A^{T}(A\mathbf {x} -\mathbf {b}}}}} Die Liniensucheminimierung, die lokal optimale Schrittgr√∂√üe Œ≥ {\displaystyle \gamma } bei jeder Iteration zu finden, kann analytisch f√ºr quadratische Funktionen durchgef√ºhrt werden, und es sind explizite Formeln f√ºr die lokal optimale Œ≥ {\displaystyle \gamma } bekannt. Beispielsweise kann f√ºr die reale symmetrische und positiv definierte Matrix A {\displaystyle A} ein einfacher Algorithmus wie folgt sein, in der Schleife wiederholen: r := b - A x Œ≥:= r T r / r T A r x := x + Œ≥ r wenn r T r ist ausreichend klein, dann Ausgang-Schleife-End-Repeat x als Ergebnis {\displaystyle begin{align}&{\text{repeat in der Schlaufe:}\\&\qquad \mathbf {r} \=:mathbf {b} - Ja. \{=:mathbf {r} ^{\mathsf {T}\mathbf {r} }{\/mathbf {r} ^{\mathf {T}\mathbf {Ar} }&\qquad \mathbf {x} \=mathbf {x} +\gamma \mathbf {r} &\qquad \hbox{if }\mathbf {r} ^{\mathsf {T}\mathbf {r} \text ist ausreichend klein, dann ausgangsschleife}\\&{\text{end wiederholschleife}\\\&{text{return }\mathbf {x} \text als Ergebnis}\end{ausgerichtet Um die Multiplikation mit A {\displaystyle A} zweimal pro Iteration zu vermeiden, beachten wir, dass x := x + Œ≥ r {\displaystyle \mathbf {x} \=:mathbf {x} +\gamma \mathbf {r} impliziert r A r {\displaystyle \mathbf {r} \=:mathbf {r} -\gamma \mathbf {Ar} , die den herk√∂mmlichen Algorithmus gibt, r:= b - A x Wiederholung in der Schleife: Œ≥ := r T r / r T A r x := x + Œ≥ r, wenn r T r ausreichend klein ist, dann Austrittsschleife r:= r - Œ≥ Ein r End-Repeatschleife zur√ºck x als Ergebnis {\displaystyle begin{aligned}&\mathbf {r} \=:mathbf {b} -\mathbf {Ax} &\text{repeat in der Schleife:}\&\qquad \gamma \{=:mathbf r}^{\mathf {T}}\mathbf {r} }{\/mathbf {r} ^{\mathf {T}\mathbf {Ar} }&\qquad \mathbf {x} \=mathbf {x} +\gamma \mathbf {r} &\qquad \hbox{if}\mathbf {r} ^{\mathsf {T}\mathbf {r} \text ist ausreichend klein, dann ausgangsschleife}\\\&\qquad \mathbf {r} \=:mathbf {r} - Gamma \mathbf {Ar} &\text{end Repeatschleife}\&{\text{return}\mathbf {x} \text als Ergebnis}\end{ausgerichtet Das Verfahren wird selten zur L√∂sung von linearen Gleichungen verwendet, wobei die konjugierte Gradientenmethode eine der beliebtesten Alternativen ist. Die Anzahl der Gradientenabstiegs Iterationen ist √ºblicherweise proportional zur spektralen Zustandszahl Œ∫ (A ) {\displaystyle \kappa (A}) der Systemmatrix A {\displaystyle A} (das Verh√§ltnis der maximalen zu minimalen Eigenwerte von A T A {\displaystyle A^{T}A ), w√§hrend die Konvergenz der konjugierten Gradientenmethode typischerweise durch eine quadratische Wurzel der Zustandszahl bestimmt wird, d. Beide Methoden k√∂nnen von der Vorkonditionierung profitieren, wobei die Gradientenabstieg m√∂glicherweise weniger Annahmen auf dem Vorkonditionierer erfordern. Eine L√∂sung eines nichtlinearen Systems Gradientenabstieg kann auch zur L√∂sung eines Systems nichtlinearer Gleichungen verwendet werden. Unten ist ein Beispiel, das zeigt, wie man den Gradientenabstieg f√ºr drei unbekannte Variablen, x1, x2, und x3 verwendet. Dieses Beispiel zeigt eine Iteration des Gradientenabstiegs. Betrachten Sie das nichtlineare System der Gleichungen { 3 x 1 ‚àí cos ‚Ä° ( x 2 x 3 ) ‚àí 3 2 = 0 4 x 1 2 ‚àí 625 x 2 + 2 x 2 ‚àí 1 = 0 exp ‚â† ( ‚àí x 1 x 2 ) + 20 x 3 + 10 œÄ ‚àí 3 = 0 {\displaystyle Anfang {cases}3x_{1}-\cos(x_{2}x_{3})-{\tfrac 3}{2}=0\4x_{1}{2}-625x_{2}{2}+2x_{2}-1=0\\\exp(-x_{1}x_{2}+20x_{3}+{\c\c\c\c\c\c(-x_{1}x_{2})+20x_{3}+{\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\ -3{3}===Legen Lassen Sie uns die zugeh√∂rige Funktion G ( x ) = [ 3 x 1 ‚àí cos ƒã ( x 2 x 3 ) ‚àí 3 2 4 x 1 2 ‚àí 625 x 2 2 2+ 2 x 2 - 1 exp ( ‚àí x 1 x 2 ) + 20 x 3 + 10 œÄ ‚àí 3 3 ], {\displaystyle G(\mathbf {x} = begin{bmatrix}3x_{1}-\cos(x_{2}x_{3})-{\tfrac 3}{2}{2}\\4x_{1}{2}-625x_{2}{2}}{2}x_{2}-1\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}\\\\\d{bmatrix, wobei x = [ x 1 x 2 x 3 x 3] {\displaystyle \mathbf {x} =begin{bmatrix}x_{1}\x_{2}\x_{3}\\\end{bmatrix. Man k√∂nnte nun die objektive Funktion F ( x ) = 1 2 G T ( x ) G ( x ) = 1 2 [ ( 3 x 1 - cos ƒã ( x 2 x 3 ) - 3 2 ) 2 + ( 4 x 1 2 - 625 x 2 + 2 x 2 - 1 ) 2 + ( exp x 2 x 2 ) + 20 x 3 + 10 œÄ - 3 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 1}{2}G^{\mathrm {T} (}\mathbf {x} )G(\mathbf {x} ){=\frac 1}{2}}\left(3x_{1}-\cos(x_{2}x_{3}-{\frac 3}{2}}{2}}{2}{2}}}}{2}}}}}}}}}}}}}}}}}}}}}}}\\\\\\\\\\\\\c\c\c\c\c\c\c}}}}}}}}}}}}\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c (4x_{1}^{2}-625x_{2}{2}}+2x_{2}-1\right)^{2}+\left(\exp(-x_{1}x_{2})+20x_{3}+{\frac {10\pi -3}}\right)^^{2}\right], die wir zu minimieren versuchen. Lassen Sie uns als erstes raten, x ( 0 ) = 0 = [ 0 0 0 ] . {\displaystyle \mathbf {x} ^{(0)}=\mathbf {0}=begin{bmatrix}0\\\0\\\\end{bmatrix. (x) (x_{2}x_{3}x_{2}\8x_{1}&-1250x_{2}+2&0\\-x_{2}\exp x_{1}x_{2})}&-x_{1}\exp(-x_{1}x_{2})&20\\end{bmatrix. Wir berechnen: J G ( 0 ) = [ 3 0 0 2 0 0 0 20], G ( 0 ) = [ - 2.5 - 1 10.472] . = 0 - Œ≥ 0 [ - 7.5 - 2 209.44 ], {\displaystyle \mathbf {x}{(1)}=\mathbf {0} -\gamma_0}{b\f\f\b\\2\209.44\d{bmatrix und F (0 ) = 0,5 (rechts - 2.5 ) Nun muss eine geeignete Œ≥ 0 {\displaystyle \gamma {_0} so gefunden werden, dass F ( x ( 1 ) ) ‚â§ F ( x ( 0 ) ) = F ( 0 ) . Dies kann mit einer Vielzahl von Zeilen-Suchalgorithmen geschehen. Man k√∂nnte auch einfach Œ≥ 0 = 0,001 , {\displaystyle \gamma {_0}=0.001 erraten, was x ( 1 ) = [ 0.0075 0.002 - 0.20944 ] gibt. Bewertung der Zielfunktion zu diesem Wert, Ausbeuten F ( x ( 1 ) = 0,5 ( ( - 2.48 ) 2 + ( - 1.00 ) 2 + ( 6.28 ) 2 ) = 23.306 {\displaystyle F\left(\mathbf {x} 1)}\right) = 0.5\left((-2.48)^{2}+(-1.00)^{2}+}} Die Abnahme von F ( 0 ) = 58.456 {\displaystyle F(\mathbf {0} =)58.456} auf den n√§chsten Schrittwert von F ( x ( 1 ) ) = 23.306 {\displaystyle F\left(\mathbf {x} ^{(1)}\right)=23.306 ist eine sisierbare Abnahme der Objektivfunktion. Weitere Schritte w√ºrden den Wert weiter reduzieren, bis eine ungef√§hre L√∂sung des Systems gefunden wurde. Kommentare Gradienten Abstieg arbeitet in R√§umen jeder Anzahl von Dimensionen, auch in unendlichen Dimensionen. Im letzteren Fall ist der Suchraum typischerweise ein Funktionsraum, und man berechnet das Fr√©chet-Derivat der zu minimierenden Funktion zur Bestimmung der Abw√§rtsrichtung. Dieser Gradientenabstieg funktioniert in jeder Anzahl von Dimensionen (mindestens Anzahl) kann als Folge der Cauchy-Schwarz Ungleichheit gesehen werden. Dieser Artikel beweist, dass die Gr√∂√üe des inneren (dot) Produkts von zwei Vektoren jeder Dimension maximiert wird, wenn sie kolinear sind. Bei Gradientenabstieg, d.h. wenn der Vektor unabh√§ngiger variabler Einstellungen proportional zum Gradientenvektor von Teilderivaten ist. Der Gradientenabstieg kann viele Iterationen nehmen, um ein lokales Minimum mit einer erforderlichen Genauigkeit zu berechnen, wenn die Kr√ºmmung in verschiedenen Richtungen f√ºr die gegebene Funktion sehr unterschiedlich ist. F√ºr solche Funktionen h√§rtet die Vorkonditionierung, die die Geometrie des Raumes √§ndert, um die Funktionsebene wie konzentrische Kreise zu formen, die langsame Konvergenz. Der Aufbau und das Anlegen von Vorkonditionierungen kann jedoch rechnerisch teuer sein. Der Gradientenabstieg kann mit einer Liniensuche kombiniert werden, wobei die lokal optimale Schrittgr√∂√üe Œ≥ {\displaystyle \gamma } bei jeder Iteration gefunden wird. Die Durchf√ºhrung der Zeilensuche kann zeitraubend sein. Umgekehrt kann mit einem festen kleinen Œ≥ {\displaystyle \gamma } eine schlechte Konvergenz erzielen. Methoden basierend auf Newtons Methode und Inversion des Hessischen mit konjugierten Gradiententechniken k√∂nnen bessere Alternativen sein. In der Regel konvergieren solche Verfahren in weniger Iterationen, aber die Kosten jeder Iteration ist h√∂her. Ein Beispiel ist die BFGS-Methode, die darin besteht, auf jedem Schritt eine Matrix zu berechnen, mit der der Gradientenvektor multipliziert wird, in eine bessere Richtung zu gehen, kombiniert mit einem ausgefeilteren Zeilensuchalgorithmus, um den besten Wert von Œ≥ zu finden. {\displaystyle \gamma . Bei extrem gro√üen Problemen, bei denen die Computer-Memory-Probleme dominieren, sollte anstelle von BFGS oder dem steilsten Abstieg eine limitierte Methode wie L-BFGS verwendet werden. Der Gradientenabstieg kann als Anwendung von Eulers Methode zur L√∂sung von gew√∂hnlichen Differentialgleichungen x' (t ) = - MENT f ( x (t ) ) {\displaystyle x'(t)=-\nabla f(x(t}) auf einen Gradientenfluss angesehen werden. Diese Gleichung kann wiederum als optimaler Regler f√ºr das Steuerungssystem x' (t ) = u (t ) {\displaystyle x'(t)=u(t} mit u (t ) {\displaystyle u(t}) in R√ºckkopplungsform u (t ) = - MENT f ( x (t )) ) abgeleitet werden.{\displaystyle u(t)=-\nabla f(x(t}) . Modifications Gradient Abstieg kann auf ein lokales Minimum konvergieren und in einer Nachbarschaft eines Sattelpunktes verlangsamen. Auch f√ºr unkonstrainierte quadratische Minimierung entwickelt sich der Gradientenabstieg ein Zick-Zack-Muster von nachfolgenden Iteraten, wenn es zu einer Verlangsamung kommt. Es wurden mehrere √Ñnderungen des Gradientenabstiegs vorgeschlagen, um diese M√§ngel zu beheben. Schnelle Gradientenmethoden Yurii Nesterov hat eine einfache Modifikation vorgeschlagen, die eine schnellere Konvergenz bei konvexen Problemen erm√∂glicht und seitdem weiter verallgemeinert ist. Bei ungehinderten glatten Problemen wird das Verfahren als schnelle Gradientenmethode (FGM) oder als beschleunigte Gradientenmethode (AGM) bezeichnet. Wenn n√§mlich die differenzierbare Funktion F {\displaystyle F} konvex ist und F {\displaystyle \nabla F} Lipschitz ist, und es wird nicht angenommen, dass F {\displaystyle F} stark konvex ist, dann wird der Fehler in dem bei jedem Schritt k\displaystyle k} durch die Gradientenabstiegsmethode erzeugten objektiven Wert durch O ( 1 k) Es ist bekannt, dass die Rate O (k - 2 ) {\displaystyle {\mathcal O}}\left({k^{-2}\right) f√ºr die Abnahme der Kostenfunktion optimal f√ºr Optimierungsmethoden erster Ordnung ist. Dennoch besteht die M√∂glichkeit, den Algorithmus zu verbessern, indem der konstante Faktor reduziert wird. Die optimierte Gradientenmethode (OGM) reduziert diese Konstante um einen Faktor von zwei und ist ein optimales Verfahren erster Ordnung f√ºr gro√ütechnische Probleme. Bei eingeschr√§nkten oder nicht-gl√§ttenden Problemen wird Nesterovs FGM als schnelle proximale Gradientenmethode (FPGM) eine Beschleunigung der proximalen Gradientenmethode bezeichnet. Momentum oder schwere Kugel-Methode Versuchen, das Zick-Zack-Muster der Gradientenabstieg zu brechen, verwendet die Dynamik oder schwere Kugel-Methode in Analogie zu einer schweren Kugel gleiten auf der Oberfl√§che von Werten der Funktion minimiert wird, oder zu Massenbewegung in Newtonischer Dynamik durch ein viskoses Medium in einem konservativen Kraftfeld. Gradienter Abstieg mit Impuls erinnert an das L√∂sungsupdate bei jeder Iteration und bestimmt das n√§chste Update als lineare Kombination des Gradienten und des vorherigen Updates. F√ºr eine unkonstrainierte quadratische Minimierung ist eine theoretische Konvergenzrate der Schwerkugelmethode asymptotisch gleich wie bei der optimalen konjugierten Gradientenmethode. Diese Technik wird in Stochastic Gradientenabstieg verwendet # Momentum und als Erweiterung auf die Backpropagation Algorithmen verwendet, um k√ºnstliche neuronale Netzwerke zu trainieren. Erweiterungen Gradient Abstieg kann erweitert werden, um Zw√§nge zu handhaben, indem eine Projektion auf den Satz der Zw√§nge. Dieses Verfahren ist nur dann m√∂glich, wenn die Projektion auf einem Computer effizient berechnet wird. Unter geeigneten Annahmen konvergiert diese Methode. Dieses Verfahren ist ein spezieller Fall des Vorw√§rts-R√ºckw√§rts-Algorithmus f√ºr Monoton-Einschl√ºsse (der konvexe Programmierung und variierende Ungleichheiten umfasst). Siehe auch Referenzen Weiter lesen Boyd, Stephen; Vandenberghe, Lieven (2004)." Unbeschr√§nkte Minimierung" (PDF). Convex Optimierung. New York: Cambridge University Press.pp.457‚Äì520.ISBN 0-521-83378-7. Chong, Edwin K. P;. ≈ªak, Stanislaw H. (2013)."Gradient Methods". Eine Einf√ºhrung in die Optimierung (Fourth ed.). Hoboken: Wiley.pp.131‚Äì160.ISBN 978-1-118-27901-4.Himmelblau, David M. (1972). "Unbeschr√§nkte Minimierungsverfahren mit Derivaten". Angewandte nichtlineare Programmierung. New York: McGraw-Hill.pp.63‚Äì132.ISBN 0-07-028921-2. Externe Links Mit Gradientenabstieg in C,+ Boost, Ublas for linear regression Series of Khan Academy videos diskutieren Gradientenabstieg Online Buch Lehrgradientenabstieg in tiefen neuronalen Netzwerkkontext "Gradient Descent, How Neural Networks Learn".3Blue1Brown.October 16, 2017 ‚Äì via YouTube