Bedingte Zufallsfelder (CRFs) sind eine Klasse statistischer Modellierungsmethoden, die oft in Mustererkennung und maschinellem Lernen angewendet werden und für strukturierte Vorhersage verwendet werden. Während ein Klassifikator ein Etikett für eine einzelne Probe prognostiziert, ohne benachbarte Proben zu betrachten, kann ein CRF Kontext berücksichtigen. Dazu wird die Vorhersage als grafisches Modell modelliert, das Abhängigkeiten zwischen den Vorhersagen implementiert. Welche Art von Diagramm verwendet wird, hängt von der Anwendung ab. Beispielsweise sind in der natürlichen Sprachverarbeitung lineare Ketten CRFs beliebt, die sequentielle Abhängigkeiten in den Vorhersagen implementieren. Bei der Bildverarbeitung verbindet der Graph typischerweise Orte mit nahe gelegenen und/oder ähnlichen Standorten, um sicherzustellen, dass sie ähnliche Vorhersagen erhalten. Weitere Beispiele, bei denen CRFs verwendet werden, sind: Kennzeichnung oder Parsierung von sequentiellen Daten für die natürliche Sprachverarbeitung oder biologische Sequenzen, POS-Tagging, flache Parsing, benannte Entitätserkennung, Genfindung, peptidkritisches Funktionsgebietsfinden und Objekterkennung und Bildsegmentierung in der Computervision. Warenbezeichnung CRFs sind eine Art diskriminatives und geschütztes probabilistisches grafisches Modell. Lafferty, McCallum und Pereira definieren einen CRF auf Beobachtungen X {\displaystyle {\boldsymbol {X} und zufällige Variablen Y {\displaystyle {\boldsymbol {Y} wie folgt: Let G = (V, E) {\displaystyle G=(V,E}) ein Diagramm, so dass Y = ( Y v ) v ε V {\displaystyle {\\\boldsymbol} Y}=({\boldsymbol Y}_{v})_{v\in V} , so dass Y {\displaystyle {\boldsymbol {Y} durch die Vertiken von G {\displaystyle G} indiziert wird. Y v {\displaystyle {\boldsymbol Y}_{v , conditioned on X {\displaystyle {\boldsymbol {X}, gehorcht der Markov-Eigenschaft bezüglich des Graphen: p ( Y v | X, Y w , w symbol v ≠ v ) = p ( Y v | X, Y w , w bold v |} {\displaystyle p}{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ X},{\boldsymbol ({\boldsymbol) **_{v}{\boldsymbol X},{\boldsymbol Y}_{w},w\sim v}) , wo w ̃ v {\displaystyle {\mathit {w}\sim v} bedeutet, dass w {\displaystyle w} und v {\displaystyle v} Nachbarn in G sind. Dies bedeutet, dass ein CRF ein ungerichtetes grafisches Modell ist, dessen Knoten in genau zwei disjoint-Sets X {\displaystyle {\boldsymbol {X} und Y {\displaystyle {\boldsymbol {Y}, die beobachteten und ausgegebenen Variablen unterteilt werden können; die bedingte Verteilung p ( Y s X) {\displaystyle p({\boldsymbol Y}}}}{\\\{\}}}}}\\\\\\{\boldsymbol is Inferenz Für allgemeine Grafiken ist das Problem der genauen Inferenz in CRFs unbrauchbar. Das Inferenzproblem für eine CRF ist im Grunde dasselbe wie für eine MRF und die gleichen Argumente. Es gibt jedoch spezielle Fälle, für die eine genaue Aussage möglich ist: Wenn der Graph eine Kette oder ein Baum ist, ergeben Nachrichtenübermittlungsalgorithmen genaue Lösungen. Die in diesen Fällen verwendeten Algorithmen sind analog zum Vorwärts- und Viterbi-Algorithmus für den Fall von HMMs. Enthält die CRF nur paarweise Potentiale und die Energie submodular, ergeben kombinatorische min cut/max flow Algorithmen exakte Lösungen. Ist eine genaue Inferenz nicht möglich, können mehrere Algorithmen verwendet werden, um ungefähre Lösungen zu erhalten. Dazu gehören: Loopy Glaubensausbreitung Alpha Expansion Mittelfeldinferenz Lineare Programmierrelaxationen Parameter Lernen der Parameter θ {\displaystyle \theta } wird in der Regel durch maximales Wahrscheinlichkeitslernen für p ( Y i | X i; θ ) {\displaystyle p(Y_{i}|X_{i};\theta } durchgeführt .Wenn alle Knoten exponentielle Familienverteilungen haben und alle Knoten während des Trainings beobachtet werden, ist diese Optimierung konvex. Sie kann beispielsweise mittels Gradientenabstiegsalgorithmen oder Quasi-Newton-Methoden wie dem L-BFGS-Algorithmus gelöst werden. Andererseits muss bei Nichtbeobachtung einiger Variablen das Inferenzproblem für diese Variablen gelöst werden. Die genaue Inferenz ist in den allgemeinen Schaubildern unbrauchbar, so daß Annäherungen verwendet werden müssen. Beispiele Bei der Sequenzmodellierung ist der interessierende Graph in der Regel ein Kettendiagramm. Eine Eingangssequenz der beobachteten Variablen X {\displaystyle X} stellt eine Folge von Beobachtungen dar und Y {\displaystyle Y} stellt eine versteckte (oder unbekannte) Zustandsvariable dar, die bei den Beobachtungen zu beziehen ist. Die Y i {\displaystyle Y_{i} sind zu einer Kette strukturiert, mit einer Kante zwischen je Y i - 1 {\displaystyle Y_{i-1} und Y i {\displaystyle Y_{i} . Neben einer einfachen Interpretation der Y i {\displaystyle Y_{i} als Etiketten für jedes Element in der Eingabesequenz gibt dieses Layout effiziente Algorithmen für: Modelltraining, Erlernen der bedingten Verteilungen zwischen der Y i {\displaystyle Y_{i} und Funktion von einigen Korpus von Trainingsdaten. EPMATHMARKEREP wobei die Wahrscheinlichkeit einer gegebenen Etikettenfolge Y {\displaystyle Y} bei X {\displaystyle X} bestimmt wird, wobei die wahrscheinlichste Etikettenfolge Y {\displaystyle Y} bei X {\displaystyle X} bestimmt wird. Das Modell gibt jedem ein numerisches Gewicht zu und kombiniert sie, um die Wahrscheinlichkeit eines bestimmten Wertes für Y i {\displaystyle Y_{i} zu bestimmen.Linear-chain CRFs haben viele der gleichen Anwendungen wie konzeptuell einfacher versteckte Markov-Modelle (HMMs), aber entspannen Sie bestimmte Annahmen über die Eingabe- und Ausgabesequenzverteilungen. Ein HMM kann lose als CRF mit sehr spezifischen Merkmalsfunktionen verstanden werden, die konstante Wahrscheinlichkeiten für die Modellzustandsübergänge und -emissionen verwenden. Umgekehrt kann unter einem CRF lose eine Verallgemeinerung eines HMM verstanden werden, die die konstanten Übergangswahrscheinlichkeiten in beliebige Funktionen macht, die je nach Eingangssequenz über die Positionen in der Folge versteckter Zustände variieren. Insbesondere im Gegensatz zu HMMs können CRFs eine beliebige Anzahl von Funktionsfunktionen enthalten, die Funktionsfunktionen können die gesamte Eingangssequenz X {\displaystyle X} an jedem Punkt während der Inferenz inspizieren und der Bereich der Funktionsfunktionen braucht keine probabilistische Interpretation. Variants Higher-order CRFs und Semi-Markov CRFs CRFs können in höhere Auftragsmodelle erweitert werden, indem jede Y i {\displaystyle Y_{i} von einer festen Anzahl k {\displaystyle k} früherer Variablen abhängig gemacht wird Y i - k , . . . , Y i - 1 {\displaystyle Y_{i-k}, ...,Y_{i-1 . Bei herkömmlichen Formulierungen höherer Ordnung sind CRFs, Training und Inferenz nur für kleine Werte k {\displaystyle k} (wie k ≤ 5,) praktisch, da ihre Rechenkosten mit k {\displaystyle k} exponentiell zunehmen. Ein weiterer neuer Fortschritt hat es jedoch geschafft, diese Probleme durch die Nutzung von Konzepten und Werkzeugen aus dem Bereich der Bayesischen Nichtparametrie zu klären. Insbesondere stellt der CRF-Infinity-Ansatz ein CRF-Modell dar, das in der Lage ist, unendlich lange zeitliche Dynamik skalierbar zu lernen. Dies geschieht durch Einführung einer neuartigen Potentialfunktion für CRFs, die auf dem Sequence Memoizer (SM,) basiert, einem nichtparametrischen Bayesischen Modell zum Erlernen unendlich langer Dynamik in sequentiellen Beobachtungen. Um ein solches Modell rechnerisch traktierbar zu machen, setzt CRF-Infinity eine Mittelfeld-Annäherung der postulierten neuartigen Potentialfunktionen (die von einem SM angetrieben werden) ein. Dies ermöglicht es, effiziente annähernde Trainings- und Inferenzalgorithmen für das Modell zu entwickeln, ohne dessen Fähigkeit zu unterminieren, zeitliche Abhängigkeiten beliebiger Länge zu erfassen und zu modellieren. Es besteht eine weitere Verallgemeinerung von CRFs, das semi-Markov bedingte Zufallsfeld (semi-CRF), das variable Längensegmentierungen der Etikettensequenz Y {\displaystyle Y} modelliert.Dies bietet viel von der Leistung von höherwertigen CRFs zu modelllangen Abhängigkeiten der Y i {\displaystyle Y_{i} zu angemessenen Rechenkosten. Schließlich sind Großmargin-Modelle für strukturierte Prognosen, wie die strukturierte Support Vector Machine, als alternatives Trainingsverfahren für CRFs zu sehen. Latent-dynamisches bedingtes Zufallsfeld Latent-dynamische bedingte Zufallsfelder (LDCRF) oder diskriminative probabilistische latente Variablenmodelle (DPLVM) sind eine Art von CRFs für Sequenz-Tagging-Aufgaben. Sie sind latente variable Modelle, die diskriminativ trainiert werden. In einem LDCRF ist, wie in jeder Sequenz-Tagging-Aufgabe, bei einer Sequenz von Beobachtungen x = x 1 , ... , x n {\displaystyle x_{1},\dots ,x_{n} das Hauptproblem, das das Modell lösen muss, eine Sequenz von Etiketten y= y 1 ,... Dies ermöglicht die Erfassung latenter Struktur zwischen den Beobachtungen und Etiketten. Während LDCRFs mit Quasi-Newton-Methoden trainiert werden können, wurde für sie auch eine spezielle Version des Perceptron-Algorithmus namens latent-variable Perceptron entwickelt, die auf dem strukturierten Perceptron-Algorithmus von Collins basiert. Diese Modelle finden Anwendungen in der Computer-Vision, speziell Gestenerkennung von Videostreams und flachen Parsing. Software Dies ist eine Teilliste von Software, die generische CRF-Tools implementiert. CRF-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten-Ketten Eine Python-Bindung für crfsuite (Python)Abb. Probabilistische Programmiersprache zur Definition von CRFs und anderen grafischen Modellen (Scala) CRF Modellierungs- und Rechentools für CRFs und andere ungerichtete grafische Modelle (R) OpenGM Bibliothek für diskrete Faktor-Graphen-Modelle und verteilte Operationen auf diesen Modellen (C++)UPGMpp Bibliothek für Gebäude, Schulung und Durchführung von Inferenzen mit undirected Graphical Models (C++)KEG_CRFFast Linear CRFs (C++) Dies ist eine Teilliste von Software, die CRF-bezogene Tools implementiert. MedaCy Medical Named Entity Recepter (Python) Conrad CRF basierte Gen-Vorhersage (Java)Stanford NER Named Entity Reognitioner (Java)BANNER Named Entity Reuther (Java) Siehe auch Hammersley–Clifford theorem Graphical modelMarkov random field Maximum entropy Markov model (MEMM) Referenzen Weiteres Lesen McCallum, A.:Effizient induzieren Merkmale von bedingten zufälligen Feldern. In: Proc.19th Konferenz über Ungewissheit in Künstlicher Intelligenz.(2003)Wallach, H.M: bedingte zufällige Felder: Eine Einführung. Technischer Bericht MS-CIS-04-21, University of Pennsylvania (2004)Sutton, C,. McCallum, A:. Eine Einführung in Conditional Random Fields for Relational Learning. In "Introduction to Statistical Relational Learning". Herausgegeben von Lise Getoor und Ben Taskar. MIT Press.(2006)Online PDF Klinger, R,. Tomanek, K:. Klassische Probabilistische Modelle und bedingte Randomfelder. Algorithm Engineering Report TR07-2-013, Abteilung Informatik, Technische Universität Dortmund, Dezember 2007.ISSN 1864-4503.Online PDF