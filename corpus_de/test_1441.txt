Die Geschichte der Künstlichen Intelligenz (KI) begann in der Antike, mit Mythen, Geschichten und Gerüchten von künstlichen Wesen, die von Handwerkern mit Intelligenz oder Bewusstsein versorgt wurden. Die Samen der modernen KI wurden von klassischen Philosophen gepflanzt, die versuchten, den Prozess des menschlichen Denkens als die mechanische Manipulation von Symbolen zu beschreiben. Diese Arbeit gipfelte in der Erfindung des programmierbaren digitalen Computers in den 1940er Jahren, einer Maschine basierend auf der abstrakten Essenz der mathematischen Argumentation. Dieses Gerät und die Ideen dahinter inspirierten eine Handvoll Wissenschaftler ernsthaft über die Möglichkeit des Aufbaus eines elektronischen Gehirns zu diskutieren. Der Bereich der KI-Forschung wurde in einem Workshop am Campus des Dartmouth College im Sommer 1956 gegründet. Diejenigen, die daran teilgenommen haben, werden seit Jahrzehnten die Führer der KI-Forschung. Viele von ihnen haben vorhergesagt, dass eine Maschine so intelligent wie ein Mensch in nicht mehr als einer Generation existieren würde, und sie wurden Millionen von Dollar gegeben, um diese Vision wahr zu machen. Schließlich wurde deutlich, dass kommerzielle Entwickler und Forscher die Schwierigkeit des Projekts stark unterschätzt hatten. Im Jahr 1973, als Reaktion auf die Kritik von James Lighthill und den anhaltenden Druck vom Kongress, die US und die britischen Regierungen aufgehörten, ungerichtete Forschung über künstliche Intelligenz zu finanzieren, und die folgenden schwierigen Jahre würden später als "AI-Winter" bekannt sein. Sieben Jahre später inspirierte eine visionäre Initiative der japanischen Regierung Regierungen und Industrie, KI mit Milliarden von Dollar zu versorgen, aber durch die späten 80er Jahre wurden die Investoren disillusioniert und rückten wieder Geld zurück. Investitionen und Interesse an KI boomten in den ersten Jahrzehnten des 21. Jahrhunderts, als das maschinelle Lernen aufgrund neuer Methoden, der Anwendung leistungsfähiger Computerhardware und der Sammlung immenser Datensätze auf viele Probleme in Wissenschaft und Industrie erfolgreich angewandt wurde. Precursors Mythical, fictional und spekulative Vorläufer Myth und Legende In der griechischen Mythologie war Talos ein Riesen aus Bronze, der als Hüter für die Insel Kreta fungierte. Er würde Boulder auf die Schiffe von Invasoren werfen und würde täglich 3 Kreise rund um den Umfang der Insel vervollständigen. Laut Pseudo-Apollodorus' Bibliotheke schmiedete Hephaestus Talos mit Hilfe eines Zyklopen und stellte den Automaten als Geschenk an Minos vor. In der Argonautica besiegten Jason und die Argonauten ihn über einen einzigen Stecker in der Nähe seines Fußes, der, einmal entfernt, dem lebenswichtigen Ichor erlaubte, aus seinem Körper zu strömen und ihn unbelebt zu lassen. Pygmalion war ein legendärer König und Bildhauer der griechischen Mythologie, berühmt in Ovids Metamorphosen vertreten. Im 10. Buch des Erzählgedichts von Ovid wird Pygmalion mit Frauen ekelhaft, wenn er Zeuge der Art, wie die Propoetides selbst prostituieren. Trotzdem, er macht Angebote im Tempel der Venus und bittet die Göttin, ihm eine Frau wie eine Statue zu bringen, die er Carvexre Der früheste schriftliche Bericht über die Golem-Gestaltung findet sich in den Schriften von Eleazar ben Judah von Worms um 12-13. C. Im Mittelalter wurde angenommen, dass die Animation eines Golems durch das Einfügen eines Papiers mit einem beliebigen Namen Gottes in den Mund der Tonfigur erreicht werden konnte. Im Gegensatz zu legendären Automaten wie Brazen Heads konnte ein Golem nicht sprechen. Alchemische Mittel der künstlichen Intelligenz In Of the Nature of Things, geschrieben von der schweizerischen Alchemistin Paracelsus, beschreibt er ein Verfahren, das er behauptet, einen "künstlichen Mann" herzustellen. Durch die Platzierung des "Sperm eines Mannes" in Pferdeschleuse und die Fütterung des "Arcanums des Mans Blutes" nach 40 Tagen wird die Konkoktion ein lebendes Kind. Predating Paracelsus war Jābir ibn Hayyāns Übernahme des Homunculus: Takwin In Faust, Der zweite Teil der Tragödie von Johann Wolfgang von Goethe, ein alchemisch hergestellter Homunculus, der dazu bestimmt ist, ewig in der Flasche zu leben, in der er hergestellt wurde, versucht, in einen vollen menschlichen Körper geboren zu werden. Bei der Einleitung dieser Transformation zerstreut sich jedoch der Kolben und der Homunculus. Moderne Fiktion Im 19. Jahrhundert wurden Ideen über künstliche Männer und Denkmaschinen in Fiktion entwickelt, wie in Mary Shelleys Frankenstein oder Karel Čapeks R.U.R. (Rossum's Universal Robots) und Spekulationen wie Samuel Butlers "Darwin unter den Maschinen" und in realen Weltfällen, darunter Edgar Allan Poes "Maelzels Ches Player". KI ist durch die Gegenwart zu einem regelmäßigen Thema der Science Fiction geworden. Automata Realistic humanoid automata wurden von Handwerker aus jeder Zivilisation gebaut, darunter Yan Shi,Hero von Alexandria,Al-Jazari,Pierre Jaquet-Droz und Wolfgang von Kempelen. Die älteste bekannte automata waren die heiligen Statuen des alten Ägyptens und Griechenlands. Die Gläubigen glaubten, dass Handwerker diese Figuren mit sehr realen Köpfen, fähig zu Weisheit und Emotion – Hermes Trismegistus schrieb, dass "durch die Entdeckung der wahren Natur der Götter, der Mensch in der Lage war, es zu reproduzieren. " Während der frühen modernen Zeit, diese legendären Automaten wurden gesagt, um die magische Fähigkeit, Fragen an sie gestellt zu beantworten. Der spätmittelalterliche Alchemist und Gelehrte Roger Bacon wurde angeblich einen braten Kopf hergestellt haben, nachdem er eine Legende entwickelt hatte, ein Zauberer gewesen zu sein. Diese Legenden ähnelten dem nordischen Mythos des Leiters von Mímir. Laut Legende war Mímir für seinen Verstand und seine Weisheit bekannt und wurde im Æsir-Vanir-Krieg enthauptet. Odin soll den Kopf mit Kräutern versiegelt haben und darüber mit Beschwörungen gesprochen haben, so dass Mímirs Kopf weiterhin Weisheit zu Odin sprechen konnte. Odin hielt dann den Kopf in seiner Nähe zum Rat. Die formale Begründung künstlicher Intelligenz beruht auf der Annahme, dass der Prozess des menschlichen Denkens mechanisiert werden kann. Die Studie der mechanischen – oder formalen“ – Überlegungen hat eine lange Geschichte. Chinesische, indische und griechische Philosophen entwickelten im ersten Jahrtausend BCE strukturierte Methoden der formalen Abführung. Ihre Ideen wurden im Laufe der Jahrhunderte von Philosophen wie Aristoteles (der eine formale Analyse des Syllogismus gab), Euclid (die Elemente war ein Modell der formalen Argumentation), al-Khwārizmī (die Algebra entwickelte und seinen Namen Algorithmus gab) und europäischen scholastic Philosophen wie William von Ockham und Duns Scotus entwickelt. Der spanische Philosoph Ramon Llull (1232–1315) entwickelte mehrere logische Maschinen, die auf logische Weise der Wissensproduktion gewidmet sind; Llull beschreibt seine Maschinen als mechanische Einheiten, die grundlegende und unleugbare Wahrheiten durch einfache logische Operationen, die von der Maschine durch mechanische Bedeutungen erzeugt werden, kombinieren könnten, um alle möglichen Kenntnisse zu erzeugen. Llulls Arbeit hatte einen großen Einfluss auf Gottfried Leibniz, der seine Ideen neu entwickelte. Im 17. Jahrhundert untersuchten Leibniz, Thomas Hobbes und René Descartes die Möglichkeit, dass alle rationalen Gedanken so systematisch wie Algebra oder Geometrie gemacht werden konnten. Hobbes schrieb bekanntlich in Leviathan: "Fluss ist nichts als zu rechnen". Leibniz stellte eine universelle Sprache der Argumentation vor (sein charakteristischea universalis), die die Argumentation zur Berechnung reduzieren würde, so dass "es nicht mehr Notwendigkeit der Disputation zwischen zwei Philosophen als zwischen zwei Buchhaltern wäre. Denn es genügt, ihre Stifte in die Hand zu nehmen, bis zu ihren Schiefen und einander zu sagen (mit einem Freund als Zeuge, wenn sie möchten): Berechnen wir. " Diese Philosophen hatten begonnen, die physische Symbolsystemhypothese zu artikulieren, die der Führungsgläubige der KI-Forschung werden würde. Im 20. Jahrhundert lieferte die Studie der mathematischen Logik den wesentlichen Durchbruch, der künstliche Intelligenz plausibel erscheinen ließ. Die Grundlagen wurden von solchen Werken wie Boole's The Laws of Thought und Frege's Begriffsschrift gesetzt. Aufbauend auf Freges System, Russell und Whitehead präsentiert eine formale Behandlung der Grundlagen der Mathematik in ihrem Meisterwerk, die Principia Mathematica in 1913. Inspiriert von Russells Erfolg forderte David Hilbert Mathematiker der 1920er und 30er Jahre auf, diese grundlegende Frage zu beantworten: "Kann alle mathematischen Argumentation formalisiert werden? "Seine Frage wurde von Gödels Unvollkommenheitsnachweis, Turings Maschine und der Lambda-Kalkül der Kirche beantwortet. Ihre Antwort war auf zwei Arten überraschend. Zuerst bewiesen sie, dass es tatsächlich Grenzen, was mathematische Logik erreichen könnte. Aber zweitens (und wichtiger für KI)ihre Arbeit schlug vor, dass innerhalb dieser Grenzen jede Form der mathematischen Argumentation mechanisiert werden könnte. Die Church-Turing-Thesis deutete darauf hin, dass ein mechanisches Gerät, Schuppen Symbole so einfach wie 0 und 1 imitieren könnte jede denkbare Prozess der mathematischen Reduktion. Die zentrale Erkenntnis war die Turing-Maschine – ein einfaches theoretisches Konstrukt, das die Essenz der abstrakten Symbolmanipulation erfasste. Diese Erfindung würde eine Handvoll Wissenschaftler anregen, zunächst über die Möglichkeit des Denkens von Maschinen zu diskutieren. Informatik Berechnungsmaschinen wurden in der Antike gebaut und durch viele Mathematiker in der ganzen Geschichte verbessert, einschließlich (nur wieder) Philosoph Gottfried Leibniz. Im frühen 19. Jahrhundert entwarf Charles Babbage einen programmierbaren Computer (die Analytische Maschine), obwohl er nie gebaut wurde. Ada Lovelace spekuliert, dass die Maschine "might komponieren aufwendige und wissenschaftliche Stücke von Musik irgendeiner Art von Komplexität oder Ausmaß". ( Sie wird oft als erster Programmierer wegen einer Reihe von Notizen, die sie schrieb, dass ganz detailliert eine Methode zur Berechnung Bernoulli Zahlen mit dem Motor.) Die ersten modernen Computer waren die massiven Code-Bruchmaschinen des Zweiten Weltkriegs (z.B. Z3, ENIAC und Colossus). Letztere beiden dieser Maschinen basieren auf der theoretischen Grundlage von Alan Turing und entwickelt von John von Neumann. Die Geburt der künstlichen Intelligenz 1952–1956 In den 1940er und 50er Jahren begann eine Handvoll Wissenschaftler aus verschiedenen Bereichen (Mathematik, Psychologie, Ingenieurwesen, Wirtschaft und Politikwissenschaft) über die Möglichkeit, ein künstliches Gehirn zu schaffen. Der Bereich der künstlichen Intelligenzforschung wurde 1956 als akademische Disziplin gegründet. Cybernetik und frühe neuronale Netze Die früheste Forschung an Denkmaschinen war inspiriert von einem Zusammenfluss von Ideen, die in den späten 1930er, 1940er und frühen 1950er Jahren verbreitet wurden. Die jüngste Forschung in der Neurologie hatte gezeigt, dass das Gehirn ein elektrisches Netz von Neuronen war, die in All-oder-Nichts-Impulsen gefeuert. Norbert Wieners Kybernetik beschreibt Kontrolle und Stabilität in elektrischen Netzwerken. Die Informationstheorie von Claude Shannon beschreibt digitale Signale (d.h. All-oder-Nichts-Signale). Alan Turings Berechnungstheorie zeigte, dass jede Form der Berechnung digital beschrieben werden konnte. Die enge Beziehung zwischen diesen Ideen schlug vor, dass es möglich sein könnte, ein elektronisches Gehirn aufzubauen. Beispiele für die Arbeit in dieser Vene sind Roboter wie W. Grey Walters Schildkröten und das Johns Hopkins Beast. Diese Maschinen nutzten keine Computer, digitale Elektronik oder symbolische Vernunft; sie wurden vollständig durch analoge Schaltung gesteuert. Walter Pitts und Warren McCulloch analysierten Netzwerke idealisierter künstlicher Neuronen und zeigten, wie sie 1943 einfache logische Funktionen ausführen könnten. Sie waren die ersten, um zu beschreiben, was später Forscher ein neuronales Netzwerk nennen würden. Einer der von Pitts und McCulloch inspirierten Studenten war ein junger Marvin Minsky, dann ein 24-jähriger Student. 1951 (mit Dean Edmonds) baute er die erste neuronale Netzmaschine, die SNARC. Minsky sollte für die nächsten 50 Jahre einer der wichtigsten Führer und Innovatoren in KI werden. Turing's Test Im Jahr 1950 Alan Turing veröffentlichte ein Wahrzeichen Papier, in dem er spekuliert über die Möglichkeit der Erstellung von Maschinen, die denken. Er stellte fest, dass das Denken schwierig ist, seinen berühmten Turing Test zu definieren und zu entwickeln. Wenn eine Maschine ein Gespräch (über einen Teleprinter) führen könnte, das aus einem Gespräch mit einem Menschen nicht zu unterscheiden war, dann war es vernünftig, zu sagen, dass die Maschine dachte". Diese vereinfachte Version des Problems erlaubte Turing überzeugend zu argumentieren, dass eine "Dinkmaschine" zumindest plausibel war und das Papier alle häufigsten Einwände gegen die Behauptung beantwortete. Das Turing Test war der erste ernsthafte Vorschlag in der Philosophie der künstlichen Intelligenz. Spiel AIIn 1951, mit der Ferranti Mark 1 Maschine der Universität Manchester, Christopher Strachey schrieb ein Kassenprogramm und Dietrich Prinz schrieb eine für Schach. Arthur Samuels Scheckerprogramm, entwickelt in den mittleren 50er und frühen 60er Jahren, erreichte schließlich ausreichend Geschick, um einen respektablen Amateur herauszufordern. Spiel KI würde weiterhin als Maß für Fortschritt in KI in seiner gesamten Geschichte verwendet werden. Symbolische Argumentation und der Logiktheoretiker Als der Zugriff auf digitale Computer in den mittleren fünfziger Jahren möglich wurde, erkannten einige Wissenschaftler instinktiv, dass eine Maschine, die Zahlen manipulieren könnte, auch Symbole manipulieren könnte und dass die Manipulation von Symbolen das Wesen des menschlichen Denkens sein könnte. Dies war ein neuer Ansatz, um Denkmaschinen zu schaffen. 1955 schuf Allen Newell und (zukünftig Nobelpreisträger)Herbert A. Simon den "Logic Theorist" (mit Hilfe von J. C. Shaw). Das Programm würde schließlich 38 der ersten 52 Theoremen in Russell und Whitehead Principia Mathematica beweisen, und finden neue und elegantere Beweise für einige. Simon sagte, sie hätten "das ehrwürdige Geist/Körper-Problem gelöst und erklärt, wie ein System aus Materie die Eigenschaften des Verstandes haben kann. "(Dies war eine frühe Aussage der philosophischen Position John Searle würde später "Strong AI" nennen: dass Maschinen Geiste enthalten können, wie menschliche Körper tun). Dartmouth Workshop 1956:Die Geburt von AI Der Dartmouth Workshop von 1956 wurde von Marvin Minsky, John McCarthy und zwei hochrangigen Wissenschaftlern organisiert: Claude Shannon und Nathan Rochester von IBM. Der Vorschlag für die Konferenz umfasste diese Behauptung: "Jeder Aspekt des Lernens oder irgendein anderes Merkmal der Intelligenz kann so genau beschrieben werden, dass eine Maschine zur Simulation gemacht werden kann". Zu den Teilnehmern zählten Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell und Herbert A. Simon, die in den ersten Jahrzehnten der AI-Forschung wichtige Programme erstellen würden. Auf der Konferenz debütierten Newell und Simon den "Logic Theorist" und McCarthy überredeten die Teilnehmer, "Künstliche Intelligenz" als den Namen des Feldes zu akzeptieren. Die Dartmouth-Konferenz von 1956 war der Moment, in dem KI seinen Namen, seine Mission, seinen ersten Erfolg und seine wichtigsten Akteure erlangte und weithin als die Geburt von KI gilt. Der Begriff "Künstliche Intelligenz" wurde von McCarthy gewählt, um Assoziationen mit Cybernetik und Verbindungen mit dem einflussreichen Cybernetiker Norbert Wiener zu vermeiden. Die goldenen Jahre 1956–1974 Die Programme, die in den Jahren nach dem Dartmouth Workshop entwickelt wurden, waren für die meisten Menschen einfach erstaunlich: Computer lösten Algebra Wortprobleme, beweisen Theorien in Geometrie und lernen, Englisch zu sprechen. Wenige damals hätten geglaubt, dass ein solches intelligentes Verhalten durch Maschinen überhaupt möglich wäre. Forscher äußerten einen intensiven Optimismus im privaten und im Druck, vorauszusagen, dass eine voll intelligente Maschine in weniger als 20 Jahren gebaut würde. Regierungsbehörden wie DARPA haben Geld in das neue Feld gegossen. Die Arbeit In den späten 50er und 1960er Jahren gab es viele erfolgreiche Programme und neue Wege. Zu den einflussreichsten waren diese: Viele frühe KI-Programme nutzten den gleichen Grundalgorithmus. Um ein Ziel zu erreichen (wie ein Spiel zu gewinnen oder ein Theorem zu beweisen), gingen sie Schritt für Schritt darauf (durch eine Bewegung oder eine Absenkung) als ob durch ein Labyrinth suchen, Rückverfolgung, wenn sie ein totes Ende erreicht. Dieses Paradigma wurde als "Erinnerung als Suche" bezeichnet. Die Hauptschwierigkeit war, dass für viele Probleme die Anzahl der möglichen Wege durch das Labyrinth einfach astronomisch war (eine Situation, die als "kombinatorische Explosion" bezeichnet wird). Forscher würden den Suchraum durch die Verwendung von Heuristiken oder "Regeln des Daumens" reduzieren, die jene Pfade beseitigen würden, die unwahrscheinlich zu einer Lösung führten. Newell und Simon versuchten, eine allgemeine Version dieses Algorithmus in einem Programm namens "General Problem Solver" zu erfassen. Andere Suchprogramme konnten beeindruckende Aufgaben erfüllen wie die Lösung von Problemen in Geometrie und Algebra, wie Herbert Gelernter's Geometry Theorem Prover (1958) und SAINT, geschrieben von Minskys Student James Slagle (1961). Andere Programme durch Tore und Subgoals gesucht, um Aktionen zu planen, wie das STRIPS-System in Stanford entwickelt, um das Verhalten ihres Roboters Shakey zu steuern. Natürliche Sprache Ein wichtiges Ziel der KI-Forschung ist es, Computer in natürlichen Sprachen wie Englisch kommunizieren zu lassen. Ein früher Erfolg war Daniel Bobrows Programm STUDENT, das High School Algebra Wort Probleme lösen könnte. Ein semantisches Netz stellt Konzepte (z.B. Haus", "Tür) als Knoten und Beziehungen zwischen Konzepten (z.B. has-a) als Verbindungen zwischen den Knoten dar. Das erste KI-Programm, um ein semantisches Netz zu verwenden, wurde von Ross Quillian geschrieben und die erfolgreichste (und umstrittene) Version war Roger Schanks konzeptuelle Abhängigkeitstheorie. Joseph Weizenbaums ELIZA konnte Gespräche führen, die so realistisch waren, dass die Nutzer gelegentlich ins Denken getäuscht wurden, dass sie mit einem Menschen kommunizieren und kein Programm (siehe ELIZA-Effekt). Aber in der Tat hatte ELIZA keine Ahnung, wovon sie sprach. Sie gab einfach eine gekonnte Antwort oder wiederholte zurück, was ihr gesagt wurde, und wiederholte ihre Antwort mit einigen Grammatikregeln. ELIZA war der erste Chatterbot. Mikrowelten In den späten 60er Jahren schlugen Marvin Minsky und Seymour Papert des MIT AI Laboratory vor, dass sich die AI-Forschung auf künstlich einfache Situationen konzentrieren sollte, die als Mikrowelten bekannt sind. Sie wiesen darauf hin, dass in erfolgreichen Wissenschaften wie Physik oft Grundprinzipien mit vereinfachten Modellen wie reibungsfreien Ebenen oder perfekt starren Körpern am besten verstanden wurden. Ein Großteil der Forschung konzentrierte sich auf eine "Blöcke-Welt", die aus farbigen Blöcken verschiedener Formen und Größen besteht, die auf einer flachen Oberfläche angeordnet sind. Dieses Paradigma führte zu innovativer Arbeit in der Bildverarbeitung von Gerald Sussman (der das Team leitete), Adolfo Guzman, David Waltz (der "constraint propagation" erfunden hat) und insbesondere Patrick Winston. Gleichzeitig bauten Minsky und Papert einen Roboterarm, der Blöcke stapeln konnte und die Blöcke Welt zum Leben brachte. Die krönende Errungenschaft des Mikroprogramms war Terry Winograds SHRDLU. Es könnte in gewöhnlichen englischen Sätzen kommunizieren, planen Operationen und ausführen. Der Optimismus Die erste Generation von KI-Forschern machte diese Vorhersagen über ihre Arbeit: 1958, H. A. Simon und Allen Newell: "Mit zehn Jahren wird ein digitaler Computer der Schachmeister der Welt sein" und "innerhalb von zehn Jahren wird ein digitaler Computer ein wichtiges neues mathematisches Theorem entdecken und beweisen."1965, H. A. Simon: "Maschinen werden innerhalb von zwanzig Jahren in der Lage sein, irgendeine Arbeit zu machen, ein Mann kann."1967, Marin "In drei bis acht Jahren haben wir eine Maschine mit der allgemeinen Intelligenz eines durchschnittlichen Menschen". Das Geld Im Juni 1963 erhielt das MIT von der neu geschaffenen Advanced Research Projects Agency (später DARPA) einen Zuschuss von 2,2 Millionen Dollar. Das Geld wurde verwendet, um Projekt MAC zu finanzieren, die die "AI-Gruppe" von Minsky und McCarthy fünf Jahre früher gegründet. DARPA lieferte weiterhin drei Millionen Dollar pro Jahr bis in die 70er Jahre. DARPA hat ähnliche Zuschüsse an Newell und Simons Programm an der CMU und an das Stanford AI Project (gegründet von John McCarthy 1963) vergeben. Ein weiteres wichtiges KI-Labor wurde 1965 an der Edinburgh University von Donald Michie gegründet. Diese vier Institutionen wären weiterhin die wichtigsten Zentren der KI-Forschung (und Finanzierung) in der Wissenschaft für viele Jahre. Das Geld wurde mit wenigen Streichern angehängt: J. C. R. Licklider, dann der Direktor von ARPA, glaubte, dass seine Organisation sollte "die Menschen finanzieren, nicht Projekte"! und erlaubte den Forschern, alle Richtungen zu verfolgen, die sie interessieren könnten.Dies schaffte eine Freilauf-Atmosphäre am MIT, die der Hacker-Kultur gebären, aber dieser "hands off" Ansatz würde nicht dauern. Robotik In Japan initiierte die Waseda University 1967 das WABOT-Projekt und beendete 1972 den WABOT-1, den weltweit ersten intelligenten humanoiden Roboter oder Android. Seine Gliedersteuerung erlaubte es, mit den unteren Gliedmaßen zu gehen und Objekte mit Händen zu greifen und zu transportieren, mit taktilen Sensoren. Sein Vision-System erlaubte es, Entfernungen und Richtungen zu Objekten mit externen Rezeptoren, künstlichen Augen und Ohren zu messen. Und sein Gesprächssystem erlaubte es, mit einer Person auf Japanisch, mit einem künstlichen Mund zu kommunizieren. Der erste KI-Winter 1974–1980 In den 1970er-Jahren war KI Kritiken und finanziellen Rückschlägen ausgesetzt. KI-Forscher hatten die Schwierigkeit der Probleme, denen sie gegenüber standen, nicht erkannt. Ihr enormer Optimismus hatte die Erwartungen unmöglich hoch angehoben, und als die verheißenen Ergebnisse nicht materialisiert wurden, verschwand die Finanzierung von KI. Gleichzeitig wurde der Bereich des Verbindungsismus (oder neuronale Netze) von Marvin Minskys verheerender Kritik an Perceptronen fast vollständig abgeschaltet. Trotz der Schwierigkeiten mit der öffentlichen Wahrnehmung von KI in den späten 70er Jahren wurden neue Ideen in der Logik-Programmierung, gemeinsamen Verständnis und viele andere Bereiche erforscht. Die Probleme Anfang der siebziger Jahre waren die Fähigkeiten von KI-Programmen begrenzt. Selbst die eindrucksvollsten konnten nur triviale Versionen der Probleme behandeln, die sie lösen sollten; alle Programme waren in gewisser Weise Spielzeug". KI-Forscher hatten begonnen, in mehrere grundlegende Grenzen zu laufen, die in den 1970er Jahren nicht überwunden werden konnten. Obwohl einige dieser Grenzen in späteren Jahrzehnten erobert werden würden, stymie noch andere das Feld bis heute. Begrenzte Computerleistung: Es gab nicht genug Speicher- oder Verarbeitungsgeschwindigkeit, um alles wirklich nützlich zu erreichen. Zum Beispiel wurde Ross Quillians erfolgreiche Arbeit an natürlicher Sprache mit einem Wortschatz von nur zwanzig Wörtern demonstriert, denn das war alles, was in Erinnerung passen würde. Hans Moravec argumentierte 1976, dass Computer noch Millionen Mal zu schwach seien, um Intelligenz zu zeigen. Er schlug eine Analogie vor: künstliche Intelligenz erfordert Computerleistung in der gleichen Weise, wie Flugzeuge PS benötigen. Unter einer bestimmten Schwelle ist es unmöglich, aber, wie die Macht steigt, schließlich könnte es leicht werden. In Bezug auf die Computer-Vision, Moravec geschätzt, dass einfach Anpassung der Rand- und Bewegungserkennung Fähigkeiten der menschlichen Netzhaut in Echtzeit würde einen universellen Computer in der Lage zu 109 Operationen / Sekunden (1000 MIPS). Seit 2011 benötigen praktische Computer Vision Anwendungen 10.000 bis 1.000.000 MIPS. Im Vergleich dazu war Cray-1, der schnellste Supercomputer im Jahr 1976, nur in der Lage, etwa 80 bis 130 MIPS, und ein typischer Desktop-Computer zu der Zeit erreichte weniger als 1 MIPS. Intraktivität und die kombinatorische Explosion. 1972 zeigte Richard Karp (Gebäude auf Stephen Cook's 1971 Theorem) viele Probleme, die wahrscheinlich nur in exponentieller Zeit (in der Größe der Eingänge) gelöst werden können. Die Suche nach optimalen Lösungen für diese Probleme erfordert unvorstellbare Mengen an Computerzeit, außer wenn die Probleme trivial sind. Dies bedeutete fast sicher, dass viele der von AI verwendeten Spielzeuglösungen wahrscheinlich nie in nützliche Systeme einsteigen würden. Kenntnisse und Argumentation. Viele wichtige künstliche Intelligenz-Anwendungen wie Vision oder natürliche Sprache erfordern einfach enorme Mengen an Informationen über die Welt: Das Programm muss eine Vorstellung davon haben, was es sehen könnte oder wovon es spricht. Dies erfordert, dass das Programm die meisten der gleichen Dinge über die Welt kennen, die ein Kind tut. Forscher entdeckten bald, dass dies eine wirklich große Menge an Informationen war. Niemand im Jahr 1970 konnte eine Datenbank so groß bauen und niemand wusste, wie ein Programm so viele Informationen erlernen könnte. Moravec's paradox: Die bewährten Theorems und die Lösung von Geometrieproblemen ist für Computer vergleichsweise einfach, aber eine vermeintlich einfache Aufgabe wie das Erkennen eines Gesichts oder das Überqueren eines Raumes, ohne in etwas zu stoßen, ist äußerst schwierig. Dies hilft zu erklären, warum die Forschung an Vision und Robotik bis Mitte der 1970er Jahre so wenig vorangekommen war. Rahmen- und Qualifikationsprobleme. KI-Forscher (wie John McCarthy), die Logik verwendet haben, entdeckten, dass sie keine gewöhnlichen Abzüge darstellen konnten, die Planungen oder Standardvernunften beinhalteten, ohne Änderungen an der Struktur der Logik selbst vorzunehmen. Sie entwickelten neue Logiken (wie nicht monotone Logiken und modale Logiken), um die Probleme zu lösen. Das Ende der Finanzierung Die Agenturen, die die AI-Forschung (wie die britische Regierung, DARPA und NRC) finanzierten, wurden mit dem Fehlen von Fortschritten frustriert und schließlich fast alle Finanzierungen für ungerichtete Forschung in KI abgeschnitten. Das Muster begann bereits 1966, als der ALPAC-Bericht die maschinellen Übersetzungsbemühungen kritisierte. Nachdem die NRC 20 Millionen Dollar ausgegeben hatte, beendete sie alle Unterstützung. Im Jahr 1973 kritisierte der Lighthill-Bericht über den Stand der KI-Forschung in England das völlige Versagen der KI, ihre "Grandiose-Ziele" zu erreichen und führte zum Abbau der KI-Forschung in diesem Land. ( Der Bericht erwähnte ausdrücklich das kombinatorische Explosionsproblem als Grund für KI-Versagen.) DARPA war zutiefst enttäuscht von Forschern, die an dem Forschungsprogramm "Speech Understanding Research" bei der CMU arbeiteten und einen Jahreszuschuss von drei Millionen Dollar kündigten. Bis 1974 war die Finanzierung von KI-Projekten schwer zu finden. Hans Moravec gab der Krise die unrealistischen Vorhersagen seiner Kollegen vor. " Viele Forscher wurden in einem Netz zunehmender Übertreibung gefangen. " Es gab jedoch ein weiteres Problem: Seit der Passage des Mansfield-Änderungsvorschlags 1969 war DARPA unter zunehmendem Druck, "missionsorientierte Direktforschung zu finanzieren, anstatt grundlegende ungerichtete Forschung". Die Förderung der kreativen, freilaufenden Exploration, die in den 60er Jahren angelaufen war, würde nicht von DARPA kommen. Stattdessen richtete sich das Geld an konkrete Projekte mit klaren Zielen, wie autonome Tanks und Schlachtmanagementsysteme. Kritiken aus dem gesamten Campus Mehrere Philosophen hatten starke Einwände gegen die Behauptungen von KI-Forschern. Einer der ersten war John Lucas, der argumentierte, dass Gödels Unvollkommenheit Theorem zeigte, dass ein formales System (wie ein Computerprogramm) nie die Wahrheit bestimmter Aussagen sehen konnte, während ein Mensch könnte. Hubert Dreyfus verspottete die gebrochenen Versprechen der 1960er Jahre und kritisierte die Annahmen von KI, argumentierte, dass menschliches Denken tatsächlich sehr wenig "Symbolverarbeitung" und eine Menge verkörperter, instinktiver, unbewusster "Wissen wie". John Searle's chinesisches Room Argument, das 1980 vorgestellt wurde, versuchte zu zeigen, dass ein Programm nicht gesagt werden konnte, um die Symbole zu verstehen, die es verwendet (eine Qualität genannt Intentionalität). Wenn die Symbole keine Bedeutung für die Maschine haben, argumentierte Searle, dann kann die Maschine nicht als Denken bezeichnet werden". Diese Kritiken wurden von KI-Forschern nicht ernst genommen, oft weil sie so weit vom Punkt ab schienen. Probleme wie Intraktivität und Commonsense-Wissen schienen viel unmittelbarer und ernster. Es war unklar, welchen Unterschied "Wissen, wie" oder Intentionalität zu einem eigentlichen Computerprogramm gemacht. Minsky sagte von Dreyfus und Searle "sie missverstehen, und sollten ignoriert werden. " Dreyfus, der am MIT lehrte, erhielt eine kalte Schulter: Er sagte später, dass KI-Forscher "nicht mit mir essen zu sehen. "Joseph Weizenbaum, der Autor von ELIZA, fühlte, dass die Behandlung seiner Kollegen von Dreyfus unprofessionell und kindisch war. Obwohl er ein ausgesprochener Kritiker der Positionen von Dreyfus war, machte er "entsprechend klar, dass sie nicht der Weg waren, einen Menschen zu behandeln. " Weizenbaum fing an, ernsthafte ethische Zweifel an AI zu haben, als Kenneth Colby ein "Computerprogramm schrieb, das psychotherapeutischen Dialog auf Basis von ELIZA führen kann. Weizenbaum wurde gestört, dass Colby ein achtloses Programm als ernstes therapeutisches Werkzeug sah. Ein Feud begann, und die Situation wurde nicht geholfen, als Colby Weizenbaum für seinen Beitrag zum Programm nicht gutgeschrieben hat. Im Jahr 1976 veröffentlichte Weizenbaum Computer Power und menschliche Ursache, die argumentierte, dass der Missbrauch künstlicher Intelligenz das Potenzial hat, das menschliche Leben abzuwerten. Perceptrons und der Angriff auf den VerbindungsismusEin Perceptron war eine Form des neuronalen Netzwerks, das 1958 von Frank Rosenblatt eingeführt wurde, der Schulkamerad von Marvin Minsky an der Bronx High School of Science war. Wie die meisten KI-Forscher war er optimistisch über ihre Macht und sagte, dass "Perceptron kann schließlich lernen, Entscheidungen treffen und Sprachen übersetzen können". Ein aktives Forschungsprogramm zum Paradigma wurde in den 1960er Jahren durchgeführt, kam aber plötzlich zum Stillstand mit der Veröffentlichung von Minsky und Paperts 1969er Buch Perceptrons. Es schlug vor, dass es strenge Einschränkungen für das gab, was Perceptrons tun konnten und dass Frank Rosenblatts Vorhersagen grob übertrieben wurden. Die Wirkung des Buches war verheerend: fast keine Forschung wurde im Verbindungsismus seit 10 Jahren durchgeführt. Irgendwann würde eine neue Forschergeneration das Feld wiederbeleben und danach ein lebenswichtiger und nützlicher Teil der künstlichen Intelligenz werden. Rosenblatt würde das nicht sehen, da er in einem Bootsunfall kurz nach der Veröffentlichung des Buches starb. Logische und symbolische Argumentation Logic wurde bereits 1959 von John McCarthy in seinem Advice Taker-Vorschlag in die KI-Forschung eingeführt. 1963 hatte J. Alan Robinson eine einfache Methode entdeckt, um die Ableitung auf Computern, den Auflösungs- und Vereinigungsalgorithmus durchzuführen. Allerdings waren einfache Implementierungen, wie die von McCarthy und seinen Schülern in den späten 1960er Jahren versuchten, besonders anstrengend: die Programme erforderten astronomische Stufenzahlen, um einfache Theoremen zu beweisen. In den 1970er Jahren wurde von Robert Kowalski an der University of Edinburgh ein fruchtbarerer Ansatz zur Logik entwickelt, der bald zur Zusammenarbeit mit französischen Forschern Alain Colmerauer und Philippe Roussel führte, die die erfolgreiche logische Programmiersprache Prolog erstellten. Prolog verwendet eine Teilmenge von Logik (Horn-Klausel, eng mit Regeln und "Produktionsregeln"), die eine lenkbare Berechnung ermöglichen. Regeln würden weiterhin einflussreich sein, eine Grundlage für Edward Feigenbaums Expertensysteme und die weitere Arbeit von Allen Newell und Herbert A. Simon, die zu Soar und ihren einheitlichen Theorien der Kognition führen würde. Kritik des logischen Ansatzes bemerkte, wie Dreyfus hatte, dass Menschen selten Logik verwendet, wenn sie Probleme gelöst. Experimente von Psychologen wie Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman und anderen lieferten Beweise. McCarthy antwortete, dass das, was die Menschen tun, irrelevant sei. Er argumentierte, dass das, was wirklich benötigt wird, sind Maschinen, die Probleme lösen können – nicht Maschinen, die denken, wie Menschen tun. Frames und Skripte Unter den Kritikern von McCarthys Ansatz waren seine Kollegen im ganzen Land am MIT. Marvin Minsky, Seymour Papert und Roger Schank versuchten Probleme wie "Geschichteverständnis" und "Objekterkennung" zu lösen, die eine Maschine erforderten, wie eine Person zu denken. Um gewöhnliche Konzepte wie Stuhl oder Restaurant zu verwenden, mussten sie alle gleichen unlogischen Annahmen machen, die die Menschen normalerweise gemacht. Leider sind ungenaue Konzepte wie diese in der Logik schwer zu vertreten. Gerald Sussman beobachtete, dass "genaue Sprache, um im Wesentlichen unpräzise Konzepte zu beschreiben, sie nicht präziser macht. " Schank beschrieb ihre antilogischen Ansätze als schruffig, im Gegensatz zu den ordentlichen Paradigmen von McCarthy, Kowalski, Feigenbaum, Newell und Simon. Im Jahr 1975, in einem halbnalen Papier, Minsky bemerkte, dass viele seiner Kollegen scruffy Forscher waren mit der gleichen Art von Werkzeug: ein Rahmen, der alle unsere gemeinsamen Sinn Annahmen über etwas erfasst. Zum Beispiel, wenn wir das Konzept eines Vogels verwenden, gibt es eine Konstellation von Fakten, die sofort in den Sinn kommen: wir könnten annehmen, dass es fliegt, frisst Würmer und so weiter. Wir wissen, dass diese Tatsachen nicht immer wahr sind und dass Abzüge mit diesen Tatsachen nicht logisch sind, aber diese strukturierten Annahmen sind Teil des Kontexts von allem, was wir sagen und denken. Er nannte diese Strukturen Rahmen". Schank benutzte eine Version von Frames, die er Skripte nannte, um Fragen über Kurzgeschichten auf Englisch erfolgreich zu beantworten. Viele Jahre später würde eine objektorientierte Programmierung die wesentliche Idee der Vererbung aus der KI-Forschung an Rahmen übernehmen. Boom 1980–1987 In den 1980er Jahren wurde eine Form von KI-Programm "Expertensysteme" von Unternehmen auf der ganzen Welt angenommen und Wissen wurde zum Schwerpunkt der KI-Forschung. In den gleichen Jahren hat die japanische Regierung mit ihrem Computerprojekt der fünften Generation aggressiv AI finanziert. Ein weiteres ermutigendes Ereignis in den frühen 1980er Jahren war die Wiederbelebung des Verbindungsismus in der Arbeit von John Hopfield und David Rumelhart. Noch einmal hatte AI Erfolg. Der Aufstieg von Expertensystemen Ein Expertensystem ist ein Programm, das Fragen beantwortet oder Probleme über eine bestimmte Wissensdomäne löst, unter Verwendung logischer Regeln, die aus dem Wissen von Experten abgeleitet werden. Die frühesten Beispiele wurden von Edward Feigenbaum und seinen Studenten entwickelt. Dendral, begonnen 1965, identifizierte Verbindungen aus Spektrometer-Lesungen. MYCIN, entwickelt 1972, diagnostiziert Infektionskrankheiten. Sie zeigten die Machbarkeit des Ansatzes. Expertensysteme beschränkten sich auf eine kleine Domäne von spezifischem Wissen (d.h. die Vermeidung des gängigen Wissensproblems) und ihr einfaches Design machte es relativ einfach, Programme zu bauen und dann zu ändern, sobald sie vorhanden waren. Insgesamt erwiesen sich die Programme als nützlich: etwas, das KI bis zu diesem Punkt nicht erreichen konnte. 1980 wurde bei der CMU ein Expertensystem namens XCON für die Digital Equipment Corporation abgeschlossen. Es war ein enormer Erfolg: Das Unternehmen rettete bis 1986 jährlich 40 Millionen Dollar. Unternehmen auf der ganzen Welt begannen, Expertensysteme zu entwickeln und zu implementieren, und bis 1985 hatten sie mehr als eine Milliarde Dollar für KI ausgegeben, die meisten davon für interne KI-Abteilungen. Eine Industrie wuchs auf, um sie zu unterstützen, darunter Hardware-Unternehmen wie Symbolics und Lisp Machines und Software-Unternehmen wie IntelliCorp und Aion. Die Wissensrevolution Die Macht der Expertensysteme kam aus dem Expertenwissen, das sie enthielten. Sie waren Teil einer neuen Richtung in der AI-Forschung, die in den 70er Jahren an Boden gewonnen hatte. "Al-Forscher begannen zu verdächtigen, denn sie verletzten den wissenschaftlichen Kanon der Parsimony, dass Intelligenz könnte sehr gut auf der Fähigkeit basieren, große Mengen an unterschiedlichem Wissen auf unterschiedliche Weise zu verwenden", schreibt Pamela McCorduck. "[T]he große Lektion aus den 1970er Jahren war, dass intelligentes Verhalten sehr stark vom Umgang mit Wissen, manchmal ziemlich detaillierte Kenntnisse, einer Domäne, wo eine bestimmte Aufgabe lag." Wissensbasierte Systeme und Wissenstechnik wurden in den 1980er Jahren zum Schwerpunkt der KI-Forschung. Die 1980er Jahre sahen auch die Geburt von Cyc, der erste Versuch, das Problem des gemeinsamen Verständnisses direkt anzugreifen, indem eine massive Datenbank erstellt wurde, die alle mundane Fakten enthalten würde, die die durchschnittliche Person weiß. Douglas Lenat, der das Projekt anfing und leitete, argumentierte, dass es keinen Kurzschluss gibt – der einzige Weg, um Maschinen die Bedeutung menschlicher Konzepte zu kennen, ist, sie, ein Konzept zu einer Zeit, von Hand zu lehren. Das Projekt wurde nicht für viele Jahrzehnte abgeschlossen. Schachspielprogramme HiTech und Deep Thought besiegten Schachmeister 1989. Beide wurden von der Carnegie Mellon University entwickelt; Deep Thought Entwicklung ebnete den Weg für Deep Blue. Das Geld kehrt zurück: Das Projekt der Fünften Generation Im Jahr 1981 hat das japanische Ministerium für internationalen Handel und Industrie für das Computerprojekt der Fünften Generation 850 Millionen Dollar bereitgestellt. Ihre Ziele waren es, Programme zu schreiben und Maschinen zu bauen, die Gespräche führen könnten, Sprachen übersetzen, Bilder interpretieren, und Grund wie Menschen. Viel zu dem chagrin der scruffies, sie wählten Prolog als die primäre Computersprache für das Projekt. Andere Länder reagierten mit neuen eigenen Programmen. Das Vereinigte Königreich begann mit 350 Millionen Pfund Alvey Projekt. Ein Konsortium amerikanischer Unternehmen bildete die Microelectronics and Computer Technology Corporation (oder MCC) zur Finanzierung von Großprojekten in der KI- und Informationstechnologie. DARPA reagierte auch auf die Gründung der Strategischen Computing Initiative und stolperte ihre Investitionen in KI zwischen 1984 und 1988. Die Wiederbelebung des Verbindungsismus 1982 konnte der Physiker John Hopfield beweisen, dass eine Form des neuronalen Netzes (jetzt "Hopfield net" genannt) Informationen auf völlig neue Weise lernen und verarbeiten könnte. Um die gleiche Zeit, Geoffrey Hinton und David Rumelhart populär eine Methode zur Ausbildung von neuronalen Netzwerken genannt Backpropagation, auch bekannt als die umgekehrte Modus der automatischen Differenzierung veröffentlicht von Seppo Linnainmaa (1970) und angewendet auf neuronale Netzwerke von Paul Werbos. Diese beiden Entdeckungen halfen, den Bereich des Verbindungsismus zu revidieren. Das neue Feld war vereint und inspiriert von der Erscheinung von Parallel Distributed Processing im Jahr 1986 – eine zweibändige Sammlung von Papieren von Rumelhart und Psychologe James McClelland. Neurale Netzwerke würden in den 1990er-Jahren kommerziell erfolgreich werden, als sie als Motoren eingesetzt werden, die Programme wie optische Charaktererkennung und Spracherkennung antreiben. Die Entwicklung von Metall-Oxid-Halbleiter (MOS) sehr großflächiger Integration (VLSI) in Form komplementärer MOS (CMOS)-Technologie ermöglichte die Entwicklung praktischer künstlicher neuronaler Netzwerke (ANN)-Technologie in den 1980er Jahren. Ein Wahrzeichen auf dem Feld war das 1989 Buch Analog VLSI Implementierung von Neural Systems von Carver A. Mead und Mohammed Ismail. Büste: der zweite KI Winter 1987–1993 Die Faszination der Geschäftsgemeinschaft mit KI stieg in den 1980er Jahren im klassischen Muster einer Wirtschaftsblase. Der Zusammenbruch war auf das Scheitern kommerzieller Anbieter zurückzuführen, eine Vielzahl von arbeitbaren Lösungen zu entwickeln. Da Dutzende von Unternehmen scheiterten, war die Wahrnehmung, dass die Technologie nicht lebensfähig war. Trotz der Kritik hat das Feld jedoch weiterhin Fortschritte gemacht. Zahlreiche Forscher, darunter Robotik-Entwickler Rodney Brooks und Hans Moravec argumentierten für einen völlig neuen Ansatz für künstliche Intelligenz. AI Winter Der Begriff "AI-Winter" wurde von Forschern geprägt, die die Finanzierungskürzungen von 1974 überlebt hatten, als sie besorgt wurden, dass die Begeisterung für Expertensysteme aus der Kontrolle geflüchtet war und dass Enttäuschung sicherlich folgen würde. Ihre Ängste waren gut begründet: In den späten 1980er und frühen 1990er Jahren erlitt AI eine Reihe von finanziellen Rückschlägen. Die erste Anzeige einer Wetteränderung war der plötzliche Zusammenbruch des Marktes für spezialisierte KI-Hardware im Jahr 1987. Desktop-Computer von Apple und IBM waren immer schneller und leistungsstark gewesen und im Jahr 1987 wurden sie leistungsfähiger als die teureren Lisp-Maschinen von Symbolics und anderen. Es gab keinen guten Grund mehr, sie zu kaufen. Eine ganze Branche im Wert von einer halben Milliarde Dollar wurde über Nacht abgerissen. Schließlich erwiesen sich die frühesten erfolgreichen Expertensysteme, wie XCON, als zu teuer zu halten. Sie waren schwer zu aktualisieren, sie konnten nicht lernen, sie waren spröde (d.h. sie konnten groteske Fehler machen, wenn man ungewöhnliche Eingaben gegeben), und sie fielen auf Probleme (wie das Qualifizierungsproblem), die vor Jahren identifiziert worden waren. Expertensysteme erwiesen sich als nützlich, aber nur in einigen speziellen Kontexten. In den späten 1980er Jahren hat die Strategische Computing-Initiative die KI "zutiefst und brutal" finanziert. Die neue Führung bei DARPA hatte beschlossen, dass KI nicht "die nächste Welle" war und Mittel für Projekte richtete, die eher zu sofortigen Ergebnissen führen könnten. 1991 war die beeindruckende Liste der Ziele, die 1981 für das Projekt der Fünften Generation Japans gestiftet wurden, nicht erfüllt worden. Tatsächlich waren einige von ihnen, wie "Werbe auf einem lässigen Gespräch" bis 2010 nicht getroffen worden. Wie bei anderen KI-Projekten waren die Erwartungen viel höher als das, was tatsächlich möglich war.Mehr als 300 KI-Unternehmen hatten abgeschaltet, in Konkurs gegangen oder bis Ende 1993 erworben, die erste kommerzielle Welle von KI effektiv beendet. Nouvelle AI und verkörperter Grund In den späten 1980er Jahren befürworteten mehrere Forscher einen völlig neuen Ansatz für künstliche Intelligenz, basierend auf Robotik. Sie glaubten, dass, um echte Intelligenz zu zeigen, eine Maschine einen Körper haben muss – sie muss wahrnehmen, bewegen, überleben und mit der Welt umzugehen. Sie argumentierten, dass diese sensorimotorischen Fähigkeiten essentiell für höhere Qualifikationen sind, wie zum Beispiel Commonsense Argumentation und dass abstrakte Argumentation tatsächlich die am wenigsten interessante oder wichtige menschliche Fähigkeit war (siehe Moravec paradox). Sie befürworteten den Bau von Intelligenz "von unten nach oben. " Der Ansatz revivierte Ideen von Cybernetik und Steuerungstheorie, die seit den sechziger Jahren unbeliebt waren. Ein weiterer Vorläufer war David Marr, der in den späten 1970er Jahren von einem erfolgreichen Hintergrund in der theoretischen Neurowissenschaften zum MIT gekommen war, um die Gruppe zu führen, die Vision studierte. Er lehnte alle symbolischen Ansätze ab (beide McCarthys Logik und Minskys Frames), wobei er argumentierte, dass KI brauchte, um die physischen Maschinen der Vision von unten nach oben zu verstehen, bevor jede symbolische Verarbeitung stattgefunden hat. (Die Arbeit von Marr würde 1980 durch Leukämie verkürzt werden.) In einer 1990 erschienenen Zeitung "Elephants Don't Play Chess" verfolgte der Robotikforscher Rodney Brooks direktes Ziel auf die physikalische Symbolsystemhypothese, die argumentierte, dass Symbole nicht immer notwendig seien, da "die Welt ihr eigenes bestes Modell ist. Es ist immer genau aktuell. Es hat immer jedes Detail, es ist bekannt zu sein. Der Trick ist es sinnvoll und oft genug zu spüren. " In den 1980er und 1990er Jahren lehnten viele kognitive Wissenschaftler auch das Symbol-Verarbeitungsmodell des Geistes ab und argumentierten, dass der Körper für die Vernunft unerlässlich war, eine Theorie, die die verkörperte Denktheorie genannt. KI 1993–2011 Der Bereich der KI, jetzt mehr als ein halbes Jahrhundert alt, schließlich erreichte einige seiner ältesten Ziele. Es begann erfolgreich in der gesamten Technologiebranche zu verwenden, obwohl etwas hinter den Kulissen. Einige der Erfolge waren auf die Erhöhung der Computerleistung zurückzuführen und einige wurden durch die Fokussierung auf bestimmte isolierte Probleme und die Verfolgung mit den höchsten Standards der wissenschaftlichen Rechenschaftspflicht erreicht. Dennoch war der Ruf der KI, zumindest in der Geschäftswelt, weniger als unberührt. Im Inneren des Feldes gab es wenig Übereinstimmung über die Gründe für AIs Versagen, den Traum von menschlicher Intelligenz zu erfüllen, die die Vorstellungskraft der Welt in den 1960er Jahren erfasst hatte. Zusammen halfen all diese Faktoren, KI in konkurrierende Subfelder zu zersplittern, die sich auf bestimmte Probleme oder Ansätze konzentrierten, manchmal sogar unter neuen Namen, die den abgeschotteten Pedigree der "künstlichen Intelligenz" verkleideten. KI war sowohl vorsichtiger als auch erfolgreicher als je zuvor. Milestones and Moore's lawAm 11. Mai 1997, Tief Blue wurde das erste Computer-Chassis-Playing-System, um einen amtierenden Welt-Chassis-Champion, Garry Kasparov zu schlagen. Der Supercomputer war eine spezialisierte Version eines von IBM produzierten Rahmens und war in der Lage, doppelt so viele Bewegungen pro Sekunde zu verarbeiten, wie es während des ersten Spiels hatte (was Deep Blue verloren hatte), berichtet 200,000,000 bewegt sich pro Sekunde. Die Veranstaltung wurde live über das Internet übertragen und erhielt über 74 Millionen Treffer. Im Jahr 2005 gewann ein Stanford-Roboter die DARPA Grand Challenge, indem er autonom für 131 Meilen entlang eines unbekannten Wüstenweges fuhr. Zwei Jahre später gewann ein Team von CMU die DARPA Urban Challenge durch autonomes Navigieren von 55 Meilen in einer städtischen Umgebung, während es an Verkehrsgefahren und allen Verkehrsgesetzen festhielt. Im Februar 2011, in einem Jeopardy! quiz show exhibition match, IBMs Frage-Beantwortung System, Watson, besiegte die zwei größten Jeopardy! Champions, Brad Rutter und Ken Jennings, mit einem erheblichen Spielraum. Diese Erfolge waren nicht auf ein revolutionäres neues Paradigma zurückzuführen, sondern vor allem auf die mühsame Anwendung von Technik und auf die enorme Zunahme der Geschwindigkeit und Kapazität des Computers durch die 90er. In der Tat, Deep Blues Computer war 10 Millionen Mal schneller als die Ferranti Mark 1, dass Christopher Strachey lehrte, Schach zu spielen 1951. Diese dramatische Zunahme wird durch Moore's Gesetz gemessen, was voraussagt, dass die Geschwindigkeit und Speicherkapazität von Computern alle zwei Jahre verdoppelt, aufgrund von Metall-Oxid-Halbleiter (MOS)-Transistor verdoppelt alle zwei Jahre. Das grundlegende Problem der "Rohcomputerleistung" wurde langsam überwunden. Intelligente Agenten Ein neues Paradigma namens "intelligente Agenten" wurde in den 1990er Jahren weit verbreitet. Obwohl frühere Forscher modulare "geteilte und erobernde" Ansätze für KI vorgeschlagen hatten, erreichte der intelligente Agent seine moderne Form nicht, bis Judea Pearl, Allen Newell, Leslie P. Kaelbling und andere Konzepte aus Entscheidungstheorie und Ökonomie in die Studie von KI brachten. Als die Definition eines rationalen Agenten des Wirtschaftswissenschaftlers mit der Definition eines Objekts oder Moduls der Informatik verheiratet war, war das intelligente Agentenparadigma vollständig. Ein intelligenter Agent ist ein System, das seine Umgebung wahrnimmt und Aktionen durchführt, die seine Erfolgschancen maximieren. Durch diese Definition sind einfache Programme, die spezifische Probleme lösen, "intelligente Agenten", wie Menschen und Organisationen des Menschen, wie Firmen. Das intelligente Agentenparadigma definiert AI-Forschung als "die Studie von intelligenten Agenten". Dies ist eine Verallgemeinerung einiger früherer Definitionen von KI: es geht über das Studium der menschlichen Intelligenz hinaus; es untersucht alle Arten von Intelligenz. Das Paradigma gab Forschern Lizenz, isolierte Probleme zu studieren und Lösungen zu finden, die sowohl verifizierbar als auch nützlich waren. Es gab eine gemeinsame Sprache, um Probleme zu beschreiben und ihre Lösungen miteinander zu teilen, und mit anderen Bereichen, die auch Konzepte von abstrakten Agenten verwendet, wie Ökonomie und Kontrolle Theorie. Es wurde gehofft, dass eine komplette Agent-Architektur (wie Newells SOAR) eines Tages es Forschern ermöglichen würde, vielseitigere und intelligente Systeme aus interagierenden intelligenten Agenten aufzubauen. Die "Sieg der Nieren" KI-Forscher begannen, anspruchsvolle mathematische Werkzeuge mehr zu entwickeln und zu verwenden, als sie jemals in der Vergangenheit hatten. Es gab eine weit verbreitete Erkenntnis, dass viele der Probleme, die KI zu lösen brauchte, bereits von Forschern in Bereichen wie Mathematik, Elektrotechnik, Wirtschaft oder Betriebsforschung bearbeitet wurden. Die gemeinsame mathematische Sprache erlaubte sowohl eine höhere Zusammenarbeit mit etablierteren und erfolgreichen Feldern als auch die Errungenschaft der Ergebnisse, die messbar und nachweisbar waren; KI war zu einer strengeren wissenschaftlichen Disziplin geworden. Russell & Norvig (2003) beschreiben dies als nichts weniger als eine Revolution und "den Sieg der Neger". Judea Pearls einflussreiches 1988-Buch brachte Wahrscheinlichkeit und Entscheidungstheorie in KI. Unter den vielen neuen Tools im Einsatz waren Bayesische Netzwerke, versteckte Markov-Modelle, Informationstheorie, stochastische Modellierung und klassische Optimierung. Genaue mathematische Beschreibungen wurden auch für "computational Intelligence" Paradigmen wie neuronale Netzwerke und evolutionäre Algorithmen entwickelt. KI hinter den Kulissen Algorithmen, die ursprünglich von KI-Forschern entwickelt wurden, begannen als Teile größerer Systeme zu erscheinen. KI hatte viele sehr schwierige Probleme gelöst und ihre Lösungen erwiesen sich als nützlich in der gesamten Technologieindustrie, wie Datenbergbau, Industrierobotik, Logistik, Spracherkennung, Banksoftware, medizinische Diagnose und Google Suchmaschine. Der Bereich KI erhielt in den 1990er und Anfang 2000er Jahren wenig oder kein Guthaben für diese Erfolge. Viele der größten Innovationen von KI wurden auf den Status eines anderen Produkts in der Werkzeugkiste der Informatik reduziert. Nick Bostrom erklärt "Viele Schneide KI hat in allgemeine Anwendungen gefiltert, oft ohne KI genannt zu werden, weil einmal etwas nützlich genug wird und häufig genug ist es nicht mehr KI markiert. " Viele Forscher in KI in den 1990er Jahren nannten bewusst ihre Arbeit mit anderen Namen wie Informatik, wissensbasierte Systeme, kognitive Systeme oder computergestützte Intelligenz. Zum Teil könnte dies gewesen sein, weil sie ihren Bereich als grundlegend anders als KI betrachteten, aber auch die neuen Namen helfen, die Finanzierung zu fördern. In der kommerziellen Welt zumindest, die gescheiterten Versprechen des AI Winters weiterhin AI-Forschung in die 2000er Jahre, wie die New York Times im Jahr 2005 berichtet: "Computer-Wissenschaftler und Software-Ingenieure vermeiden den Begriff künstliche Intelligenz für Angst, als wilde Träumer betrachtet zu werden". Predictions (oder "Wo ist HAL 9000?") 1968 hatten sich Arthur C. Clarke und Stanley Kubrick vorgestellt, dass bis zum Jahr 2001 eine Maschine mit einer Intelligenz existierte, die die Fähigkeit von Menschen übertraf. Der von ihnen geschaffene Charakter, HAL 9000, basierte auf einem Glauben, der von vielen führenden KI-Forschern geteilt wurde, dass eine solche Maschine bis zum Jahr 2001 existieren würde. Im Jahr 2001 fragte KI-Gründer Marvin Minsky: "So die Frage ist, warum haben wir nicht HAL im Jahr 2001 bekommen?" Minsky glaubte, dass die Antwort darin besteht, dass die zentralen Probleme, wie die gemeinsense Argumentation, vernachlässigt wurden, während die meisten Forscher Dinge wie kommerzielle Anwendungen von neuronalen Netzen oder genetischen Algorithmen verfolgten. John McCarthy dagegen hat das Qualifizierungsproblem noch beschuldigt. Für Ray Kurzweil ist das Thema Computerleistung und mit Moore's Law prognostizierte er, dass Maschinen mit menschlicher Intelligenz bis 2029 erscheinen werden. Jeff Hawkins argumentierte, dass die neurale Nettoforschung die wesentlichen Eigenschaften der menschlichen Kortex ignoriert und einfache Modelle bevorzugt, die bei der Lösung einfacher Probleme erfolgreich waren. Es gab viele andere Erklärungen und für jeden gab es ein entsprechendes Forschungsprogramm im Gange. Deep Learning, Big Data und künstliche allgemeine Intelligenz: 2011–präsent In den ersten Jahrzehnten des 21. Jahrhunderts wurde der Zugriff auf große Datenmengen (bekannt als "große Daten"), billigere und schnellere Computer und fortgeschrittene maschinelle Lerntechniken erfolgreich auf viele Probleme in der gesamten Wirtschaft angewendet. In der Tat, McKinsey Global Institute geschätzt in ihrem berühmten Papier "Big-Daten: Die nächste Grenze für Innovation, Wettbewerb und Produktivität", dass "bis 2009 fast alle Sektoren in der US-Wirtschaft hatte mindestens einen Durchschnitt von 200 Terabyte von gespeicherten Daten". Bis 2016 erreichte der Markt für KI-bezogene Produkte, Hardware und Software mehr als 8 Milliarden Dollar, und die New York Times berichtete, dass das Interesse an KI einen Fress erreicht hatte". Die Anwendungen von Big Data begannen auch in andere Bereiche zu gelangen, wie z.B. Ausbildungsmodelle in der Ökologie und für verschiedene Anwendungen in der Wirtschaft. Fortschritte beim Deep Learning (insbesondere tiefe konvolutionale neuronale Netzwerke und wiederkehrende neuronale Netzwerke) trieben Fortschritte und Forschung in der Bild- und Videoverarbeitung, Textanalyse und sogar Spracherkennung. Deep Learning Deep Learning ist ein Zweig des maschinellen Lernens, der hochgradige Abstraktionen in Daten durch die Verwendung eines tiefen Diagramms mit vielen Verarbeitungsschichten modelliert. Nach dem Universal Approximationstheorem ist Tiefe nicht notwendig, damit ein neuronales Netz beliebige kontinuierliche Funktionen annähern kann. Auch gibt es viele Probleme, die für flache Netzwerke (z.B. Überbelegung) häufig sind, die tiefe Netzwerke vermeiden. Als solche können tiefe neuronale Netze im Vergleich zu ihren flachen Gegenstücken realistisch viel komplexere Modelle erzeugen. Das tiefe Lernen hat jedoch eigene Probleme. Ein häufiges Problem für wiederkehrende neuronale Netze ist das verschwindende Gradientenproblem, bei dem Gradienten zwischen Schichten allmählich schrumpfen und buchstäblich verschwinden, da sie auf Null abgerundet sind. Es gibt viele Methoden, die entwickelt wurden, um dieses Problem zu nähern, wie z.B. langfristige Speichereinheiten. Moderne tiefe neuronale Netzwerkarchitekturen können manchmal sogar menschliche Genauigkeit in Bereichen wie Computer Vision, speziell auf Dinge wie die MNIST-Datenbank, und Verkehrszeichenerkennung rivalisieren. Sprachverarbeitungsmotoren, die von intelligenten Suchmaschinen betrieben werden, können Menschen leicht schlagen, um allgemeine trivia Fragen (wie IBM Watson) zu beantworten, und die jüngsten Entwicklungen im Deep Learning haben erstaunliche Ergebnisse im Wettbewerb mit Menschen, in Dingen wie Go, und Doom (die, ein First-Person-Shooter-Spiel, hat einige Kontroverse gezündet). Große Daten Big Data bezieht sich auf eine Sammlung von Daten, die innerhalb eines bestimmten Zeitrahmens nicht erfasst, verwaltet und verarbeitet werden können. Es ist eine riesige Menge an Entscheidungs-, Einsichts- und Prozessoptimierungsfunktionen, die neue Verarbeitungsmodelle erfordern. In der Big Data Era geschrieben von Victor Meyer Schonberg und Kenneth Cooke bedeutet Big Data, dass statt zufälliger Analyse (Probenerhebung) alle Daten zur Analyse verwendet werden. Die 5V-Charakteristiken der Big Data (vorgestellt von IBM): Volume, Velocity, Variety, Value, Veracity. Die strategische Bedeutung der Big Data-Technologie ist es nicht, riesige Dateninformationen zu beherrschen, sondern sich auf diese sinnvollen Daten zu spezialisieren. Mit anderen Worten, wenn große Daten einer Branche ähnlich sind, ist der Schlüssel zur Realisierung der Rentabilität in dieser Branche, die "Prozessfähigkeit" der Daten zu erhöhen und die "Wertschöpfung" der Daten durch Verarbeitung zu realisieren. Allgemeine Intelligenz ist die Fähigkeit, jedes Problem zu lösen, anstatt eine Lösung für ein bestimmtes Problem zu finden. Künstliche allgemeine Intelligenz (oder AGI) ist ein Programm, das Intelligenz auf eine Vielzahl von Problemen anwenden kann, in vielerlei Hinsicht die Menschen können. Ben Goertzel und andere argumentierten in den frühen 2000er Jahren, dass KI-Forschung weitgehend auf dem ursprünglichen Ziel des Feldes der Schaffung künstlicher allgemeiner Intelligenz aufgegeben hatte. Die AGI-Forschung wurde als separates Unterfeld gegründet und bis 2010 gab es akademische Konferenzen, Labore und Universitätskurse für die AGI-Forschung sowie private Konsortien und neue Unternehmen. Künstliche allgemeine Intelligenz wird auch als "starke KI", "full AI" oder synthetische Intelligenz im Gegensatz zu "schwache KI" oder "schmal KI" bezeichnet. ( Akademische Quellen behalten sich "starke KI" vor, um sich auf Maschinen zu beziehen, die in der Lage sind, Bewusstsein zu erfahren). Siehe auch Outline of künstliche Intelligenz Fortschritt in der künstlichen Intelligenz Zeitleiste der künstlichen Intelligenz Geschichte der natürlichen Sprachverarbeitung Zeitleiste des maschinellen Lernens Geschichte der Wissensdarstellung und Argumentation Notes == Referenzen = Ein Notchback ist ein Design eines Autos mit dem hintersten Abschnitt, der vom Fahrgastraum unterscheidet und wo die Rückseite des Fahrgastraums in einem Winkel zu der Oberseite des ist, was typischerweise der hintere Gepäckraum ist. Notchback Autos haben " einen Kofferraum, dessen Deckel ein ausgeprägtes Deck bildet". In der Profilansicht hat der Körper einen Schritt nach unten vom Dach mit einer nach unten geneigten Fahrgastraum-Heckscheibe, um einen nahezu horizontalen Kofferraumdeckel zu treffen, der sich nach hinten erstreckt. Die Kategorie kann als dreifach ausgeführt sein, wobei das Stammvolumen weniger ausgeprägt ist als die Motor- und Fahrgasträume. Viele Modelle von Limousinen, Coupés oder Haschbacks könnten als Notchbacks eingestuft werden. Allerdings hat die Kategorie begrenzte Salience außerhalb von amerikanischen Autoherstellern unterscheiden die Drei-Box-Modelle von anderen Körperarten in der gleichen Modellreihe. Zum Beispiel umfasste die Chevrolet Vega Reihe sowohl einen Notchback-Gutschein als auch einen Fastback-Gutschein. Nordamerika Einer der ersten Autos, die als Notchback vermarktet werden, ist das 1938 Cadillac Sixty Special. 1952 wurde eine Notchback-Version des Nash-Botschafters eingeführt. Im Jahr 1971 vermarktete Chevrolet die dreifachen Limousinenmodelle des Chevrolet Vega als Notchback, um sie von den Fastback Vega-Modellen zu unterscheiden. Für das Modelljahr 1973 wurde der Name des Autos in "Vega Notchback" geändert. Englisch-sprachige Länder Während viele Automodelle Kerbrück-Charakteristiken haben, ist die Kategorie außerhalb Nordamerikas weitgehend ungenutzt, wobei ihr Körperstil mit anderen Begriffen beschrieben wird. Zum Beispiel ist eine dreifache Limousine allgemeiner als Saloon in British English bekannt. Notchback ist in einigen britischen englischen Publikationen erschienen, aber es ist kein Begriff, der in Großbritannien im allgemeinen verwendet wird. Siehe auch Karosseriestil Ponton Styling == Referenzen ==