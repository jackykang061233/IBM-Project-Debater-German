In der rechnerischen Komplexität und Optimierung ist das keine freie Mittagstheorem ein Ergebnis, dass für bestimmte Arten von mathematischen Problemen die Rechenkosten der Suche nach einer Lösung, gemittelt über alle Probleme in der Klasse, ist die gleiche für jede Lösungsmethode. Daher bietet keine Lösung einen "kurzen Schnitt". Dies ist unter der Annahme, dass der Suchraum eine Wahrscheinlichkeitsdichtefunktion ist. Es gilt nicht für den Fall, dass der Suchraum eine zugrunde liegende Struktur hat (z.B. eine differenzierbare Funktion), die effizienter ausgenutzt werden kann (z.B. Newtons Methode in der Optimierung) als zufällige Suche oder sogar geschlossene Lösungen (z.B. das Extrema eines quadratischen Polynoms), die ohne Suche überhaupt bestimmt werden können. Für solche probabilistischen Annahmen sind die Ausgänge aller Verfahren, die eine bestimmte Art von Problem lösen, statistisch identisch. Eine bunte Art, einen solchen Umstand zu beschreiben, der von David Wolpert und William G. Macready in Verbindung mit den Problemen der Suche und Optimierung eingeführt wird, ist zu sagen, dass es kein kostenloses Mittagessen gibt. Wolpert hatte zuvor keine kostenlosen Mittagessen Theorien für maschinelles Lernen abgeleitet (statistische Inferenz.) Bevor Wolperts Artikel veröffentlicht wurde, erwies Cullen Schaffer unabhängig eine eingeschränkte Version eines der Theoreme von Wolpert und nutzte es, um den aktuellen Stand der maschinellen Lernforschung über das Problem der Induktion zu schreiben. In der "keine kostenlose Mittagsmetaphore" hat jedes Restaurant (Problemlösungsverfahren) ein Menü, das jedem "Lunchplate" (Problem) einen Preis (die Leistung des Verfahrens bei der Lösung des Problems) zugeordnet. Die Menüs der Restaurants sind identisch, außer in einer Hinsicht – die Preise werden von einem Restaurant zum nächsten geschüttelt. Für einen allgegenwärtigen, der jede Platte als jede andere bestellen wird, sind die durchschnittlichen Kosten des Mittagessens nicht von der Wahl des Restaurants abhängig. Aber ein Veganer, der regelmäßig mit einem Fleischfresser zu Mittag geht, der die Wirtschaft sucht, könnte eine hohe durchschnittliche Kosten für das Mittagessen zahlen. Um die durchschnittlichen Kosten methodisch zu reduzieren, muss man Vorkenntnisse von a) verwenden, was man bestellen wird und b) was der Auftrag zu verschiedenen Restaurants kosten wird. Das heißt, Verbesserung der Leistungsfähigkeit bei der Problemlösung Scharniere bei der Verwendung früherer Informationen, um Verfahren zu Problemen passen. In formalen Begriffen gibt es kein kostenloses Mittagessen, wenn die Wahrscheinlichkeitsverteilung auf Problemfällen so ist, dass alle Problemlöser identisch verteilte Ergebnisse haben. Bei der Suche ist eine Probleminstanz eine objektive Funktion, und ein Ergebnis ist eine Folge von Werten, die bei der Auswertung von Kandidatenlösungen im Bereich der Funktion gewonnen werden. Für typische Interpretationen der Ergebnisse ist die Suche ein Optimierungsprozess. Es gibt kein kostenloses Mittagessen in der Suche, wenn und nur, wenn die Verteilung auf objektive Funktionen invariant unter Permutation des Raums der Kandidatenlösungen ist. Diese Bedingung hält nicht genau in der Praxis, aber ein ("fast) kein kostenloses Mittagessen" Theorem deutet darauf hin, dass es ungefähr hält. Überblick Einige rechnerische Probleme werden durch die Suche nach guten Lösungen in einem Raum der Kandidatenlösungen gelöst. Eine Beschreibung der wiederholten Auswahl von Kandidatenlösungen für die Auswertung wird als Suchalgorithmus bezeichnet. Bei einem bestimmten Problem können unterschiedliche Suchalgorithmen unterschiedliche Ergebnisse erzielen, aber über alle Probleme sind sie unauffällig. Es folgt, dass, wenn ein Algorithmus überlegene Ergebnisse bei einigen Problemen erreicht, muss es mit Unfruchtbarkeit auf andere Probleme zahlen. In diesem Sinne gibt es kein kostenloses Mittagessen in der Suche. Alternativ wird nach Schaffer die Suchleistung konserviert. Üblicherweise wird die Suche als Optimierung interpretiert, was zur Beobachtung führt, dass es kein kostenloses Mittagessen in der Optimierung gibt. " Das "keine kostenlose Mittags-Theorem von Wolpert und Macready", wie es in der einfachen Sprache von Wolpert und Macready selbst gesagt wird, ist, dass "alle zwei Algorithmen gleichwertig sind, wenn ihre Leistung über alle möglichen Probleme gemittelt wird." Die "keine kostenlose Mittagspause"-Ergebnisse zeigen, dass die passenden Algorithmen zu Problemen eine höhere durchschnittliche Leistung bieten als bei allen einen festen Algorithmus anwenden. Igel und Toussaint und Englisch haben eine allgemeine Bedingung geschaffen, unter der es kein kostenloses Mittagessen gibt. Während es physisch möglich ist, hält es nicht genau. Droste, Jansen und Wegener haben sich als Theorem erwiesen, das sie als Hinweis darauf interpretieren, dass es in der Praxis ("fast) kein freies Mittagessen" gibt. Um Dinge konkreter zu machen, betrachten Sie einen Optimierungspraktizierenden mit einem Problem konfrontiert. Angesichts einiger Kenntnisse, wie das Problem aufgetreten ist, kann der Praktizierende in der Lage sein, das Wissen in der Auswahl eines Algorithmus auszunutzen, der bei der Lösung des Problems gut funktioniert. Wenn der Praktizierende nicht versteht, wie man das Wissen ausnutzt oder einfach kein Wissen hat, dann steht er oder sie vor der Frage, ob ein Algorithmus in der Regel andere über reale Probleme hinausgeht. Die Autoren des Theorem ("fast) kein freies Mittagessen" sagen, dass die Antwort im Wesentlichen nein ist, aber einige Vorbehalte zugeben, ob das Theorem Praxis anspricht. Theorems Ein Problem ist, formaler, eine objektive Funktion, die Kandidatenlösungen mit Gütewerten verbindet. Ein Suchalgorithmus übernimmt eine objektive Funktion als Eingabe und wertet Kandidatenlösungen einzeln aus. Der Ausgang des Algorithmus ist die Reihenfolge der beobachteten Gütewerte. Wolpert und Macready legen fest, dass ein Algorithmus nie eine Kandidatenlösung neu bewertet, und dass die Algorithmusleistung an Ausgängen gemessen wird. Für die Einfachheit entlüften wir die Zufallsmäßigkeit in Algorithmen. Unter diesen Bedingungen, wenn ein Suchalgorithmus auf jeder möglichen Eingabe ausgeführt wird, erzeugt er jede mögliche Ausgabe genau einmal. Da die Leistung an den Ausgängen gemessen wird, unterscheiden sich die Algorithmen nicht, wie oft sie bestimmte Leistungsstufen erreichen. Einige Maßnahmen der Leistung zeigen, wie gut Suchalgorithmen bei der Optimierung der Zielfunktion tun. Tatsächlich scheint es keine interessante Anwendung von Suchalgorithmen in der betrachteten Klasse, sondern zu Optimierungsproblemen. Ein gemeinsames Leistungsmaß ist der geringste Index des geringsten Wertes in der Ausgangssequenz. Dies ist die Anzahl der Bewertungen, die erforderlich sind, um die Zielfunktion zu minimieren. Für einige Algorithmen ist die Zeit, die benötigt wird, um das Minimum zu finden, proportional zur Anzahl der Auswertungen. Die ursprünglichen No free Lunch (NFL) Theorems gehen davon aus, dass alle objektiven Funktionen gleichermaßen an Suchalgorithmen eingegeben werden. Es ist seither festgestellt worden, dass es NFL gibt, wenn und nur, wenn, lose gesprochen, schrumpfende objektive Funktionen keinen Einfluss auf ihre Wahrscheinlichkeiten haben. Obwohl diese Bedingung für NFL physisch möglich ist, wurde argumentiert, dass es sicherlich nicht genau hält. Die offensichtliche Interpretation von "nicht NFL" ist "freies Mittagessen", aber das ist irreführend. NFL ist eine Frage des Grades, nicht eine all-oder-nothing-Bestimmung. Wenn die Bedingung für NFL ungefähr hält, dann liefern alle Algorithmen ungefähr die gleichen Ergebnisse über alle objektiven Funktionen." Nicht NFL" bedeutet nur, dass Algorithmen insgesamt durch einige Maß an Leistung unäquivalent sind. Für ein Leistungsmaß von Interesse können Algorithmen gleichwertig bleiben, oder fast so. Kolmogorov Zufälligkeit Fast alle Elemente des Satzes aller möglichen Funktionen (im set-theoretischen Sinne der Funktion) sind Kolmogorov zufällig, und daher gelten die NFL-Theorems für eine Reihe von Funktionen, die fast alle nicht kompakter ausgedrückt werden können als eine Lookup-Tabelle, die einen eindeutigen (und zufälligen) Eintrag für jeden Punkt im Suchraum enthält. Funktionen, die kompakter ausgedrückt werden können (z.B. durch einen mathematischen Ausdruck vernünftiger Größe) sind durch Definition nicht Kolmogorov zufällig. Darüber hinaus sind innerhalb der Reihe aller möglichen objektiven Funktionen auch Werte der Güte unter Kandidatenlösungen vertreten, so dass gute Lösungen im gesamten Raum der Kandidaten verstreut werden. Dementsprechend wird ein Suchalgorithmus selten mehr als einen kleinen Teil der Kandidaten auswerten, bevor eine sehr gute Lösung gefunden wird. Fast alle objektiven Funktionen sind von so hoher Kolmogorov-Komplexität, dass sie nicht in einem bestimmten Computer gespeichert werden können. Genauer gesagt, wenn wir einen bestimmten physischen Computer als Registermaschine mit einem bestimmten Größenspeicher in der Reihenfolge der Speicher von modernen Computern modellieren, dann können die meisten objektiven Funktionen nicht in ihren Speichern gespeichert werden. Es gibt mehr Informationen in der typischen objektiven Funktion oder Algorithmus als Seth Lloyd Schätzungen, das beobachtbare Universum ist in der Lage zu registrieren. Wird beispielsweise jede Kandidatenlösung als Folge von 300 0's und 1's kodiert und die Gütewerte 0 und 1 betragen, so weisen die meisten objektiven Funktionen Kolmogorov-Komplexität von mindestens 2300 Bit auf, was größer ist als Lloyd's Grenze von 1090 ≈ 2299 Bit. Es folgt, dass das ursprüngliche "kein kostenloses Mittagessen" Theorem nicht für das gilt, was in einem physischen Computer gespeichert werden kann; stattdessen müssen die sogenannten verschärften keine freien Mittagessen Theorems angewendet werden. Es wurde auch gezeigt, dass NFL-Ergebnisse für unbestrittene Funktionen gelten. Formale Synopsis Y X {\displaystyle Y^{X} ist der Satz aller objektiven Funktionen f:X→Y, wobei X {\displaystyle X} ein endlicher Lösungsraum ist und Y {\displaystyle Y} eine endliche Pose ist. Der Satz aller Permutationen von X ist J. Eine zufällige Variable F wird auf Y X verteilt {\displaystyle Y^{X} . Für alle j in J ist F o j eine auf Y X {\displaystyle Y^{X} verteilte Zufallsvariable mit P(F o j = f) = P(F = f o j-1) für alle f in Y X {\displaystyle Y^{X} .Let a(f) die Ausgabe des Suchalgorithmus a am Eingang f.If a(F) und b(F) für alle Suchalgorithmen identisch verteilt sind. Diese Bedingung gilt, wenn und nur, wenn F und F o j für alle j in J identisch verteilt sind. Mit anderen Worten gibt es kein kostenloses Mittagessen für Suchalgorithmen, wenn und nur, wenn die Verteilung der objektiven Funktionen unter Permutation des Lösungsraums invariant ist. Set-theoretisch NFL-Theorems wurden kürzlich auf willkürliche Kardialität X {\displaystyle X} und Y {\displaystyle Y} verallgemeinert. Origin Wolpert und Macready geben zwei Haupt-NFL-Theorems, die erste über objektive Funktionen, die sich nicht ändern, während der Suche im Gang ist, und die zweite über objektive Funktionen, die sich ändern können. Die Werte für jedes Paar von Algorithmen a1 und a2 Σ f P (d m y | f , m , a 1 ) = Σ f P (d m y | f , m , a 2 ) , {\displaystyle \sum f}(d_{m}{y}}}, m,a_{1})=\sumf}(d_ Y {\displaystyle f:X\rightarrow Y} ist die optimierte Funktion und P (d m y | f , m , a ) {\displaystyle P(d_{m}^{y}|f,m,a) ist die bedingte Wahrscheinlichkeit, eine bestimmte Folge von Kostenwerten aus dem Algorithmus zu erhalten, a {\displaystyle a} m {\displaystyle m} mals auf der Funktion f {\displaystyle f} ausführen. Im Wesentlichen sagt dies, dass, wenn alle Funktionen f gleichwahrscheinlich sind, die Wahrscheinlichkeit, eine beliebige Folge von m-Werten im Verlauf der Suche zu beobachten, nicht vom Suchalgorithmus abhängt. Theorem 1 stellt ein "feineres" NFL-Ergebnis für zeitverändernde objektive Funktionen fest. Interpretationen der Ergebnisse Eine konventionelle, aber nicht ganz genaue Interpretation der NFL-Ergebnisse ist, dass "eine allgemeine universelle Optimierungsstrategie theoretisch unmöglich ist, und die einzige Möglichkeit, eine Strategie zu übertreffen, ist, wenn sie sich auf das jeweilige Problem spezialisiert". Mehrere Kommentare sind in der Reihenfolge: Es besteht theoretisch ein allgemeiner nahezu universeller Optimierer. Jeder Suchalgorithmus erfüllt fast alle objektiven Funktionen gut. Wenn man sich also nicht mit den "relativ kleinen" Unterschieden zwischen Suchalgorithmen beschäftigt, z.B. weil die Computerzeit billig ist, dann sollte man sich keine Sorgen um kein freies Mittagessen machen. Ein Algorithmus kann eine andere auf einem Problem übertreffen, wenn weder auf das Problem spezialisiert ist. Tatsächlich kann es sein, dass beide Algorithmen zu den schlimmsten für das Problem gehören. In der Regel haben Wolpert und Macready ein Maß für den Grad der Ausrichtung zwischen einem Algorithmus und einer Verteilung über Probleme entwickelt (in der Regel ein inneres Produkt). Um zu sagen, dass ein Algorithmus eine bessere Verteilung als eine andere ist nicht zu sagen, dass entweder bewusst auf die Verteilung spezialisiert ist; ein Algorithmus kann eine gute Ausrichtung nur durch Glück haben. In der Praxis bewerten einige Algorithmen Kandidatenlösungen neu. Der Grund, nur die Leistung auf nie zuvor bewerteten Kandidaten zu betrachten ist, sicherzustellen, dass beim Vergleich von Algorithmen man Äpfel mit Äpfeln vergleicht. Darüber hinaus kann jede Überlegenheit eines Algorithmus, der nie neu bewertet Kandidaten über einen anderen Algorithmus, der auf einem bestimmten Problem tut, nichts mit Spezialisierung auf das Problem zu tun haben. Für fast alle objektiven Funktionen ist die Spezialisierung im Wesentlichen zufällig. Unkomprimierbare, oder Kolmogorov zufällige, objektive Funktionen haben keine Regelmäßigkeit für einen Algorithmus auszunutzen, was die universelle Turing-Bearbeitung zur Definition von Kolmogorov-Zufälligkeit betrifft. So vermuten Sie, dass es eine, eindeutig überlegene Wahl der universellen Turing-Maschine. Dann gibt es bei einer objektiven Funktion, die für diese Turing-Maschine unkomprimierbar ist, keine Grundlage für die Wahl zwischen zwei Algorithmen, wenn beide komprimierbar sind, wie mit dieser Turing-Maschine gemessen. Wenn ein gewählter Algorithmus besser als die meisten ausführt, ist das Ergebnis passiert. Eine Kolmogorov Zufallsfunktion hat keine Darstellung kleiner als eine Lookup-Tabelle, die einen (random) Wert enthält, der jedem Punkt im Suchraum entspricht; jede Funktion, die kompakter ausgedrückt werden kann, ist definitionsgemäß nicht Kolmogorov random. In der Praxis passen nur stark komprimierbare (außer zufällige) objektive Funktionen in die Speicherung von Computern, und es ist nicht der Fall, dass jeder Algorithmus gut auf fast allen komprimierbaren Funktionen ausführt. Es besteht in der Regel ein Leistungsvorteil bei der Einarbeitung von Vorkenntnissen des Problems in den Algorithmus. Während die NFL-Ergebnisse in strengem Sinne Vollbeschäftigungstheorien für Optimierungsexperten darstellen, ist es wichtig, den größeren Kontext im Auge zu behalten. Zum einen haben Menschen oft wenig Vorkenntnisse mit zu arbeiten. Für eine andere, mit Vorkenntnissen nicht viel von einem Leistungsgewinn auf einige Probleme. Schließlich ist die menschliche Zeit relativ zur Computerzeit sehr teuer. Es gibt viele Fälle, in denen ein Unternehmen wählen würde, eine Funktion langsam mit einem unmodifizierten Computerprogramm anstatt schnell mit einem humanmodifizierten Programm zu optimieren. Die NFL-Ergebnisse zeigen nicht, dass es mühelos ist, "Kalbaufnahmen" bei Problemen mit unspezialisierten Algorithmen zu nehmen. Niemand hat den Bruchteil praktischer Probleme ermittelt, für die ein Algorithmus rasch gute Ergebnisse liefert. Und es gibt ein praktisches kostenloses Mittagessen, überhaupt nicht im Konflikt mit der Theorie. Eine Implementierung eines Algorithmus auf einem Computer zu betreiben, kostet sehr wenig, bezogen auf die Kosten der menschlichen Zeit und den Nutzen einer guten Lösung. Wenn es einem Algorithmus gelingt, eine zufriedenstellende Lösung in einer akzeptablen Zeit zu finden, hat eine kleine Investition eine große Auszahlung ergeben. Wenn der Algorithmus ausfällt, dann ist wenig verloren. Coevolution Wolpert und Macready haben bewiesen, dass es kostenlose Mittagessen in der koevolutionären Optimierung gibt. Ihre Analyse "deckt Selbstspielprobleme. In diesen Problemen arbeiten die Spieler zusammen, um einen Champion zu produzieren, der dann einen oder mehrere Antagonisten in einem nachfolgenden Multiplayer-Spiel eingreift." Das ist das Ziel, einen guten Spieler zu erhalten, aber ohne eine objektive Funktion. Die Güte jedes Spielers (Kandidatlösung) wird durch Beobachtung, wie gut es gegen andere spielt beurteilt. Ein Algorithmus versucht, Spieler und ihre Spielqualität zu verwenden, um bessere Spieler zu erhalten. Der Spieler, der am besten vom Algorithmus betrachtet wird, ist der Champion. Wolpert und Macready haben gezeigt, dass einige koevolutionäre Algorithmen in der Regel anderen Algorithmen in der Qualität der gewonnenen Champions überlegen sind. Die Generierung eines Champions durch Selbstspiel ist von Interesse an evolutionären Berechnungen und Spieltheorie. Die Ergebnisse sind unanwendbar für die Koevolution biologischer Arten, die keine Champions liefern. Siehe auch Evolutionsinformatik Induktive Vorspannung Occam'srazor Simplicity Ugly Entkling theorem Notes Externe Links http://www.no-free-lunch.org Radcliffe and Surry, 1995, "Fundamental Limitations on Search Algorithms: Evolutionary Computing in Perspective" (ein früh veröffentlichtes Papier auf NFL, das bald nach Schaffers ICML-Papier erscheint, die wiederum auf Wolpert's Preprint basierte; in verschiedenen Formaten erhältlich)N WolFL-Publikationen von Thomas Englisch NFL-Publikationen von Christian Igel und Marc Toussaint NFL Das Starling wurde von Lawrence Livermore National Laboratory entwickelt. Nach Angaben des Forschers Chuck Hansen hatte er einen 11,53 Zoll (293 mm) Durchmesser, 14,25 Zoll (362 mm) Länge und ein Gewicht von 86,4 Pfund (39,2 kg). Die nominale Ausbeute betrug 10 bis 20 Kilotonnen TNT (42 bis 84 TJ). Es war ein Zweipunkt-Primär, wie Tsetse, Kinglet, Robin und Gnat. Hansen listet auch Starling als Primär für die W55 (zusammen mit Kinglet) und die W56 auf. Hansen listet es als Nachfolger von Robin (zusammen mit Kinglet.) Siehe auch Liste der AtomwaffenTeller-Ulam Design Referenzen Hansen, Chuck (2007). Die Schwerter von Armageddon, Version 2.Chukelea Veröffentlichungen.