Spionierende neuronale Netze (SNNs) sind künstliche neuronale Netzwerke, die natürliche neuronale Netzwerke stärker imitieren. Neben dem neuronalen und synaptischen Zustand integrieren SNNs das Konzept der Zeit in ihr Betriebsmodell. Die Idee ist, dass Neuronen in der SNN keine Informationen an jedem Ausbreitungszyklus übermitteln (wie es mit typischen Mehrschicht-Perceptron-Netzwerken geschieht), sondern nur dann Informationen übermitteln, wenn ein Membranpotenzial – eine intrinsische Qualität des Neurons im Zusammenhang mit seiner Membran-Elektroladung – einen bestimmten Wert erreicht, der die Schwelle genannt wird. Wenn das Membranpotential die Schwelle erreicht, feuert das Neuron und erzeugt ein Signal, das auf andere Neuronen zurückgeht, die sein Potential in Reaktion auf dieses Signal erhöhen oder verringern. Ein Neuronmodell, das im Moment der Grenzüberschreitung feuert, wird auch als spiking Neuron Modell bezeichnet. Das prominenteste Spiking Neuron Modell ist das undichte Integra-and-Feuer-Modell. Im Integrier-und-Feuer-Modell wird der momentane Aktivierungspegel (als Differentialgleichung modelliert) normalerweise als der Neuron-Zustand betrachtet, wobei ankommende Spikes diesen Wert höher oder niedriger schieben, bis der Zustand schließlich entweder abklingen oder - wenn die Schußschwelle erreicht ist - die Neuronfeuer. Nach dem Zünden wird die Zustandsgröße auf einen niedrigeren Wert zurückgesetzt. Zur Interpretation des abgehenden Spitzenzuges als Realwertzahl existieren verschiedene Dekodierverfahren, die entweder auf die Frequenz der Spitzen (Raten-Code), den Zeit-zu-Erst-Spike nach der Stimulation oder das Intervall zwischen Spießen beruhen. Geschichte Mehrschichtige künstliche neuronale Netzwerke sind in der Regel vollständig verbunden, empfangen Eingang von jedem Neuron in der vorherigen Schicht und signalisieren jedes Neuron in der nachfolgenden Schicht. Obwohl diese Netzwerke in vielen Bereichen Durchbrüche erreicht haben, sind sie biologisch ungenau und dem Operationsmechanismus von Neuronen im Gehirn einer lebendigen Sache nicht nachahmen. Das biologisch inspirierte Hodgkin-Huxley-Modell eines spikenden Neurons wurde 1952 vorgeschlagen. Dieses Modell beschreibt, wie Aktionspotentiale initiiert und propagiert werden. Die Kommunikation zwischen Neuronen, die den Austausch von chemischen Neurotransmittern in der synaptischen Lücke erfordert, ist in verschiedenen Modellen beschrieben, wie das Integrier- und Feuermodell, FitzHugh-Nagumo Modell (1961–1962) und Hindmarsh–Rose Modell (1984). Das undichte Integra-and-Feuer-Modell (oder ein Derivat) wird häufig verwendet, da es einfacher zu berechnen ist als das Hodgkin-Huxley-Modell. Im Juli 2019 auf dem Gipfel der DARPA Electronics Resurgence Initiative enthüllte Intel ein 8-Millionen-neuron-Neuromorphikum mit 64 Loihi-Forschungschips. Untermauerungen Aus der informationstheoretischen Perspektive besteht das Problem, zu erklären, wie Informationen durch eine Reihe von Impulsfolgen, d.h. Aktionspotentiale, kodiert und decodiert werden. Eine grundlegende Frage der Neurowissenschaften ist also, ob Neuronen durch einen Geschwindigkeits- oder zeitlichen Code kommunizieren. Temporale Codierung deutet darauf hin, dass ein einzelnes spikendes Neuron Hunderte von versteckten Einheiten auf einem sigmoidalen neuronalen Netz ersetzen kann. Ein spikendes neuronales Netz betrachtet zeitliche Informationen. Die Idee ist, dass nicht alle Neuronen in jeder Ausbreitungsbeschleunigung aktiviert werden (wie es in einem typischen Mehrschicht-Perceptron-Netzwerk der Fall ist), sondern nur, wenn sein Membranpotential einen bestimmten Wert erreicht. Wenn ein Neuron aktiviert wird, erzeugt es ein Signal, das an angeschlossene Neuronen geleitet wird, ihr Membranpotential erhöht oder senkt. In einem spikierenden neuronalen Netz wird der aktuelle Zustand des Neurons als seine Aktivierungsstufe definiert (als Differentialgleichung dargestellt). Ein Eingangsimpuls bewirkt, daß der aktuelle Zustandswert über einen Zeitraum ansteigt und dann allmählich abnimmt. Um diese Ausgangsimpulsfolgen unter Berücksichtigung sowohl der Pulsfrequenz als auch des Pulsintervalls als Anzahl zu interpretieren, sind Kodierschemata aufgebaut. Ein neuronales Netzmodell basierend auf der Pulserzeugungszeit kann genau festgelegt werden. Spike-Codierung wird in diesem neuen neuronalen Netz übernommen. Mit der genauen Zeit des Pulsereignisses kann ein neuronales Netzwerk mehr Informationen einsetzen und eine stärkere Rechenleistung bieten. Pulsgekoppelte neuronale Netzwerke (PCNN) werden oft mit SNNs verwechselt. Ein PCNN ist als eine Art SNN zu sehen. Der SNN-Ansatz verwendet anstelle des kontinuierlichen Outputs herkömmlicher ANNs einen binären Ausgang (Signal/kein Signal). Darüber hinaus sind Pulstrainings nicht leicht auslegbar. Aber Pulstraining erhöht die Fähigkeit, Spatiotemporaldaten zu verarbeiten (oder ständige Echtzeit-Welt-Sensorik-Datenklassifikation). Raum bezieht sich auf die Tatsache, dass Neuronen nur mit nahe gelegenen Neuronen verbinden, so dass sie Eingangsblöcke separat verarbeiten können (ähnlich CNN mit Filtern). Die Zeit bezieht sich auf die Tatsache, dass die Pulsausbildung über die Zeit erfolgt, so dass die bei der binären Codierung verlorene Information aus der Zeitinformation abgerufen werden kann. Dies vermeidet die zusätzliche Komplexität eines wiederkehrenden neuronalen Netzes (RN). Es stellt sich heraus, dass Impulsneuronen leistungsfähigere Recheneinheiten sind als herkömmliche künstliche Neuronen. SNN ist theoretisch leistungsfähiger als Netzwerke der zweiten Generation, jedoch begrenzen SNN-Trainingsprobleme und Hardware-Anforderungen ihren Einsatz. Obwohl ungesupervised biologische Lernmethoden zur Verfügung stehen, wie zum Beispiel Hebbian Learning und STDP, ist keine effektive überwachte Trainingsmethode für SNN geeignet, die eine bessere Leistung als Netzwerke der zweiten Generation bieten kann. Die Spike-basierte Aktivierung von SNNs ist nicht differenzierbar, so dass es schwierig ist, Gradienten-Abstiegs-basierte Trainingsmethoden zur Fehlerwiederholung zu entwickeln, obwohl einige neuere Algorithmen wie NormAD und Multilayer NormAD durch geeignete Annäherung des Gradienten der Spike-basierten Aktivierung eine gute Trainingsleistung nachgewiesen haben. SNNs haben viel größere Rechenkosten für die Simulation realistischer neuronaler Modelle als herkömmliche ANNs. Die Anwendungen SNNs können grundsätzlich auf die gleichen Anwendungen wie herkömmliche ANNs Anwendung finden. Darüber hinaus können SNNs das zentrale Nervensystem biologischer Organismen, wie ein Insekt, der ohne vorherige Kenntnis der Umwelt Nahrung sucht, modellieren. Aufgrund ihrer relativen Realismus können sie dazu verwendet werden, den Betrieb biologischer neuronaler Schaltungen zu untersuchen. Ausgehend von einer Hypothese zur Topologie eines biologischen Neuronalkreislaufs und seiner Funktion können Aufnahmen dieser Schaltung mit dem Ausgang der entsprechenden SN verglichen werden, wobei die Plausibilität der Hypothese beurteilt wird. Es fehlt jedoch an effektiven Trainingsmechanismen für SNNs, die für einige Anwendungen, einschließlich Computer-Visionsaufgaben, hemmungshemmend sein können. Ab 2019 lagen SNNs ANNs in Bezug auf die Genauigkeit, aber die Lücke ist abgenommen und hat auf einige Aufgaben verschwunden. Software Eine vielfältige Anwendungssoftware kann SNNs simulieren. Diese Software kann nach ihren Verwendungen klassifiziert werden: SNN-Simulation Diese simulieren komplexe neuronale Modelle mit einem hohen Detail- und Genauigkeitsgrad. Große Netzwerke benötigen in der Regel eine lange Verarbeitung. Zu den Kandidaten zählen:GENESIS (das GEnerale NEural SImulation System) – entwickelt in James Bowers Labor an der Caltech; NEURON – hauptsächlich entwickelt von Michael Hines, John W. Moore und Ted Carnevale in der Yale University und Duke University; Brian – entwickelt von Romain Brette und Danachman an der École Normale Supérieure; NEST – entwickelt von der NESTNET Initiative SpykeTorch - ein Framework auf Basis von PyTorch, das speziell für Faltungs-SNNs mit maximal einem Spike pro Neuron optimiert wurde. Lauft auf GPUs. Hardware Zukunft neuromorphe Architekturen werden Milliarden solcher Nanosynapsen umfassen, die ein klares Verständnis der für die Plastizität verantwortlichen physikalischen Mechanismen erfordern. Experimentelle Systeme auf Basis ferroelektrischer Tunnelübergänge haben gezeigt, dass STDP aus heterogener Polarisationsschaltung ausgenutzt werden kann. Durch die kombinierte Abtastsondenabbildung, den elektrischen Transport und die atomar-skalige molekulare Dynamik können Leitungsschwankungen durch kerndotierte Umkehr von Domänen modelliert werden. Simulationen zeigen, dass Arrays von ferroelektrischen Nanosynapsen autonom lernen können, Muster in einer vorhersehbaren Weise zu erkennen, den Weg in Richtung ununterbrochenes Lernen zu öffnen. Brainchips Akida NSoC behauptet, effektiv 1,2 Millionen Neuronen und 10 Milliarden Synapsen Neurogrid zu haben, ist ein Board, das Spiking-Neural-Netzwerke direkt in Hardware simulieren kann.(Stanford University) SpiNNaker (Spiking Neural Network Architecture) verwendet ARM-Prozessoren als Bausteine einer massiv parallelen Rechenplattform auf Basis eines sechsschichtigen thalamocortical Modells.(University of Manchester) Es bietet individuelle digitale Chips, die jeweils achtzehn Kerne und einen gemeinsamen lokalen 128 Mbyte RAM, mit insgesamt über 1.000.000 Kernen. Ein einziger Chip kann 16.000 Neuronen simulieren, wobei acht Millionen Kunststoffsynapsen in Echtzeit laufen. TrueNorth ist ein Prozessor, der 5,4 Milliarden Transistoren enthält, die nur 70 Milliwatt verbraucht; die meisten Prozessoren in Personalcomputern enthalten etwa 1,4 Milliarden Transistoren und benötigen 35 Watt oder mehr. IBM bezieht sich auf das Designprinzip hinter TrueNorth als neuromorphes Computing. Sein Hauptzweck ist die Mustererkennung. Während Kritiker sagen, dass der Chip nicht leistungsfähig genug ist, weisen seine Anhänger darauf hin, dass dies nur die erste Generation ist, und die Fähigkeiten von verbesserten Iterationen werden deutlich.(IBM) Dynamic Neuromorphic Asynchronous Processor (DYNAP) kombiniert langsame, Low-Power, inhomogene Sub-Threshold analog Schaltungen und schnelle programmierbare digitale Schaltungen. Es unterstützt rekonfigurierbare, allgemeine, Echtzeit-Neuralnetze von spikenden Neuronen. Dies ermöglicht die Implementierung von Echtzeit-Spike-basierten neuronalen Verarbeitungsarchitekturen, in denen Speicher und Berechnung kolokalisiert sind. Es löst das von Neumann Engpassproblem und ermöglicht die Echtzeit-Multiplex-Kommunikation von Spiking-Ereignissen zur Realisierung massiver Netzwerke. Wiederkehrende Netzwerke, Feed-Forward-Netzwerke, Faltungsnetzwerke, Lockerungsnetzwerke, Echo-State-Netzwerke, tiefe Netzwerke und Sensor-Fusions-Netzwerke sind einige der Möglichkeiten. Loihi ist eine 14-nm Intel Chip, der 128 Kerne und 130.000 Neuronen auf einem 60-mm-Paket bietet. Es integriert eine breite Palette von Funktionen, wie hierarchische Konnektivität, dendritische Fächer, synaptische Verzögerungen und programmierbare synaptische Lernregeln. Mit einer spikenden konvolutionalen Form des lokal wettbewerbsfähigen Algorithms kann Loihi LASSO-Optimierungsprobleme mit über drei Größenordnungen überlegenen Energieverzögerungsprodukten lösen, verglichen mit herkömmlichen Solvens, die auf einem CPU-Isoprozess/Spannung/Flächen laufen. Ein 64 Loihi Forschungssystem bietet 8-Millionen-neuron neuromorphes System. Loihi ist etwa 1.000 Mal so schnell wie eine CPU und 10.000 Mal als energieeffizient. Messing S basiert auf physikalischen Emulationen von Neuron, Synapse und Plastizitätsmodellen mit digitaler Konnektivität, läuft bis zu zehntausend Mal schneller als Echtzeit. Es wurde vom Europäischen Projekt Human Brain entwickelt. Die BrainScale S-System enthält 20 8-Zoll-Silizium-Wafer in 180 nm-Prozesstechnik. Jeder Wafer enthält 50 x 106 Kunststoffsynapsen und 200.000 biologisch realistische Neuronen. Das System führt nicht vorprogrammierten Code aus, sondern entwickelt sich nach den physikalischen Eigenschaften der elektronischen Geräte, die mit bis zu 10 Tausendfach schneller als Echtzeit laufen. Benchmarks Die Klassifizierungsfähigkeiten von nach nicht überwachten Lernmethoden ausgebildeten Spiking-Netzwerken wurden an den gemeinsamen Benchmark-Datensätzen, wie Iris, Wisconsin Brustkrebs oder Statlog Landsat-Datensatz, getestet. Es wurden verschiedene Ansätze zur Informationscodierung und Netzwerkgestaltung eingesetzt. Zum Beispiel ein 2-Schicht-Feedforward-Netzwerk zur Daten-Clusterung und -klassifizierung. Anhand der in Hopfield (1995) vorgeschlagenen Idee realisierten die Autoren Modelle von lokalen Aufnahmefeldern, die die Eigenschaften von radialen Basisfunktionen (RBF) und spikenden Neuronen kombinieren, um Eingangssignale (klassifizierte Daten) mit einer Floating-Point-Darstellung in eine Spiking-Darstellung umzuwandeln. Siehe auch CoDi Kognitive Architektur Kognitive Karte Kognitiver Computer Computational NeuroscienceNeural codieren Neural korreliert Neural dekodierende Neuroethologie Neuroinformatik Modelle der neuronalen Berechnung Bewegungserkennung Systeme Neurowissenschaften Referenzen Externe Links Vollständiger Text des Buches Spiking Neuron Models. Einzelne Neuronen, Populationen, Plastizität von Wulfram Gerstner und Werner M. Kistler (ISBN 0-521-89079-9)