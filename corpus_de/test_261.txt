Ein Supercomputer ist ein Computer mit einem hohen Leistungsniveau im Vergleich zu einem universellen Computer. Die Leistung eines Supercomputers wird in Floating-Point-Betrieben pro Sekunde (FLOPS) anstelle von Millionen Anweisungen pro Sekunde (MIPS) üblicherweise gemessen. Seit 2017 gibt es Supercomputer, die über 1017 FLOPS (einhundert Quadrillionen FLOPS, 100 petaFLOPS oder 100 PFLOPS) ausführen können. Seit November 2017 betreiben alle weltweit schnellsten 500 Supercomputer Linux-basierte Betriebssysteme. In den Vereinigten Staaten, der Europäischen Union, Taiwan, Japan und China werden zusätzliche Forschungen durchgeführt, um schnellere, leistungsfähigere und technologisch überlegene Exascale-Supercomputer aufzubauen. Supercomputer spielen eine wichtige Rolle im Bereich der Rechenwissenschaft und werden für eine breite Palette von rechnerisch intensiven Aufgaben in verschiedenen Bereichen verwendet, einschließlich Quantenmechanik, Wettervorhersage, Klimaforschung, Öl- und Gasexploration, Molekularmodellierung (Berechnung der Strukturen und Eigenschaften chemischer Verbindungen, biologischer Makromoleküle, Polymere und Kristalle) und physikalischer Simulationen (wie Simulationen der frühen Momente des Universums, der aerodynamischen Waffen und des Universums) Sie waren im Bereich der Kryptanalyse unerlässlich. Supercomputer wurden in den 1960er Jahren eingeführt, und seit mehreren Jahrzehnten wurde die schnellste von Seymour Cray bei Control Data Corporation (CDC), Cray Research und nachfolgenden Unternehmen mit seinem Namen oder Monogramm gemacht. Die ersten solchen Maschinen waren hoch abgestimmte konventionelle Designs, die schneller liefen als ihre allgemeineren Zeitgenossen. Im Laufe des Jahrzehnts wurden zunehmende Mengen an Parallelismus hinzugefügt, wobei ein bis vier Prozessoren typisch sind. In den 1970er Jahren wurden Vektorprozessoren, die auf großen Datenfeldern arbeiten, dominiert. Ein bemerkenswertes Beispiel ist der sehr erfolgreiche Cray-1 von 1976. In den 1990er Jahren blieben die Vektorrechner das dominante Design. Von damals bis heute wurden massiv parallele Supercomputer mit Zehntausenden von Off-the-Shelf-Prozessoren zur Norm. Die USA sind seit langem führend im Supercomputer-Bereich, zunächst durch Crays fast ununterbrochene Dominanz des Bereichs, und später durch eine Vielzahl von Technologieunternehmen. Japan hat in den 80er und 90er Jahren große Fortschritte auf diesem Gebiet gemacht, wobei China zunehmend aktiver wird. Ab Juni 2020 ist der schnellste Supercomputer auf der TOP500 Supercomputerliste Fugaku, in Japan, mit einem LINPACK Benchmark-Score von 415 PFLOPS, gefolgt von Summit, von rund 266,7 PFLOPS. Die USA haben vier der Top 10; China und Italien haben jeweils zwei, die Schweiz hat eine. Im Juni 2018 brachen alle kombinierten Supercomputer auf der TOP500-Liste die 1 exaFLOPS-Marke. Geschichte 1960 baute UNIVAC den Livermore Atomic Research Computer (LARC), heute unter den ersten Supercomputern, für das US Navy Research and Development Center. Es verwendet immer noch Hochgeschwindigkeits-Trommelspeicher, anstatt die neu entstehende Festplattenantriebstechnologie. Auch unter den ersten Supercomputern war der IBM 7030 Stretch. Der IBM 7030 wurde von IBM für das Los Alamos National Laboratory gebaut, das 1955 einen Computer 100 mal schneller als jeder vorhandene Computer angefordert hatte. Der IBM 7030 benutzte Transistoren, Magnetkernspeicher, Pipeline-Anweisungen, Prefetched-Daten über einen Speichercontroller und beinhaltete wegweisende Zufalls-Zugriffs-Disk-Antriebe. Der IBM 7030 wurde 1961 fertig gestellt und trotz der Herausforderung einer hundertfachen Leistungssteigerung wurde er vom Los Alamos National Laboratory gekauft. Kunden in England und Frankreich kauften auch den Computer, und es wurde die Grundlage für die IBM 7950 Harvest, ein Supercomputer für die Kryptanalyse gebaut. Das dritte wegweisende Supercomputer-Projekt in den frühen 1960er Jahren war der Atlas an der Universität Manchester, gebaut von einem Team unter der Leitung von Tom Kilburn. Er konzipierte den Atlas, um Speicherplatz für bis zu einer Million Wörter von 48 Bit zu haben, aber weil die magnetische Speicherung mit einer solchen Kapazität nicht beeinflussbar war, betrug der eigentliche Kernspeicher des Atlas nur 16.000 Wörter, wobei eine Trommel Speicher für weitere 96.000 Wörter bereitstellte. Das Atlas-Betriebssystem tauschte die Daten in Form von Seiten zwischen dem Magnetkern und der Trommel aus. Das Atlas-Betriebssystem führte auch eine zeitliche Aufteilung auf Supercomputing ein, so dass jederzeit mehr als ein Programm auf dem Supercomputer ausgeführt werden konnte. Atlas war ein Joint-Venture zwischen Ferranti und der Manchester University und wurde entwickelt, um mit Verarbeitungsgeschwindigkeiten zu arbeiten, die eine Mikrosekunde pro Unterricht nähern, etwa eine Million Anweisungen pro Sekunde. Die CDC 6600, entworfen von Seymour Cray, wurde 1964 fertiggestellt und markiert den Übergang von Germanium zu Siliziumtransistoren. Silicon-Transistoren könnten schneller laufen und das Überhitzungsproblem wurde durch die Einführung von Kälte auf den Supercomputer-Design gelöst. So wurde der CDC6600 zum schnellsten Computer der Welt. Da die 6600 alle anderen zeitgenössischen Computer um etwa 10 Mal übertrafen, wurde es ein Supercomputer gegraben und den Supercomputing-Markt definiert, als hundert Computer mit jeweils 8 Millionen Dollar verkauft wurden. Cray verließ 1972 CDC, um sein eigenes Unternehmen Cray Research zu bilden. Vier Jahre nach CDC lieferte Cray 1976 den 80 MHz Cray-1, der zu einem der erfolgreichsten Supercomputer der Geschichte wurde. Der Cray-2 wurde 1985 veröffentlicht. Es hatte acht zentrale Verarbeitungseinheiten (CPUs), Flüssigkeitskühlung und die Elektronik Kühlmittelflüssigkeit Fluorinert wurde durch die Supercomputerarchitektur gepumpt. Es erreicht 1,9 gigaFLOPS, so dass es der erste Supercomputer, um die Gigaflop Barriere zu brechen. Massive parallele Designs Der einzige Computer, der die Leistung von Cray-1 in den 1970er Jahren ernsthaft herausforderte, war der ILLIAC IV. Diese Maschine war das erste realisierte Beispiel eines echten massiv parallelen Computers, bei dem viele Prozessoren zusammengearbeitet haben, um verschiedene Teile eines einzigen größeren Problems zu lösen. Im Gegensatz zu den Vektorsystemen, die so schnell wie möglich einen einzigen Datenstrom ausführen sollen, speist der Rechner stattdessen separate Teile der Daten an ganz unterschiedliche Prozessoren und rekombiniert dann die Ergebnisse. Das ILLIAC-Design wurde 1966 mit 256 Prozessoren fertiggestellt und bietet eine Geschwindigkeit von bis zu 1 GFLOPS, verglichen mit dem Cray-1's Peak von 250 MFLOPS. Allerdings führten Entwicklungsprobleme zu nur 64 Prozessoren, und das System konnte nie schneller arbeiten als etwa 200 MFLOPS, während viel größer und komplexer als der Cray. Ein weiteres Problem war, dass das Schreiben von Software für das System schwierig war, und das Erreichen der Spitzenleistung von ihm war eine Frage ernsthafter Anstrengungen. Aber der teilweise Erfolg des ILLIAC IV wurde weithin als Hinweis auf den Weg zur Zukunft des Supercomputing gesehen. Cray argumentierte gegen diese, berühmt quipping, dass "Wenn Sie ein Feld pflügen, was würden Sie lieber verwenden? Zwei starke Ochsen oder 1024 Hühner?" Doch Anfang der 1980er Jahre arbeiteten mehrere Teams an parallelen Designs mit Tausenden von Prozessoren, insbesondere der Verbindungsmaschine (CM), die aus der Forschung am MIT entwickelt wurde. Der CM-1 verwendet so viele wie 65,536 vereinfachte benutzerdefinierte Mikroprozessoren in einem Netzwerk miteinander verbunden, um Daten zu teilen. Mehrere aktualisierte Versionen folgten; der CM-5 Supercomputer ist ein massiv paralleler Verarbeitungsrechner, der in der Lage ist, viele Milliarden von arithmetischen Operationen pro Sekunde. Das LINKS-1 Computer Graphics System der Osaka University nutzte 1982 eine massiv parallele Verarbeitungsarchitektur mit 514 Mikroprozessoren, darunter 257 Zilog Z8001 Steuerprozessoren und 257 iAPX 86/20 Schwimmpunktprozessoren. Es wurde hauptsächlich für realistische 3D-Computergrafiken verwendet. Fujitsu's VPP500 von 1992 ist ungewöhnlich, da, um höhere Geschwindigkeiten zu erreichen, seine Prozessoren GaAs verwendet, ein Material, das normalerweise für Mikrowellenanwendungen aufgrund seiner Toxizität reserviert ist. Fujitsu's Numerischer Wind Tunnel Supercomputer nutzte 166 Vektorprozessoren, um den Top-Spot 1994 mit einer Spitzengeschwindigkeit von 1,7 GigaFLOPS (GFLOPS) pro Prozessor zu gewinnen. Die Hitachi SR2201 erzielte 1996 eine Spitzenleistung von 600 GFLOPS durch Verwendung von 2048 Prozessoren, die über ein schnelles dreidimensionales Crossbar-Netzwerk verbunden sind. Der Intel Paragon könnte 1000 bis 4000 Intel i860 Prozessoren in verschiedenen Konfigurationen haben und wurde 1993 am schnellsten der Welt platziert. Das Paragon war eine MIMD-Maschine, die Prozessoren über ein hochgeschwindigkeits zweidimensionales Mesh angeschlossen hat, wodurch Prozesse an separaten Knoten ausgeführt werden können, die über das Message Passing Interface kommuniziert werden. Die Software-Entwicklung blieb ein Problem, aber die CM-Serie entzündete erhebliche Forschungen zu diesem Thema. Ähnliche Designs mit benutzerdefinierter Hardware wurden von vielen Unternehmen gemacht, einschließlich der Evans & Sutherland ES-1, MasPar, nCUBE, Intel iPSC und der Goodyear MPP. Aber bis Mitte der 1990er Jahre hatte die allgemeine CPU-Leistung so viel verbessert, dass ein Supercomputer mit ihnen als die einzelnen Verarbeitungseinheiten gebaut werden konnte, anstatt benutzerdefinierte Chips zu verwenden. Bis zur Wende des 21. Jahrhunderts waren Designs mit zehntausenden Waaren-CPUs die Norm, mit späteren Maschinen, die grafische Einheiten zum Mix hinzufügen. Systeme mit einer großen Anzahl von Prozessoren nehmen in der Regel einen von zwei Pfaden. Im Netz Computing-Ansatz wird die Verarbeitungsleistung vieler Computer, organisiert als verteilte, verschiedene Verwaltungsbereiche, opportunistisch verwendet, wenn ein Computer verfügbar ist. In einem anderen Ansatz werden viele Prozessoren in der Nähe zueinander verwendet, z.B. in einem Computercluster. Bei einem solchen zentralisierten massiv parallelen System wird die Geschwindigkeit und Flexibilität der Verbindung sehr wichtig und moderne Supercomputer haben verschiedene Ansätze verwendet, von verbesserten Infiniband-Systemen bis zu dreidimensionalen Torus-Verbindungen. Die Verwendung von Multi-Core-Prozessoren kombiniert mit Zentralisierung ist eine Schwellenrichtung, z.B. wie im Cyclops64-System. Da sich der Preis, die Leistung und die Energieeffizienz der allgemeinen Grafikverarbeitungseinheiten (GPGPUs) verbessert haben, haben einige petaFLOPS Supercomputer wie Tianhe-I und Nebulae begonnen, sich auf sie zu verlassen. Andere Systeme wie der K-Computer verwenden jedoch weiterhin konventionelle Prozessoren wie SPARC-basierte Designs und die generelle Anwendbarkeit von GPGPUs in universellen Hochleistungs-Computing-Anwendungen war Gegenstand der Debatte, dass eine GPGPU zwar auf bestimmte Benchmarks gut abgestimmt werden kann, aber ihre Gesamtanwendbarkeit auf alltägliche Algorithmen begrenzt werden kann, es sei denn, es wird ein erheblicher Aufwand dafür ausgegeben, die Anwendung darauf abzustimmen. GPUs gewinnen jedoch an Boden, und 2012 wurde der Jaguar Supercomputer durch Nachrüstung von CPUs mit GPUs in Titan umgewandelt. Hochleistungsrechner haben einen erwarteten Lebenszyklus von etwa drei Jahren, bevor ein Upgrade erforderlich ist. Der Gyoukou Supercomputer ist einzigartig, indem er sowohl eine massiv parallele Konstruktion als auch eine Flüssigkeitstauchkühlung verwendet. Spezialzweck-Supercomputer Es wurden eine Reihe spezieller Systeme entwickelt, die einem einzigen Problem gewidmet sind. Dies ermöglicht die Verwendung speziell programmierter FPGA-Chips oder sogar benutzerdefinierter ASICs, wodurch bessere Preis-Leistungs-Verhältnisse durch Verallgemeinerung ermöglicht werden. Beispiele für Spezial-Supercomputer sind Belle, Deep Blue und Hydra zum Spielen von Schach, Gravity Pipe für Astrophysik, MDGRAPE-3 für Proteinstrukturvorhersage und Molekulardynamik sowie Deep Crack zum Brechen der DES-Kipherie. Energienutzung und Wärmemanagement Im Laufe der Jahrzehnte ist die Verwaltung der Wärmedichte ein Schlüsselthema für die meisten zentralisierten Supercomputer. Die große Wärmemenge, die durch ein System erzeugt wird, kann auch andere Effekte haben, z.B. die Lebensdauer anderer Systemkomponenten zu reduzieren. Es gab verschiedene Ansätze zur Wärmeleitung, vom Pumpen von Fluorinert durch das System, zu einem Hybrid-Flüssig-Luft-Kühlsystem oder Luftkühlung mit normalen Klimatemperaturen. Ein typischer Supercomputer verbraucht große Mengen elektrischer Leistung, die fast alle in Wärme umgewandelt werden, was Kühlung erfordert. Beispielsweise verbraucht Tianhe-1A 4.04 Megawatt (MW) Strom. Die Kosten für die Leistung und Kühlung des Systems können signifikant sein, z.B. 4 MW bei $0.10/k $400 pro Stunde oder etwa $3,5 Millionen pro Jahr. Wärmemanagement ist ein wichtiges Thema in komplexen elektronischen Geräten und betrifft leistungsstarke Computersysteme auf verschiedene Weise. Die Wärme-Design-Leistung und CPU-Leistungsableitung Probleme bei der Supercomputing übertreffen diejenigen der traditionellen Computer-Kühltechnologien. Die ausgezeichneten Auszeichnungen für Green Computing spiegeln dieses Thema wider. Die Zusammenpackung von tausenden von Prozessoren erzeugt zwangsläufig erhebliche Mengen an Wärmedichte, die behandelt werden müssen. Der Cray-2 wurde flüssig gekühlt und verwendet einen Fluorinert "Kühlwasserfall", der unter Druck durch die Module gedrückt wurde. Der untergetauchte Flüssigkeitskühlungsansatz war jedoch für die Multi-Kabinett-Systeme auf Basis von Off-the-Shelf-Prozessoren nicht praktikabel, und in System X wurde in Verbindung mit der Firma Liebert ein spezielles Kühlsystem entwickelt, das die Klimatisierung mit Flüssigkeitskühlung kombiniert. Im Blue Gene-System nutzte IBM bewusst Low-Power-Prozessoren, um mit Wärmedichte umzugehen. Die 2011 veröffentlichte IBM Power 775 verfügt über eng gepackte Elemente, die Wasserkühlung erfordern. Das IBM Aquasar-System nutzt die Warmwasserkühlung, um Energieeffizienz zu erreichen, wobei das Wasser auch für Gebäude verwendet wird. Die Energieeffizienz von Computersystemen wird in der Regel in Bezug auf "FLOPS pro Watt" gemessen. Im Jahr 2008, Roadrunner von IBM betrieben bei 3.76 MFLOPS/W. Im November 2010 erreichte der Blue Gene/Q 1.684 MFLOPS/W und im Juni 2011 wurden die beiden Top-Spots auf der Green 500-Liste von Blue Gene Maschinen in New York besetzt (eine zu erreichen super 2097 MFLOPS/W) mit dem DEGIMA-Cluster in Nagasaki Platzierung dritten mit 1375 MFLOPS/W. Ab 2015 haben viele bestehende Supercomputer mehr Infrastrukturkapazität als die tatsächliche Spitzennachfrage der Maschine – Designer entwerfen in der Regel konservativ die Strom- und Kühlinfrastruktur, um mehr als die vom Supercomputer verbrauchte theoretische Spitzenleistung zu handhaben. Designs für zukünftige Supercomputer sind leistungsbegrenzt – die thermische Auslegungsleistung des Supercomputers insgesamt, der Betrag, den die Strom- und Kühlinfrastruktur bewältigen kann, ist etwas mehr als der erwartete normale Stromverbrauch, aber weniger als der theoretische Spitzenleistungsverbrauch der elektronischen Hardware. Software und Systemmanagement Betriebssysteme Seit Ende des 20. Jahrhunderts haben Supercomputer-Betriebssysteme aufgrund der Veränderungen in der Supercomputer-Architektur große Veränderungen erfahren. Während frühe Betriebssysteme individuell auf jeden Supercomputer zugeschnitten waren, um Geschwindigkeit zu gewinnen, war der Trend, weg von in-house-Betriebssystemen zu der Anpassung von generischen Software wie Linux. Da moderne massiv parallele Supercomputer typischerweise Berechnungen von anderen Diensten durch die Verwendung mehrerer Knotentypen trennen, laufen sie üblicherweise auf verschiedenen Knoten unterschiedliche Betriebssysteme, z.B. mit einem kleinen und effizienten Leichtbaukern wie CNK oder CNL auf Rechenknoten, aber ein größeres System wie ein Linux-Derivat auf Server- und I/O-Knoten. Während in einem traditionellen Multi-User-Computersystem Job-Scheduling ist in der Tat ein Aufgabenproblem für die Verarbeitung und periphere Ressourcen, in einem massiv parallelen System, das Job-Management-System muss die Zuordnung von sowohl Rechen- und Kommunikationsressourcen zu verwalten, sowie anmutig mit unvermeidlichen Hardware-Ausfällen, wenn Zehntausende von Prozessoren vorhanden sind. Obwohl die meisten modernen Supercomputer Linux-basierte Betriebssysteme verwenden, hat jeder Hersteller seinen eigenen spezifischen Linux-Derivat, und es gibt keinen Industriestandard, zum Teil aufgrund der Tatsache, dass die Unterschiede in Hardware-Architekturen Änderungen erfordern, um das Betriebssystem zu jedem Hardware-Design zu optimieren. Software-Tools und Nachrichtenübermittlung Die parallelen Architekturen von Supercomputern diktieren oft die Verwendung spezieller Programmiertechniken, um ihre Geschwindigkeit auszunutzen. Software-Tools für die verteilte Verarbeitung umfassen Standard-APIs wie MPI und PVM, VTL und Open Source-Software wie Beowulf. Im häufigsten Szenario werden Umgebungen wie PVM und MPI für lose verbundene Cluster und OpenMP für eng aufeinander abgestimmte gemeinsame Speichermaschinen verwendet. Wesentlicher Aufwand ist die Optimierung eines Algorithmus für die Interconnect-Eigenschaften der Maschine, auf der sie ausgeführt wird; Ziel ist es, zu verhindern, dass eine der CPUs Zeit auf Daten von anderen Knoten wartet. GPGPUs haben Hunderte von Prozessorkernen und werden mit Programmiermodellen wie CUDA oder OpenCL programmiert. Außerdem ist es sehr schwierig, parallele Programme zu debuggen und zu testen. Spezielle Techniken müssen zum Testen und Debuggen solcher Anwendungen verwendet werden. Opportunistische Supercomputing ist eine Form von Netzwerk-Gitter-Computing, bei der ein "Super-Virtual-Computer" von vielen lose gekoppelten Freiwilligen-Computing-Maschinen sehr große Rechenaufgaben ausführen kann. Grid Computing wurde auf eine Reihe von großräumigen peinlich parallelen Problemen angewendet, die supercomputing Leistungsskala erfordern. Grundsätzliche Grid- und Cloud-Computing-Ansätze, die sich auf freiwilliges Computing verlassen, können jedoch nicht mit traditionellen Supercomputing-Aufgaben wie fluiddynamische Simulationen umgehen. Das schnellste Netzrechnersystem ist das verteilte Rechenprojekt Folding@home (F@h). Ab April 2020 meldete F@h 2,5 ExaFLOPS von x86 Verarbeitungsleistung. Davon werden über 100 PFLOPS von Clients, die auf verschiedenen GPUs laufen, und der Rest von verschiedenen CPU-Systemen unterstützt. Die Plattform Berkeley Open Infrastructure for Network Computing (BOINC) beherbergt eine Reihe verteilter Rechenprojekte. Ab Februar 2017 verzeichnete BOINC eine Verarbeitungsleistung von über 166 petaFLOPS über 762 Tausend aktiven Computern (Hosts) im Netzwerk. Seit Oktober 2016 hat die Great Internet Mersenne Prime Search (GIMPS) die Mersenne Prime-Suche um 0,313 PFLOPS über 1,3 Millionen Computer erreicht. Der Internet PrimeNet Server unterstützt den GIMPS-Grid Computing-Ansatz, einer der frühesten und erfolgreichsten Grid Computing-Projekte seit 1997. Quasi-opportunistische Ansätze Quasi-opportunistische Supercomputing ist eine Form des verteilten Computing, wobei der "Super-Virtual-Computer" vieler vernetzter geographisch disperser Computer Rechenaufgaben durchführt, die enorme Verarbeitungsleistung erfordern. Quasi-opportunistische Supercomputing zielt darauf ab, eine höhere Qualität des Dienstes als opportunistisches Netz Computing zu bieten, indem mehr Kontrolle über die Aufgabenverteilung auf verteilte Ressourcen und die Verwendung von Intelligenz über die Verfügbarkeit und Zuverlässigkeit einzelner Systeme innerhalb des Supercomputing-Netzwerks erreicht wird. Die quasi-opportunistische verteilte Ausführung anspruchsvoller paralleler Rechensoftware in Netzen sollte jedoch durch die Umsetzung von netzweisen Zuordnungsvereinbarungen, Co-Allocation-Subsystemen, Kommunikationstopologie-Aware-Zuordnungsmechanismen, Fehler tolerante Nachricht, die Bibliotheken und Datenvorkonditionierung weitergibt, erreicht werden. Hochleistungs-Computing-Wolken Cloud Computing mit seinen jüngsten und schnellen Erweiterungen und Entwicklung haben die Aufmerksamkeit von High-Performance Computing (HPC) Benutzern und Entwicklern in den letzten Jahren ergriffen. Cloud Computing versucht HPC-as-a-Service genau wie andere in der Cloud verfügbare Dienste wie Software als Service, Plattform als Service und Infrastruktur als Service zu bieten. HPC-Nutzer können von der Cloud in unterschiedlichen Winkeln wie Skalierbarkeit, Ressourcen auf Anfrage, schnell und kostengünstig profitieren. Auf der anderen Seite haben bewegte HPC-Anwendungen auch eine Reihe von Herausforderungen. Gute Beispiele solcher Herausforderungen sind die Virtualisierung in der Cloud, die Vielschichtigkeit der Ressourcen und die Probleme der Netzwerk-Latenz. Viele Forschungen werden derzeit durchgeführt, um diese Herausforderungen zu überwinden und HPC in der Cloud zu einer realistischeren Möglichkeit zu machen. 2016 starteten Penguin Computing, R-HPC, Amazon Web Services, Univa, Silicon Graphics International, Sabalcore und Gomput HPC Cloud Computing. Die Penguin On Demand (POD) Cloud ist ein Bare-Metall-Compute-Modell, um Code auszuführen, aber jeder Benutzer wird virtualisiert Anmeldeknoten gegeben. POD-Computing-Knoten werden über nicht-virtualisierte 10 Gbit/s Ethernet oder QDR InfiniBand-Netzwerke verbunden. Die Benutzerkonnektivität zum POD-Datenzentrum reicht von 50 Mbit/s bis 1 Gbit/s. Mit der EC2 Elastische Compute Cloud von Amazon argumentiert Penguin Computing, dass die Virtualisierung von Rechenknoten nicht für HPC geeignet ist. Penguin Computing hat auch kritisiert, dass HPC Clouds möglicherweise Computing-Knoten für Kunden zugewiesen haben, die weit auseinander sind, wodurch Latenz, die die Leistung für einige HPC-Anwendungen beeinträchtigt. Leistungsmessung Kapazität gegen Kapazität Supercomputer zielen in der Regel auf die maximale Leistungsberechnung anstatt auf die Kapazitätsberechnung. Capability Computing wird in der Regel als die Verwendung der maximalen Rechenleistung gedacht, um ein einziges großes Problem in kürzester Zeit zu lösen. Oft kann ein Fähigkeitssystem ein Problem einer Größe oder Komplexität lösen, die kein anderer Computer kann, z.B. eine sehr komplexe Wettersimulation Anwendung. Kapazitäts Computing dagegen wird typischerweise als effiziente kostengünstige Rechenleistung gedacht, um einige etwas große Probleme oder viele kleine Probleme zu lösen. Architekturen, die sich für die Unterstützung von vielen Benutzern für routinemäßige Alltagsaufgaben eignen, können eine Menge Kapazität haben, werden aber in der Regel nicht als Supercomputer betrachtet, da sie kein einziges sehr komplexes Problem lösen. Leistungskennzahlen Im allgemeinen wird die Geschwindigkeit der Supercomputer in FLOPS (Floating-Point-Operationen pro Sekunde) gemessen und bewertet, nicht in Bezug auf MIPS (Millionen-Anweisungen pro Sekunde), wie dies bei Universalrechnern der Fall ist. Diese Messungen werden häufig mit einem SI-Präfix wie tera,- kombiniert in den Kurzhand-TFLOPS (1012 FLOPS, ausgeprägte Teraflops) oder peta,- kombiniert in den Kurzhand-PFLOPS (1015 FLOPS, ausgeprägte Petaflops) verwendet. Petascale Supercomputer können einen Quadrillion (1015) (1000 Trillion) FLOPS verarbeiten. Exascale ist die Rechenleistung im exaFLOPS (EFLOPS) Bereich. Ein EFLOPS ist ein Quittillion (1018) FLOPS (eine Million TFLOPS). Keine einzelne Zahl kann die Gesamtleistung eines Computersystems widerspiegeln, aber das Ziel der Linpack Benchmark ist es, wie schnell der Computer numerische Probleme löst und es ist in der Industrie weit verbreitet. Die FLOPS-Messung wird entweder auf der Grundlage der theoretischen Floating Point-Performance eines Prozessors (aus den Prozessorspezifikationen des Herstellers abgeleitet und als Rpeak in den TOP500-Listen angezeigt) angegeben, der im Allgemeinen bei realen Workloads oder dem erzielbaren Durchsatz, der aus den LINPACK-Benchmarks abgeleitet und als Rmax in der TOP500-Liste angezeigt wird. Der LINPACK Benchmark führt typischerweise eine LU-Zersetzung einer großen Matrix durch. Die LINPACK-Leistung gibt einige Anzeichen für Leistung für einige real-world-Probleme, entspricht aber nicht unbedingt den Verarbeitungsanforderungen vieler anderer Supercomputer-Workloads, die zum Beispiel mehr Speicherbandbreite erfordern, oder kann eine bessere Ganzzahl-Computing-Leistung erfordern, oder kann ein High-Performance-I/O-System benötigen, um hohe Leistungsstufen zu erreichen. Die TOP500-Liste Seit 1993 sind die schnellsten Supercomputer nach ihren LINPACK Benchmark-Ergebnissen auf der TOP500-Liste platziert. Die Liste behauptet nicht, unvoreingenommen oder endgültig zu sein, aber es ist eine weithin zitierte aktuelle Definition des schnellsten Supercomputers, der jederzeit verfügbar ist. Dies ist eine aktuelle Liste der Computer, die an der Spitze der TOP500-Liste erschien, und die "Peak-Geschwindigkeit" wird als Rmax-Rating gegeben. 2018 wurde Lenovo weltweit größter Anbieter für die TOP500 Supercomputer mit 117 produzierten Einheiten. Anwendungen Die Stufen der Supercomputeranwendung können in der folgenden Tabelle zusammengefaßt sein: Der IBM Blue Gene/P-Computer wurde verwendet, um eine Anzahl von künstlichen Neuronen zu simulieren, die etwa einem Prozent eines menschlichen zerebralen Kortex entspricht, der 1,6 Milliarden Neuronen mit etwa 9 Trillion-Verbindungen enthält. Die gleiche Forschungsgruppe gelang es auch, einen Supercomputer zu verwenden, um eine Reihe von künstlichen Neuronen zu simulieren, die dem gesamten Gehirn einer Ratte entspricht. Auch die moderne Wettervorhersage stützt sich auf Supercomputer. Die National Oceanic and Atmospheric Administration verwendet Supercomputer, um Hunderte von Millionen von Beobachtungen zu knacken, um die Wettervorhersagen genauer zu machen. Im Jahr 2011 wurden die Herausforderungen und Schwierigkeiten, den Umschlag in Supercomputing zu verschieben, durch IBMs Aufgabe des Projekts Blue Waters petascale unterstrichen. Das Advanced Simulation and Computing Program verwendet derzeit Supercomputer, um die US-Kernbestände zu erhalten und zu simulieren. Anfang 2020 war COVID-19 in der Welt mittendrin. Supercomputer nutzten verschiedene Simulationen, um Verbindungen zu finden, die die Ausbreitung möglicherweise stoppen könnten. Diese Computer laufen für zehn Stunden mit mehreren parallel laufenden CPU's zu Modell unterschiedlichen Prozessen. Entwicklung und Trends In den 2010er Jahren, China, die Vereinigten Staaten, die Europäische Union und andere konkurrieren, um die erste zu sein, um ein 1 exaFLOP (1018 oder ein Quitillion FLOPS) Supercomputer zu schaffen. Erik P. DeBenedictis von Sandia National Laboratories hat theorisiert, dass ein zettaFLOPS (1021 oder ein Sextillion FLOPS) Computer benötigt wird, um Vollwettermodellierung zu erreichen, die eine zweiwöchige Zeitspanne genau abdecken könnte. Solche Systeme könnten etwa 2030 gebaut werden. Viele Monte Carlo-Simulationen verwenden den gleichen Algorithmus, um einen zufällig generierten Datensatz zu verarbeiten; insbesondere integro-differentiale Gleichungen, die physikalische Transportprozesse beschreiben, die zufälligen Pfade, Kollisionen und Energie- und Impulsablagerungen von Neutronen, Photonen, Ionen, Elektronen usw. Der nächste Schritt für Mikroprozessoren kann in die dritte Dimension sein; und auf Monte Carlo spezialisiert, könnten die vielen Schichten identisch sein, vereinfachen den Konstruktions- und Herstellungsprozess. Die Kosten für den Betrieb von Hochleistungs-Supercomputern sind gestiegen, vor allem aufgrund des steigenden Stromverbrauchs. Mitte der 1990er Jahre benötigt ein Top 10 Supercomputer im Bereich von 100 Kilowatt, im Jahr 2010 die Top 10 Supercomputer zwischen 1 und 2 Megawatt. Eine 2010 Studie, die von DARPA in Auftrag gegeben wurde, identifizierte den Stromverbrauch als die überzeugendste Herausforderung bei der Realisierung von Exascale Computing. Zur Zeit kostete ein Megawatt pro Jahr im Energieverbrauch etwa 1 Million Dollar. Supercomputing-Anlagen wurden gebaut, um die zunehmende Wärmemenge, die durch moderne Multicore-Zentral-Verarbeitungseinheiten erzeugt wird, effizient zu entfernen. Auf der Grundlage des Energieverbrauchs der Green 500 Liste der Supercomputer zwischen 2007 und 2011 hätte ein Supercomputer mit 1 exaFLOPS 2011 fast 500 Megawatt benötigt. Für bestehende Hardware wurden Betriebssysteme entwickelt, um Energie jederzeit zu erhalten. CPU-Kerne, die während der Ausführung einer parallelisierten Anwendung nicht im Einsatz waren, wurden in Low-Power-Zustände eingesetzt, wodurch Energieeinsparungen für einige supercomputing-Anwendungen entstehen. Die steigenden Kosten für den Betrieb von Supercomputern waren ein treibender Faktor für die Bündelung von Ressourcen durch eine verteilte Supercomputer-Infrastruktur. Nationale Supercomputing-Zentren entstanden zunächst in den USA, gefolgt von Deutschland und Japan. Die Europäische Union hat die Partnerschaft für Fortgeschrittene in Europa (PRACE) mit dem Ziel gestartet, eine dauerhafte paneuropäische Supercomputer-Infrastruktur mit Dienstleistungen zu schaffen, um Wissenschaftlern in der gesamten Europäischen Union bei der Portierung, Skalierung und Optimierung von Supercomputing-Anwendungen zu unterstützen. Island baute den weltweit ersten Null-Emission-Supercomputer. Im Thor Data Center in Reykjavík, Island, liegt dieser Supercomputer auf völlig erneuerbaren Quellen für seine Macht und nicht fossilen Brennstoffen. Das kältere Klima reduziert auch die Notwendigkeit einer aktiven Kühlung, so dass es eine der grünsten Einrichtungen der Welt der Computer. Auch die Förderung von Supercomputer-Hardware wurde immer schwieriger. Mitte der 1990er Jahre kostete ein Top 10 Supercomputer etwa 10 Millionen Euro, während 2010 die Top 10 Supercomputer eine Investition zwischen 40 und 50 Millionen Euro erforderten. In den 2000er Jahren haben nationale Regierungen verschiedene Strategien zur Finanzierung von Supercomputern eingeführt. Im Vereinigten Königreich wurde die nationale Regierung von Supercomputern vollständig finanziert und ein hochleistungsfähiges Computing unter die Kontrolle einer nationalen Finanzbehörde gestellt. Deutschland hat ein gemischtes Fördermodell entwickelt, mit dem die Landesfinanzierung und die Bundesfinanzierung zusammengeführt werden. In der Fiktion Viele Science-Fiction-Autoren haben Supercomputer in ihren Werken dargestellt, sowohl vor und nach der historischen Konstruktion solcher Computer. Ein Großteil dieser Fiktion beschäftigt sich mit den Beziehungen der Menschen mit den Computern, die sie bauen, und mit der Möglichkeit, Konflikte schließlich zwischen ihnen zu entwickeln. Beispiele für Supercomputer in Fiktion sind HAL-9000, Multivac, The Machine Stops, GLaDOS, The Evitable Conflict, Vulcan's Hammer, Colossus, WOPR und Deep Thought. Siehe auch Hinweise und Hinweise Externe Links McDonnell, Marshall T. (2013) Supercomputer Design: Ein erstes Anliegen, die Umwelt-, Wirtschafts- und gesellschaftlichen Auswirkungen zu erfassen. Chemische und biomolekulare Veröffentlichungen und andere Werke.