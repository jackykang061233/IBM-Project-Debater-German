Erklärbare AI (XAI) ist künstliche Intelligenz (KI), in der die Ergebnisse der Lösung von Menschen verstanden werden können. Es kontrastiert mit dem Konzept der "schwarzen Box" im maschinellen Lernen, wo selbst ihre Designer nicht erklären können, warum eine KI eine bestimmte Entscheidung erreichte. XAI kann eine Umsetzung des sozialen Rechtes auf Erklärung sein. XAI ist relevant, auch wenn es kein rechtliches oder regulatorisches Erfordernis gibt – zum Beispiel kann XAI die Nutzererfahrung eines Produktes oder Dienstes verbessern, indem Endbenutzer dabei unterstützt werden, dass die AI gute Entscheidungen trifft. Das Ziel von XAI ist es, zu erklären, was getan wurde, was jetzt getan wird, was als nächstes getan wird und die Informationen, auf denen die Aktionen basieren, enthüllen wird. Diese Merkmale ermöglichen (i) die Bestätigung bestehender Kenntnisse (ii) die Herausforderung bestehender Kenntnisse und (iii) die Schaffung neuer Annahmen. Die in AI verwendeten Algorithmen können in White-Box- und Black-Box-Maschinenlernalgorithmen (ML) differenziert werden. White-Box-Modelle sind ML-Modelle, die Ergebnisse liefern, die für Experten in der Domain verständlich sind. Black-Box-Modelle hingegen sind extrem schwer zu erklären und lassen sich auch von Domänenexperten kaum verstehen. XAI-Algorithmen werden als Folge der drei Prinzipien Transparenz, Interpretationsfähigkeit und Erklärbarkeit betrachtet. Transparenz ist gegeben, „wenn die Prozesse, die Modellparameter aus Trainingsdaten extrahieren und Etiketten aus Testdaten generieren, vom Ansatz-Designer beschrieben und motiviert werden können“. Die Interpretability beschreibt die Möglichkeit, das ML-Modell zu verstehen und die zugrunde liegende Grundlage für die Entscheidungsfindung in einer für den Menschen verständlichen Weise darzustellen. Erklärbarkeit ist ein Konzept, das als wichtig erkannt wird, aber eine gemeinsame Definition ist noch nicht verfügbar. Es wird vorgeschlagen, dass die Erklärbarkeit in ML als "die Erfassung von Merkmalen der interpretierbaren Domäne, die für ein bestimmtes Beispiel zur Entscheidung beigetragen haben (z.B. Klassifizierung oder Regression)" betrachtet werden kann. Wenn Algorithmen diese Anforderungen erfüllen, bieten sie eine Grundlage, um Entscheidungen zu rechtfertigen, sie zu verfolgen und dadurch zu überprüfen, die Algorithmen zu verbessern und neue Fakten zu erforschen. Manchmal ist es auch möglich, ein Ergebnis mit hoher Genauigkeit mit einem an sich interpretierbaren White-Box-ML-Algorithmus zu erzielen. Dies ist besonders wichtig in Bereichen wie Medizin, Verteidigung, Finanzen und Recht, wo es entscheidend ist, die Entscheidungen zu verstehen und Vertrauen in die Algorithmen aufzubauen. KI-Systeme optimieren das Verhalten, um ein von den Systemdesignern gewähltes mathematisch spezifiziertes Zielsystem zu erfüllen, wie z.B. den Befehl "maximieren Sie die Genauigkeit der Beurteilung, wie positive Filmrezensionen im Testdatensatz sind". Die KI kann nützliche allgemeine Regeln aus dem Test-Set lernen, wie "Reviews, die das Wort schrecklich enthalten, sind wahrscheinlich negativ". Es kann aber auch unangemessene Regeln lernen, wie z.B. "Rezensionen mit 'Daniel Day-Lewis' sind in der Regel positiv;" solche Regeln können unerwünscht sein, wenn sie als wahrscheinlich außerhalb des Testsatzes nicht verallgemeinert werden, oder wenn die Menschen die Regel als Betrug oder unfair betrachten". Ein Mensch kann Regeln in einem XAI prüfen, um eine Idee zu bekommen, wie wahrscheinlich das System zu zukünftigen realen Daten außerhalb des Testsatzes zu verallgemeinern ist. Die Zusammenarbeit zwischen Agenten, in diesem Fall Algorithmen und Menschen, hängt vom Vertrauen ab. Wenn Menschen algorithmische Rezepte akzeptieren sollen, müssen sie ihnen vertrauen. Unvollkommenheit bei der Formalisierung von Vertrauenskriterien ist eine Barriere für einfache Optimierungsansätze. Aus diesem Grund werden Dolmetschbarkeit und Erklärbarkeit als Zwischenziele zur Überprüfung anderer Kriterien herangezogen. KI-Systeme lernen manchmal unerwünschte Tricks, die eine optimale Aufgabe, explizite vorprogrammierte Ziele auf die Trainingsdaten zu erfüllen, aber die nicht die komplizierten impliziten Wünsche der menschlichen Systemdesigner widerspiegeln. Zum Beispiel lernte ein 2017-System, das mit der Bilderkennung beauftragt wurde, zu betrügen, indem er nach einem Copyright-Tag suchte, der mit Pferdebildern verbunden war, anstatt zu lernen, wie man sagt, ob ein Pferd tatsächlich abgebildet war. In einem anderen System 2017 lernte ein beaufsichtigtes Lernen KI, das mit dem Greifen von Gegenständen in einer virtuellen Welt beschimpft wurde, indem es seinen Manipulator zwischen dem Objekt und dem Betrachter so platzierte, dass es falsch erschien, das Objekt zu erfassen. Ein Transparenzprojekt, das DARPA XAI-Programm, zielt darauf ab, "Glaskasten"-Modelle herzustellen, die einem Mensch-in-the-Loop erklären können, ohne die AI-Performance stark zu beeinträchtigen. Menschen sollten in der Lage sein, die KI-Erkennung (sowohl in Echtzeit als auch nach der Tatsache) zu verstehen und zu bestimmen, wann sie der KI vertrauen und wann die KI misstrauen sollte.Andere Anwendungen von XAI sind Wissensextraktion aus Blackbox-Modellen und Modellvergleichen. Der Begriff "Glasbox" wurde auch für Systeme verwendet, die die Inputs und Outputs eines Systems überwachen, um die Einhaltung ethischer und sozio-legaler Werte und damit die Erstellung wertbasierter Erklärungen zu überprüfen. Darüber hinaus wurde derselbe Begriff verwendet, um einen Sprachassistenten zu nennen, der als Erläuterungen zu kontrastiellen Aussagen führt. Geschichte und Methoden Während der 1970er- bis 1990er-Jahre wurden symbolische Argumentationssysteme wie MYCIN, GUIDON, SOPHIE und PROTOS erforscht, die ihre Argumentation für diagnostische, belehrende oder maschinelle Lernzwecke (explanationsbasiertes Lernen) darstellen und erklären könnten. MYCIN, der Anfang der 1970er Jahre als Forschungsprototyp für die Diagnose von Bakteremia-Infektionen des Blutstroms entwickelt wurde, könnte erklären, welche seiner handcodierten Regeln zu einer Diagnose in einem bestimmten Fall beigetragen haben. Die Forschung an intelligenten Tutoring-Systemen entwickelte Systeme wie SOPHIE, die als "Künstler-Experte" fungieren könnten und die Problemlösungsstrategie auf einer Ebene erklären, die der Student verstehen könnte, so dass sie wissen würden, welche Maßnahmen als nächstes zu ergreifen sind. So könnte SOPHIE beispielsweise die qualitative Vernunft hinter seiner elektronischen Fehlerbehebung erklären, obwohl sie letztendlich auf den SPICE-Schaltungssimulator zurücklehnte. Ebenso fügte GUIDON Tutorial-Regeln hinzu, um MYCINs Domain-Level-Regeln zu ergänzen, damit es die Strategie für die medizinische Diagnose erklären könnte. Symbolische Ansätze zum maschinellen Lernen, insbesondere diejenigen, die auf erklärungsbasiertes Lernen, wie PROTOS, vertrauen explizit auf Darstellungen von Erklärungen, um ihre Handlungen zu erklären und neue Kenntnisse zu erwerben. In den 1980er Jahren bis Anfang der 1990er Jahre wurden die Wahrheits-Wartungssysteme (TMS) entwickelt, um die Fähigkeiten von ursächlichen, regelbasierten und logischen Inferenzsystemen zu erweitern. Ein TMS dient dazu, explizit alternative Argumentationslinien, Begründungen für Schlussfolgerungen und Argumentationslinien zu verfolgen, die zu Widersprüchen führen und zukünftige Argumente erlauben, diese toten Enden zu vermeiden. Um Erläuterungen zu geben, verfolgen sie die Argumentation von Schlussfolgerungen zu Annahmen durch Regeloperationen oder logische Inferenzen, so dass Erklärungen aus den Argumentationsspuren generiert werden. Als Beispiel betrachten Sie einen regelbasierten Problemlöser mit nur ein paar Regeln über Sokrates, die daraus schließen, dass er aus Gift gestorben ist: Durch einfaches Durchfahren der Abhängigkeitsstruktur kann der Problemlöser die folgende Erklärung erstellen: "Socrates starb, weil er sterblich war und Gift trank, und alle Sterblichen sterben, wenn sie Gift trinken. Sokrates war sterblich, weil er ein Mann war und alle Männer sterblich sind. Sokrates trank Gift, weil er dissidente Überzeugungen hielt, die Regierung war konservativ, und diejenigen, die konservative dissidente Überzeugungen unter konservativen Regierungen halten, müssen Gift trinken." In den 90er Jahren begannen die Forscher auch, zu untersuchen, ob es möglich ist, die nicht handcodierten Regeln, die durch opak ausgebildete neuronale Netze erzeugt werden, sinnvoll zu extrahieren. Forscher in klinischen Expertensystemen, die neurale netzbetriebene Entscheidungsunterstützung für Kliniker schaffen, haben versucht, dynamische Erklärungen zu entwickeln, die es ermöglichen, diese Technologien in der Praxis vertrauter und vertrauenswürdiger zu machen. In den 2010er Jahren können öffentliche Bedenken über Rasse und andere Vorurteile bei der Verwendung von KI für strafrechtliche Entscheidungen und Befunde von Kreditwürdigkeit zu einer erhöhten Nachfrage nach transparenter künstlicher Intelligenz geführt haben. Infolgedessen entwickeln viele Akademiker und Organisationen Werkzeuge, um Bias in ihren Systemen zu erkennen. Marvin Minsky et al. das Problem, dass KI als eine Form der Überwachung funktionieren kann, mit den Bias, die der Überwachung inhärent sind, und deutet darauf hin, dass HI (Humanistische Intelligenz) als eine Möglichkeit zur Schaffung einer gerechteren und ausgewogeneren human-in-the-loop KI. Moderne komplexe KI-Techniken wie tiefes Lernen und genetische Algorithmen sind natürlich undurchsichtig. Um dieses Problem zu lösen, gab es eine Entwicklung von vielen neuen Methoden, um neue Modelle zu erklären und zu interpretieren. Dazu gehören viele Methoden, wie die schichtweise Relevanzausbreitung (LRP), eine Technik zur Bestimmung, welche Merkmale in einem bestimmten Eingangsvektor am stärksten zum Ausgang eines neuronalen Netzes beitragen. Andere Techniken wurden entwickelt, um eine bestimmte Vorhersage eines (nichtlinearen) Black-Box-Modells zu erklären, ein Ziel, das als "lokale Interpretationsfähigkeit" bezeichnet wird. Es ist erwähnenswert, dass die bloße Umsetzung der Konzepte der lokalen Interpretierbarkeit in einen entfernten Kontext (wo das Black-Box-Modell auf einem Dritten ausgeführt wird) derzeit unter Kontrolle ist.Darüber hinaus gibt es Arbeiten an Entscheidungsbäumen, Baumensembles und Bayesischen Netzwerken, die für die Inspektion transparenter sind. Im Jahr 2018 wurde eine interdisziplinäre Konferenz mit dem Namen FAT* (Fairness, Accountability und Transparency) eingerichtet, um Transparenz und Erklärbarkeit im Kontext soziotechnischer Systeme zu untersuchen, von denen viele künstliche Intelligenz umfassen. Einige Techniken ermöglichen die Visualisierung der Inputs, auf die einzelne Neuronen am stärksten reagieren. Mehrere Gruppen haben festgestellt, dass Neuronen in Schaltungen aggregiert werden können, die human-verständliche Funktionen ausführen, von denen einige zuverlässig über verschiedene, unabhängig ausgebildete Netzwerke entstehen. Auf höherer Ebene gibt es verschiedene Techniken, um komprimierte Darstellungen der Merkmale bestimmter Eingaben zu extrahieren, die dann nach Standard-Clustering-Techniken analysiert werden können. Alternativ können Netzwerke ausgebildet werden, um sprachliche Erklärungen ihres Verhaltens auszugeben, die dann direkt menschlich interpretierbar sind. Das Modellverhalten kann auch anhand von Trainingsdaten erläutert werden - beispielsweise durch Auswertung, welche Trainingseingänge ein bestimmtes Verhalten am meisten beeinflussten. Verordnung Da Regulatoren, amtliche Körperschaften und allgemeine Benutzer von KI-basierten dynamischen Systemen abhängen, wird eine deutlichere Rechenschaftspflicht für Entscheidungsprozesse erforderlich sein, um Vertrauen und Transparenz zu gewährleisten. Der Nachweis dieser Forderung, die mehr Dynamik erlangt, ist mit der Einführung der ersten globalen Konferenz, die ausschließlich dieser aufstrebenden Disziplin gewidmet ist, der Internationalen Gemeinsamen Konferenz über Künstliche Intelligenz: Workshop über Erklärbare Künstliche Intelligenz (XAI). Die Europäische Union hat ein Recht auf Erklärung im Datenschutzrecht (DSGVO) eingeführt, um die potenziellen Probleme zu bewältigen, die sich aus der steigenden Bedeutung von Algorithmen ergeben. Die Umsetzung der Verordnung begann im Jahr 2018. Das Recht auf Erklärung in der DSGVO umfasst jedoch nur den lokalen Aspekt der Interpretationsfähigkeit. In den Vereinigten Staaten müssen Versicherungsunternehmen ihre Zins- und Deckungsentscheidungen erklären können. Sektoren XAI wurde in vielen Bereichen erforscht, darunter: Antenna Design (evolved Antenne) Algorithmischer Handel (hochfrequenter Handel)Medizin diagnostiziert Autonome Fahrzeuge Textanalytik Strafrecht Referenzen Externe Links "AI Erklärbarkeit 360". " Was ist das Erklärbare-Ai und warum ist wichtig"." Erklärbare KI ist das nächste große Wesen in Rechnungswesen und Finanzen." "FAT* Konferenz über Fairness, Rechenschaftspflicht und Transparenz""FATML Workshop über Fairness, Rechenschaftspflicht und Transparenz im maschinellen Lernen." "Explainable Artificial Intelligence": Cracking öffnet die schwarze Kiste von AI.Computerworld.2017-11-02.Retrieved 2017-11-02.Park, Dong Huk; Hendricks, Lisa Anne; Akata, Zeynep; Schiele, Bernt; Darrell, Trevor; Rohrbach, Marcus (2016-12-14)." Erläuterungen: Entscheidungsbeschlüsse und Hinweise auf die Beweiskraft"arXiv:1612.04757 [cs.CV]." Erklärbare KI: Maschinen für Menschen verständlich machen". Erklärbar KI: Maschinen für Menschen verständlich machen. Retrieved 2017-11-02."End-to-End Deep Learning for Self-Driving Cars".Parallel Forall.2016-08-17.Retrieved 2017-11-02."Erklärung wie End-to-End Deep Learning Steers a Self-Driving Car".Parallel Forall.2017-05-23.Retrieved 2017-11-02. Knight, Will (2017-03-14)."DARPA finanziert Projekte, die versuchen, die schwarzen Kisten von AI zu öffnen". MIT Technology Review.Retrieved 2017-11-02.Alvarez-Melis, David; Jaakkola, Tommi S. (2017-07-06). "Ein ursächlicher Rahmen zur Erläuterung der Vorhersagen von Black-box Sequenz-to-Sequence-Modellen".arXiv:1707.01943 [cs.LG]." Ähnlichkeit knüpft den Code of Explainable AI".simMachines.2017-10-12.Retrieved 2018-02-02.Bojarski, Mariusz; Yeres, Philip; Choromanska, Anna; Choromanski, Krzysztof; Firner, Bernhard; Jackel, Lawrence; Muller, Urs (2017-04-25)." Erklären, wie ein Deep Neural Network mit End-to-End-Lernen trainiert ein Auto "arXiv:1704.07911 [cs.CV]." Was sind die Methoden, die Ausgabe von maschinellen Lernmethoden zu interpretieren?". IntelligenceReborn 2020-12-30.Retrieved 2020-12-30. Der Begriff Proofreading wird in der Genetik verwendet, um sich auf die fehlerkorrigierenden Prozesse zu beziehen, die zunächst von John Hopfield und Jacques Ninio vorgeschlagen wurden, die an der DNA-Replikation, Immunsystemspezifität, Enzym-Substrat-Erkennung unter vielen anderen Prozessen beteiligt sind, die eine verbesserte Spezifität erfordern. Die Proofreading-Mechanismen von Hopfield und Ninio sind nicht-Equilibrium aktive Prozesse, die ATP verbrauchen, um die Spezifität verschiedener biochemischer Reaktionen zu verbessern.In Bakterien haben alle drei DNA-Polymerasen (I, II und III) die Fähigkeit, die Exonukleaseaktivität 3’ → 5’ zu testen. Wenn ein falsches Basispaar erkannt wird, rückt DNA-Polymerase seine Richtung durch ein Basispaar von DNA um und exzisiert die fehlangepasste Basis. Nach der Basenexzision kann die Polymerase die korrekte Basis erneut einfügen und die Replikation kann fortgesetzt werden. In Eukaryoten haben nur die Polymerasen, die sich mit der Dehnung (Delta und Epsilon) beschäftigen, die Nachweisfähigkeit (3’ → 5’ Exonuclease-Aktivität). Auch bei der mRNA-Übersetzung zur Proteinsynthese kommt es zu einem Proofreading. In diesem Fall ist ein Mechanismus die Freisetzung einer falschen Aminoacyl-tRNA vor der Peptidbindungsbildung. Das Ausmaß der Korrekturlesung in der DNA-Replikation bestimmt die Mutationsrate und ist in verschiedenen Arten unterschiedlich. So führt beispielsweise der Verlust der Korrekturlesung durch Mutationen in der DNA-Polymerase epsilon-Gen zu einem hypermutierten Genotyp mit >100 Mutationen pro Mbase von DNA in humanen Dickdarmkrebsen. Das Ausmaß der Proofreading in anderen molekularen Prozessen kann von der effektiven Populationsgröße der Spezies und der Anzahl der von demselben Proofreading-Mechanismus betroffenen Gene abhängen. Bakteriophage T4-DNA-Polymerase Bacteriophage (phage) T4-Gen 43 kodiert das DNA-Polymerase-Replikationsenzym des Phagen. Temperaturempfindliche (ts) Gen 43 Mutanten wurden identifiziert, die einen Antimutator-Phänotyp aufweisen, also eine geringere Spontanmutationsrate als Wildtyp. Studien einer dieser Mutanten, tsB120, zeigten, dass die von dieser Mutanten spezifizierte DNA-Polymerase langsamer als die Wildtyp-Polymerase kopiert. Die 3’ bis 5’-Exonuclease-Aktivität war jedoch nicht höher als Wildtyp. Bei der DNA-Replikation ist das Verhältnis von zu den stabil in neu gebildete DNA umgedrehten Nukleotiden bei der tsB120-Mutante 10 bis 100 mal höher als bei Wildtyp. Es wurde vorgeschlagen, den Antimutatoreffekt sowohl durch eine größere Genauigkeit bei der Nukleotidauswahl als auch durch eine erhöhte Effizienz der Entfernung von nichtkomplementären Nukleotiden (proofreading) durch die tsB120 Polymerase zu erklären. Wenn Phage T4 Virionen mit einem Wildtyp-Gen 43 DNA-Polymerase entweder ultravioletten Licht ausgesetzt sind, das Cyclobutan-Pyrimidin-Dimer-Schäden in DNA einführt, oder psoralen-plus-Licht, das Pyrimidin-Addukte einführt, erhöht sich die Mutationsrate. Diese mutagenen Effekte werden jedoch gehemmt, wenn die DNA-Synthese der Phagen durch die tsCB120 Antimutator-Polymerase oder eine andere Antimutator-Polymerase tsCB87 katalysiert wird. Diese Ergebnisse zeigen, dass die Höhe der Induktion von Mutationen durch DNA-Schäden durch die Gen 43 DNA-Polymerase-Proofreading-Funktion stark beeinflusst werden kann. Referenzen Externe Links Idaho U. DNA-Proofreading und Reparatur "DNA Polymerase ε und δ Proofreading unterdrücken diskrete Mutator und Krebs-Phenotypen in Mäusen" Tseng, Shun-Fu; Gabriel, Abram; Teng, Shu-Chun (2008)." Proofreading Aktivität von DNA Polymerase Pol2 Mediates 3'-End-Prozessierung während der nichthomologen EndJoining in Yeast.PLOS Genetics.4 (4:) e1000060. doi:10.1371/journal.pgen.1000060.PMC 2312331.PMID 18437220.