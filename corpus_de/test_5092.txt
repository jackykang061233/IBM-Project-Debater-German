Allgemeines Anwendungs-Computing auf Grafik-Verarbeitungseinheiten (GPGPU, oder weniger oft GPGP) ist die Verwendung einer Grafik-Verarbeitungseinheit (GPU), die typischerweise nur für Computergrafiken Berechnungen durchführt, in Anwendungen, die traditionell von der zentralen Verarbeitungseinheit (CPU) bearbeitet werden. Die Verwendung mehrerer Videokarten in einem Computer oder großer Anzahl von Grafikchips parallelisiert die bereits parallele Natur der Grafikverarbeitung. Im Wesentlichen handelt es sich bei einer GPGPU-Pipeline um eine Art paralleler Verarbeitung zwischen einer oder mehreren GPUs und CPUs, die Daten wie in Bild- oder anderen Grafikform analysiert. Während GPUs mit niedrigeren Frequenzen arbeiten, haben sie typischerweise oft die Anzahl der Kerne. So können GPUs viel mehr Bilder und grafische Daten pro Sekunde verarbeiten als eine traditionelle CPU. Daten in graphische Form migrieren und dann mit der GPU scannen und analysieren, kann eine große Beschleunigung verursachen. GPGPU-Pipelines wurden zu Beginn des 21. Jahrhunderts für die grafische Verarbeitung entwickelt (z.B. für bessere Shader). Diese Pipelines wurden gefunden, um den wissenschaftlichen Rechenbedarf gut anzupassen und sind seitdem in dieser Richtung entwickelt worden. Geschichte Grundsätzlich kann jede beliebige boolesche Funktion, einschließlich der Additions-, Multiplikations- und andere mathematische Funktionen, aus einem funktionell kompletten Satz von Logik-Operatoren aufgebaut werden. Im Jahr 1987 wurde Conway's Game of Life eines der ersten Beispiele für das Universal-Computing mit einem frühen Stream-Prozessor namens blitter, um eine spezielle Sequenz von logischen Operationen auf Bitvektoren aufgerufen. General-purpose Computing auf GPUs wurde nach etwa 2001 praktischer und populärer, mit dem Aufkommen von sowohl programmierbaren Shadern als auch der Floating Point-Support auf Grafikprozessoren. Insbesondere Probleme mit Matrizen und/oder Vektoren – insbesondere zwei,- drei,- oder vierdimensionale Vektoren – waren leicht zu einer GPU zu übersetzen, die mit nativer Geschwindigkeit und Unterstützung auf diese Typen wirkt. Die Experimente der wissenschaftlichen Computing Community mit der neuen Hardware begannen mit einer Matrix-Multiplikationsroutine (2001); eines der ersten gemeinsamen wissenschaftlichen Programme, die auf GPUs schneller laufen als CPUs war eine Implementierung der LU-Faktorisierung (2005). Diese frühen Bemühungen, GPUs als universelle Prozessoren zu verwenden, erforderten eine Reform der Rechenprobleme in Bezug auf Grafik-Primitiven, wie von den beiden großen APIs für Grafikprozessoren, OpenGL und DirectX unterstützt. Diese umständliche Übersetzung wurde durch das Aufkommen allgemeiner Programmiersprachen und APIs wie Sh/RapidMind, Brook und Accelerator vernichtet. Darauf folgten Nvidias CUDA, die es den Programmierern ermöglichte, die zugrunde liegenden grafischen Konzepte zugunsten gemeinsamer Hochleistungs-Computing-Konzepte zu ignorieren. Neue, hardware-vendor-unabhängige Angebote umfassen Microsofts DirectCompute und die OpenCL der Apple/Khronos Group. Dies bedeutet, dass moderne GPGPU-Pipelines die Geschwindigkeit einer GPU nutzen können, ohne dass eine vollständige und explizite Umwandlung der Daten in eine grafische Form erforderlich ist. Durchführung Jede Sprache, die den Code, der auf der CPU läuft, ermöglicht, einen GPU-Schaler für Rückgabewerte zu erstellen, kann ein GPGPU-Framework erstellen. Seit 2016 ist OpenCL die dominante offene allgemeine GPU-Computing-Sprache und ein von der Khronos Group definierter offener Standard. OpenCL bietet eine plattformübergreifende GPGPU-Plattform, die zusätzlich Daten parallel auf CPUs unterstützt. OpenCL wird aktiv auf Intel-, AMD-, Nvidia- und ARM-Plattformen unterstützt. Die Khronos Group hat auch SYCL standardisiert und implementiert, ein übergeordnetes Programmiermodell für OpenCL als ein-Source-Domain-spezifische Embedded-Sprache basierend auf purem C++11. Der dominante proprietäre Rahmen ist Nvidia CUDA. Nvidia startete 2006 CUDA, ein Software-Entwicklungskit (SDK) und Anwendungs-Programmierschnittstelle (API), mit dem die Programmiersprache C Algorithmen für die Ausführung auf GeForce 8 und später GPUs kodieren kann. Programmierstandards für Parallel Computing sind OpenCL (vendor-unabhängig), OpenACC und OpenHMPP. Mark Harris, der Gründer von GPGPU.org, prägte den Begriff GPGPU. Das von Xcelerit erstellte Xcelerit SDK ist darauf ausgelegt, große C+- oder C#-Code-Basen auf GPUs mit minimalem Aufwand zu beschleunigen. Es bietet ein vereinfachtes Programmiermodell, automatisiert die Parallelisierung, verwaltet Geräte und Speicher und kompiliert an CUDA Binaries. Zusätzlich können Multi-Core-CPUs und andere Beschleuniger aus demselben Quellcode zielgerichtet werden. OpenVIDIA wurde von 2003 bis 2005 in Zusammenarbeit mit Nvidia an der University of Toronto entwickelt. Altimesh Hybridizer erstellt von Altimesh kompiliert Common Intermediate Language an CUDA Binaries. Es unterstützt Generika und virtuelle Funktionen. Debugging und Profiling sind mit Visual Studio und Nsight integriert. Es ist als Visual Studio-Erweiterung auf Visual Studio Marketplace verfügbar. Microsoft stellte die DirectCompute GPU Computing API vor, die mit der DirectX 11 API veröffentlicht wurde. Alea GPU erstellt von QuantAlea stellt native GPU-Computing-Funktionen für die Microsoft .NETlanguage F# und C#.Alea GPU ein vereinfachtes GPU-Programmiermodell auf Basis von GPU parallel-für und parallele Aggregate mit Delegierten und automatischem Speichermanagement zur Verfügung. MATLAB unterstützt GPGPU-Beschleunigung mit der Parallel Computing Toolbox und MATLAB Distributed Computing Server und Drittanbieter-Paketen wie Jacket. Die GPGPU-Verarbeitung wird auch verwendet, um Newtonische Physik durch Physik-Motoren zu simulieren, und kommerzielle Implementierungen umfassen Havok Physik, FX und PhysX, die beide typischerweise für Computer- und Videospiele verwendet werden. In der Nähe von Metal, jetzt Stream genannt, ist die GPGPU-Technologie von AMD für ATI Radeon-basierte GPUs. C+ Accelerated Massive Parallelism (C+ AMP) ist eine Bibliothek, die die Ausführung von C+-Code durch die Nutzung der datenparallelen Hardware auf GPUs beschleunigt. Mobile Computer Durch den Trend der zunehmenden Leistungsfähigkeit mobiler GPUs wurde die allgemeine Programmplanung auch auf mobilen Geräten mit großen mobilen Betriebssystemen zur Verfügung gestellt. Google Android 4.2 ermöglichte die Ausführung von RenderScript-Code auf dem mobilen Gerät GPU. Apple stellte die proprietäre Metall-API für iOS-Anwendungen vor, in der Lage, beliebigen Code durch Apples GPU-Compute-Shader auszuführen. Hardware-Unterstützung Computer-Videokarten werden von verschiedenen Anbietern wie Nvidia, AMD und ATI produziert. Karten von solchen Anbietern unterscheiden sich bei der Implementierung von Datenformat-Unterstützung, wie ganze und schwimmende Formate (32-Bit und 64-Bit). Microsoft hat einen Shader Model Standard eingeführt, um die verschiedenen Features von Grafikkarten in eine einfache Shader Model-Versionsnummer zu ordnen (1.0, 2.0, 3.0, etc.) Integer Zahlen Pre-DirectX 9 Videokarten unterstützt nur paletten- oder ganzzahlige Farbtypen. Es stehen verschiedene Formate zur Verfügung, die jeweils ein rotes Element, ein grünes Element und ein blaues Element enthalten. Manchmal wird ein anderer alpha-Wert hinzugefügt, der für die Transparenz verwendet wird. Gemeinsame Formate sind: 8 Bit pro Pixel – Manchmal Palette Modus, wobei jeder Wert ein Index in einer Tabelle mit dem realen Farbwert in einem der anderen Formate angegeben ist. Manchmal drei Bits für Rot, drei Bits für Grün und zwei Bits für Blau. 16 Bits pro Pixel – Üblicherweise werden die Bits als fünf Bits für Rot, sechs Bit für Grün und fünf Bit für Blau zugeordnet. 24 Bit pro Pixel – Für jeden von Rot, Grün und Blau gibt es acht Bits. 32 Bit pro Pixel – Für jeden von Rot, Grün, Blau und Alpha gibt es acht Bits. Schwimmpunktzahlen Für eine frühe Festfunktion oder begrenzte Programmierbarkeitsgrafik (d.h. bis einschließlich DirectX 8.1-konforme GPUs) reichte dies aus, da dies auch die Darstellung in Displays ist. Es ist wichtig zu beachten, dass diese Darstellung gewisse Einschränkungen aufweist. Bei ausreichender Grafikverarbeitung möchten sogar Grafikprogrammierer bessere Formate, wie z.B. Floating Point Data Formate verwenden, um Effekte wie hochdynamische Bildgebung zu erhalten. Viele GPGPU-Anwendungen erfordern eine schwimmende Punktgenauigkeit, die mit Videokarten entsprechend der DirectX 9 Spezifikation kam. Direkt 9 Shader Modell 2.x schlug die Unterstützung von zwei Präzisionstypen vor: Voll- und Teilgenauigkeit. Volle Präzisionsunterstützung könnte entweder FP32 oder FP24 (Floating Point 32- oder 24-Bit pro Bauteil) oder größer sein, während die Teilgenauigkeit FP16 war. Die Radeon R300-Serie von ATI unterstützte die FP24-Präzision nur in der programmierbaren Fragmentpipeline (obwohl FP32 in den Vertex-Prozessoren unterstützt wurde), während die NV30-Serie von Nvidia sowohl FP16 als auch FP32 unterstützte; andere Anbieter wie S3 Graphics und XGI unterstützten eine Mischung von Formaten bis FP24. Die Implementierungen von Floating Point auf Nvidia GPUs sind meist IEEE-konform; dies gilt jedoch nicht für alle Anbieter. Dies hat Auswirkungen auf die Richtigkeit, die für einige wissenschaftliche Anwendungen als wichtig angesehen werden. Während auf CPUs häufig 64-Bit Floating-Point-Werte (Doppel-Präzisions-Boot) verfügbar sind, werden diese nicht universell auf GPUs unterstützt. Einige GPU-Architekturen opfern die IEEE-Compliance, während andere keine Doppelpräzision haben. Es hat sich jedoch gezeigt, dass die Geschwindigkeitsabnahme einen Vorteil verschafft, das Computing auf die GPU zu entlasten. Vectoris Die meisten Operationen auf der GPU funktionieren vektorisiert: Eine Operation kann auf bis zu vier Werten gleichzeitig durchgeführt werden. Soll beispielsweise eine Farbe <R1, G1, B1> durch eine andere Farbe <R2, G2, B2,> moduliert werden, so kann die GPU in einem Arbeitsgang die resultierende Farbe <R1*R2, G1*G2, B1*B2> erzeugen. Diese Funktionalität ist in Grafiken nützlich, da fast jeder Grunddatentyp ein Vektor ist (entweder 2,- 3,- oder 4-dimensional). Beispiele sind Vertikale, Farben, normale Vektoren und Texturkoordinaten. Viele andere Anwendungen können dies gut nutzen, und aufgrund ihrer höheren Leistung, Vektor-Anweisungen, als Einzelanweisung, mehrere Daten (SIMD,) sind seit langem auf CPUs verfügbar. GPU gegen CPU Ursprünglich wurden die Daten einfach einwegig von einer zentralen Verarbeitungseinheit (CPU) an eine Grafikverarbeitungseinheit (GPU) übergeben, dann an ein Anzeigegerät. Im Laufe der Zeit wurde es jedoch für GPUs wertvoll, zunächst einfache, dann komplexe Strukturen von Daten an die CPU zurückzugeben, die ein Bild analysiert, oder eine Reihe von wissenschaftlichen Daten, die als 2D- oder 3D-Format dargestellt sind, die eine Videokarte verstehen kann. Da die GPU Zugriff auf jeden Ziehvorgang hat, kann sie Daten in diesen Formularen schnell analysieren, während eine CPU jedes Pixel oder Datenelement viel langsamer abfragen muss, da die Zugriffsgeschwindigkeit zwischen einer CPU und ihrem größeren Pool an Zufalls-Zugriffsspeicher (oder in einem noch schlechteren Fall eine Festplatte) langsamer ist als GPUs und Videokarten, die in der Regel kleinere Mengen teurerer Speicher enthalten, die viel schneller zuzugreifen sind. Die Übertragung des zu analysierenden Datensatzes auf diesen GPU-Speicher in Form von Texturen oder anderen leicht lesbaren GPU-Formen führt zu einer Geschwindigkeitserhöhung. Das Unterscheidungsmerkmal eines GPGPU-Designs ist die Fähigkeit, Informationen bidirektional von der GPU auf die CPU zu übertragen; in der Regel ist der Datendurchsatz in beiden Richtungen ideal hoch, was zu einem Multiplikatoreffekt auf die Geschwindigkeit eines bestimmten High-Use-Algorithmus führt. GPGPU-Pipelines können die Effizienz auf besonders großen Datensätzen und/oder Daten mit 2D- oder 3D-Bilder verbessern. Es wird in komplexen Grafik-Pipelines sowie wissenschaftlichem Computing verwendet; mehr so in Feldern mit großen Datensätzen wie Genom-Mapping oder wo zwei- oder dreidimensionale Analyse nützlich ist – vor allem derzeit Biomolekülanalyse, Proteinstudie und andere komplexe organische Chemie. Solche Pipelines können unter anderem auch die Effizienz in der Bildverarbeitung und in der Computer-Vision erheblich verbessern; sowie die parallele Verarbeitung im Allgemeinen. Einige sehr stark optimierte Pipelines haben Geschwindigkeitserhöhungen von mehreren hundertfachen der ursprünglichen CPU-basierten Pipeline auf einer Hochnutzungsaufgabe ergeben. Ein einfaches Beispiel wäre ein GPU-Programm, das Daten über mittlere Beleuchtungswerte sammelt, da es entweder eine Kamera oder ein Computergrafikprogramm zurück zum Hauptprogramm der CPU zeigt, so dass die CPU dann Anpassungen an die Gesamtbildschirmansicht vornehmen kann. Ein fortgeschritteneres Beispiel könnte die Kantenerkennung verwenden, um sowohl numerische Informationen als auch ein verarbeitetes Bild zurückzugeben, das Umrisse an ein Computer-Visionsprogramm darstellt, das beispielsweise einen mobilen Roboter steuert. Da die GPU einen schnellen und lokalen Hardwarezugriff auf jedes Pixel oder ein anderes Bildelement in einem Bild hat, kann sie es (zum ersten Beispiel) analysieren und durchschnittlichisieren oder einen Sobel Randfilter oder einen anderen Faltfilter (zum zweiten) mit viel größerer Geschwindigkeit als eine CPU anwenden, die typischerweise auf langsamere zufällige Zugriffsspeicherkopien der betreffenden Grafik zugreifen muss. GPGPU ist grundsätzlich ein Software-Konzept, kein Hardware-Konzept; es ist eine Art Algorithmus, kein Gerät. Spezielle Gerätedesigns können jedoch noch die Effizienz von GPGPU-Pipelines verbessern, die traditionell relativ wenige Algorithmen auf sehr großen Datenmengen durchführen. So können massiv parallelisierte, gigantische Datenebenenaufgaben noch weiter über spezialisierte Setups wie Rack Computing (viele ähnliche, hoch zugeschnittene Maschinen, die in ein Rack eingebaut sind) parallelisiert werden, was eine dritte Schicht hinzufügt – viele Recheneinheiten verwenden jeweils viele CPUs, um vielen GPUs zu entsprechen. Einige Bitcoin Bergleute nutzten solche Setups für die Hochmengenverarbeitung. Kasein Historisch, CPUs haben Hardware-managed Caches verwendet, aber die früheren GPUs lieferten nur softwaregestützte lokale Speicher. Da GPUs jedoch zunehmend für allgemeine Anwendungen eingesetzt werden, werden hochmoderne GPUs mit hardwarebetriebenen Multi-Level-Caches entwickelt, die den GPUs geholfen haben, sich in Richtung Mainstream Computing zu bewegen. Zum Beispiel, GeForce 200 Serie GT200 Architektur GPUs nicht mit einem L2 Cache, die Fermi GPU hat 768 KiB last-level Cache, die Kepler GPU hat 1,5 MiB last-level Cache, die Maxwell GPU hat 2 MiB last-level Cache, und die Pascal GPU hat 4 MiB last-level Cache. Registrieren Sie die Datei GPUs haben sehr große Registerdateien, die es ihnen ermöglichen, kontextvermittelte Latenz zu reduzieren. Die Registerdateigröße steigt auch über verschiedene GPU-Generationen, z.B. die Gesamtregisterdateigröße auf Maxwell (GM200,) Pascal und Volta GPUs sind 6 MiB, 14 MiB bzw. 20 MiB. Im Vergleich dazu ist die Größe einer Registerdatei auf CPUs klein, typischerweise Zehner oder Hunderte von Kilobytes. Energieeffizienz Die hohe Leistung von GPUs kommt zu den Kosten des hohen Stromverbrauchs, die unter voller Last ist in der Tat so viel Leistung wie der Rest des PC-Systems kombiniert. Der maximale Stromverbrauch der Pascal Serie GPU (Tesla P100) wurde auf 250W angegeben. Die Stream-Verarbeitung von GPUs ist speziell für Grafiken konzipiert und sind somit sehr restriktiv in Betrieb und Programmierung. Aufgrund ihres Designs sind GPUs nur für Probleme wirksam, die mit der Stream-Verarbeitung gelöst werden können und die Hardware nur auf bestimmte Weise verwendet werden kann. Die folgende Diskussion über Vertikale, Fragmente und Texturen betrifft vor allem das Vermächtnismodell der GPGPU-Programmierung, wo Grafik-APIs (OpenGL oder DirectX) verwendet wurden, um allgemeine Berechnungen durchzuführen. Mit der Einführung der CUDA (Nvidia, 2007) und OpenCL (vendor-unabhängig, 2008) Universal-Computing-APIs, in neuen GPGPU-Codes ist es nicht mehr notwendig, die Berechnung auf Grafik-Primitiven abzubilden. Die stromverarbeitende Natur von GPUs bleibt unabhängig von den verwendeten APIs gültig. (siehe z.B.) GPUs können nur unabhängige Vertiken und Fragmente verarbeiten, können aber viele von ihnen parallel verarbeiten. Dies ist besonders dann wirksam, wenn der Programmierer in gleicher Weise viele Vertikale oder Fragmente verarbeiten will. In diesem Sinne sind GPUs Stream-Prozessoren – Prozessoren, die parallel arbeiten können, indem ein Kernel auf vielen Datensätzen in einem Stream auf einmal ausgeführt wird. Ein Stream ist einfach eine Reihe von Datensätzen, die eine ähnliche Berechnung erfordern. Streams liefern Datenparallelität. Kernel sind die Funktionen, die auf jedes Element im Strom aufgebracht werden. Bei den GPUs sind Vertices und Fragmente die Elemente in Strömen und Scheitel- und Fragment-Shader die darauf zu laufenden Kerne. Für jedes Element können wir nur von der Eingabe lesen, Operationen durchführen und an die Ausgabe schreiben. Es ist zulässig, mehrere Eingänge und mehrere Ausgänge zu haben, aber niemals ein Speicher, der sowohl lesbar als auch schreibbar ist. Die arithmetische Intensität wird definiert als die Anzahl der Operationen pro übertragenem Speicherwort. Es ist wichtig, dass GPGPU-Anwendungen eine hohe arithmetische Intensität haben, sonst wird die Speicherzugriffslatenz die Rechengeschwindigkeit begrenzen. Ideale GPGPU-Anwendungen haben große Datensätze, hohe Parallelität und minimale Abhängigkeit zwischen Datenelementen. GPU Programmierkonzepte Rechenressourcen Auf der GPU gibt es eine Vielzahl von Rechenressourcen: Programmierbare Prozessoren – Vertex, Primitive, Fragmente und vor allem Rechenpipeline ermöglichen es Programmierern, Kernel auf Datenströme Rasterizer durchzuführen – erstellt Fragmente und Interpolate per-vertex Konstanten wie Textur-Koordinaten und Farbe Texture Unit – read-only Memory Interface Framebuffer – Schreib-only Memory Interface Tatsächlich kann ein Programm anstelle des Framebuffers nur eine Schreibtextur für die Ausgabe ersetzen. Dies geschieht entweder durch Render to Texture (RTT,) Render-To-Backbuffer-Copy-To-Texture (RTBCTT) oder durch den neueren Stream-out. Texturen als Strom Die häufigste Form für einen Strom, der GPGPU einnimmt, ist ein 2D-Gitter, da dies natürlich mit dem in GPUs eingebauten Rendering-Modell passt. Viele Berechnungen ordnen natürlich in Gitter: Matrix Algebra, Bildverarbeitung, physikalisch basierte Simulation, und so weiter. Da Texturen als Speicher verwendet werden, werden dann Textur-Lookups als Speicherlese verwendet. Hierdurch können bestimmte Operationen automatisch von der GPU durchgeführt werden. Kernels Compute Kernels können als Körper von Schleifen gedacht werden. Beispielsweise kann ein Programmierer, der auf einem Raster auf der CPU arbeitet, einen Code haben, der so aussieht: Auf der GPU gibt der Programmierer nur den Körper der Schleife als Kernel an und welche Daten durch das Aufrufen der Geometrieverarbeitung zu überschleifen. Durchflussregelung In sequentiellem Code ist es möglich, den Ablauf des Programms mit if-then-else-Anweisungen und verschiedenen Formen von Schleifen zu steuern. Solche Strömungsleitstrukturen wurden erst kürzlich zu GPUs hinzugefügt. Bedingte Schriften konnten mit einer richtig handgefertigten Reihe von Rechen-/Bit-Operationen durchgeführt werden, aber Schlaufen und bedingte Verzweigungen waren nicht möglich. Jüngste GPUs erlauben Verzweigung, aber in der Regel mit einer Leistungsstrafe. Eine Verzweigung sollte in der Regel in inneren Schleifen vermieden werden, sei es in CPU- oder GPU-Code, und verschiedene Methoden, wie statische Zweigauflösung, Vorkomputation, Vorgabe, Schleifenaufspaltung und Z-Cub können verwendet werden, um Verzweigungen zu erzielen, wenn Hardware-Unterstützung nicht existiert. GPU Methoden Karte Die Kartenoperation wendet einfach die angegebene Funktion (der Kernel) an jedes Element im Stream an. Ein einfaches Beispiel ist die Multiplizierung jedes Wertes im Strom um eine Konstante (Erhöhung der Helligkeit eines Bildes). Der Kartenbetrieb ist einfach auf der GPU zu implementieren. Der Programmierer erzeugt für jeden Pixel auf dem Bildschirm ein Fragment und wendet jedem ein Fragmentprogramm an. Der gleich große Ergebnisstrom wird im Ausgangspuffer gespeichert. Reduzierung Einige Berechnungen erfordern die Berechnung eines kleineren Stroms (möglicherweise eines Stroms von nur einem Element) aus einem größeren Strom. Dies wird als Reduktion des Stroms bezeichnet. Generell kann eine Reduktion in mehreren Schritten durchgeführt werden. Die Ergebnisse aus dem vorherigen Schritt werden als Eingang für den aktuellen Schritt verwendet und der Bereich, über den die Operation angewendet wird, reduziert, bis nur ein Stromelement verbleibt. Die Streamfilterung ist im Wesentlichen eine ungleichmäßige Reduktion. Die Filterung beinhaltet das Entfernen von Gegenständen aus dem Stream basierend auf einigen Kriterien. Scannen Der Scanvorgang, auch als parallele Präfixsumme bezeichnet, nimmt einen Vektor (Strom) von Datenelementen und eine (arbiträre) assoziative Binärfunktion '+' mit einem Identitätselement i' ein. Wenn der Eingang [a0, a1, a2, a3, ...] ist, erzeugt ein exklusiver Scan den Ausgang [i, a0, a0 + a1, a0 + a1 + a2, ...], während ein inklusiver Scan den Ausgang [a0, a0 + a1, a0 + a1 + a2, a0 + a1 + a2 + a3 ...] erzeugt und keine Identität benötigt.Während auf den ersten Blick die Operation inhärent seriell erscheinen kann, sind effiziente parallele Scanalgorithmen möglich und wurden auf Grafikverarbeitungseinheiten implementiert. Der Scan-Betrieb hat Verwendungen in z.B. Quicksort und sparse Matrix-Vector Multiplikation. Streuung Die Scatter-Operation wird am besten auf dem Scheitelprozessor definiert. Der Scheitelprozessor ist in der Lage, die Position des Scheitels einzustellen, was es dem Programmierer ermöglicht, zu kontrollieren, wo Informationen auf dem Raster abgelegt werden. Es sind auch andere Erweiterungen möglich, wie beispielsweise die Steuerung, wie groß ein Bereich der Scheitel wirkt. Der Fragmentprozessor kann keine direkte Streuung durchführen, da der Ort jedes Fragments auf dem Raster zum Zeitpunkt der Erstellung des Fragments festgelegt ist und vom Programmierer nicht verändert werden kann. Eine logische Scatter-Operation kann jedoch manchmal mit einem anderen Sammelschritt neucastbar oder implementiert werden. Eine Scatter-Implementierung würde zunächst sowohl einen Ausgangswert als auch eine Ausgangsadresse abgeben. Ein unmittelbar darauffolgender Sammelvorgang verwendet Adressvergleiche, um zu sehen, ob der Ausgangswert auf den aktuellen Ausgabeschlitz abbildet. In dedizierten Rechenkernen kann die Streuung durch indizierte Schriften durchgeführt werden. Gather Gather ist die Rückseite der Streuung. Nach Scatter-Reorders-Elemente nach einer Karte, sammeln kann die Reihenfolge der Elemente nach der verwendeten Kartenstreuung wiederherzustellen. In dedizierten Rechenkernen kann das Sammeln durch indizierte Lesezeichen durchgeführt werden. In anderen Shadern wird es mit Textur-Lookups durchgeführt. Sortierung Der Sortiervorgang verwandelt einen ungeordneten Satz von Elementen in einen geordneten Satz von Elementen. Die häufigste Implementierung auf GPUs verwendet Radyx-Sort für ganzzahlige und schwimmende Punktdaten sowie grobkörnige Merge-Sorten und feinkörnige Sortiernetzwerke für allgemeine vergleichbare Daten. Suche Die Suchoperation ermöglicht es dem Programmierer, ein bestimmtes Element innerhalb des Stroms zu finden oder eventuell Nachbarn eines bestimmten Elements zu finden. Die GPU wird nicht verwendet, um die Suche nach einem einzelnen Element zu beschleunigen, sondern wird verwendet, um mehrere Suchvorgänge parallel auszuführen. Meistens wird als Suchmethode eine binäre Suche auf sortierten Elementen verwendet. Datenstrukturen Auf der GPU können eine Vielzahl von Datenstrukturen dargestellt werden: Dichte Arrays Sparse Matrices (sparse array) – statische oder dynamische Adaptive Strukturen (union type) Anwendungen Die folgenden Bereiche sind einige der Bereiche, in denen GPUs für allgemeine Zwecke verwendet wurden Computing: Automatische Parallelisierung Physikalische Simulations- und Physik-Motoren (in der Regel basierend auf Newtonischen Physik-Modellen) Conway's Game of Life, Tuchsimulation, Fluid inkompressibler Fluss durch Lösung von Euler Gleichungen (Flüssigdynamik) oder Navier–Stokes Gleichungen Statistische Physik Ising Modell Lattice Messgerät Theorie Segmentation – 2D und 3D Level set Methoden CT Rekonstruktion Fast Fourier transform GPU Lernen – Machine Learning und Data Mining Berechnungen, z.B. mit Software BIDMach k-nearest Nachbaralgorithmus Fuzzy Logik Tonabbildung AudiosignalverarbeitungAudio- und Toneffektverarbeitung, zur Verwendung einer GPU für die digitale Signalverarbeitung (DSP) Analoge Signalverarbeitung Redeverarbeitung Digitale Bildverarbeitung VideoverarbeitungHardware beschleunigte Videodekodierung und Nachbearbeitung Bewegungskompensation (Mo comp)Inverse diskrete Cosinus-Transformation (iDCT) Variable-Länge-Dekodierung (VLD,) Computer Vision Digitale Signalverarbeitung / Signalverarbeitung Control engineering Operations researchImplementations of: the GPU Tabu Der Algorithmus zur Lösung des Problems Resource Constrained Project Scheduling ist auf GitHub frei verfügbar; der GPU-Algorithmus zur Lösung des Nurse Rerostering Problems ist auf GitHub frei verfügbar. Neurale Netzwerke Datenbank Operationen Computational Fluid Dynamics vor allem mit Lattice Boltzmann Methoden Kryptographie und Kryptanalyse Leistungsmodellierung: rechnerisch intensive Aufgaben auf GPU Implementierungen von: MD6, Advanced Encryption Standard (AES,) Data Encryption Standard (DES,) RSA, elliptische Kurven Kryptographie (ECC) Passwort-Cracking Cryptocurrency-Transaktionen (Mining) Molekulare Dynamik † Erwartete Beschleunigungen sind stark von der Systemkonfiguration abhängig. GPU Leistung verglichen mit Multi-Core x86 CPU Buchse. GPU-Performance, die auf GPU-gestützten Funktionen Benchmarking basiert, kann ein Kernel zum Kernel-Performance-Vergleich sein. Für Details zur verwendeten Konfiguration, siehe Anwendungs-Website. Geschwindigkeiten nach Nvidia-Inhouse-Tests oder ISV-Dokumentation.‡ Q=Quadro GPU, T=Tesla GPU.Nvidia empfohlen GPUs für diese Anwendung. Überprüfen Sie mit Entwickler oder ISV, um Zertifizierungsinformationen zu erhalten. Siehe auch Graphics Processing UnitOpen CL MP OpenACC OpenHMPP Fastra II Stream Processing Mark Harris (Programmer) Physik Advanced Simulation Library Physics Processing Unit (PPU)Havok (Software) Physik, FX PhysX In der Nähe von Metal C+ AMP DirectCompute RenderScript Audio-Verarbeitungseinheit Larrabee (Mikroarchitecture)Compute Kernel AI Beschleuniger Deep Learning Prozessor (DLP) Referenzen Externe Links openhmpp.org – Neuer Open Standard für Many-Core OCLTools Open Source OpenCL Compiler und Linker GPGPU.org – General-Purpose Computation using Graphics Hardware GPGPU Wiki SIGGRAPH 2005 GPGPU Kursnotizen IEEE VIS 2005 GPGPU Kursnotizen Nvidia Developer Zone AMD GPU Tools CPU vs. GPGPU Was ist GPU Computing? Tech Report article: "ATI setzt Ansprüche auf Physik, GPGPU Boden" von Scott Wasson Preis, Tobias; Virnau, Peter; Paul, Wolfgang; Schneider, Johannes J (2009). "GPU beschleunigte Monte Carlo Simulation des 2D- und 3D-Ising-Modells". Journal of Computational Physics. 228 (12:) 4468.Bibcode:2009JCoPh.228.4468P doi:10.1016/j.jcp.2009.03.018.GPGPU Computing @ Duke Statistische Wissenschaft GPGPU Programmierung in F# mit dem Microsoft Research Accelerator System