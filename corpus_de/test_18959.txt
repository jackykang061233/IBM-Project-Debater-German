Mustererkennung ist die automatische Anerkennung von Mustern und Ordnungsmäßigkeiten in Daten. Sie hat Anwendungen in der statistischen Datenanalyse, der Signalverarbeitung, der Bildanalyse, der Informationsgewinnung, der Bioinformatik, der Datenkomprimierung, der Computeranalyse und des Maschinenlernens. Mustererkennung hat seine Herkunft in Statistiken und Ingenieurwissenschaften; einige moderne Ansätze zur Mustererkennung umfassen die Nutzung des maschinellen Lernens, da größere Verfügbarkeit großer Daten und eine neue Fülle von Verarbeitungsleistungen. Diese Aktivitäten können jedoch als zwei Aspekte des gleichen Anwendungsfeldes angesehen werden, und sie haben in den letzten Jahrzehnten eine erhebliche Entwicklung erlebt. Eine moderne Definition der Mustererkennung ist: Der Bereich der Mustererkennung ist besorgt über die automatische Entdeckung von Ordnungsmäßigkeiten in Daten durch die Verwendung von Computeralgorithmen und die Verwendung dieser Regelmäßigkeiten, um Maßnahmen wie die Einstufung der Daten in verschiedene Kategorien zu ergreifen. Mustererkennungssysteme sind in vielen Fällen aus etikettierten Schulungsdaten ausgebildet, aber wenn keine Etikettierten Daten verfügbar sind, können andere Algorithmen verwendet werden, um vorher unbekannte Muster zu entdecken. KDD und Data Mining haben einen größeren Schwerpunkt auf unüberwachten Methoden und einer stärkeren Verbindung zur Geschäftsnutzung. Mustererkennung konzentriert sich stärker auf das Signal und berücksichtigt auch den Erwerb und die Signalverarbeitung. Er stammt aus der Technik, und der Begriff ist im Zusammenhang mit der Computervision populär: Eine führende Computer-Vision-Konferenz ist die Konferenz über Computer Vision und Mustererkennung. Mustererkennung ist die Zuteilung eines Etiketts an einen bestimmten Inputwert. Statistiken zufolge wurde im Jahr 1936 eine diskriminierende Analyse eingeführt. Ein Beispiel für die Mustererkennung ist die Einstufung, die versucht, jedem Inputwert einer Reihe von Klassen (z.B. festzustellen, ob eine bestimmte E-Mail Spam oder Spam ist). Mustererkennung ist jedoch ein allgemeineres Problem, das auch andere Arten von Output umfasst. Andere Beispiele sind Regression, die für jeden Input eine echte Wertschöpfung zugewiesen; Sequenzetikettierung, die jedem Mitglied einer Reihe von Werten (z.B. ein Teil der Redemarkierung, die einen Teil der Rede zu jedem Wort in einer Eingangsstrafe darstellt), und Parsing, die einen Parsebaum zu einem Beitragssatz verleiht, in dem die Struktur der Strafe beschrieben wird. Mustererkennungsgorithmen zielen generell darauf ab, eine angemessene Antwort für alle möglichen Inputs zu geben und "zumeist" an den Input anzupassen, wobei deren statistische Variationen berücksichtigt werden. Dies ist gegen Mustervergleiche, die genaue Spiele im Input mit bestehenden Mustern sehen. Ein gemeinsames Beispiel für einen musterorientierten Algorithmus ist eine regelmäßige Darstellung, die Muster einer bestimmten Art in Textdaten beleuchtet und in die Suchfähigkeiten vieler Textverarbeiter und Wortverarbeiter aufgenommen wird. Übersicht Mustererkennung wird in der Regel nach Art des Lernverfahrens, das zur Erzeugung des Outputs verwendet wird, kategorisiert. Supervised Learning geht davon aus, dass eine Reihe von Ausbildungsdaten (die Ausbildungseinrichtung) bereitgestellt wurde, bestehend aus einer Reihe von Fällen, die mit der korrekten Produktion richtig gekennzeichnet sind. Ein Lernverfahren schafft dann ein Modell, das versucht, zwei manchmal widersprüchliche Ziele zu erreichen: Durchführbarkeit der Ausbildungsdaten und eine möglichst breite Verbreitung neuer Daten (normalerweise bedeutet dies eine möglichst einfache, technische Definition von einfacher, im Einklang mit dem nachstehend diskutierten Thema Razor). Unüberwachtes Lernen hingegen geht davon aus, dass es sich um Ausbildungsdaten handelt, die nicht handgedeckt wurden, und versucht, inhärente Muster in den Daten zu finden, die dann verwendet werden können, um den richtigen Ausgangswert für neue Datenfälle zu bestimmen. Eine Kombination der beiden, die vor kurzem untersucht wurden, ist halbbeaufsichtigtes Lernen, das eine Kombination aus gekennzeichneten und unlabelten Daten (in der Regel ein kleines Paket von Etikettendaten in Kombination mit einem großen Betrag unlabelter Daten) nutzt. Hinweis darauf, dass im Falle eines unkontrollierten Lernens überhaupt keine Ausbildungsdaten vorliegen, d. h. die zu kennzeichnenden Daten sind die Ausbildungsdaten. Hinweis darauf, dass manchmal unterschiedliche Bedingungen verwendet werden, um die entsprechenden überwachten und unkontrollierten Lernverfahren für die gleiche Art von Output zu beschreiben. Zum Beispiel ist das unkontrollierte Klassifizierungsäquivalent in der Regel als Cluster bekannt, das auf der gemeinsamen Wahrnehmung der Aufgabe beruht, keine Ausbildungsdaten zu sprechen, und die Inputdaten in Cluster zu bündeln, die auf einer gewissen inhärenten ähnlichen Maßnahme basieren (z.B. die Entfernung zwischen den Instanzen, die als Vektor in einem multidimensionalen Vektorraum angesehen werden), anstatt die einzelnen Beiträge in eine Reihe von vordefinierten Klassen aufzunehmen. In einigen Bereichen unterscheidet sich die Terminologie: Zum Beispiel wird die Begriffsklassifikation verwendet, um auf das zu verweisen, was gemeinhin als Cluster bekannt ist. Die Eingabedaten, für die ein Outputwert generiert wird, werden förmlich als Beispiel bezeichnet. Das Beispiel wird förmlich von einem Vektor von Merkmalen beschrieben, der zusammen eine Beschreibung aller bekannten Merkmale des Falles darstellt. () Diese Merkmalsvektoren können als Definition von Punkten in einem geeigneten multidimensionalen Raum angesehen werden, und Methoden für den Einsatz von Vectors in Vektorflächen können entsprechend auf sie angewendet werden, wie z.B. das Punktprodukt oder den Winkel zwischen zwei Vektoren.) Merkmale sind in der Regel entweder als nominiertes, d.h. aus einer Reihe von unbeschränkten Gegenständen, wie z.B. Geschlecht von Männern oder Frauen oder Bluttyp A, B, AB oder O), Ordinal (Konsistenz eines einer Reihe von bestellten Gegenständen, z.B. groß, mittel oder klein), Numerierung (z.B. eine Anzahl von Ereignissen eines bestimmten Wortes in einer E-Mail) oder reale Werte (z.B. eine Messung des Blutdrucks). Häufig werden kategorische und ordinale Daten zusammen zusammengefasst; auch für zahlwertige und reale Daten. Außerdem arbeiten viele Algorithmen nur in Bezug auf kategorische Daten und erfordern, dass reale oder zahlbare Daten in Gruppen (z.B. weniger als 5, zwischen 5 und 10 oder mehr als 10) zusammengefasst werden. Probabilistische Klassentoren Viele gemeinsame Mustererkennungsgorithmen sind probabilistisch in der Natur, da sie statistische Unterschiede verwenden, um ein bestimmtes Beispiel zu finden. Anders als andere Algorithmen, die einfach ein bestes Zeichen setzen, führen oft probabilistische Algorithmen auch zu einer Wahrscheinlichkeit, dass das von dem angegebenen Etikett beschriebene Beispiel beschrieben wird. Zusätzlich führen viele probabilistische Algorithmen eine Liste der N-best-Labels mit zugehörigen Probabilities, für einen gewissen Wert von N, anstelle eines einzigen besten Etiketts. Wenn die Anzahl möglicher Etiketten recht klein ist (z.B. im Falle der Einstufung), kann N so festgesetzt werden, dass die Wahrscheinlichkeit aller möglichen Etiketten ausfällt. Probabilistische Algorithmen haben viele Vorteile gegenüber nicht-probabilistischen Algorithmen: Sie machen einen Vertrauenswert, der mit ihrer Wahl verbunden ist. (Anmerkung, dass einige andere Algorithmen auch Vertrauenswerte erzeugen können, im Allgemeinen aber nur für probabilistische Algorithmen ist dieser Wert, der in der Wahrscheinlichkeitstheorie mathematisch verankert ist. Nicht-probabilistische Vertrauenswerte können im Allgemeinen keine spezifische Bedeutung erhalten und nur verwendet werden, um andere Vertrauenswerte zu vergleichen, die durch denselben Algorithmus hergestellt werden. Entsprechend können sie sich entschließen, wenn das Vertrauen, eine bestimmte Produktion auszuwählen, zu gering ist. Probabilistische Muster-Anerkennungsgorithmen können wirksamer in größere maschinenlesbare Aufgaben integriert werden, um das Problem der Fehler. teilweise oder vollständig zu vermeiden. Anzahl der wichtigen Merkmalsvariablen-Auswahlgorithmen versuchen, sich direkt auf überflüssige oder irrelevante Merkmale einzustellen. Eine allgemeine Einführung zur Auswahl, die Konzepte und Herausforderungen zusammenfasst, wurde gegeben. Die Komplexität der Merkmalsauswahl ist aufgrund ihres nicht-Montonischen Charakters ein Optimierungsproblem, bei dem eine Gesamtmenge von n {\displaystyle n} die Kraft aus allen 2 n n − 1 {\displaystyle 2 2n}-1 Untersets von Merkmalen untersucht werden müssen. Der branchen-und-Bound-Algorithmus verringert diese Komplexität, ist aber für mittlere bis große Werte der Anzahl der verfügbaren Merkmale n n HANAdisplaystyle n} tragbar. Für einen groß angelegten Vergleich von charakteristischen Algorithmen siehe . Techniken zur Umwandlung der Rohstoffvektoren (Leistungsgewinnung) werden manchmal vor der Anwendung des Muster-Konflikts verwendet. Kennzeichnend ist beispielsweise der Versuch, einen großendimensionalitätsfaktor in einen kleinerendimensionalen Vektor zu reduzieren, der leichter mit weniger Entlassungen arbeiten und mit mathematischen Techniken wie der Hauptkomponentenanalyse (PCA) verschlüsseln kann. Unterscheidung zwischen der Merkmalsauswahl und der Materialgewinnung ist, dass die sich daraus ergebenden Merkmale nach der Materialgewinnung unterschiedlich sind als die ursprünglichen Merkmale und möglicherweise nicht leicht zu interpretieren sind, während die nach der Produktauswahl verbleibenden Merkmale einfach ein Teil der ursprünglichen Merkmale sind. Problemlösung Formell kann das Problem der Mustererkennung wie folgt erklärt werden: Angesichts einer unbekannten Funktion g : X → Y WELLdisplaystyle g {X}}\rightarrow WELLcal {Y} (Grundlage) die Karten, die Inputs x {\ X {\displaystyle {x{\in {X} zu Output y  Y Y KINGstyle y\in {Y) {Y mathematisch {Y}, sowie Ausbildungsdaten D = { x 1, y 1 }, ... ( x n n , y n) } · s Y KING KING \in {Y \in {Y) {Y {Y {Y} · \d= } 7.8 Mathematik {Y}, die so eng wie möglich die korrekte Kartierung g Memestyle g} (z.B. wenn das Problem Spamfiltert, dann x i {\displaystyle x{\_{i ist eine Darstellung einer E-Mail und y KINGstyle y} ist entweder Spam oder Spam). Um ein klar definiertes Problem zu sein, müssen "caimates so eng wie möglich" streng definiert werden. In der Entscheidungstheorie wird dies durch die Angabe einer Verlustfunktion oder einer Kostenfunktion definiert, die einen bestimmten Wert für Verluste aus der Erstellung eines falschen Etiketts verleiht. Ziel ist es, den erwarteten Verlust zu minimieren, wobei die Erwartung über die wahrscheinliche Verteilung von X {\displaystyle {X} .In der Praxis weder die Verteilung von X {\displaystyle {X} noch die Boden wahrheitsrechtliche Funktion g : X → Y WELLdisplaystyle g {X}}\rightarrow WELLcal {Y} ist genau bekannt, kann aber nur empirische Berechnungen vorgenommen werden, indem eine große Anzahl von Proben von X {\displaystyle {X} gesammelt und mithilfe des richtigen Werts von Y {\displaystyle {Y} (ein zeitaufwendiger Prozess, der in der Regel den Faktor der Daten dieser Art beschränkt, die gesammelt werden können). Die besondere Verlustfunktion hängt von der Art der Kennzeichnung ab. Zum Beispiel ist die einfache Nullverlustfunktion häufig ausreichend. Dies entspricht lediglich einem Verlust von 1 zu einem falschen Etikett und bedeutet, dass der optimale Klassenprüfer die Fehlerquote auf unabhängigen Testdaten minimiert (d. h. den Bruchteil der Fälle, die die erworbene Funktion h : X → Y WELLdisplaystyle h. {X}}\rightarrow JPY {Y} Etiketten falsch, was der Maximierung der Anzahl der korrekt klassifizierten Fälle entspricht. Ziel des Lernverfahrens ist es, die Fehlerquote (maximiert die Richtigkeit) auf einem typischen Testsatz zu minimieren. Für ein probabilistisches Muster-Anerkennungsgerät ist das Problem stattdessen die Wahrscheinlichkeit jedes möglichen Output-Labels, d. h. eine Funktion der Form p ( l b e l.V. x ,   ) = f ( x ;   ) ) Memestyle p(E ) KING-Label}}-Marke x}}, E-Mail: }f\link Symbol x}}; {\ }}} . . . . . . . . . . . . . . . . . . . . . . ., . .  f  f  f  f  f  f . .  where . . . . . . . . . . . . . . . . . . . . In In In In . . . . . . . . . . . . . . . . . In einem physikalisch-technischen Ansatz wird jedoch die umgekehrte Wahrscheinlichkeit p ( x | l a b e l ) displaystyle p({{\zeichen x| {label)} mit der vorherigen Wahrscheinlichkeit p ( l b e l  l  | ) 050style p( }} }} | | | {\ {\ {\ {\ {\ {\    l  l  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |        |  |  |  l  l  |  l  l  l  l  l  l  l  | ) ) ) ) L . alle Etiketten p ( x ) p (L [  L ) . . Memedisplaystyle p(Getrm Label}} ·|, E-Mail: )}=1,0frac p({{\ . . . | | | | {\ {\ {\ . . . {\ {\ {\ {\ {\ {\ | {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ L L {\ {\ L {\ {\ {\ {\ {\ {\ {\ L {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ Wenn die Etiketten kontinuierlich verteilt werden (z.B. in Regressionsanalyse), führt der Nenner eine Integration statt eine Zusammenfassung ein: p ( l a b e l ,   ) = p ( x | a b e l ,   ) p ( l a b e l | θ  )   L . alle Etiketten p ( x ) p (L.  L ) d  L L . . Memedisplaystyle p(Getrm Label}} ·|, fiskalische Symbol )}= {\frac p({{\ . | | | | | | | | . . . . . . . . . . . . . . . . . . . L L . . L L L L L L L L L L L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Dies ist der beste Wert, der gleichzeitig zwei widersprüchliche Objekte trifft: Um die Ausbildungsdaten (kleinste Fehlerrate) so gut wie möglich durchzuführen und das einfachste Modell zu finden. Konkret bedeutet dies eine maximale Wahrscheinlichkeitsschätzung mit einem Regularisierungsverfahren, das einfachere Modelle über komplexere Modelle begünstigt. In einem Bayesischen Kontext kann das Regularisierungsverfahren als eine Vorwahrscheinlichkeit p ( p ) {\displaystyle p( Memeprint Symbol SSOtheta }})} zu verschiedenen Werten von θ HANAdisplaystyle HANA Stempel HANAtheta {\ Ma mathematic: Ma = arg  = p ( |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ }*={^\arg \max Meme_print Symbol Memetheta }p(Markenzeichen \| Mathematik} {D} {D} )}, wo ∗ {\displaystyle HANA Stempel HANAtheta *** der Wert für θ {\ {\ {\  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  7.8displaystyle p(Eßzeichen) \|} {D} {D} )=left[\prod i=1}^{n}p(y_{i} x__{i}, E-Mail-Symbol Memetheta )} Im Bayesian-Ansatz zu diesem Problem wird anstelle der Wahl eines einzigen Parametervektors {\ HANAdisplaystyle HANAtheta **}} , die Wahrscheinlichkeit eines bestimmten Etiketts für ein neues Beispiel x {\displaystyle {x} durch die Integration aller möglichen Werte von . {\ {\ {\ {\ {\ {\ {\ {\  l  |  | .  |  |  |, gewichtet nach der Wahrscheinlichkeit: p l  e  |  |  |  |  |  |  |  |  |  |  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D  D 7.8displaydisplaystyle p(Getrm Label}}) {x}} {x))=\int p(Getrm Label| ·|, Glühbirnetheta )}p(E-Marke HANAtheta \8.5} {D} ..} Frequentist oder Bayesian-Ansatz zur Mustererkennung Die erste Musterklasse – der lineare Disriminant von Fisher – wurde in der häufigen Tradition entwickelt. Der häufige Ansatz bedeutet, dass die Modellparameter als unbekannt, aber objektiv gelten. Die Parameter werden dann aus den gesammelten Daten (geschätzt) berechnet. Diese Parameter sind genau die Mittelwerte und die Kovarianzmatrix. Auch die Wahrscheinlichkeit jeder Klasse ( l a b e l [  |  | ) Memedisplaystyle p(Getrm Label| · E-Mail Memetheta }})} wird von der gesammelten Datengruppe geschätzt. Hinweis darauf, dass die Verwendung von 'Bayes-Regelung' in einem Musterklasseifier den Klassifikationsansatz Bayesian nicht darstellt. Bayesische Statistiken haben ihren Ursprung in der griechischen Philosophie, wo bereits eine Unterscheidung zwischen dem „a priori“ und dem „ein Posteriori“-Know besteht. Später legte Kant seinen Unterschied zwischen dem, was vor der Beobachtung bekannt ist – und dem empirischen Wissen, das durch Beobachtungen gewonnen wurde. In einer Biesischen Musterklasse, kann die Klasse Probabilities p ( l a b e l | ) ) Memedisplaystyle p(Getrm Label| · olitheta }})} vom Nutzer gewählt werden, der dann eine Vorabwahl ist. Darüber hinaus können die Erfahrungen, die als vorherrschende Parameterwerte quantifiziert werden, mit empirischen Beobachtungen gewichtet werden, z.B. mit dem Beta- (conjugate vor) und Dirichlet-distributionen. Der Bayesische Ansatz erleichtert einen nahtlosen Austausch zwischen Fachkenntnissen in Form von subjektiven Fähigkeiten und objektiven Beobachtungen. Probabilistische Musterklassen können nach einem häufigen oder einem Bayesischen Ansatz verwendet werden. Verwendungen innerhalb der medizinischen Wissenschaft sind die Grundlage für computergestützte Diagnosesysteme (CAD). CAD beschreibt ein Verfahren, das die Auslegungen und Feststellungen des Arztes unterstützt. Andere typische Anwendungen von Mustererkennungstechniken sind automatische Spracherkennung, Sprecheridentifizierung, Einstufung von Texten in mehrere Kategorien (z.B. Spam/Nicht-E-Mail-Nachrichten), automatische Anerkennung von Handverschreibungen auf Postsendungen, automatische Anerkennung von Bildern menschlicher Gesichter oder Bildgewinnung aus medizinischen Formen. Die letzten beiden Beispiele bilden die subtopische Bildanalyse der Mustererkennung, die sich mit digitalen Bildern als Input für Mustererkennungssysteme befasst. optische Charaktererkennung ist ein klassisches Beispiel für die Anwendung eines Musterklasseifiers, siehe OCR-Beispiel. Im Jahr 1990 wurde die Methode der Unterzeichnung eines Namens mit einem Wort und einer Überlagerung erfasst. Schlagzeilen, Geschwindigkeit, relative min, relative max, Beschleunigung und Druck werden verwendet, um die Identität eindeutig zu erkennen und zu bestätigen. Banks wurden zuerst diese Technologie angeboten, aber sie waren Inhalte, um von der FDIC für jeden Bankbetrug zu sammeln und keine Unannehmlichkeiten für Kunden einzugehen. Mustererkennung hat viele echte Anwendungen in der Bildverarbeitung, einige Beispiele sind: Identifizierung und Authentifizierung: z.B. Lizenzzeichenerkennung, Fingerabdruckanalyse, Gesichtserkennung/Verifizierung und Sprach-Authentifizierung. medizinische Diagnose: z.B. Screening für Gebärmutterhalskrebs (Papnet), Brust Tumoren oder Herzgeräusche; Verteidigung: verschiedene Navigations- und Leitsysteme, Zielerkennungssysteme, Formerkennungstechnik usw. Mobilität: fortschrittliche Fahrerassistenzsysteme, autonome Fahrzeugtechnik usw. In der Psychologie ist die Mustererkennung (der Sinn und die Identifizierung von Gegenständen) eng mit der Wahrnehmung verknüpft, die erklärt, wie die sensorischen Inputs sinnvoll sind. Mustererkennung kann auf zwei unterschiedliche Weise erwogen werden: das erste Modell, das sich anpasst, und die zweite ist Erkennung. Eine Vorlage ist ein Muster, das verwendet wird, um Gegenstände derselben Proportionen herzustellen. In der Prägetationshypothesis wird vorgeschlagen, dass neue Stimululi mit Mustern im langfristigen Gedächtnis verglichen werden. Liegt ein Spiel, werden die Impulse ermittelt. Spielfilmerkennungsmodelle, wie das Pandemonium-System für die Einstufung von Buchstaben (Selfridge, 1959), weisen darauf hin, dass die Stimulus in ihre Komponenten für die Identifizierung zerbrochen werden. Beispielsweise verfügt ein Kapital E über drei horizontale Linien und eine vertikale Linie. Algorithms Algorithms für die Mustererkennung hängen von der Art der Kennzeichnungsleistung ab, davon, ob das Lernen überwacht oder unüberwacht ist und ob der Algorithmus statistisch oder nichtstatistisch ist. Statistische Algorithmen können weiter als physikalisch oder diskriminierend eingestuft werden. Klassifikationsmethoden (Methoden, die kategorische Etiketten vorhersagen) Parameter: Lineare diskriminierende Analyse von Grimic diskriminant Analyse der höchstmöglichen Eutropy-Klasse (aka logistische Regression, multinomiale logistische Regression): Hinweis darauf, dass logistische Regression trotz ihres Namens ein Algorithmus für die Einstufung ist. () Der Name kommt aus der Tatsache, dass logistische Regression eine Verlängerung eines linearen Regressionsmodells verwendet, um die Wahrscheinlichkeit eines Inputs in einer bestimmten Klasse zu modellieren.) Nichtparametrisch: Entscheidungsbäume, Entscheidungslisten für die Schätzung des Adapters und K-norest-neighbor-Algorithmen Naive Bayes-Klasseifier Neural Netze (Multi-layer-Perceptrons) Perceptrons Antriebsmaschinen Gen Expression Programmierung von Clustering-Methoden (Methoden zur Einstufung und Vorhersage von Kategorithmen) Kathogorische Mischungsmodelle Hierarchisches Clustering (agglomerative oder divisive) K-means Clustering Correlation Clustering Teilanalyse (Kernel PCA) Ensemble Learning Algorithmen (überarbeitete Meta-Algorithmen für die Kombination mehrerer Lernalgorithmen)Boosting (meta-algorithm)Bootstrap Aggregation (bagging) Ensemble durchschnittlicher Mixture von Sachverständigen, hierarchische Mischung von Sachverständigen Allgemeine Methoden zur Vorhersehbarung von willkürlichen (Säulen) Etiketten Bayesian Networks Markov zufällige Multilineare Subspace Learning-Algorithmen (vorhergestellte Etiketten von multidimensionalen Daten mit zehnteren Darstellung) Keine Überwachung: Multilineare Hauptkomponentenanalyse (MPCA) Echtzeitetikettierungsmethoden (vorhergestellte Sequenzierungssequenzen der realen Wertwerte)Kalman Filter Particle Filter Regressionsmethoden (vorhergestellte tatsächliche Wertwerte)Gaussian-Regression (kriging)Linear Regression und Erweiterung der unabhängigen Komponentenanalyse (ICA)Principal-Komponentenanalyse (PCA) Sequence-Label-Methoden (Vorurteilung von Kategorisierungssequenzen))e Zufallsfelder (CRFs) versteckten Markov-Modelle (HMMs) Höchste entropy Markov Modelle (MEMMs) wiederkehrende Neuralnetze (RNNs)Dynamic-Zeitkrieg (DTW) Siehe auch Hinweise Dieser Artikel basiert auf dem Material, das vor dem 1. November 2008 aus dem frei zugänglichen Computer-System entnommen wurde und unter den neuen Bedingungen der GFDL, Version 1,3 oder später aufgenommen wurde. Weitere Lesung Shinnaga, Blattosuke (1990). Einführung in die Statistische Mustererkennung (2. Boston: wissenschaftliche Presse.ISBN gegen 0,469851-4.Hornegger, Joachim; Christoph, Dietrich W. R. (1999). Anwendung der Mustererkennung: Eine praktische Einführung zur Bild- und Sprachverarbeitung in C+ (2. ed). San Francisco: Morgan Kaufmann Publishers.ISBN UV3-528-15558-2.Schuermann, Juergen (1996). Musterklassifikation: Ein einheitlicher Überblick über die statistischen und Neuralkonzepte. New York: Julius.ISBN, DIE0-471-13534-0. Godfried T. Toussaint, ed. (1988) Computational Morphologie. Amsterdam: Nord-Holland Verlagsgesellschaft.ISBNcad483296722.Kulikowski, Kasimir A; Weiss, Sholom M. (1991). Computersysteme Dieses Lernen: Klassifikations- und Prognosemethoden aus Statistiken, Neural Nets, Maschinenbau und Expertensystemen. Maschinenbau. San Francisco: Morgan Kaufmann Publishers.ISBN gegen 17.00-065-2.Duda, Richard O; Hart, Peter E; Stork, David G. (2000). Musterklassifikation (2. Nick-Interscience.ISBN gegen0471056690.Jain, Anil.K; Duin, Robert.P.W; Mao, Jianchange (2000). " Statistische Mustererkennung: Überprüfung. Prüfverfahren über Musteranalyse und Maschinennachrichten.22 (1): 4–37.CiteSeerX 10.1.123.8151.doi:10.1109/34.824819.An ein Einführungs-Werkzeug an Klassenifiers (Einführung der Grundbegriffe mit numerischem Beispiel) Externe Links The International Association for Mustererkennungs-Liste der Mustererkennungs-Website der Muster Research Muster Anerkennung Info Muster (International Journal of Muster Society) International Journal of Muster Society (International Journal of Muster Society) Bessere Anpassung an den schnellen Mustern