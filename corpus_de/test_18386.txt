lineare Diskriminierung (LDA), normale diskriminierende Analyse (NDA) oder Diskriminierende Funktionanalyse ist eine allgemeine Erfassung der linearen Unterscheidungskraft der Fischer, eine Methode, die in Statistiken und anderen Bereichen verwendet wird, um eine lineare Kombination von Merkmalen zu finden, die zwei oder mehrere Arten von Gegenständen oder Veranstaltungen kennzeichnet. Die daraus resultierende Kombination kann als linearer Prüfer oder, allgemeiner, zur Verringerung der Dimension vor einer späteren Einstufung verwendet werden. LDA ist eng mit der Analyse der Varianz (ANOVA) und der Regressionsanalyse verknüpft, die auch versuchen, eine Abhängigkeit von anderen Merkmalen oder Messungen zu äußern. ANOVA verwendet jedoch kategorische unabhängige Variablen und eine ständige abhängige variable Variablen, während die diskriminierende Analyse kontinuierliche unabhängige Variablen und eine kategorisch abhängige Variablen (d.h. das Klassenetikett) aufweist. Logistische Regression und Probit-Regression sind ähnlich wie LDA als ANOVA, da sie auch eine kategorische Variablen durch die Werte kontinuierlicher unabhängiger Variablen erklären. Diese anderen Methoden sind vorzugsweise in Anwendungen, in denen es nicht zumutbar ist, dass die unabhängigen Variablen normalerweise verteilt werden, was eine grundlegende Annahme der LDA-Methode ist. LDA ist auch eng mit der Hauptkomponentenanalyse (PCA) und der Faktoranalyse verbunden, dass sie sowohl lineare Kombinationen von Variablen suchen, die die Daten am besten erklären. LDA versucht ausdrücklich, den Unterschied zwischen den Datenklassen zu modellieren. PCA berücksichtigt dagegen keine Unterschiede in der Klasse, und die Faktoranalyse baut die charakteristischen Kombinationen auf, die sich auf Differenzen und nicht auf Ähnlichkeiten stützen. Diskriminierende Analyse unterscheidet sich auch von der Faktoranalyse, da es keine Interdependence-Methode ist: eine Unterscheidung zwischen unabhängigen Variablen und abhängigen Variablen (auch als Kriteriumvariable bezeichnet) muss gemacht werden. LDA funktioniert, wenn die Messungen auf unabhängigen Variablen für jede Beobachtung kontinuierliche Mengen sind. Bei der Behandlung von kategorischen unabhängigen Variablen wird die gleichwertige Technik diskriminiert. Diskriminierende Analysen werden verwendet, wenn Gruppen bekannt sind (ungleich in der Clusteranalyse). Jeder Fall muss über eine oder mehrere quantitative Vorhersehbarkeitsmaßnahmen verfügen und über eine Gruppenmaßnahme verfügen. Kurz gesagt, eine diskriminierende Funktionsanalyse ist die Einstufung - der Akt der Verteilung der Dinge in Gruppen, Klassen oder Kategorien derselben Art. Geschichte Im Jahr 1936 wurde die ursprüngliche Dichotomous Discriminant-Analyse von Sir Ronald Fisher entwickelt. Es unterscheidet sich von einer ANOVA oder MANOVA, die verwendet wird, um eine (ANOVA) oder mehrere (MANOVA) kontinuierliche, abhängige Variablen durch eine oder mehrere unabhängige Kategorisierungsvariablen vorherzusagen. Diskriminierende Funktionsanalyse ist sinnvoll, um festzustellen, ob eine Reihe von Variablen wirksam ist, um eine Mitgliedschaft in der Kategorie vorherzusagen. LDA für zwei Klassen erwägen eine Reihe von Beobachtungen x → Memedisplaystyle Memevec {x} (auch als Merkmale, Eigenschaften, Variablen oder Messungen bezeichnet) für jede Probe eines Gegenstands oder Veranstaltung mit bekannter Klasse Yendisplaystyle y}. Dieser Satz von Proben wird als Ausbildungseinrichtung bezeichnet. Das Klassifizierungsproblem besteht dann darin, einen guten Vorhersehbaren für die Klasse y KINGstyle y} einer beliebigen Probe der gleichen Verteilung (nicht notwendigerweise aus der Ausbildungseinrichtung) zu finden, da nur ein Beobachtungsmerkmal x → KINGstyle HANAvec {x} vorliegt. LDA bringt das Problem mit der Annahme an, dass die bedingte Wahrscheinlichkeitsdichte p ( x → ) {\displaystyle p( {x|=0) und p ( x → [ y = 1 ] ) {\displaystyle p( {x| {x|||=1) sowohl die normale Verteilung mit Mittelwerten als auch die Kovarianzparameter ( μ → 0,  0 0   0   0   0         ve {\ {\ {\ {\ {\ {\ {\ mu mu mu mu mu mu  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu mu Unter dieser Annahme besteht die optimale Lösung darin, Punkte, die von der zweiten Klasse ausgehen, wenn das Protokoll der Wahrscheinlichkeitsraten größer ist als einige Schwellenwerte T, so dass: ( x → · · · · ·   0 ) T  0 0 − 1 ( x → · ·   0 ) + ln  | 0  | 0 ( x → → 1 ) T 1 1 → 1 ( x · · · · · → 1 ·  l 1 · → 1 ·  l 1 ) ) 1 ) ) 1 ) ) )  l 1 ) 1 ) ) 1  | ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) 1  1 1  1 1  1  1  1  1  1  1  1  1  1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 T {\displaystyle {(\vec x}}-ggiovec Mememu _0})TT _Sigma _0}^{-1}(Dok. ) {_0})+\lnn \|Sigma _0} x}}-Spavec 7.8mu _1}TTSSigma _1-1-1}(Dok. {_1})-\ln \|Sigma {_1\|} T} Ohne weitere Annahmen wird der nachfolgende Klassenempfänger als QDA (Wertpapiere diskriminierende Analyse) bezeichnet. LDA macht stattdessen die zusätzliche Vereinfachung der Homoscedasticity Annahme (d. h. dass die Klassenkombinationen identisch sind, so  so 0 =  1 1 = {\ KINGstyle \Sigma {_0} {_1}=\Sigma } ) und die Kovarien haben einen vollen Rang. In diesem Fall werden mehrere Bedingungen aufgehoben: x → T  0 0 − 1 x → x → T  1 1 − 1 x → {\displaystyle Memevec x}}^{T}\ Sigma _0}^{-1}{\ x== xTT _Sigma _1}^{-1}{\vec {x} x → T  i i − 1 μ → i = μ → i T  i i − 1 x → displaystyle Memevec x}}^{TTTTT Sigma _i-1-1}{\vec 7.8mu _i} 7.8mu _iTT}{\ Sigma _i-1-1}{\vec {x} wegen  i i KINGstyle \Sigma {_i} ist Hermitian und das obengenannte Entscheidungskriterium wird zu einem Schwellenwert für das Punkteprodukt w → · · · · · · {cdot Memevec {w}}\cdot {x>>c für einige Schwellen c, wo w →  1 − 1 ( · · · · · · · · 0 → 0 → 0 {\ {\ {\ ) ) {\ {\ {\ {\ } {w==\ Sigma ^-1}(Getvec Mememu _1}-Portvec 7.8mu {_0) c = w → 1 2 (μ → 1 + μ → 0 ) 7.8displaystyle c= {w}}\cdot Memef 1 12((Portvec Mememu _1} + Memevec 574 7.8vec KINGmu {0) Dies bedeutet, dass das Kriterium eines Inputs x → {\displaystyle Memevec {x} in einer Klasse Yendisplaystyle y} rein eine Funktion dieser linearen Kombination der bekannten Beobachtungen darstellt. Es ist oft sinnvoll, diesen Abschluss in geometrischer Hinsicht zu sehen: das Kriterium eines Inputs x → Memedisplaystyle {x} in einer Klasse y Memestyle y} ist rein eine Funktion der Projektion von multidimensionalem Raumpunkt x → → KINGstyle Memevec {x} auf Vektor W → Memestyle Memevec {w} (thus, wir betrachten nur seine Richtung). In anderen Worten gehört die Beobachtung zu Yendisplaystyle y}, wenn dies x → KINGstyle Memevec {x} auf einer bestimmten Seite eines Hyperflugzeugs per Flugzeug auf w → {\displaystyle HANAvec {w} liegt. Die Lage des Flugzeugs wird durch die Schwelle c. AssumrationenDie Annahmen einer diskriminierenden Analyse sind dieselben wie für MANOVA. Die Analyse ist sehr empfindlich, und die Größe der kleinsten Gruppe muss größer sein als die Zahl der vorhergesagten Variablen. Multivariative Normalität: Unabhängige Variablen sind für die einzelnen Ebenen der Gruppenvariable normal. Homogenität der Varianz/Kovarianz (homsedasticity): Unterschiede zwischen den Gruppenvariablen sind die gleichen Werte auf allen Ebenen der Vorhersehbaren. Kann mit der M-Statistik von Box getestet werden. Es wurde jedoch vorgeschlagen, dass lineare diskriminierende Analysen verwendet werden, wenn Kovariationen gleich sind und dass quadratische diskriminierende Analysen verwendet werden können, wenn Kovariationen nicht gleich sind. Multicollinearität: Die konjunkturelle Macht kann mit einer erhöhten Korrelation zwischen vorhergesagten Variablen sinken. Unabhängigkeit: Die Teilnehmer werden als Zufallsstichprobe angenommen, und ein Teilnehmer-Score auf einer Variablen wird davon ausgegangen, dass es unabhängig von den Ergebnissen dieser Variablen für alle anderen Teilnehmer ist. Es wurde vorgeschlagen, dass diskriminierende Analysen relativ robust sind, um leichte Verletzungen dieser Annahmen zu verhindern, und es wurde auch gezeigt, dass diskriminierende Analysen immer noch zuverlässig sein können, wenn man Dichototische Variablen verwendet (wo multivariate Normalität oft verletzt wird). Diskriminierende FunktionenDiscriminant-Analysearbeiten durch die Schaffung einer oder mehrerer linearer Kombinationen von Vorhersehbaren, die eine neue latente Variablen für jede Funktion schaffen. Diese Funktionen werden als diskriminierende Funktionen bezeichnet. Die Zahl der Funktionen ist entweder N g − 1 Memedisplaystyle N_{g}-1, wo N g {\displaystyle N_{g} = Anzahl der Gruppen oder p KINGstyle p} (die Zahl der Vorhersehbaren), die immer kleiner ist. In der ersten Funktion werden die Unterschiede zwischen Gruppen auf dieser Funktion maximiert. Die zweite Funktion maximiert die Unterschiede in dieser Funktion, muss aber auch nicht mit der vorherigen Funktion korreliert werden. Nach wie vor funktioniert dies mit dem Erfordernis, dass die neue Funktion nicht mit einer der bisherigen Funktionen korreliert. In Anbetracht der Gruppe j Memedisplaystyle j} \mathbb {R} {_j} Stichprobenfläche) gibt es eine diskriminierende Regel, die, wenn x  j R j {\displaystyle x\in \thebb {R} {R} {F}, dann x  j j Memestyle x\in j} .Discriminant Analyse findet dann „gute“ Regionen der j displaystyle \R \R {R} {R}, so dass die Einstufung als hoch eingestuft wird}. Jede Funktion wird einer diskriminierenden Bewertung unterzogen, um zu ermitteln, wie gut sie die Gruppenplatzierung vorhersagen. Struktur Correlation Coeffizients: Die Korrelation zwischen jedem Berechtigten und dem diskriminierenden Wert der einzelnen Funktionen. Hierbei handelt es sich um eine grenzüberschreitende Korrelation (d. h. nicht für die anderen Berechtigten berichtigt). Standardisierte Koeffizienten: Jedes vorhergesagte Gewicht in der linearen Kombination, die die diskriminierende Funktion ist. Wie in einer Regressionsform sind diese Koeffizienten teilweise (d. h. für die anderen Berechtigten berichtigt). erklärt den einzigartigen Beitrag jedes Berechtigten zur Vorhersehbarkeit der Gruppenzuteilung. Funktionen in der Gruppe Centroids: Mean diskriminierende Ergebnisse für die einzelnen Gruppenvariablen werden für jede Funktion angegeben. Neben den Mitteln ist der weniger Fehler in der Klassifizierung. Diskriminierungsregeln Höchstwahrscheinlichkeit: Lieferungen x an die Gruppe, die die Bevölkerungsdichte (Gruppe) maximiert. Laes Discriminant Regel: Assigns x an die Gruppe, die die  i i f i f i ( x ) Memedisplaystyle \pi i}f_{i}(x)  maximiert, wo  priori die Vorbedingung für diese Einstufung darstellt, und f i ( x ) Memestyle f_{i}(x) repräsentiert die Bevölkerungsdichte. lineare Unterscheidungsregel der Fischer: Maximierung des Verhältnisses zwischen SS zwischen SS und SSin und stellt eine lineare Kombination der Vorhersehbaren für die Gruppe fest. Eigenwerte Ein Eigenwert in diskriminanter Analyse ist das Merkmal jeder Funktion. Es ist ein Hinweis darauf, wie gut diese Funktion die Gruppen unterscheidet, in denen der größere Eigenwert, die bessere Funktion jet. Man sollte jedoch mit Vorsicht interpretiert werden, da Eigenwerte keine Obergrenze haben. Der Eigenwert kann als Verhältnis von SS zwischen SS und SSin angesehen werden, wie in ANOVA, wenn die abhängige Variablen die diskriminierende Funktion ist, und die Gruppen sind das Niveau der IV. Dies bedeutet, dass der größte Eigenwert mit der ersten Funktion, dem zweithöchsten mit dem zweiten usw., verbunden ist. Wirkungsgröße Manche schlagen die Verwendung von Eigenwerten als Wirkungsgröße vor, doch wird dies in der Regel nicht unterstützt. stattdessen ist die canonische Korrelation die bevorzugte Maßnahme der Wirkungsgröße. Es ähnelt dem Eigenwert, ist aber die Quadratwurzel des Verhältnisses von SS zwischen und SS Total. Es ist der Zusammenhang zwischen Gruppen und Funktion. Eine weitere beliebte Maßnahme der Wirkungsgröße ist das Prozent der Varianz für jede Funktion. Diese wird berechnet durch: (Σλx/Σλi) X 100, wo λx den Eigenwert für die Funktion darstellt und Σλi die Summe aller Eigenwerte ist. Dies zeigt uns, wie stark die Vorhersage für diese besondere Funktion im Vergleich zu den anderen ist.Percent kann auch als Wirkungsgröße analysiert werden. Der Kappa-Wert kann dies beschreiben und gleichzeitig die Vereinbarung über die Chance korrigieren. Kappa normalisiert in allen Kategorisierungen statt durch eine deutlich gute oder schlecht ausführende Klassen. Canonische Diskriminierende Analyse für k Klassen Canonical Discriminant Analyse (CDA) stellt Achsen (k ‐ 1 canonische Koordinierung, k die Anzahl der Klassen) fest, die die Kategorien am besten trennen. Diese linearen Funktionen sind unkorrekt und definieren in Wirklichkeit einen optimalen k ‐ 1 Raum durch die n-dimensionale Cloud von Daten, die am besten voneinander getrennt sind (die Projektionen in diesem Bereich). siehe „Multiclass LDA“ für nachfolgende Details. lineare Unterscheidung der Fischer Die Begriffe Fischer lineare Disriminant und LDA werden häufig interaktiv genutzt, obwohl der ursprüngliche Artikel der Fischer tatsächlich eine leicht unterschiedliche Diskriminierung darstellt, die nicht einige der Annahmen von LDA wie normalerweise verteilte Klassen oder gleiche Klassenkombinationen enthält. Zwei Klassen von Beobachtungen: μ → 0 , μ → 1 {\displaystyle Memevec Mememu _0}, {_1 und Kovariances  0 0 ,  1 1 {\displaystyle \Sigma {_0},\Sigma {1} .Die lineare Kombination von Merkmalen w · · x · ·  of  of {\ ve }}\ }}\  1  1  1  1  1  1  1  1  1  1  1 2 = (w · · · · · 1 · · · →                   → → → → → →  1  1  1  1  1  1  1  1  1  1                                   0 ) 2 w → T Σ 1 w → + w → T   0 w → = ( w → ) ( μ → 1 − μ → 0 ) 2 w → T (  0 0 +  1 1 ) → KINGstyle S= Marafrac HANAsigma _text{zwischen}}^{2}}{\sigma _text{mitin}}^{2== {v(\vecdot 574)cT{\Sigma _1}v W++Sigma _0}{\vec {(\vec {w}}\cdot {(\vec Mememu _1}-Barvec Mememu _0})^{2 wvec w}}^{T}(\vec \v) Sigma {_0}+\Sigma _1} {w} Diese Maßnahme ist in gewisser Weise eine Maßnahme des Signal-to-noise-Verhältnisses für die Klassenkennzeichnung. Man kann nachweisen, dass die maximale Trennung erfolgt, wenn w →  be (  0 0 +  1 1 ) − 1 ( μ · · · ·  0 0 ) Memestyle SSOvec {w}}\propto (\Sigma) {_0}+\Sigma _1})-1-1}(Kapitalvec Mememu _1}-Barvec Mememu {_0), wenn die Annahmen der LDA erfüllt sind, entspricht die vorstehende Formel der LDA. Vermerken Sie, dass der Vektor w → Memedisplaystyle Memevec {w} der Normalwert für das diskriminierende Hyperflugzeug ist. In einem zweidimensionalen Problem wird die Linie, die die beiden Gruppen am besten spalten, perspektiv bis w → {\displaystyle Memevec {w} . generell werden die zu diskriminierenden Daten auf die w → KINGstyle Memevec {w} projiziert; dann wird der Schwellenwert, der die Daten am besten von der Analyse einesdimensionalen Vertriebs unterscheidet. Es gibt keine allgemeine Regel für die Schwelle. · wenn die Projektionen der beiden Klassen etwa die gleiche Verteilung aufweisen, wäre eine gute Wahl das Hyperflugzeug zwischen den Projektionen der beiden Mittel, w · · μ · 0 {\displaystyle Memevec {w}}\cdot Memevec Mememu {0 und w → → →  1  1  1  1  1 {\ {\ {\ {w{\cdot Memevec Memevec Mememu {1 { In diesem Fall der Parameter c in Schwellenzustand w →  In x → {w>cdot } ist ausdrücklich zu finden: c = →  1 1 2 ( μ → 0 + μ → 1 → 1 → 1 T ) 1 − 1 μ · 1 2 μ · 0 T Σ 0 T Σ 0 - 1 μ · 0 · 0 · 0 · 0 · · · 0  steuerliche 7.8displaystyle c= {w}}\cdot Memefrac 1 12{\(Getvec Mememu _0} + cuvec Mememu _1})= 7.8frac 1 12{\vec 7.8mu _1TT1Sigma _1TTSSigma _1}{\-1vev vevec Mememu _1}-Spafrac 1}{vec 7.8mu _0TT-1Sigma }{\-1vec {\-1vec 7.8mu {O0 {O-Methode) ist eine lineare und mit einem schwarzen Bild verbunden. Multiclass LDAIn, wenn es mehr als zwei Klassen gibt, kann die Analyse, die bei der Ableitung des Fischer-Griminanten verwendet wird, auf einen Teilraum ausgedehnt werden, der alle Sorten enthalten scheint. Diese Generalisierung ist auf C. R. Rao zurückzuführen. Legt fest, dass jeder der C-Klassen eine durchschnittliche μ i {\displaystyle \mu {_i} und die gleiche Kovarianz {\ \Sigma } .Die Zersplitterung zwischen Klassenvariabel kann durch die Kombinationskovarianz der Klasse definiert werden:  1 b = 1 C  i i = 1 C ( μ − μ ) ( μ i ) T KINGstyle \Sigma _b} 1CC _sum _i=1CC}(\mu {_i}-\mu )(\mu {_i}-\mu )TT}, wo μ KINGstyle \mu } das Mittel der Klasse ist. S = → T  in b → T {\ w → {\displaystyle S= Memef Racvec wTT _Sigma _bvevec {w} Dies bedeutet, dass, wenn w → Memedisplaystyle Memevec {w} ein Eigenrektor von  1 - 1 {\ b RARstyle \Sigma ^-1 of Sigma_{b die Trennung wird dem entsprechenden Eigenwert entsprechen.   − 1 {\ b {\displaystyle \Sigma ^-1-1 Sigma_{b ist diagonalizierbar, die Variabilität zwischen den Merkmalen wird im Teilraum enthalten, der durch die in den C ‐ 1 größten Eigenwerten (da  b b RARstyle \Sigma {_b} am meisten von Rang C - 1) bedeckt ist. Letztere werden in erster Linie als PCA-Reduktion verwendet. Selbstständige, die den kleineren Eigenwerten entsprechen, sind in der Regel sehr empfindlich gegenüber der genauen Auswahl an Ausbildungsdaten, und es ist oft notwendig, die Regularisierung im nächsten Abschnitt zu verwenden. Wenn die Einstufung erforderlich ist, statt der Verringerung der Dimension, gibt es eine Reihe alternativer Techniken. Zum Beispiel können die Klassen aufgeteilt werden, und ein Standard-Fischerei diskriminant oder LDA verwendet werden, um jede Teilung einzustufen. Ein gemeinsames Beispiel hierfür ist "ein gegen den Rest", wo die Punkte einer Klasse in einer Gruppe und alles andere in der anderen Gruppe gestellt werden und anschließend die LDA angewendet wird. Dies führt zu C-Klassenifiers, deren Ergebnisse kombiniert werden. Eine weitere gemeinsame Methode ist die Paarsklassifikation, bei der ein neuer Klassenprüfer für jedes Klassenpaar (Beteiligung C(C) - 1)/2 Klassenstellen insgesamt) geschaffen wird, wobei die einzelnen Klassenprüfer kombiniert werden, um eine endgültige Einstufung zu erstellen. LDA Die typische Umsetzung der LDA-Technik erfordert, dass alle Proben im Voraus verfügbar sind. Jedoch gibt es Situationen, in denen die gesamte Datenmenge nicht verfügbar ist und die Inputdaten als Strafeinheit beobachtet werden. In diesem Fall ist es wünschenswert, dass die LDA-Aufnahme die Fähigkeit hat, die berechneten LDA-Funktionen zu aktualisieren, indem sie die neuen Proben überwacht, ohne den Algorithmus auf dem gesamten Datensatz zu führen. In vielen Echtzeit-Anwendungen wie mobile Robotik oder Online-Herkunft ist es beispielsweise wichtig, die gesammelten LDA-Funktionen zu aktualisieren, sobald neue Beobachtungen vorliegen. Eine LDA-Aufnahmetechnik, die die LDA-Funktionen durch die bloße Beobachtung neuer Proben aktualisieren kann, ist ein incrementaler LDA-Algorithmus, und diese Idee wurde in den letzten zwei Jahrzehnten umfassend untersucht. Chatterjee und Roychowdhury schlugen einen inkrementellen selbst organisierten LDA-Algorithmus zur Aktualisierung der LDA-Funktionen vor. Demir und Ozmehmet schlugen im Rahmen anderer Arbeiten lokale Lernalgorithmen vor, um die LDA-Merkmale durch Fehlerkorrekturen und die Hebbian Lernregeln zu aktualisieren. Später, Aliyari et al.derived schnell inkrementelle Algorithmen zur Aktualisierung der LDA-Funktionen durch Beobachtung der neuen Proben. Praktische Anwendung In der Praxis sind die Klassenmittel und Kovariationen nicht bekannt. Man kann jedoch von der Ausbildungseinrichtung geschätzt werden. Entweder kann die maximale Wahrscheinlichkeitsschätzung oder die maximale nachträgliche Schätzung zum genauen Wert in den vorstehenden Berechnungen verwendet werden. Obwohl die Schätzungen der Kovarianz in gewissem Maße als optimal angesehen werden können, bedeutet dies nicht, dass die daraus resultierende Diskriminierung, die durch die Substitution dieser Werte erlangt wird, in jedem Sinne optimal ist, auch wenn die Annahme der normalerweise verteilten Klassen korrekt ist. Eine weitere Komplikation bei der Anwendung von LDA und Fishers diskriminieren reale Daten, wenn die Anzahl der Messungen jeder Probe (d. h. die Dimension der einzelnen Datenvektoren) die Anzahl der Proben in jeder Klasse übersteigt. In diesem Fall haben die Kovarianz-Schätzungen keinen vollen Rang, und dies kann daher nicht umgeleitet werden. Hier gibt es eine Reihe von Möglichkeiten. Eines ist die Verwendung einer Pseudo imverse anstelle der üblichen Matrix in der vorstehenden Formel. Eine bessere Rechenstabilität kann jedoch erreicht werden, indem zunächst das Problem auf den Teilraum, der von {\ b {\displaystyle \Sigma {_b} geteilt wird.Eine weitere Strategie zur Lösung kleiner Stichprobengröße besteht darin, einen schrumpfenden Ester der Kovarianzmatrix zu verwenden, der mathematisch ausgedrückt werden kann als  1 = ( 1 λ )  + + λ I KINGstyle \Sigma (=1-\lambda )\Sigma +\lambda I, where, wo I KINGstyle I} die Identitätsmatrix ist, und   KINGstyle \lambda } ist der Schrumpfungs- oder Ordnungsparameter. Dies führt zu einer regelmäßigen diskriminierenden Analyse oder einer schrumpfenden Analyse. In vielen praktischen Fällen sind lineare Diskriminierungen nicht geeignet. LDA und Fisher's Discriminant können für die Verwendung in nichtlinearer Einstufung über den Kernschwierigkeiten erweitert werden. Hier werden die ursprünglichen Beobachtungen effektiv in einen mehrdimensionalen nicht-linearen Raum zusammengefasst. Lineare Klassifizierung in diesem nicht-linearen Raum entspricht dann der nichtlinearen Klassifikation im ursprünglichen Raum. Das am häufigsten verwendete Beispiel hierfür ist der Kern Fisher diskriminant. LDA kann auf mehrere diskriminierende Analysen ausgedehnt werden, bei denen c anstelle von nur zwei zu einer kategorischen variablen Variablen mit N möglichen Staaten wird. Analog, wenn die Klassen-Bedingungen p ( x →  c c = i ) Memestyle p(Getvec {x cmid c=i)} normal mit gemeinsamen Kovariationen sind, sind die ausreichenden Statistiken für P (c ∣ x → ) WELLdisplaystyle P(c\mid Memevec {x)} die Werte der N Projektionen, die durch die N-Mittel abgedeckt sind, die durch die Kovarianzprojektierung der Kovarianz in der Kombinationsmatrix geteilt werden. Diese Projektionen lassen sich durch die Lösung eines allgemeinisierten Selbstwertproblems feststellen, bei dem der Numerator die Kovarianzmatrix ist, die durch die Behandlung der Mittel als Proben gebildet wird, und der Nenner ist die gemeinsame Kovarianzmatrix. siehe „Multiclass LDA“. Anwendungen Neben den unten aufgeführten Beispielen wird LDA in Positions- und Produktmanagement angewendet. Kontaminierte Vorhersage Konkursvorhersage auf der Grundlage von Buchhaltungsquoten und anderen Finanzvariablen war die lineare Diskriminierungsanalyse die erste statistische Methode, um systematisch darzulegen, welche Unternehmen Konkurs gegen Überlebten einlegen. Trotz Beschränkungen, einschließlich bekannter Nichtübereinstimmung der Buchführungsquoten an die normalen Verbreitungsannahmen von LDA, ist das Modell von Edward Altman von 1968 nach wie vor ein führendes Modell in praktischen Anwendungen. Gesichtserkennung In der computerisierten Gesichtserkennung ist jedes Gesicht durch eine große Anzahl von Pixelwerten vertreten. Lineare diskriminierende Analyse wird hauptsächlich hier verwendet, um die Anzahl der Merkmale auf eine effizientere Zahl vor der Einstufung zu reduzieren. Jede der neuen Dimensionen ist eine lineare Kombination von Pixelwerten, die ein Muster bilden. Die linearen Kombinationen, die bei der linearen Unterscheidung von Fischern erzielt wurden, werden als Fischer-Pflanzen bezeichnet, während diejenigen, die mit der zugehörigen Hauptkomponentenanalyse gesammelt wurden, als Eigenständige bezeichnet werden. Marketing In der Marketingbranche wurde die diskriminierende Analyse einmal häufig verwendet, um die Faktoren zu bestimmen, die verschiedene Arten von Kunden und/oder Produkten auf der Grundlage von Umfragen oder anderen Formen der gesammelten Daten unterscheiden. Logistische Regression oder andere Methoden werden nun häufiger verwendet. Die Verwendung von Diskriminierungen im Marketing lässt sich durch folgende Schritte beschreiben: Formelierung des Problems und Erfassung von Daten –Identifizierung der wichtigsten Eigenschaften, die Verbraucher verwenden, um Produkte in dieser Kategorie zu bewerten – Einsatz quantitativer Marketing-Forschungstechniken (wie Erhebungen) zur Erhebung von Daten aus einer Stichprobe potenzieller Kunden über ihre Ratings aller Produkteigenschaften. Die Datenerhebungsphase erfolgt in der Regel durch Marketing-Forschungsfachleute. Erhebungsfragen fragen den Antwortgegner, ein Produkt von einem auf fünf (oder 1 bis 7 oder 1 bis 10) auf eine Reihe von vom Forscher ausgewählten Eigenschaften zu berechnen. Künftig werden fünf bis zwanzig Ausweise ausgewählt. Sie könnten u. a. folgende Themen umfassen: Leichte Nutzung, Gewicht, Genauigkeit, Haltbarkeit, Farben, Preis oder Größe. Je nach untersuchtem Produkt variieren die gewählten Eigenschaften. In der Studie wird die gleiche Frage gestellt. Die Daten für mehrere Produkte werden kodifiziert und in ein statistisches Programm wie R, SPSS oder SAS eingebracht.( Dieser Schritt ist derselbe Schritt wie in der Faktoranalyse). Schätzung der diskriminierenden Funktion Coeffizienten und Bestimmung der statistischen Bedeutung und Gültigkeit Wählen Sie die geeignete diskriminierende Analysemethode. Die direkte Methode beinhaltet die Schätzung der diskriminierenden Funktion, damit alle Berechtigten gleichzeitig bewertet werden. Die schrittweise Methode tritt den Berechenten auf, die sich sequenziell befinden. Die Zwei-Gruppe-Methode sollte verwendet werden, wenn die abhängige Variablen zwei Kategorien oder Staaten aufweisen. Die mehrfache Diskriminierungsmethode wird verwendet, wenn die abhängige Variablen drei oder mehrere kategorische Staaten haben. Verwendung der Lambda von Wilks zur Prüfung der Bedeutung in SPSS oder F stat in SAS. Die häufigste Methode, die zur Prüfung der Gültigkeit verwendet wird, besteht darin, die Probe in eine Schätzung oder Analyseprobe zu spalten und eine Validierungs- oder Einstellungsprobe durchzuführen. Die Schätzungsprobe wird beim Aufbau der diskriminierenden Funktion verwendet. Die Validierungsprobe wird verwendet, um eine Klassifikationsmatrix zu erstellen, die die Anzahl der korrekt klassifizierten und falsch klassifizierten Fälle enthält. Der Prozentsatz der korrekt eingestuften Fälle wird als Treffer bezeichnet. Bilden Sie die Ergebnisse auf einer zweidimensionalen Karte, definieren die Abmessungen und interpretieren die Ergebnisse. Das statistische Programm (oder ein entsprechendes Modul) wird die Ergebnisse aufzeigen. Die Karte wird jedes Produkt (normalerweise in zweidimensionalem Raum) festlegen. Kennzeichnend für die Entfernung von Produkten zueinander sind die unterschiedlichen Produkte. Die Abmessungen müssen vom Forscher gekennzeichnet werden. Dies erfordert subjektives Urteil und ist oft sehr schwierig. Lesen Sie die jeweilige Kartierung. Biomedizinische Studien Hauptanwendung einer diskriminierenden Analyse in der Medizin ist die Beurteilung des Schwerezustands eines Patienten und die Vorhersage von Krankheitsergebnis. Zum Beispiel werden Patienten während der rückwirkenden Analyse in Gruppen unterteilt, die nach Schwere der Krankheit – mild, moderat und schwer. Ergebnisse klinischer und Laboranalysen werden untersucht, um Variablen zu erkennen, die statistisch unterschiedlich in untersuchten Gruppen sind. Verwendung dieser Variablen werden diskriminierende Funktionen aufgebaut, die dazu beitragen, Krankheiten in einem künftigen Patienten in milder, mäßiger oder schwerer Form objektiv einzustufen. In der Biologie werden ähnliche Grundsätze verwendet, um Gruppen unterschiedlicher biologischer Gegenstände einzuordnen und zu definieren, z.B. um Phage-Typen von Salmonella initidis zu definieren, die sich auf viertere Umwandlung in Infrarot-Spektrale stützen, die Tierquelle von Escherichia coli zu entdecken, die ihre Umweltfaktoren usw. untersuchen. Geowissenschaft Diese Methode kann verwendet werden, um die Änderungszonen zu trennen. Zum Beispiel, wenn verschiedene Daten aus verschiedenen Zonen zur Verfügung stehen, kann eine diskriminierende Analyse das Muster innerhalb der Daten finden und effektiv einstufen. Vergleich zur logistischen Regression Diskriminant-Funktionsanalyse ist sehr ähnlich wie logistische Regression, und beide können genutzt werden, um dieselben Forschungsfragen zu beantworten. Logistische Regression hat nicht so viele Annahmen und Beschränkungen als diskriminierende Analyse. Wenn die Annahmen der diskriminierenden Analyse eingehalten werden, ist es jedoch effizienter als logistische Regression. Im Gegensatz zur logistischen Regression können diskriminierende Analysen mit kleinen Stichprobengrößen verwendet werden. Es wurde gezeigt, dass die Unterscheidungsgrößen bei gleicher Größe und Homogenität des Varianz-/Kovarianzvermögens genauer analysiert werden. Trotz all dieser Vorteile hat die logistische Regression keine Rolle bei der gemeinsamen Wahl, da die Annahmen einer diskriminierenden Analyse selten erfüllt werden. Lineare Diskriminierung in hochdimensionierter geometrischer Aomalities in hoher Dimension führt zu dem bekannten Hemmnis der Dimension. Gleichwohl kann eine ordnungsgemäße Nutzung von Messphänomenen die Berechnung erleichtern. Es wurde ein wichtiger Fall dieser Beeinträchtigungsphänomene von Donoho und Tanner herausgestellt: Wenn eine Probe im Wesentlichen hochdimensional ist, kann jeder Punkt von der übrigen Probe durch lineare Ungleichheit mit hoher Wahrscheinlichkeit getrennt werden, auch für exponentielle Großproben. Diese linearen Ungleichheiten können in der Standardform (Fisher's) der linearen Diskriminierung für eine reiche Familie der Wahrscheinlichkeitsverteilung ausgewählt werden. Insbesondere werden solche Theorems für Log-concave-Vertriebe, einschließlich multidimensionaler normaler Verteilung (der Nachweis basiert auf den Konzentrationsunterschieden bei Log-concave-Maßnahmen) und für Produktmaßnahmen auf einem multidimensionalen Preis nachgewiesen (dies wird nachgewiesen, dass die Konzentrationsunterschiede von Talagrand für Produktwahrscheinlichräume verwendet werden). Datentrennung durch klassische lineare Diskriminierungen vereinfacht das Problem der Fehlerkorrektur für künstliche Intelligenzsysteme in hoher Dimension. Siehe auch Data Mining-Beschluss Baum Lernfaktoranalyse modular Fisher diskriminant Analyse Logit (für logistische Regression)Linear Regression Multiple Discriminant Analyse multidimensionaler skaling Muster Anerkennung Preference Regression die Statistische Klassifikationsreferenz. Duda, R. O; Hart, P. E; Stork, D. H. (2000). Musterklassifikation (2. Nick Interscience.ISBN:0-471-05669-0.MR 1802993. Hilbe, J. M. (2009). Logistische Regressionsmodelle. Chapman & Hall/KRK Presse.ISBN: 1-4200-75-5.Mika, S. et al.(1999) "Fisher Discriminant Analysis with Vectors". Neuralnetze für die Signalverarbeitung IX: Proceedings of the 1999 Chem Signal Society Workshop (Kat. No.98TH8468).IEEE Konferenz über Neuralnetze für Signalverarbeitung IX.pp.41–48.CiteSeerX 10.1.35.9904.doi:10.1109/NNSP.1999.788121.ISBN 7803-5673-3.S2CID 8473401. McFarland, H. Richard; Donald, St. P. Richards (2001). " genaue Misclassation Möglichkeiten für die Beatmungs-In-Normandidaten. I.The Equal-Means Case.Journal of Multivariate Analysis.77 (1): 21–53.doi:10.1006/jmva2000.1924. McFarland, H. Richard; Donald, St. P. Richards (2002). " genaue Misclassation Möglichkeiten für die Beatmungs-In-Normandidaten. II.The Heterogeneous Case.Journal of Multivariate Analysis.82 (2): 299-330.doi:10.1006/jmva.2001.2034 Haghighat, M; Abdel-Motta, M; Alhalabi, W. (2016)." Diskriminierende Correlationsanalyse: Real-Time-Label Level Fusion for Multimodal Biometric Anerkennung.IEEE Transaktions on Information Forensics and Security.11 (9): 1984-96.doi:10.1109/TIFS.2569061.S2CID 15624506. Externe Links Discriminant Correlation Analysis (DCA) des High-Artikels (siehe oben) ALGLIB enthält die Einführung von LDA in C#+ / CLD / LDA / LDA / LDA / LEA. Diskriminierende Analyse StatFrage:Linear Discriminant Analysis (LDA) erläuterte eindeutig auf YouTube-Kursen, Diskriminant-Funktionsanalyse von G. David Garson, NC State University Discriminant Analyse in Microsoft von Kardi Teknomo Kurs Notes, Diskriminant-Funktionsanalyse von David W. Stockburger, Missouri State University Discriminant-Analyse (DA) von John Poulsen und Microsoft, San Francisco University