Eine Time-of-Flight-Kamera (ToF-Kamera) ist ein Fernbildkamera-System mit Zeit-of-Flight-Techniken, um Abstand zwischen der Kamera und dem Thema für jeden Punkt des Bildes zu lösen, indem die Rundfahrtzeit eines künstlichen Lichtsignals, das von einem Laser oder einer LED bereitgestellt wird, gemessen wird. Laserbasierte Zeit-of-Flight-Kameras sind Teil einer breiteren Klasse von Scannerless LIDAR, in der die gesamte Szene mit jedem Laserpuls erfasst wird, im Gegensatz zu Punkt-by-Point mit einem Laserstrahl wie beim Scannen von LIDAR-Systemen. Um das Jahr 2000 begannen die Time-of-Flight-Kamera-Produkte für zivile Anwendungen, da die Halbleiter-Prozesse die Produktion von Bauteilen für solche Geräte schnell genug ermöglichten. Die Systeme decken Reichweiten von wenigen Zentimetern bis zu mehreren Kilometern ab. Die Entfernungsauflösung beträgt ca. 1 cm. Die räumliche Auflösung von Zeit-of-Flight-Kameras ist im Allgemeinen im Vergleich zu Standard 2D-Videokameras, mit den meisten kommerziell erhältlichen Geräten bei 320 × 240 Pixeln oder weniger ab 2011. Im Vergleich zu anderen 3D-Laser-Scanverfahren zur Erfassung von 3D-Bildern arbeiten TOF-Kameras schneller, indem bis zu 160 Operationen pro Sekunde bereitgestellt werden. Arten von Geräten Mehrere unterschiedliche Technologien für Flugkameras wurden entwickelt. HF-modulierte Lichtquellen mit Phasendetektoren Photonic Mixer Devices (PMD,) der Swiss Ranger und CanestaVision arbeiten durch Modulation des abgehenden Strahls mit einem HF-Träger, dann Messung der Phasenverschiebung dieses Trägers auf der Empfängerseite. Dieser Ansatz hat eine modulare Fehlerherausforderung: Messbereiche sind modulo die HF-Trägerwellenlänge. Der Swiss Ranger ist ein kompaktes, kurzes Gerät mit Reichweiten von 5 oder 10 Metern und einer Auflösung von 176 x 144 Pixeln. Mit phasenlosen Algorithmen kann der maximale Einzigartigkeitsbereich erhöht werden. Die PMD kann Reichweiten bis zu 60 m bereitstellen. Beleuchtung ist gepulste LEDs statt ein Laser. CanestaVision Entwickler Canesta wurde 2010 von Microsoft gekauft. Der Kinect2 für Xbox One basierte auf ToF-Technologie von Canesta. Range gated imagers Diese Geräte haben eine eingebaute Blende im Bildsensor, die mit der gleichen Geschwindigkeit wie die Lichtimpulse aussendet und schließt. Die meisten zeitnahen 3D-Sensoren basieren auf diesem von Medina erfundenen Prinzip. Da ein Teil eines jeden Rücklaufimpulses von der Blende nach ihrer Ankunft blockiert wird, bezieht sich die empfangene Lichtmenge auf die zurückgelegte Strecke. Der Abstand kann mit der Gleichung z = R (S2 - S1) / 2(S1 + S2) + R / 2 für eine ideale Kamera berechnet werden. R ist der Kamerabereich, bestimmt durch die Rundfahrt des Lichtpulses, S1 die Menge des empfangenen Lichtpulses und S2 die Menge des gesperrten Lichtpulses. Der ZCam von 3DV Systems ist ein bereichsweises System. Microsoft hat im Jahr 2009 3DV gekauft. Der Kinect-Sensor der zweiten Generation von Microsoft wurde mit Kenntnissen von Canesta und 3DV Systems entwickelt. Ähnliche Prinzipien werden in der vom Fraunhofer-Institut für mikroelektronische Schaltungen und Systeme und TriDiCam entwickelten Kameralinie ToF verwendet. Diese Kameras verwenden Photodetektoren mit einer schnellen elektronischen Blende. Die Tiefenauflösung von ToF-Kameras kann mit ultraschnellen gating verstärkten CCD-Kameras verbessert werden. Diese Kameras sorgen für Gating-Zeiten bis zu 200ps und ermöglichen das ToF-Setup mit Tiefenauflösung unter Millimetern. Range gated imagers können auch in 2D-Bildgebung verwendet werden, um alles außerhalb eines bestimmten Entfernungsbereichs zu unterdrücken, wie durch Nebel zu sehen. Ein gepulster Laser sorgt für Beleuchtung, und ein optisches Tor ermöglicht es, das Bildgerät nur während der gewünschten Zeitspanne zu erreichen. Direkte Time-of-Flight-Bilder Diese Geräte messen die für einen einzigen Laserpuls benötigte direkte Flugzeit, um die Kamera zu verlassen und auf das Brennebenen-Array zurückzureflektieren. Auch bekannt als "Trigger-Modus", die 3D-Bilder, die mit dieser Methodik aufgenommen werden, komplettieren räumliche und zeitliche Daten, die Aufnahme von vollen 3D-Szenen mit einzigen Laserpuls. Dies ermöglicht eine schnelle Erfassung und schnelle Echtzeitverarbeitung von Szeneinformationen. Für zeitsensitive autonome Operationen wurde dieser Ansatz für autonome Raumtests und -operationen, wie sie auf der OSIRIS-REx Bennu-Asteroidenprobe und der Rückfahrtsmission und der autonomen Hubschrauberlandung verwendet werden, demonstriert. Advanced Scientific Concepts, Inc. bietet Anwendungsspezifisch (z.B. Luft-, Automobil-, Raumfahrt) Direkte TOF-Visionssysteme als 3D Flash LIDAR Kameras bekannt. Ihr Ansatz nutzt InGaAs Avalanche Photo Diode (APD) oder PIN-Photodetektor-Arrays, die in der Lage sind, Laserpuls in den 980 nm bis 1600 nm Wellenlängen abzubilden. Komponenten Eine Time-of-Flight-Kamera besteht aus folgenden Komponenten: Beleuchtungseinheit: Es beleuchtet die Szene. Für HF-modulierte Lichtquellen mit Phasendetektor-Bildern muss das Licht mit hohen Geschwindigkeiten bis zu 100 MHz moduliert werden, nur LEDs oder Laserdioden sind denkbar. Für Direct TOF-Bilder wird ein einziger Impuls pro Frame (z.B. 30 Hz) verwendet. Die Beleuchtung verwendet normalerweise Infrarotlicht, um die Beleuchtung unauffällig zu machen. Optik: Eine Linse sammelt das reflektierte Licht und bildet die Umgebung auf den Bildsensor (Focal-Ebenen-Array). Ein optischer Bandpassfilter passiert nur das Licht mit der gleichen Wellenlänge wie die Beleuchtungseinheit. Dies hilft, nicht dauerhaftes Licht zu unterdrücken und Geräusche zu reduzieren. Bildsensor: Das ist das Herz der TOF-Kamera. Jedes Pixel misst die Zeit, in der das Licht von der Beleuchtungseinheit (Laser oder LED) zum Objekt und zurück zum Brennebenenfeld gelangt. Für das Timing werden verschiedene Ansätze verwendet; siehe Gerätearten oben. Treiberelektronik: Sowohl die Beleuchtungseinheit als auch der Bildsensor müssen durch Hochgeschwindigkeitssignale gesteuert und synchronisiert werden. Diese Signale müssen sehr genau sein, um eine hohe Auflösung zu erhalten. Wenn sich beispielsweise die Signale zwischen der Beleuchtungseinheit und dem Sensor um nur 10 Picosekunden verschieben, ändert sich der Abstand um 1,5 mm. Zum Vergleich: Strom CPUs erreichen Frequenzen von bis zu 3 GHz, entsprechend Taktzyklen von etwa 300 ps - die entsprechende Auflösung beträgt nur 45 mm. Computation/Interface: Der Abstand wird direkt in der Kamera berechnet. Um eine gute Leistung zu erhalten, werden auch einige Kalibrierdaten verwendet. Die Kamera bietet dann ein Fernbild über eine Schnittstelle, beispielsweise USB oder Ethernet. Grundsatz Die einfachste Version einer Time-of-Flight Kamera verwendet Lichtpulse oder einen einzigen Lichtpuls. Die Beleuchtung wird sehr kurze Zeit eingeschaltet, der resultierende Lichtimpuls leuchtet die Szene aus und wird von den Objekten im Blickfeld reflektiert. Das Kameraobjektiv sammelt das reflektierte Licht und bildet es auf das Sensor- oder Brennebenenfeld. Je nach Abstand erfährt das ankommende Licht eine Verzögerung. Da Licht eine Geschwindigkeit von ca. c = 300.000.000 Meter pro Sekunde hat, ist diese Verzögerung sehr kurz: ein Objekt 2,5 m entfernt wird das Licht um: t D = 2 ⋅ D c = 2 ⋅ 2,5 m 300 000 m s = 0,000 000 016 66 s = 16.66 n s \{displaystyle t_{D}=2\cdot \{frac D}{c}=2\cdot \{frac {2.5\;\mathrm {m} }300\;000\;{\frac \{mathrm {m} \}{mathrm {s} =}0.000\;000\;016\;66\;\mathrm {m} =16.66\;\hrm {ns} Bei einer Pulsbreite von z.B. 50 ns ist der Bereich auf D m a x = 1 2 ⋅ c ⋅ t 0 = 1 2 ⋅ 300 000 m s ⋅ 0,000 000 05 s = 7,5 m \{displaystyle D_{\mathrm {max} =\{frac 1}{2}\cdotc\cdot t_{0}={\frac 1}{2}\cdot 300\;000\;000\;{\frac \{mathrm}{mathrm} \}cdot 0,000\;000\;05\\\\\\mathrm {\\\\\\\\\\\}\cdot 300\;000\;\\\;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\c\c\c\c{\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c 7.5\;\mathrm {m} Diese kurzen Zeiten zeigen, dass die Beleuchtungseinheit ein kritischer Teil des Systems ist. Nur mit speziellen LEDs oder Lasern können solche kurzen Pulse erzeugt werden. Das einzelne Pixel besteht aus einem fotosensitiven Element (z.B. einer Fotodiode). Es wandelt das ankommende Licht in einen Strom um. An die Fotodiode angeschlossene analoge Zeitgeber sind schnelle Schalter, die den Strom auf eines von zwei (oder mehreren) Speicherelementen (z.B. Kondensator) lenken, die als Summenelemente wirken. Bei digitalen Timing-Bildern ist ein Zeitzähler, der bei mehreren Gigahertz ausgeführt werden kann, mit jedem Photodetektor-Pixel verbunden und stoppt das Zählen, wenn Licht erfasst wird. Im Diagramm eines amplitudenmodulierten Array-Analog-Timers verwendet das Pixel zwei Schalter (G1 und G2) und zwei Speicherelemente (S1 und S2). Die Schalter werden durch einen Impuls mit der gleichen Länge wie der Lichtimpuls gesteuert, wobei das Steuersignal des Schalters G2 durch genau die Pulsbreite verzögert wird. Je nach Verzögerung wird durch G1 in S1 nur ein Teil des Lichtimpulses abgetastet, der andere Teil wird in S2 gespeichert. Je nach Abstand ändert sich das Verhältnis zwischen S1 und S2 wie in der Zeichnung dargestellt. Da innerhalb von 50 ns nur geringe Lichtmengen auf den Sensor auftreffen, werden nicht nur ein, sondern mehrere tausend Impulse ausgesandt (Repetitionsrate t R) und gesammelt, wodurch das Signal-Rausch-Verhältnis erhöht wird. Nach der Belichtung wird das Pixel ausgelesen und die folgenden Stufen messen die Signale S1 und S2. Da die Länge des Lichtimpulses definiert ist, kann der Abstand mit der Formel berechnet werden: D = 1 2 ⋅ c ⋅ t 0 ⋅ S 2 S 1 + S 2 \{displaystyle D={\frac 1}{2}\cdot c\cdot t_{0\cdot \{frac S2{S1+S2 Im Beispiel weisen die Signale folgende Werte auf: S1 = 0,66 und S2 = 0,33 Der Abstand ist also: D = 7.5 m ⋅ 0.33 0.33 + 0.66 = 2.5 m \{displaystyle D=7.5\;\mathrm {m} \cdot \{frac 0.33}{0.33+0.66}=2.5\;\mathrm {m} } Bei Anwesenheit von Hintergrundlicht erhalten die Speicherelemente einen zusätzlichen Teil des Signals. Dies würde die Abstandsmessung stören. Um den Hintergrundteil des Signals zu eliminieren, kann die gesamte Messung ein zweites Mal mit ausgeschalteter Beleuchtung durchgeführt werden. Sind die Objekte weiter weg als der Entfernungsbereich, so ist auch das Ergebnis falsch. Dabei hilft eine zweite Messung mit den durch eine zusätzliche Pulsbreite verzögerten Steuersignalen, solche Objekte zu unterdrücken. Andere Systeme arbeiten mit einer sinusförmig modulierten Lichtquelle anstelle der Impulsquelle. Für direkte TOF-Bilder, wie 3D Flash LIDAR, wird vom Laser ein einziger kurzer Puls von 5 bis 10 ns abgegeben. Das T-Null-Ereignis (die Zeit, in der der Puls die Kamera verlässt) wird durch die Erfassung des Pulses direkt und das Routing dieses Timings auf das Brennebenen-Array festgelegt. Mit T-Null wird die Rücklaufzeit des rücklaufenden reflektierten Impulses auf den verschiedenen Pixeln des Brennebenen-Arrays verglichen. Durch den Vergleich von T-Null und dem erfassten zurückgegebenen Puls und den Vergleich der Zeitdifferenz gibt jedes Pixel genau eine direkte Zeitmessung aus. Die Rundfahrt eines Einzelpulses für 100 Meter beträgt 660 ns. Mit einem 10 ns Puls wird die Szene beleuchtet und der Bereich und die Intensität in weniger als 1 Mikrosekunde erfasst. Vorteile Einfachheit Im Gegensatz zu Stereo- oder Triangulationssystemen ist das gesamte System sehr kompakt: Die Beleuchtung liegt direkt neben der Linse, während die anderen Systeme eine bestimmte Mindestbasislinie benötigen. Im Gegensatz zu Laser-Scansystemen werden keine mechanischen Bewegungsteile benötigt. Effizienter Entfernungsalgorithmus Es ist ein direkter Prozess, die Entfernungsinformation aus den Ausgangssignalen des TOF-Sensors zu extrahieren. Dadurch verwendet diese Aufgabe nur eine geringe Menge an Verarbeitungsleistung, wiederum im Gegensatz zur Stereosicht, wo komplexe Korrelationsalgorithmen implementiert sind. Nach der Entnahme der Abstandsdaten ist die Objekterkennung beispielsweise auch ein unkomplizierter Prozess, um durchzuführen, da die Algorithmen nicht durch Muster am Objekt gestört werden. Speed Time-of-Flight Kameras können die Entfernungen innerhalb einer kompletten Szene mit einem einzigen Schuss messen. Da die Kameras bis zu 160 Frames pro Sekunde erreichen, eignen sie sich ideal für den Einsatz in Echtzeitanwendungen. Nachteile Hintergrundlicht Bei Verwendung von CMOS oder anderen integrierenden Detektoren oder Sensoren, die sichtbares oder nahe Infrarotlicht (400 nm - 700 nm) verwenden, obwohl die meisten Hintergrundlicht aus künstlicher Beleuchtung oder Sonne unterdrückt wird, muss das Pixel noch einen hohen Dynamikbereich bieten. Das Hintergrundlicht erzeugt auch Elektronen, die gespeichert werden müssen. Beispielsweise können die Beleuchtungseinheiten in vielen der heutigen TOF-Kameras eine Beleuchtungsstärke von etwa 1 Watt bereitstellen. Die Sonne hat eine Beleuchtungsleistung von etwa 1050 Watt pro Quadratmeter und 50 Watt nach dem optischen Bandpassfilter. Wenn also die beleuchtete Szene eine Größe von 1 Quadratmeter hat, ist das Licht der Sonne 50 mal stärker als das modulierte Signal. Für nicht integrierende TOF-Sensoren, die Licht nicht über die Zeit integrieren und Nah-Infrarot-Detektoren (InGaAs) verwenden, um den kurzen Laserpuls zu erfassen, ist die direkte Betrachtung der Sonne ein Nicht-Problem, weil das Bild nicht über die Zeit integriert ist, sondern innerhalb eines kurzen Erfassungszyklus erfasst, typischerweise weniger als 1 Mikrosekunde. Derartige TOF-Sensoren werden in Raumfahrtanwendungen eingesetzt und werden für Automobilanwendungen berücksichtigt. Interferenz In bestimmten Arten von TOF-Geräten (aber nicht alle), wenn mehrere Zeit-of-Flight-Kameras gleichzeitig laufen, können die TOF-Kameras einander die Messungen stören. Es gibt verschiedene Möglichkeiten, sich mit diesem Problem zu befassen: Zeitmultiplex: Ein Steuerungssystem startet die Messung der einzelnen Kameras nacheinander, so dass nur eine Beleuchtungseinheit zu einem Zeitpunkt aktiv ist. Verschiedene Modulationsfrequenzen: Wenn die Kameras ihr Licht mit unterschiedlichen Modulationsfrequenzen modulieren, wird ihr Licht nur als Hintergrundbeleuchtung in den anderen Systemen gesammelt, stört aber die Entfernungsmessung nicht. Für Direkt TOF-Typ-Kameras, die einen einzigen Laserpuls zur Beleuchtung verwenden, weil der einzelne Laserpuls kurz ist (z.B. 10 Nanosekunden), ist die Rundfahrt TOF zu und von den Objekten im Bereich der Sicht entsprechend kurz (z.B. 100 Meter = 660 ns TOF Rundfahrt). Für eine Bilderfassung bei 30 Hz ist die Wahrscheinlichkeit einer störenden Interaktion die Zeit, in der das Kameraaufnahmegate geöffnet ist, geteilt durch die Zeit zwischen Laserimpulsen oder etwa 1 in 50.000 (0,66 μs geteilt durch 33 ms). Mehrere Reflexionen Im Gegensatz zu Laser-Scansystemen, bei denen ein einzelner Punkt beleuchtet wird, beleuchten die Zeit-Flight-Kameras eine ganze Szene. Bei einer Phasendifferenzeinrichtung (amplitudemoduliertes Array) durch Mehrfachreflexionen kann das Licht auf mehreren Wegen die Objekte erreichen. Daher kann der gemessene Abstand größer als der wahre Abstand sein. Direkte TOF-Bilder sind anfällig, wenn das Licht von einer spekulären Oberfläche reflektiert wird. Es gibt publizierte Papiere, die die Stärken und Schwächen der verschiedenen TOF-Geräte und -Ansätze beschreiben. Anwendungen Automotive Anwendungen Zeit-of-Flight-Kameras werden in Assistenz- und Sicherheitsfunktionen für fortgeschrittene Automobilanwendungen wie aktive Fußgängersicherheit, Precrash-Erkennung und Innenanwendungen wie Out-of-Position (OOP)-Erkennung eingesetzt. Mensch-Maschine-Schnittstellen und Gaming Da Zeit-of-Flight-Kameras Entfernungsbilder in Echtzeit bereitstellen, ist es einfach, Bewegungen des Menschen zu verfolgen. Dies ermöglicht neue Interaktionen mit Verbrauchergeräten wie Fernsehern. Ein weiteres Thema ist, diese Art von Kameras zu verwenden, um mit Spielen auf Videospielkonsolen zu interagieren. Der ursprünglich mit der Xbox One-Konsole ausgestattete Kinect-Sensor der zweiten Generation nutzte eine Time-of-Flight-Kamera für seine Reichweiten-Imaging, die natürliche Benutzeroberflächen und Gaming-Anwendungen mithilfe von Computer-Visions- und Gestenerkennungstechniken ermöglichte. Creative und Intel bieten auch eine ähnliche Art von interaktiven Gesten-Zeit-of-Flight-Kamera für Gaming, die Senz3D basierend auf der DepthSense 325 Kamera von Softkinetic. Infineon und PMD Technologien ermöglichen winzige integrierte 3D-Tiefenkameras zur Nahbereichs-Gestensteuerung von Verbrauchergeräten wie All-in-One PCs und Laptops (Picco flexx und Picco monstar Kameras). Smartphone-Kameras Ab 2019 umfassen mehrere Smartphones Time-of-Flight Kameras. Diese werden hauptsächlich verwendet, um die Qualität der Fotos durch die Bereitstellung der Kamera-Software mit Informationen über Vordergrund und Hintergrund zu verbessern. Das erste Mobiltelefon, das diese Technologie nutzt, ist das LG G3, das Anfang 2014 veröffentlicht wurde. Mess- und Bildverarbeitung Weitere Anwendungen sind Messaufgaben, z.B. für die Füllhöhe in Silos. In der industriellen Bildverarbeitung hilft die Time-of-Flight-Kamera, Objekte für den Einsatz von Robotern zu klassifizieren und zu lokalisieren, z.B. Gegenstände, die auf einem Förderer passieren. Türsteuerungen können leicht zwischen Tieren und Menschen unterscheiden, die die Tür erreichen. Roboter Eine weitere Verwendung dieser Kameras ist das Gebiet der Robotik: Mobile Roboter können eine Karte ihrer Umgebung sehr schnell aufbauen, so dass sie Hindernisse vermeiden oder einer führenden Person folgen. Da die Abstandsberechnung einfach ist, wird nur wenig Rechenleistung verwendet. Erdtopographie ToF-Kameras wurden verwendet, um digitale Höhenmodelle der Erdoberflächentopographie zu erhalten, für Studien in der Geomorphologie. Marken Aktive Marken (Stand 2011) ESPROS - 3D TOF Imager Chips, TOF Kamera und Modul für Automobil-, Robotik-, Industrie- und IoT-Anwendungen 3D Flash LIDAR Kameras und Vision Systems von Advanced Scientific Concepts, Inc. für Luft-, Automobil- und Raumfahrtanwendungen DepthSense - TOF-Kameras und -Module, einschließlich RGB-Sensor und Mikrofone von SoftKinetic IRMA MATRIX - TOF-Kamera, zur automatischen Personenzählung auf mobilen und stationären Anwendungen von iris-GmbH Kinect - Freisprechschnittstellenplattform von Microsoft für Videospielkonsolen und PCs, pmd - Kamera Referenzdesigns und Software (pmd[vision], einschließlich TOF-Module [CamBoard]) und TOF-Bilder (PhotonICs) von PMD Technologies real. IZ 2+3D - Hochauflösende SXGA (1280 x 1024) TOF-Kamera, die von der Start-Unternehmens-Odos-Imaging entwickelt wurde, integriert konventionelle Bildaufnahme mit TOF-Bereich im gleichen Sensor. Basierend auf der von Siemens entwickelten Technologie. Senz3D - TOF Kamera von Creative und Intel basierend auf DepthSense 325 Kamera von Softkinetic, verwendet für Spiele. SICK - 3D industrielle TOF-Kameras (Visionary-T) für industrielle Anwendungen und Software 3D MLI Sensor - TOF-Bilder, Module, Kameras und Software von IEE (International Electronics & Engineering), basierend auf modulierter Lichtintensität (MLI) TOFCam Stanley - TOF-Kamera von Stanley Electric TriDiCam - TOF-Module und Software, der TOF-Bilder ursprünglich von Fraunhofer Institut für Mikroelektronik entwickelt Defunct Marken CanestaVision - TOF Module und Software von Canesta (Firma erworben von Microsoft im Jahr 2010) D-IMager - TOF-Kamera von Panasonic Electric Works OptriCam - TOF-Kameras und Module von Optrima (rebranded DepthSense vor SoftKinetic Fusion 2011) ZCam - TOF Kameraprodukte von 3DV Systems, Integration von Vollfarben-Video mit Tiefeninformationen (Ass verkauft an Microsoft im Jahr 2009) SwissRanger - eine industrielle TOF-nur Kameralinie ursprünglich von der Centre Suisse d'Electronique et Microtechnique, S.A (CSEM,) entwickelt von Mesa Imaging (Mesa Imaging erworben von Heptagon im Jahr 2014) Fotonic - TOF Kameras und Software powered by Panasonic CMOS Chip (Fotonic erworben von Autoliv 2018) S.Cube - ToF Kamera und Module von Cube Eye Siehe auch Laser Dynamic Range Imager Structured-light 3D-Scanner Kinect Referenzen Weiter lesen Hansard, Miles; Lee, Seungkyu; Choi, Ouk; Horaud, Radu (2012). "Time-of-flight Kameras: Prinzipien, Methoden und Anwendungen" (PDF). SpringerBriefs in Informatik (PDF). doi:10.1007/978-1-4471-4658-2 ISBN 978-1-4471-4657-5. S2CID 5494636. Dieses Buch beschreibt eine Vielzahl neuer Forschungen zur Zeit-of-Flight-Bildgebung: [...] das zugrunde liegende Messprinzip [...] die zugehörigen Fehler- und Mehrdeutigkeitsquellen [...] die geometrische Kalibrierung von Zeit-of-Flight-Kameras, insbesondere bei Verwendung in Kombination mit normalen Farbkameras [...und] verwenden Zeit-of-Flight-Daten in Verbindung mit herkömmlichen Stereo-Anpassungstechniken. Die fünf Kapitel beschreiben zusammen eine komplette Tiefe und Farbe 3D Rekonstruktion Pipeline.