BIRCH (ausgeglichene iterative Reduktion und Clustering mit Hierarchien) ist ein nicht überwachter Data Mining-Algorithmus, der zur Durchführung hierarchischer Clustering über besonders große Datensätze verwendet wird. Mit Modifikationen kann man auch k-Means Clustering und Gaussian Mix Modeling mit dem Erwartungs-Maximalisierungsalgorithmus beschleunigen. Ein Vorteil von BIRCH ist seine Fähigkeit, inkremental und dynamisch Cluster eingehende, multidimensionale metrische Datenpunkte in einem Versuch zu produzieren, die beste Qualität Clustering für eine bestimmte Menge von Ressourcen (Speicher- und Zeitzwänge). In den meisten Fällen benötigt BIRCH nur einen einzigen Scan der Datenbank. Seine Erfinder behaupten, BIRCH sei der "erste Clustering-Algorithmus, der im Datenbankbereich vorgeschlagen wird, um Rauschen (Datenpunkte, die nicht Teil des zugrunde liegenden Musters sind) effektiv zu handhaben", schlagen DBSCAN um zwei Monate. Der BIRCH-Algorithmus erhielt 2006 den SIGMOD 10-jährigen Test der Zeitvergabe. Problem mit früheren Methoden Vorherige Clustering Algorithmen durchgeführt weniger effektiv über sehr große Datenbanken und nicht ausreichend betrachtet den Fall, dass ein Datensatz zu groß war, um in den Hauptspeicher passen. Infolgedessen gab es eine Menge Overhead, die hohe Clustering-Qualität beibehielt und gleichzeitig die Kosten für zusätzliche IO (Eingang/Ausgang) Operationen minimierte. Die meisten Vorgänger von BIRCH inspizieren außerdem alle Datenpunkte (oder alle derzeit vorhandenen Cluster) für jede "Clustering-Entscheidung" und führen keine heuristische Gewichtung aufgrund der Entfernung zwischen diesen Datenpunkten durch. Vorteile mit BIRCH Es ist lokal, dass jede Clusterentscheidung getroffen wird, ohne alle Datenpunkte und aktuell vorhandene Cluster abzutasten. Sie nutzt die Beobachtung aus, dass der Datenraum in der Regel nicht gleichmäßig besetzt ist und nicht jeder Datenpunkt gleichermaßen wichtig ist. Es macht die volle Nutzung des verfügbaren Speichers, um die besten möglichen Subcluster abzuleiten, während die I/O-Kosten minimiert werden. Es ist auch eine inkrementelle Methode, die nicht den gesamten Datensatz im Voraus erfordert. Algorithmen Der BIRCH-Algorithmus nimmt als Eingabe eine Menge von N-Datenpunkten, dargestellt als echtwertige Vektoren, und eine gewünschte Anzahl von Clustern K. Sie arbeitet in vier Phasen, deren zweite optional ist. Die erste Phase baut ein Clustering-Feature (C F {\displaystyle CF} ) Baum aus den Datenpunkten, eine höhensymmetrische Baumdatenstruktur, wie folgt definiert: Bei einem Satz von N d-dimensionalen Datenpunkten wird das Clustering-Feature C F {\displaystyle CF} des Sets als das Tripel C F = (N, L S →, S S ) definiert. {\displaystyle CF=(N,{\overrightarrow {LS},SS) , wobei L S → = Σ i = 1 N X i → {\displaystyle {\overrightarrow {LS}=\sum * i=1{N}{\overrightarrow X_{i} die lineare Summe und S S = Σ i = 1 N (X i → ) 2 {\displaystyle SS=\sum _i=1}^{N}({\overrightarrow X_{i}}})^{2 ist die quadratische Summe der Datenpunkte. Die Clustering-Funktionen werden in einem CF-Baum organisiert, einem höhenausgeglichenen Baum mit zwei Parametern: Verzweigungsfaktor B {\displaystyle B} und Schwelle T {\displaystyle T} .Jeder nichtblatt-Knoten enthält höchstens B {\displaystyle B}-Einträge des Formulars [C F\, c h i l d i ] Ein Blattknoten enthält höchstens L {\displaystyle L} Einträge jedes Formulars [C F i] {\displaystyle [CF_{i}] . Es hat auch zwei Zeiger vor und als nächstes, die verwendet werden, um alle Blattknoten zusammen zu ketten. Die Baumgröße ist abhängig vom Parameter T {\displaystyle T} .Ein Knoten ist erforderlich, um in eine Seite der Größe P {\displaystyle P} zu passen. B {\displaystyle B} und L {\displaystyle L} werden durch P {\displaystyle P} bestimmt.So P {\displaystyle P} kann für Performance Tuning variiert werden. Es ist eine sehr kompakte Darstellung des Datensatzes, da jeder Eintrag in einem Blattknoten kein einziger Datenpunkt, sondern ein Subcluster ist. Im zweiten Schritt scannt der Algorithmus alle Blatteinträge im anfänglichen C F {\displaystyle CF} Baum, um einen kleineren C F {\displaystyle CF} Baum wieder aufzubauen, während Ausreißer und Gruppierung überfüllter Subcluster in größere zu entfernen. Dieser Schritt ist optional in der ursprünglichen Präsentation von BIRCH markiert. In Schritt drei wird ein bestehender Clustering-Algorithmus verwendet, um alle Blatteinträge zu bündeln. Hier wird ein agglomerierender hierarchischer Clustering-Algorithmus direkt auf die Subcluster angewendet, die durch ihre C F {\displaystyle CF}-Vektoren repräsentiert werden. Es bietet auch die Flexibilität, dem Benutzer die gewünschte Anzahl von Clustern oder die gewünschte Durchmesserschwelle für Cluster anzugeben. Nach diesem Schritt wird eine Gruppe von Clustern erhalten, die ein großes Verteilungsmuster in den Daten erfasst. Es gibt jedoch kleinere und lokalisierte Ungenauigkeiten, die durch einen optionalen Schritt bearbeitet werden können. 4. In Schritt 4 werden die Schwerpunkte der in Schritt 3 erzeugten Cluster als Samen verwendet und die Datenpunkte auf die nächstliegenden Samen umverteilen, um einen neuen Clustersatz zu erhalten. Schritt 4 bietet uns auch eine Möglichkeit, Ausreißer zu verwerfen. Das ist ein Punkt, der zu weit von seinem nächsten Samen ist, kann als Ausreißer behandelt werden. Berechnungen mit den Cluster-Funktionen Bei nur dem Clustering-Feature C F = [N, L S →, S S ] {\displaystyle CF=[N,{\overrightarrow {LS},SS] können die gleichen Maßnahmen ohne Kenntnis der zugrunde liegenden Istwerte berechnet werden. Centroid: C → = Σ i = 1 N X i → N = L S → N {\displaystyle {\overrightarrow C}={\fra\frac\sum * i=1{N}{\overrightarrow * - Ja. LS Radius: R = Σ i = 1 N ( X i → - C → ) 2 N = N ⋅ C → 2 + S S S - 2 ⋅ C → ⋅ L S → N = S S N - ( L S → N ) 2 {\displaystyle R={\\sqrt {\frac} (\overrightarrow) {\cH00FF} {\cH00FF} {\cH00FF}} {\cH00FF}} {\cH00FF}} {\cH00FF}}}} {\cH00FF}}} {\c{\c{\cdot}}{\c{\c{\c}}}} {\c}}}}}}}} {\c}}}}}}}}}}}}}}}}{\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\cdot{\c{\c{\c{\c{\c{\c\c\c\c\c{\c\c\c\c{\c{\c{\c{\c{\c{\c{\c{\c\c\c\c{\c\c\c{\c\c\c\c{\c{\c{\c{\ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ {C}\cdot;\overrightarrow LS}{N}}}={\sqrt {\frac SS}{N}{\frac {\frac {\frac {\overrightarrow LS}}{N}})^{2 Durchschnittliche Verknüpfungsabstand zwischen den Clustern C F 1 = [N 1 , L S 1 → , S S S 1 ]\displaystyle CF_{1}=[N_{1},{\overrightarrow LS_{1},SS_{1] und C F 2 = [N 2 , L S 2 → , S S S 2 ] LS_{2}},SS_{2] : D 2 = Σ i = 1 N 1 Σ j = 1 N 2 (X i → - Y j → ) 2 N 1 ∙ N 2 = N 1 ∙ S 2 + N 2 ∙ S 1 - 2 ∙ L S 1 → D_{2}={\sqrt {\fra\frac} I=1^N_{1} ***________________________________________________________________ - Nein. (J) **** ************************************************************************************* ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ - Nein. In mehrdimensionalen Fällen sollte die Quadratwurzel durch eine geeignete Norm ersetzt werden. Numerische Probleme in BIRCH Clustering Features Leider gibt es numerische Probleme, die mit der Verwendung des Begriffs S S {\displaystyle SS} in BIRCH verbunden sind. Bei Subtraktion von S N - ( L S → N) 2 {\displaystyle} S.N. LS}{N}{\\big {^)2 oder ähnlich in den anderen Abständen wie D 2 {\displaystyle D_{2} kann eine katastrophale Löschung auftreten und eine schlechte Genauigkeit ergeben, die in manchen Fällen sogar dazu führen kann, dass das Ergebnis negativ ist (und die Quadratwurzel dann undefiniert wird). Dies kann durch Verwendung von BETULA-Clustermerkmalen C F = (N, μ, S ) {\displaystyle CF=(N,\mu ,S}) behoben werden, die den Zählerstand N {\displaystyle N} speichern, mittlere μ {\displaystyle \mu } und Summe von quadratischen Abweichungen statt basierend auf numerisch zuverlässigeren Online-Algorithmen zur Berechnung der Varianz. Für diese Merkmale hält ein ähnliches Additivitätstheorem. Bei der Speicherung eines Vektors bzw. einer Matrix für die quadratischen Abweichungen kann der resultierende BIRCH CF-Tree auch dazu verwendet werden, Gaussian Mixture Modeling mit dem Erwartungs-Maximalisierungsalgorithmus zu beschleunigen, neben k-Means Clustering und hierarchischer agglomerischer Clustering. = Anmerkungen ==