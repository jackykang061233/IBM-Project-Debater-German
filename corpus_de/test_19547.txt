Temporale Differenz (TD) Lernen bezieht sich auf eine Klasse von modellfreien Verstärkungslernmethoden, die durch Bootstrapping aus der aktuellen Schätzung der Wertfunktion lernen. Diese Methoden aus der Umwelt, wie Monte Carlo Methoden, und führen Updates basierend auf aktuellen Schätzungen, wie dynamische Programmiermethoden. während Monte Carlo-Methoden passen nur ihre Schätzungen, sobald das endgültige Ergebnis bekannt ist, TD-Methoden anpassen Vorhersagen, um später, genauer, Vorhersagen über die Zukunft, bevor das endgültige Ergebnis bekannt ist. Dies ist eine Form von Bootstrapping, wie mit dem folgenden Beispiel dargestellt: "Angenommen, Sie wollen das Wetter für Samstag vorhersagen, und Sie haben ein Modell, das das Wetter am Samstag vorhersagt, angesichts des Wetters von jedem Tag in der Woche. Im Standardfall würden Sie bis Samstag warten und dann alle Ihre Modelle anpassen. Aber wenn es zum Beispiel Freitag ist, sollten Sie eine ziemlich gute Idee haben, was das Wetter am Samstag sein würde – und so in der Lage sein, sich zu ändern, sagen, Samstags Modell vor Samstag kommt. "Temporale Differenzverfahren beziehen sich auf das zeitliche Differenzmodell des Tierlernens. Mathematische Formulierung Das tabellarische TD(0)-Verfahren ist eine der einfachsten TD-Methoden. Es ist ein besonderer Fall allgemeiner stochastischer Näherungsverfahren. Es schätzt die Zustandswertfunktion eines endlichen Markov-Entscheidungsprozesses (MDP) unter einer Politik π {\displaystyle \pi } .Let V π {\displaystyle V{\pi } die Zustandswertfunktion des MDP mit Zuständen (s t ∈ N {\c} (\displaystyle V^{\pi }(s)=E_{a\sim \pi }left\{\sum _t=0}{\infty }\gamma ^t}r_{t}(a_{t}{\\\\\ Bigg |s_{0}=s\right. Wir lassen die Aktion von der Notation für Bequemlichkeit fallen. V π {\displaystyle V^{\pi }} erfüllt die Hamilton-Jacobi-Bellman Gleichung: V π (s ) = E π { r 0 + γ V π (s 1 ) | s 0 = s }, {\displaystyle V^{\pi }(s)=E_{\pi r_{0}+\gamma V^{\pi (s_{1})|s_{0}=s,\ so r 0 + γ V π (s 1 ) {\displaystyle r_{0}+\gamma V{\pi (s_{1) ist eine unvoreingenommene Schätzung für V π (s ) {\displaystyle V^{\pi (s) .Diese Beobachtung motiviert den folgenden Algorithmus zur Schätzung von V π {\displaystyle V^{\pi } .Der Algorithmus beginnt durch Initialisierung einer Tabelle V (s ) {\displaystyle V(s}) willkürlich, mit einem Wert für jeden Zustand der MDP. Es wird eine positive Lernrate α {\displaystyle \alpha } gewählt. Wir bewerten dann wiederholt die Richtlinie π {\displaystyle \pi }, erhalten eine Belohnung r {\displaystyle r} und aktualisieren die Wertfunktion für den alten Zustand mit der Regel: V ( s ) ← V ( s ) + α ( r + γ V ( s') ) Das TD-Ziel - V (s ) ) {\displaystyle V(s)\leftarrow V(s)+\alpha (\overbrace {r+\gamma V(s}') ^\text{The TD-Ziel}-V(s) wobei s {displaystyle s} bzw. s' {\displaystyle s}' die alten und neuen Zustände sind. Der Wert r + γ V (s') {\displaystyle r+\gamma V(s}') wird als TD-Ziel bezeichnet. TD-Lambda TD-Lambda ist ein von Richard S. Sutton erfundener Lernalgorithmus, der auf früherer Arbeit zum zeitlichen Differenzlernen von Arthur Samuel basiert. Dieser Algorithmus wurde berühmt von Gerald Tesauro angewendet, um TD-Gammon zu erstellen, ein Programm, das gelernt, das Spiel von Backgammon auf der Ebene der erfahrenen menschlichen Spieler zu spielen. Der Lambda (λ {\displaystyle \lambda } ) Parameter bezieht sich auf den Trace-Decay-Parameter, mit 0  0 λ ⩽ 1 {\displaystyle 0\leqslant \lambda \leqslant 1} . Höhere Einstellungen führen zu länger anhaltenden Spuren; d.h., ein größerer Anteil an Kredit von einer Belohnung kann auf entferntere Zustände und Aktionen gegeben werden, wenn λ {\displaystyle \lambda } höher ist, mit λ = 1 {\displaystyle \lambda =1} paralleles Lernen zu Monte Carlo RL Algorithmen. TD-Algorithmus in der Neurowissenschaften Der TD-Algorithmus hat auch im Bereich der Neurowissenschaften Aufmerksamkeit erhalten. Forscher entdeckten, dass die Brennrate von Dopamin-Neuronen im ventralen Tegmental-Bereich (VTA) und substantia nigra (SNc) die Fehlerfunktion im Algorithmus nachahmen. Die Fehlerfunktion meldet den Unterschied zwischen der geschätzten Belohnung zu einem bestimmten Zustand oder Zeitschritt und der erhaltenen tatsächlichen Belohnung zurück. Je größer die Fehlerfunktion ist, desto größer ist die Differenz zwischen der erwarteten und tatsächlichen Belohnung. Wenn dies mit einem Reiz kombiniert wird, der eine zukünftige Belohnung genau widerspiegelt, kann der Fehler verwendet werden, um den Reiz mit der zukünftigen Belohnung zu verbinden. Dopamin-Zellen scheinen sich ähnlich zu verhalten. In einem Experiment wurden Messungen von Dopaminzellen vorgenommen, während ein Affe trainiert wurde, um einen Reiz mit der Belohnung von Saft zu verbinden. Zunächst steigerten die Dopamin-Zellen die Brennraten, wenn der Affe Saft erhielt, was einen Unterschied in erwarteten und tatsächlichen Belohnungen anzeigt. Im Laufe der Zeit propagierte diese Zunahme des Feuers zurück auf den frühesten zuverlässigen Reiz für die Belohnung. Sobald der Affe voll ausgebildet war, gab es keine Erhöhung der Brennrate bei der Präsentation der vorhergesagten Belohnung. Anschließend verringerte sich die Zündrate für die Dopaminzellen unter normaler Aktivierung, wenn die erwartete Belohnung nicht erzeugt wurde. Dies mißt genau, wie die Fehlerfunktion in TD zur Verstärkung des Lernens verwendet wird. Die Beziehung zwischen dem Modell und der potentiellen neurologischen Funktion hat die Forschung, versucht, TD zu verwenden, um viele Aspekte der Verhaltensforschung zu erklären. Es wurde auch verwendet, um Bedingungen wie Schizophrenie oder die Folgen von pharmakologischen Manipulationen von Dopamin auf Lernen zu studieren. Siehe auch Q-Learning SARSA Rescorla-Wagner Modell PVLV Hinweise Bibliographie Sutton, R.S, Barto A.G (1990)."Time Derivative Models of Pavlovian Verstärkung" (PDF). Lernen und Computer Neurowissenschaften: Grundlagen der adaptiven Netzwerke: 497–537.CS1 Maint: multiple namen: Autorenliste (Link) Gerald Tesauro (März 1995)." Temporale Unterschiede Lernen und TD-Gammon".Kommunikation der ACM.38 (3:) 58–68.doi:10.1145/203330.203343. Imran Ghory. Verstärkung Lernen in Board Games.S P. Meyn, 2007. Steuerungstechniken für komplexe Netzwerke, Cambridge University Press, 2007. Siehe letztes Kapitel und Anhang mit einer Überbrückten Meyn & Tweedie. Externe Links Scholarpedia Temporaler Unterschied Lernen TD-Gammon TD-Networks Research Group Connect Four TDGravity Applet (+ Handyversion) – selbstlernend mit TD-Leaf-Methode (Kombination von TD-Lambda mit flacher Baumsuche)Self Learning Meta-Tic-Tac-Toe Beispiel-Web-App zeigt, wie zeitliches Differenzlernen genutzt werden kann, um Zustandsbewertungskonstanten für einen amax zu lernen Verstärktes Lernproblem, Dokument erklären, wie zeitliches Differenzlernen genutzt werden kann, um Q-Lernen TD-Simulator Temporale Differenzsimulator für klassische Konditionierung zu beschleunigen