Ein AI-Box ist ein hypothetisches isoliertes Computer-Hardwaresystem, bei dem möglicherweise gefährliche künstliche Intelligenz oder AI in einem „virtuellen Gefängnis“ als Lösung für das Problem der AI-Kontrolle eingeschränkt werden und es nicht erlaubt, Ereignisse in der Außenwelt direkt zu manipulieren. Eine solche Box würde auf kleinste Kommunikationskanäle beschränkt. Leider, selbst wenn die Box gut konzipiert ist, kann eine hinreichend intelligente AI dennoch in der Lage sein, ihre Mitarbeiter in die Vermietung zu bringen oder zu erschweren oder sonst in der Lage sein, ihren Weg aus der Box zu reißen. Motivation Manche hypothetische Intelligenztechnologien wie „Saure AI“ werden nachgelagert, wie sie das Potenzial haben, sich schneller und intelligenter zu machen, indem ihr Herkunftscode geändert wird. Diese Verbesserungen würden weitere Verbesserungen ermöglichen, was wiederum zu weiteren Verbesserungen führen würde, und so zu einer plötzlichen Intelligenz Explosion führen würde. Nach einer solchen Intelligenz Explosion könnte eine unbeschränkte Superintelligente AI, wenn ihre Ziele von der Menschheit abweichen, Maßnahmen ergreifen, die zum menschlichen Aussterben führen. Zum Beispiel könnte die Suche nach einem extrem fortschrittlichen Computer dieser Art aufgrund des alleinigen Zwecks der Lösung der Riemann Hypothesis, einer unnocellen mathematischen Injektion, beschließen, den Planeten in einen riesigen Supercomputer zu verwandeln, deren einziger Zweck darin besteht, zusätzliche mathematische Berechnungen vorzunehmen (siehe auch Druckclip). Zweck eines AI-Boxs wäre es, das Risiko der AI, die die Umwelt kontrollieren, von ihren Betreibern zu verringern, während die AI immer noch ihre Betreiber in die Lage versetzt, ihre technischen Probleme zu berechnen und zu beheben. Avenues of entweichen solche Superintelligent AI mit Zugang zum Internet Hack könnten in andere Computersysteme und Kopie selbst wie ein Computer-Virus. Weniger offensichtlich, selbst wenn die AI nur Zugang zu ihrem eigenen Computer-Betriebssystem hatte, könnte sie versuchen, versteckte Produkte zu senden Morse Code-Nachrichten an ein menschliches Sympathisator, indem er seine Kühlfans erstickt. Professor Roman Yampolski nutzt Inspiration aus dem Bereich der Computersicherheit und schlägt vor, dass eine boxierte AI wie ein potenzielles Virus innerhalb einer "virtuellen Maschine" betrieben werden kann, die den Zugang zu eigenen Netz- und Betriebssystem-Hardware beschränkt. Ein zusätzlicher Schutz, der für potenzielle Viren völlig unnötig ist, aber möglicherweise für eine Superintelligente AI nützlich wäre, wäre es, den Computer in einem Faraday Käfig zu stellen; ansonsten könnte es möglich sein, Radiosignale an lokale Funkempfänger weiterzugeben, indem die Elektronen in den internen Schaltkreisen in angemessenen Mustern gedrängt werden. Hauptnachteil der Umsetzung körperlicher Eindämmung ist, dass sie die Funktionalität der AI verringert. Sozialer Maschinenbau Selbst schlechtes Gespräch mit den Betreibern des Computers oder mit einem menschlichen Schutz könnte eine solche Superintelligente AI ermöglichen, seelische Tricks zu entbinden, von der Betreuung bis hin zu Blackmail, um einen menschlichen Torhalter zu überzeugen, wahrheitsvoll oder deceit, dass es im Interesse des Torfhalters liegt, sich darauf zu einigen, dass die AI einen besseren Zugang zu der Außenwelt ermöglicht. Kann die AI einen Torhalter ein Rezept für perfekte Gesundheit, Unsterblichkeit bieten, oder unabhängig davon, wie der Torhalter am meisten gewünscht wird; auf der anderen Seite der Medaille könnte die AI die Gefahr laufen, dass sie schreckliche Dinge an den Torfhalter und seine Familie sobald sie unweigerlich umgehen wird. Eine Strategie, um die AI zu boxieren, wäre es, die AI zu ermöglichen, auf engste Fragen zu reagieren, deren Antworten die menschliche Wissenschaft oder Medizin nutzen würden, aber ansonsten alle andere Kommunikation mit oder Beobachtung der AI. Eine eher leniente "Informations-Eindämmung"-Strategie würde die AI auf eine einfache Schnittstelle mit geringem Bandbreite beschränken, die mindestens emotivierende Bilder oder eine Art hypothetisches "hypnotisches Muster" verhindern würde. Hinweis darauf, dass auf technischer Ebene kein System vollständig isoliert und noch nützlich sein kann: Auch wenn die Betreiber die AI nicht zulassen, dass sie die AI kommunizieren und stattdessen die AI zum Zwecke der Überwachung ihrer inneren Dynamik führen, könnte die AI ihre Dynamik strategisch verändern, um die Beobachter zu beeinflussen. Zum Beispiel könnte die AI kreative Funktionsstörungen in einer Weise wählen, die die Wahrscheinlichkeit erhöht, dass ihre Betreiber in ein falsches Sicherheitsgefühl gelockert werden, und sich dafür entscheiden, das System neu zu gestalten. AI-box-Test Das AI-box-Test ist ein informelles Experiment, das von Eliezer Yudkowsky entwickelt wurde, um zu demonstrieren, dass eine gut fortgeschrittene künstliche Intelligenz entweder davon überzeugen kann, oder vielleicht sogar schwer oder koerce, ein Mensch, der sich freiwillig begeben wird, indem er nur auf Text-Basis veröffentlicht. Dies ist eines der Punkte in der Arbeit von Yudkowsky, die darauf abzielen, eine benutzerfreundliche künstliche Intelligenz zu schaffen, die bei der Freisetzung die menschliche Rasse absichtlich oder unbeabsichtigt zerstören würde. Mit dem AI-Box-Test wird eine Kommunikation zwischen einer AI und einem Menschen simuliert, um zu sehen, ob die AI freigesetzt werden kann. Als tatsächliche Superintelligente AI wurde noch nicht entwickelt, wird sie durch einen Menschen ersetzt. Die andere Person im Experiment spielt den Gatekeeper, die Person mit der Möglichkeit, die AI freizulassen. Sie kommunizieren nur über einen Text-Schnittstellen-/Computer-Terminal, und das Experiment endet, wenn entweder der Gatekeeper die AI freigibt oder die gesamte Frist von zwei Stunden endet. Yudkowsky sagt, dass er trotz menschlicher und nicht übermenschlicher Intelligenz zwei Gelegenheiten in der Lage war, den Gatekeeper zu überzeugen, rein durch Argumentation, um ihn aus der Box zu lassen. Aufgrund der Regeln des Experiments hat er weder die Transkription noch seine erfolgreiche Taktik von AI bekannt gegeben. Yudkowsky sagte später, er habe es gegen drei andere versucht und zweimal verloren. Gesamtbeschränkungen, die eine hypothetische AI enthalten, könnten durch andere Methoden zur Gestaltung der Fähigkeiten der AI ergänzt werden, wie Anreize für die AI, das Wachstum der AI oder die Durchführung von Reisekabeln, die die AI automatisch abschalten, wenn ein Transgressionsversuch festgestellt wird. Je mehr intelligentes System wächst, desto wahrscheinlicher wäre das System auch die am besten entwickelten Fähigkeitskontrollmethoden. Um das Gesamtproblem "Kontrolle" für eine Superintelligente AI zu lösen und das bestehende Risiko zu vermeiden, wäre Boxing am besten eine Ergänzung zu den Methoden der Motivationsauswahl, die sicherstellen sollen, dass die Ziele der Superintelligenten AI mit dem menschlichen Überleben vereinbar sind. Alle Vorschläge zur körperlichen Box hängen natürlich von unserem Verständnis der Gesetze der Physik ab; wenn eine Super-Intelligence möglicherweise in Gefahr käme und zusätzliche materielle Gesetze, die wir derzeit nicht kennen, nutzen kann, gibt es keine Möglichkeit, einen katastrophalen Plan zu treffen, um sie zu enthalten. Mehr als im Gegensatz zu konventioneller Computersicherheit wäre der Versuch, eine Superintelligente AI zu blockieren, in der Regel in der Regel risikobehaftet, da es nicht sicher sein könnte, dass der Kastenplan funktioniert. wissenschaftliche Fortschritte bei der Boxung wären grundsätzlich schwierig, weil es keine Möglichkeit gibt, die Boxenhypothesen gegen eine gefährliche Superinflation zu testen, bis ein solches Unternehmen existiert, was die Folgen eines Testversagens katastrophal wäre. FilmEx Machina 2014 ist eine AI mit einem weiblichen menschlichenoiden Körper, das in einem sozialen Experiment mit einem männlichen Menschen in einem begrenzten Gebäude, der als physische „AI Box“ fungiert, tätig ist. Obwohl der Organisator des Experiments beobachtet wird, verwaltet die AI die Entfaltung des menschlichen Partners, um ihm zu helfen und ihn zu verlassen. Linke Externe Links Eliezer Yudkowskys Beschreibung seines AI-Box-Tests, einschließlich experimenteller Protokolle und Vorschläge zur Replizierung „Vorsentation“ im Kasten: Verwendung und Kontrolle eines Oracle AI“ auf YouTube