Die Geschichte des Internets hat ihren Ursprung in den Bemühungen, Computernetzwerke aufzubauen und zu verbinden, die aus Forschung und Entwicklung in den Vereinigten Staaten entstanden sind und internationale Zusammenarbeit, insbesondere mit Forschern im Vereinigten Königreich und Frankreich, involviert sind. Die Informatik war in den späten 1950er-Jahren eine aufstrebende Disziplin, die mit der Zeitteilung zwischen den Computernutzern begann, und später die Möglichkeit, diese über weite Netzwerke zu erreichen. Unabhängig davon schlug Paul Baran Anfang der 1960er Jahre ein verteiltes Netzwerk auf Basis von Daten in Nachrichtenblöcken vor und Donald Davies wurde 1965 im National Physical Laboratory (NPL) von Paketvermittlungen konzipiert und schlug vor, ein nationales kommerzielles Datennetz in Großbritannien aufzubauen. Die Advanced Research Projects Agency (ARPA) des US-Verteidigungsministeriums hat 1969 Verträge für die Entwicklung des ARPANET-Projekts unter der Leitung von Robert Taylor vergeben und von Lawrence Roberts verwaltet. ARPANET nahm die von Davies und Baran vorgeschlagene Paketvermittlungstechnologie an, die Anfang der 1970er Jahre von Leonard Kleinrock bei UCLA untermauert wurde. Das Netzwerk wurde von Bolt, Beranek und Newman gebaut. Frühe Paketvermittlungsnetze wie das NPL-Netzwerk, ARPANET, Merit Network und CYCLADES haben in den frühen 1970er-Jahren die Datenvernetzung erforscht und bereitgestellt. ARPA-Projekte und internationale Arbeitsgruppen führten zur Entwicklung von Protokollen für die Internetbearbeitung, in denen mehrere separate Netzwerke in ein Netzwerk von Netzwerken eingebunden werden konnten, das verschiedene Standards produzierte. Bob Kahn, bei ARPA und Vint Cerf, an der Stanford University, veröffentlichte 1974 Forschung, die in das Übertragungskontrollprotokoll (TCP) und Internet Protocol (IP,) die beiden Protokolle der Internetprotokoll-Suite entwickelt. Das Design umfasste Konzepte des französischen CYCLADES-Projekts unter der Leitung von Louis Pouzin. In den frühen 1980er Jahren finanzierte die National Science Foundation (NSF) nationale Supercomputing-Zentren an mehreren Universitäten in den Vereinigten Staaten und bot 1986 die Vernetzung mit dem NSFNET-Projekt. So schaffen Netzwerkzugang zu diesen Supercomputer-Standorten für Forschung und akademische Organisationen in den Vereinigten Staaten. Internationale Verbindungen zu NSFNET, die Entstehung von Architektur wie dem Domain Name System und die Übernahme von TCP/IP international in bestehenden Netzwerken markierten die Anfänge des Internets. Die kommerziellen Internet-Dienstleister (ISPs) sind 1989 in den Vereinigten Staaten und Australien entstanden. Das ARPANET wurde 1990 stillgelegt. Ende 1989 und 1990 entstanden in mehreren amerikanischen Städten begrenzte private Verbindungen zu Teilen des Internets durch offiziell kommerzielle Einrichtungen. Das NSFNET wurde 1995 stillgelegt und die letzten Beschränkungen für die Nutzung des Internets für den kommerziellen Verkehr entfernt. Die Forschung am CERN in der Schweiz durch den britischen Informatiker Tim Berners-Lee 1989–90 führte dazu, dass das World Wide Web Hypertext-Dokumente in ein Informationssystem verknüpft, das von jedem Knoten des Netzwerks zugänglich ist. Seit Mitte der 1990er Jahre hat das Internet einen revolutionären Einfluss auf Kultur, Handel und Technologie, einschließlich des Anstiegs der nahen Kommunikation per E-Mail, Instant Messaging, Stimme über Internet Protocol (VoIP) Telefonanrufe, Video-Chat und das World Wide Web mit seinen Diskussionsforen, Blogs, Social Networking-Services und Online-Shopping-Seiten. Über Glasfasernetze, die mit 1 Gbit/s, 10 Gbit/s oder mehr arbeiten, werden zunehmende Datenmengen mit höheren und höheren Geschwindigkeiten übertragen. Die Übernahme der globalen Kommunikationslandschaft im Internet war historisch rasant: Sie übermittelte nur 1 % der Informationen, die im Jahr 1993 durch zweiseitige Telekommunikationsnetze fließen, 51 % bis 2000 und mehr als 97 % der telekommunikationsbezogenen Informationen bis 2007. Das Internet wächst weiter, angetrieben von immer größeren Mengen von Online-Informationen, Handel, Unterhaltung und Social Networking-Diensten. Die Zukunft des globalen Netzes kann jedoch durch regionale Unterschiede geprägt werden. Stiftungen Precursors Das Konzept der Datenkommunikation – Datenübermittlung zwischen zwei verschiedenen Orten durch ein elektromagnetisches Medium wie Funk oder einen elektrischen Draht – setzt die Einführung der ersten Rechner voraus. Solche Kommunikationssysteme waren typischerweise auf eine Punkt-Kommunikation zwischen zwei Endgeräten beschränkt. Semaphore-Linien, Telegraph-Systeme und Telex-Maschinen können als frühe Vorläufer dieser Art von Kommunikation betrachtet werden. Das Telegraph im späten 19. Jahrhundert war das erste volldigitale Kommunikationssystem.Die grundlegende theoretische Arbeit zur Informationstheorie wurde von Harry Nyquist und Ralph Hartley in den 1920er Jahren entwickelt. Die Informationstheorie, wie Claude Shannon in den 1940er-Jahren verkündete, bot eine feste theoretische Grundlage, um die Kompromisse zwischen Signal-Rausch-Verhältnis, Bandbreite und fehlerfreie Übertragung in Gegenwart von Rauschen in der Telekommunikationstechnologie zu verstehen. Dies war eine der drei wichtigsten Entwicklungen, zusammen mit Fortschritten in der Transistortechnologie (insbesondere MOS-Transistoren) und der Lasertechnologie, die das schnelle Wachstum der Telekommunikationsbandbreite im nächsten halben Jahrhundert ermöglichten. Frührechner in den 1940er Jahren hatten eine zentrale Verarbeitungseinheit und Benutzerterminals. Bei der Entwicklung der Technologie in den 1950er Jahren wurden neue Systeme entwickelt, um eine Kommunikation über längere Strecken (für Terminals) oder mit höherer Geschwindigkeit (für die Verbindung lokaler Geräte) zu ermöglichen, die für das Computermodell des Hauptrechners erforderlich waren. Diese Technologien ermöglichten es, Daten (wie Dateien) zwischen Remotecomputern auszutauschen. Das Punkt-zu-Punkt-Kommunikationsmodell war jedoch begrenzt, da es keine direkte Kommunikation zwischen zwei beliebigen Systemen ermöglichte; eine physische Verbindung war notwendig. Die Technologie wurde auch für den strategischen und militärischen Einsatz als verletzlich angesehen, da es keine alternativen Wege für die Kommunikation im Falle eines gebrochenen Links gab. Inspiration für die Vernetzung und Interaktion mit Computern Die frühesten Computer wurden direkt mit Endgeräten verbunden, die von einem einzelnen Benutzer verwendet wurden. Christopher Strachey, der erste Professor der Universität Oxford wurde, reichte im Februar 1959 eine Patentanmeldung für die Zeitverteilung ein. Im Juni dieses Jahres gab er eine Zeitung "Time Sharing in Large Fast Computers" auf der UNESCO Informationsverarbeitung Konferenz in Paris, wo er das Konzept an J. C. R. Licklider von M.I.T..Licklider, Vice President bei Bolt Beranek und Newman, Inc., weiter, um ein Computernetzwerk in seinem Januar 1960 Papier Man-Computer Symbiosis vorzuschlagen: Ein Netz von solchen Zentren, die durch Breitband-Kommunikationsleitungen miteinander verbunden [...] die Funktionen der heutigen Bibliotheken zusammen mit erwarteten Fortschritten in der Informationsspeicherung und Retrieval und symbiotischen Funktionen bereits im August 1962, Licklider und Welden Clark veröffentlichte das Papier "On-Line Man-Computer Communication", das eine der ersten Beschreibungen einer vernetzten Zukunft war. Im Oktober 1962 wurde Licklider von Jack Ruina als Direktor des neu gegründeten Informationsverarbeitungstechnik-Büros (IPTO) innerhalb von DARPA angeheuert, mit dem Auftrag, die Hauptcomputer des US-Verteidigungsministeriums in Cheyenne Mountain, dem Pentagon und SAC HQ zu verbinden. Dort gründete er eine informelle Gruppe innerhalb von DARPA zur weiteren Computerforschung. Er begann 1963 mit dem Schreiben von Memos, indem er ein verteiltes Netzwerk an das IPTO-Personal beschreibt, das er "Mitglieder und Partner des intergalaktischen Computernetzwerks" nannte. Obwohl er 1964 die IPTO verlassen hatte, fünf Jahre vor dem ARPANET live ging, war es seine Vision der universellen Vernetzung, die den Anstoß für einen seiner Nachfolger Robert Taylor bot, die ARPANET-Entwicklung zu initiieren. Licklider kehrte später für zwei Jahre zur Leitung der IPTO in 1973 zurück. Paketwechsel Das Problem, separate physikalische Netzwerke zu einem logischen Netzwerk zu verbinden, war das erste von vielen Problemen. Frühe Netzwerke verwendet Nachrichten geschaltete Systeme, die starre Routing-Strukturen anfällig für einen einzigen Punkt des Ausfalls erforderlich. In den 1960er Jahren produzierte Paul Baran von der RAND Corporation eine Studie über überlebensfähige Netzwerke für das US-Militär im Falle des Atomkriegs. Informationen, die über Barans Netzwerk übertragen werden, würden in das, was er als "Botschaftsblöcke" bezeichnet. Unabhängig davon hat Donald Davies (National Physical Laboratory, UK) ein lokales Netzwerk vorgeschlagen und in die Praxis umgesetzt, basierend auf dem, was er Paketvermittlung nannte, dem Begriff, der letztendlich angenommen würde. Paketvermittlung ist ein schnelles Speicher- und Weiternetzwerkdesign, das Nachrichten in willkürliche Pakete unterteilt, mit Routing-Entscheidungen pro Paket. Es bietet eine bessere Bandbreitenauslastung und Ansprechzeiten als die herkömmliche schaltungsschaltende Technologie, die für die Telefonie verwendet wird, insbesondere auf ressourcenbegrenzten Verbindungsverbindungen. Netzwerke, die zum Internet NPL-Netzwerk geführtNach Gesprächen mit J. C. R. Licklider im Jahr 1965, Donald Davies wurde an Datenkommunikation für Computernetzwerke interessiert. Später in diesem Jahr, am National Physical Laboratory (Großbritannien), Davies entworfen und schlug ein nationales kommerzielles Datennetz auf der Grundlage der Paketvermittlung.Im folgenden Jahr beschrieb er die Verwendung eines "Interface-Computers" als Router. Der Vorschlag wurde nicht national aufgegriffen, sondern er erstellte ein Design für ein lokales Netzwerk, um den Bedürfnissen von NPL zu dienen und die Machbarkeit der Paketvermittlung durch schnelle Datenübertragung zu beweisen. Er und sein Team waren einer der ersten, die 1967 den Begriff Protokoll in einem Daten-Kommutationskontext verwenden. 1969 hatte er damit begonnen, das paketvermittelte Netzwerk Mark I aufzubauen, um den Bedürfnissen des multidisziplinären Labors gerecht zu werden und die Technik unter betrieblichen Bedingungen zu beweisen. Im Jahr 1976 wurden 12 Computer und 75 Endgeräte angeschlossen, und mehr wurden hinzugefügt, bis das Netz 1986 ersetzt wurde. Das lokale Netzwerk NPL und das ARPANET waren die ersten beiden Netzwerke der Welt, um Paketvermittlung zu nutzen und wurden Anfang der 1970er Jahre miteinander verbunden. Das NPL-Team hat Simulationsarbeiten an Paketnetzen durchgeführt, einschließlich Datagram-Netzwerken und der Forschung zur Internetbearbeitung. ARPANET Robert Taylor wurde 1966 zum Leiter des Informationsverarbeitungstechnikamts (IPTO) der Defense Advanced Research Projects Agency (DARPA) gefördert. Er wollte Lickliders Ideen eines vernetzten Netzwerksystems erkennen. Im Rahmen der IPTO-Rolle wurden drei Netzwerkterminals installiert: ein für die System Development Corporation in Santa Monica, ein für Projekt Genie an der University of California, Berkeley, und ein für das kompatible Time-Sharing System Projekt am Massachusetts Institute of Technology (MIT). Taylors identifizierte Notwendigkeit für die Vernetzung wurde offensichtlich aus der Verschwendung von Ressourcen, die ihm offensichtlich sind. Für jedes dieser drei Terminals hatte ich drei verschiedene Sätze von Benutzerbefehlen. Also, wenn ich online mit jemandem in S.D.C sprach und ich mit jemandem sprechen wollte, den ich in Berkeley oder M.I.T davon kannte, musste ich vom S.D.C-Terminal aufstehen, mich einloggen und mit ihnen in Kontakt treten. Ich sagte, Mann, es ist offensichtlich, was zu tun ist: Wenn Sie diese drei Terminals haben, sollte es ein Terminal sein, das überall hingeht, wo Sie wollen, wohin Sie interaktives Computing haben. Diese Idee ist das ARPAnet. Mit Larry Roberts vom MIT im Januar 1967 initiierte er ein Projekt, um ein solches Netzwerk aufzubauen. Roberts und Thomas Merrill haben die Computer-Zeitverteilung über weite Netzwerke untersucht. In den 50er Jahren entstanden breite Flächennetze (WANs) und wurden in den 1960er Jahren gegründet. Auf dem ersten ACM-Symposium über Betriebssystemprinzipien im Oktober 1967 präsentierte Roberts einen Vorschlag für das "ARPA-Netz", basierend auf Wesley Clarks Vorschlag, Interface Message Processors zur Schaffung eines Nachrichtenvermittlungsnetzes einzusetzen. Auf der Konferenz präsentierte Roger Scantlebury die Arbeit von Donald Davies zur Paketvermittlung für die Datenkommunikation und erwähnte die Arbeit von Paul Baran bei RAND. Roberts hat die Paketvermittlungskonzepte in das ARPANET-Design integriert und die vorgeschlagene Kommunikationsgeschwindigkeit von 2,4 kbps auf 50 kbps aktualisiert. Leonard Kleinrock entwickelte anschließend die mathematische Theorie hinter der Leistung dieses Technologie-Gebäudes auf seiner früheren Arbeit auf Wartetheorie. ARPA erhielt den Auftrag, das Netzwerk an Bolt Beranek & Newman zu bauen, und die erste ARPANET-Verbindung wurde zwischen der University of California, Los Angeles (UCLA) und dem Stanford Research Institute um 22:30 Stunden am 29. Oktober 1969 gegründet. "Wir haben eine Telefonverbindung zwischen uns und den Jungs bei SRI aufgebaut...", sagte Kleinrock in einem Interview: "Wir tippten das L und wir fragten am Telefon: "Sehen Sie das L?" "Ja, wir sehen das L", kam die Antwort. Wir tippten das O und wir fragten: "Sehen Sie das O." "Ja, wir sehen das O." Dann gaben wir die G, und das System stürzte ab. Doch eine Revolution hatte begonnen"... Bis Dezember 1969 wurde ein Vier-Node-Netzwerk durch die Hinzufügung der Universität Utah und der University of California, Santa Barbara verbunden. Im selben Jahr half Taylor ALOHAnet, ein System von Professor Norman Abramson und anderen an der University of Hawaii an Manoa, die Daten per Funk zwischen sieben Computern auf vier Inseln auf Hawaii übertragen. Die Software zum Aufbau von Verbindungen zwischen Netzwerk-Websites im ARPANET war das Netzwerk Control Program (NCP), das im Jahr 1970 fertiggestellt wurde. ARPANET-Entwicklung wurde um die Anfrage für Kommentare (RFC) Prozess, noch heute verwendet, um Internet-Protokolle und Systeme vorzuschlagen und zu verteilen. RFC 1, mit dem Titel "Host Software", wurde von Steve Crocker von der University of California, Los Angeles geschrieben und am 7. April 1969 veröffentlicht. Diese frühen Jahre wurden in den 1972 Film Computer Networks dokumentiert: The Heralds of Resource Sharing.Frühe internationale Kooperationen auf ARPANET waren spärlich. Die Verbindungen wurden 1973 mit dem norwegischen Seismic Array (NORSAR) über eine Satellitenverbindung an der Tanum Earth Station in Schweden und Peter Kirsteins Forschungsgruppe am University College London hergestellt, die ein Tor zu britischen akademischen Netzwerken bot. Die Zahl der Gastgeber war bis 1981 auf 213 angewachsen. ARPANET wurde der technische Kern dessen, was das Internet werden würde, und ein primäres Werkzeug für die Entwicklung der verwendeten Technologien. Netzwerk vernetzen Das Merit Network wurde 1966 als Michigan Educational Research Information Triad gegründet, um Computer-Netzwerke zwischen drei der öffentlichen Universitäten Michigan als Mittel zu erkunden, um der bildungs- und wirtschaftspolitischen Entwicklung des Staates zu helfen. Mit anfänglicher Unterstützung des Staates Michigan und der National Science Foundation (NSF) wurde das paketvermittelte Netzwerk erstmals im Dezember 1971 demonstriert, als ein interaktiver Host zur Host-Verbindung zwischen den IBM-Mainframe-Computersystemen an der University of Michigan in Ann Arbor und Wayne State University in Detroit hergestellt wurde. Im Oktober 1972 beendeten die Verbindungen zum CDC-Mainframe an der Michigan State University in East Lansing die Triade. In den nächsten Jahren wurde das Netzwerk zusätzlich zu Host für interaktive Verbindungen erweitert, um Terminal zu Host-Verbindungen zu unterstützen, Stapelverbindungen zu hosten (Job-Einreichung, Remote-Druck, Batch-Datei-Transfer,) interaktive Dateiübertragung, Gateways zu den Tymnet und Telenet Public Data Networks, X.25 Host-Anhänge, Gateways zu X.25-Datennetzwerken, Ethernet-Anbieter, und schließlich TCP/IP und zusätzliche öffentliche Universitäten in Michigan zu hosten. All dies setzte die Bühne für die Rolle von Merit im NSFNET-Projekt ab Mitte der 1980er Jahre. CYCLADES Das CYCLADES Paketvermittlungsnetz war ein französisches Forschungsnetzwerk, das von Louis Pouzin entworfen und geleitet wurde. Ausgehend von den Ideen von Donald Davies entwickelte Pouzin das Netzwerk, um Alternativen zum frühen ARPANET-Design zu erkunden und die Internet-working-Forschung zu unterstützen. Als erstes im Jahr 1973 gezeigt wurde, war es das erste Netzwerk, die Hosts für eine zuverlässige Datenlieferung zu verantwortlich zu machen, anstatt das Netzwerk selbst, indem sie unzuverlässige Datengramme und zugehörige End-to-End-Protokollmechanismen verwenden. Konzepte dieses Netzwerks beeinflussten später die ARPANET Architektur. X.25 und öffentliche Datennetze Basierend auf internationalen Forschungsinitiativen, insbesondere den Beiträgen von Rémi Després, wurden Paketvermittlungsnetzstandards vom Internationalen Telegraphen- und Telefonberatungsausschuss (ITU-T) in Form von X.25 und verwandten Standards entwickelt. X.25 basiert auf dem Konzept der virtuellen Schaltungen, die traditionelle Telefonverbindungen nachahmen. 1974 gründete X.25 die Basis für das SERCnet-Netzwerk zwischen britischen akademischen und Forschungsstandorten, das später JANET wurde. Der erste ITU-Standard auf X.25 wurde im März 1976 genehmigt. Das British Post Office, Western Union International und Tymnet haben zusammengearbeitet, um 1978 das erste internationale Paketvermittlungsnetz zu schaffen, das als International Packet Switched Service (IPSS) bezeichnet wird. Dieses Netzwerk wuchs von Europa und den USA bis 1981 auf Kanada, Hongkong und Australien. In den 90er Jahren wurde eine weltweite Vernetzungsinfrastruktur bereitgestellt. Im Gegensatz zu ARPANET war X.25 häufig für den geschäftlichen Gebrauch verfügbar. Telenet bot seinen Telemail-E-Mail-Service an, der auch auf den Einsatz von Unternehmen anstatt auf das allgemeine E-Mail-System des ARPANET zielte. Die ersten öffentlichen Einwahlnetze nutzten asynchrone TTY-Endprotokolle, um einen im öffentlichen Netz betriebenen Konzentrator zu erreichen. Einige Netzwerke, wie Telenet und CompuServe, nutzten X.25, um die Terminalsitzungen in ihre paketvermittelten Backbones zu multiplexen, während andere, wie Tymnet, proprietäre Protokolle benutzten. Im Jahr 1979 wurde CompuServe der erste Service, der elektronische Mail-Fähigkeiten und technische Unterstützung für PC-Benutzer bietet. Das Unternehmen brach 1980 wieder neue Wege als erstes, um Echtzeit-Chat mit seinem CB-Simulator anzubieten. Andere große Einwahl-Netzwerke waren America Online (AOL) und Prodigy, die auch Kommunikation, Inhalte und Unterhaltungs-Funktionen zur Verfügung gestellt. Viele Bullen-Board-System (BBS) Netzwerke auch Online-Zugang, wie FidoNet, die unter Hobby-Computer-Nutzer, viele von ihnen Hacker und Amateur-Radio-Operatoren beliebt war. In der UdSSR erschienen erste Computernetzwerke in den 1950er Jahren im Raketenabwehrsystem von Sary Shagan (erst wurden sie in Moskau am Lebedev Institut für Präzisionsmechanik und Computertechnik getestet).In den 1960er Jahren wurde das massive Computernetzwerkprojekt OGAS vorgeschlagen, aber nicht umgesetzt. Apollo–Soyuz USA–USSR-Gemeinschaftsraumprogramm (1972–1975) verwendet digitale Daten für Raumschiffe, die zwischen zwei Ländern übertragen werden. Seit den späten 1970er Jahren begannen X.25 sowjetische Netzwerke zu erscheinen und Academset entstand 1978 in Leningrad. Im Jahr 1982 wurde das VNIIPAS-Institut in Moskau gegründet, um als zentrale Knoten von Academset zu dienen, der X.25 regelmäßige Verbindung zu IIASA in Österreich (die den Zugang zu anderen weltweiten Netzwerken erlaubte). 1983 gründete VNIIPAS zusammen mit der US-Regierung und George Soros den sowjetischen X.25-Dienstleister namens SFMT ("San Francisco - Moscow Teleport), der später Sovam Teleport ("Sowjet-American Teleport") wurde. VNIIPAS lieferte auch X.25 Dienstleistungen, einschließlich über Satelliten, in östliche Blockländer zusammen mit Mongolei, Kuba und Vietnam. Zu der Zeit, westliche Benutzer von Usenet waren in der Regel nicht bewusst, dass, und als solche Vernetzung in der UdSSR nicht vorhanden, so einer von ihnen am 1. April 1984 machte eine "April-Tor"-Hoax über Kremvax ("Kremlin VAX"), die einige Popularität für die folgenden Jahre gewonnen. UdSSR trat im Oktober 1990 in privates Fidonet-Netzwerk, als erster Knoten der Region 50 in Novosibirsk erschien. Sovam Teleport wurde Anfang der 1990er Jahre zum ersten SWIFT-Netzwerkanbieter für aufstrebende russische Banken (über X.25). Einige der frühen sowjetischen/russischen Netzwerke wurden auch als Teile von BITNET initiiert. UUCP und UsenetIn 1979, zwei Studenten an der Duke University, Tom Truscott und Jim Ellis, entstand die Idee, Bourne Shell-Skripte zu verwenden, um Nachrichten und Nachrichten auf einer seriellen Linie UUCP Verbindung mit der nahe gelegenen University of North Carolina an Chapel Hill zu übertragen. Nach der Veröffentlichung der Software im Jahr 1980 wurde das Netz von UUCP-Hosts, die auf den Usenet News weitergeleitet wurden, rasch erweitert. UUCPnet, wie es später benannt werden würde, erstellt auch Gateways und Verbindungen zwischen FidoNet und Dia-up-BBS-Hosts. UUCP-Netzwerke breiteten sich aufgrund der geringeren Kosten, der Fähigkeit, bestehende Mietlinien, X.25-Links oder sogar ARPANET-Verbindungen zu verwenden, und der fehlenden strengen Nutzungsrichtlinien im Vergleich zu späteren Netzwerken wie CSNET und Bitnet. Alle Verbindungen waren lokal. Bis 1981 war die Zahl der UUCP-Hosts auf 550 angewachsen, was 1984 fast auf 940 verdoppelt wurde. Sublink Network, seit 1987 tätig und 1989 offiziell in Italien gegründet, basiert seine Vernetzung auf UUCP, um Post- und Nachrichtengruppen-Nachrichten in seinen italienischen Knoten (ca. 100 zu der Zeit) sowohl von Privatpersonen als auch von kleinen Unternehmen zu verteilen. Sublink Network stellte möglicherweise eines der ersten Beispiele der Internet-Technologie dar, die durch populäre Diffusion vorangetrieben wird. 1973–1989:Merging the network and create the Internet TCP/IP Mit so vielen verschiedenen Netzwerkmethoden wurde etwas benötigt, um sie zu vereinen. Bob Kahn von DARPA rekrutiert Vinton Cerf von Stanford University, um mit ihm an dem Problem zu arbeiten. Steve Crocker gründete eine ARPA "Networking Working Group" mit Vint Cerf. Gleichzeitig gründete 1972 eine internationale Netzwerk-Arbeitsgruppe; aktive Mitglieder waren Vint Cerf, Alex McKenzie, Donald Davies, Roger Scantlebury, Louis Pouzin und Hubert Zimmermann. Bis 1973 hatten diese Gruppen eine grundlegende Reformulierung erarbeitet, bei der die Unterschiede zwischen Netzwerkprotokollen durch die Verwendung eines gemeinsamen Internetwork-Protokolls verdeckt wurden, und statt des Netzwerks, das für die Zuverlässigkeit verantwortlich ist, wie im ARPANET, wurden die Hosts verantwortlich. Diese Arbeit prägte auch den Begriff catenet (concatenated network). Im Jahre 1974 veröffentlichten Kahn und Cerf ihre Ideen, die von Louis Pouzin und Hubert Zimmermann, den Designern des CYCLADES-Netzwerks, vorgeschlagenen Konzepten enthalten. Die Spezifikation des resultierenden Protokolls, das Transmission Control Program, wurde von der Network Working Group im Dezember 1974 als RFC 675 veröffentlicht. Es enthält die erste geprüfte Nutzung des Begriffs Internet, als Kurzhand für Internetwork. Diese Software war monolithisch im Design mit zwei Simplex-Kommunikationskanälen für jede Benutzersitzung. Mit der auf einen Kern der Funktionalität reduzierten Rolle des Netzes wurde es möglich, den Verkehr mit anderen Netzwerken unabhängig von ihren detaillierten Eigenschaften auszutauschen und so die grundlegenden Probleme der Internetbearbeitung zu lösen. DARPA stimmte der Entwicklung von Prototyp-Software zu. Die Tests begannen 1975 durch gleichzeitige Implementierungen an Stanford, BBN und University College London. Nach mehreren Jahren Arbeit wurde die erste Demonstration eines Gateways zwischen dem Paketfunknetz (PRNET) im Bereich SF Bay und dem ARPANET vom Stanford Research Institute durchgeführt.Am 22. November 1977 wurden drei Netzwerkdemonstrationen durchgeführt, darunter das ARPANET, das SRI's Packet Radio Van auf dem Packet Radio Network und das Atlantic Packet Satellite Network (SATNET). Die Software wurde mit Vollduplex-Kanälen als modularer Protokollstapel neu gestaltet. Zwischen 1976 und 1977 schlug Yogen Dalal vor, die Routing- und Übertragungssteuerungsfunktionen von TCP in zwei diskrete Schichten zu trennen, was zur Spaltung des Transmission Control Program in das Transmission Control Protocol (TCP) und das IP-Protokoll (IP) in Version 3 1978 führte. Ursprünglich als IP/TCP bezeichnet, wurde die Version 4 in der IETF-Veröffentlichung RFC 791 (September 1981,) 792 und 793 beschrieben. Es wurde 1982 auf SATNET installiert und das ARPANET im Januar 1983, nachdem die DoD es Standard für alle militärischen Computer-Netzwerke gemacht. Dies führte zu einem Netzwerkmodell, das informell als TCP/IP bekannt wurde. Es wurde auch als das Verteidigungsministerium (DoD) Modell, DARPA-Modell oder ARPANET-Modell bezeichnet. Cerf würdigt seine Absolventen-Studenten Yogen Dalal, Carl Sunshine, Judy Estrin und Richard Karp, mit wichtigen Arbeiten an Design und Test. DARPA förderte die Entwicklung von TCP/IP-Implementierungen für viele Betriebssysteme. IPv4 verwendet 32-Bit-Adressen, die den Adressraum auf 232 Adressen, d.h. 4294967296 Adressen begrenzt. Die letzte verfügbare IPv4-Adresse wurde im Januar 2011 zugewiesen. IPv4 wird durch seinen Nachfolger IPv6 ersetzt, der 128 Bit-Adressen verwendet und 2128 Adressen bereitstellt, d.h. 340282366920938463463374607431768211456. Dies ist ein enorm erhöhter Adressraum. Die Umstellung auf IPv6 wird voraussichtlich viele Jahre, Jahrzehnte oder vielleicht länger dauern, da es vier Milliarden Maschinen mit IPv4 gab, als die Verschiebung begann. Von ARPANET nach NSFNET Nachdem das ARPANET seit mehreren Jahren auf dem Laufenden war, suchte ARPA eine andere Agentur, um das Netzwerk auszuhändigen; die Hauptaufgabe von ARPA war die Finanzierung von Spitzenforschung und -entwicklung, nicht die Durchführung eines Kommunikations-Dienstprogramms. Letztendlich, im Juli 1975, wurde das Netzwerk an die Verteidigungsagentur, auch Teil des Verteidigungsministeriums, umgestellt. 1983 wurde der US-Militärteil des ARPANET als separates Netzwerk, das MILNET, abgebrochen. MILNET wurde anschließend zum unklassifizierten, aber militärischen NIPRNET, parallel zum SECRET-Level SIPRNET und JWICS für TOP SECRET und oben. NIPRNET hat Sicherheits-Gateways in das öffentliche Internet gesteuert. Die auf dem ARPANET basierenden Netzwerke wurden von der Regierung finanziert und daher auf nichtkommerzielle Anwendungen wie Forschung beschränkt; die unbezogene kommerzielle Nutzung war streng verboten. Dies beschränkte zunächst die Verbindungen zu militärischen Standorten und Universitäten. In den 1980er Jahren erweiterten sich die Verbindungen zu mehr Bildungseinrichtungen und sogar zu einer wachsenden Zahl von Unternehmen wie Digital Equipment Corporation und Hewlett-Packard, die an Forschungsprojekten beteiligt waren oder Dienstleistungen für diejenigen erbrachten. Mehrere weitere Zweige der US-Regierung, der National Aeronautics and Space Administration (NASA), der National Science Foundation (NSF) und der Department of Energy (DOE) wurden stark an der Internetforschung beteiligt und begannen die Entwicklung eines Nachfolgers von ARPANET. Mitte der 1980er Jahre entwickelten alle drei dieser Zweige die ersten Wide Area Networks auf Basis von TCP/IP. NASA entwickelte das NASA Science Network, NSF entwickelte CSNET und DOE entwickelte das Energy Sciences Network oder ESNet. Die NASA entwickelte Mitte der 1980er Jahre das TCP/IP-basierte NASA Science Network (NSN) und verbindet Raumfahrtwissenschaftler mit Daten und Informationen, die überall auf der Welt gespeichert sind. 1989 wurden das DECnet-basierte Space Physics Analysis Network (SPAN) und das TCP/IP-basierte NASA Science Network (NSN) am NASA Ames Research Center zusammengebracht, um das erste Multiprotocol Wide Area Network namens NASA Science Internet oder NSI zu schaffen. Die NSI wurde gegründet, um der NASA-Wissenschaft eine vollständig integrierte Kommunikationsinfrastruktur für die Förderung von Erd-, Raum- und Lebenswissenschaften zur Verfügung zu stellen. Als High-Speed-, Multiprotocol-, internationales Netzwerk bot die NSI Konnektivität zu über 20.000 Wissenschaftlern auf allen sieben Kontinenten. 1981 unterstützte NSF die Entwicklung des Computer Science Network (CSNET). CSNET verbunden mit ARPANET mit TCP/IP, und lief TCP/IP über X.25, aber es unterstützte auch Abteilungen ohne ausgeklügelte Netzwerkverbindungen, unter Verwendung automatisierter Dia-up-Mail-Austausch. 1986 schuf die NSF NSFNET, ein 56 kbit/s Backbone, um die NSF-gesponserten Supercomputing-Zentren zu unterstützen.Das NSFNET unterstützte auch die Schaffung regionaler Forschungs- und Bildungsnetze in den Vereinigten Staaten sowie die Anbindung von Hochschul- und Hochschulcampus-Netzwerken an die regionalen Netzwerke. Der Einsatz von NSFNET und den regionalen Netzwerken war nicht auf Supercomputer-Nutzer beschränkt und das 56 kbit/s Netzwerk wurde schnell überlastet. NSFNET wurde 1988 im Rahmen eines Kooperationsabkommens mit dem Merit Network in Zusammenarbeit mit IBM, MCI und dem Staat Michigan auf 1,5 Mbit/s aufgerüstet. Die Existenz von NSFNET und die Gründung von Federal Internet Exchanges (FIXes) erlaubte es, das ARPANET 1990 stillzusetzen. NSFNET wurde 1991 auf 45 Mbit/s ausgebaut und aufgerüstet und 1995 zurückgelassen, als es durch Backbones ersetzt wurde, die von mehreren kommerziellen Internet-Dienstleistern betrieben wurden. Die Forschungs- und Wissenschaftsgemeinschaft entwickelt und nutzt weiter fortgeschrittene Netzwerke wie Internet2 in den Vereinigten Staaten und JANET in Großbritannien. Transition ins InternetDer Begriff Internet spiegelt sich im ersten RFC wider, der im TCP-Protokoll (RFC 675: Internet Transmission Control Program, Dezember 1974) veröffentlicht wurde, als eine kurze Form von Internetworking, als die beiden Begriffe austauschbar verwendet wurden. Im Allgemeinen war ein Internet eine Sammlung von Netzwerken, die durch ein gemeinsames Protokoll verknüpft wurden. In der Zeit, in der das ARPANET in den späten 1980er Jahren mit dem neu gebildeten NSFNET-Projekt verbunden war, wurde der Begriff als Name des Netzes, Internet, als das große und globale TCP/IP-Netzwerk verwendet. Da das Interesse an der Vernetzung durch die Bedürfnisse der Zusammenarbeit, des Austauschs von Daten und des Zugriffs von Remote Computing-Ressourcen wuchs, verbreiteten sich die TCP/IP-Technologien auf der ganzen Welt. Der hardware-agnostische Ansatz in TCP/IP unterstützte die Nutzung bestehender Netzwerkinfrastrukturen, wie der International Packet Switched Service (IPSS) X.25 Netzwerk, um den Internetverkehr zu führen. Viele Websites, die nicht direkt mit dem Internet verbinden können, haben einfache Gateways für die Übertragung von elektronischen Posten geschaffen, die wichtigste Anwendung der Zeit. Websites mit nur intermittierenden Verbindungen verwendet UUCP oder FidoNet und auf die Gateways zwischen diesen Netzwerken und dem Internet. Einige Gateway-Dienste gingen über einfache Mail-Peering, wie den Zugang zu File Transfer Protocol (FTP) Websites über UUCP oder Post. Schließlich wurden Routing-Technologien für das Internet entwickelt, um die verbleibenden zentralen Routing-Aspekte zu entfernen. Das Exterior Gateway Protocol (EGP) wurde durch ein neues Protokoll, das Border Gateway Protocol (BGP) ersetzt. Dies lieferte eine netzierte Topologie für das Internet und reduzierte die zentrische Architektur, die ARPANET betont hatte. Im Jahr 1994 wurde Classless Inter-Domain Routing (CIDR) eingeführt, um eine bessere Erhaltung des Adressraums zu unterstützen, der die Nutzung der Routenaggregation ermöglichte, um die Größe der Routing-Tabellen zu verringern. TCP/IP geht global (1980) CERN, das Europäische Internet, die Verbindung zum Pazifik und darüber hinaus Anfang 1982 verließ NORSAR und Peter Kirsteins Gruppe am University College London (UCL) das ARPANET und begannen, TCP/IP über SATNET zu verwenden. UCL bietet Zugang zwischen dem Internet und akademischen Netzwerken in Großbritannien. Zwischen 1984 und 1988 begann CERN mit der Installation und dem Betrieb von TCP/IP, um seine wichtigsten internen Computersysteme, Workstations, PCs und ein Beschleuniger-Kontrollsystem zu verbinden. Das CERN betreibt weiterhin ein begrenztes selbstentwickeltes System (CERNET) intern und mehrere inkompatible (typischerweise proprietäre) Netzwerkprotokolle extern. In Europa gab es einen erheblichen Widerstand gegen eine stärkere Nutzung von TCP/IP, und die CERN TCP/IP-Intranets blieben bis 1989 vom Internet isoliert, als eine transatlantische Verbindung zur Cornell University aufgebaut wurde. 1988 wurde die erste internationale Verbindung zu NSFNET von Frankreichs INRIA und Piet Beertema am Centrum Wiskunde & Informatica (CWI) in den Niederlanden aufgebaut. Daniel Karrenberg, von CWI, besuchte Ben Segal, CERN TCP/IP Koordinator, auf der Suche nach Beratung über den Übergang EUnet, die europäische Seite des UUCP Usenet-Netzwerks (viel davon über X.25 Links,) über TCP/IP. Im vergangenen Jahr hatte Segal mit Len Bosack von der damals noch kleinen Firma Cisco getroffen, um einige TCP/IP Router für CERN zu kaufen, und Segal konnte Karrenberg beraten und ihn für die entsprechende Hardware an Cisco weiterleiten. Dies erweiterte den europäischen Teil des Internets über die bestehenden UUCP-Netzwerke. Die NORDUnet-Verbindung zu NSFNET wurde bald danach eingerichtet und bietet einen offenen Zugang für Studenten in Dänemark, Finnland, Island, Norwegen und Schweden.Im Januar 1989 eröffnete CERN seine ersten externen TCP/IP-Verbindungen. Dies fiel mit der Schaffung von Réseaux IP Européens (RIPE), zunächst einer Gruppe von IP-Netzwerkadministratoren, die regelmäßig zusammenkamen, um die Koordinierungsarbeit gemeinsam durchzuführen. Später, 1992, wurde RIPE offiziell als Genossenschaft in Amsterdam registriert. 1991 verabschiedete das nationale Forschungs- und Bildungsnetz des Vereinigten Königreichs das Internetprotokoll über das bestehende Netz. Im selben Jahr führte Dai Davies die Internet-Technologie in die paneuropäische NREN, EuropaNet ein, die auf dem X.25-Protokoll gebaut wurde. Das Europäische Wissenschafts- und Forschungsnetzwerk (EARN) und RARE haben IP zur gleichen Zeit angenommen und das europäische Internet-Backbone EBONE wurde 1992 in Betrieb genommen. Gleichzeitig mit dem Anstieg der Internet-Bearbeitung in Europa bildeten sich Ad-hoc-Netzwerke zu ARPA und zwischen den australischen Universitäten, basierend auf verschiedenen Technologien wie X.25 und UUCPNet. Diese waren in ihrem Zusammenhang mit den globalen Netzwerken aufgrund der Kosten für die individuelle internationale UUCP-Zeichnung oder X.25-Verbindungen begrenzt. 1989 schlossen sich australische Universitäten mit IP-Protokollen zusammen, um ihre Netzwerkinfrastrukturen zu vereinheitlichen. AARNet wurde 1989 vom australischen Vizekanzlerausschuss gegründet und bietet ein dediziertes IP-basiertes Netzwerk für Australien. Die erste internationale Internetverbindung Neuseelands wurde im selben Jahr gegründet. Im Mai 1982 hat Südkorea ein Zwei-Node-Inlands-TCP/IP-Netzwerk aufgebaut und im folgenden Jahr einen dritten Knoten hinzugefügt. Japan, das 1984 das UUCP-basierte Netzwerk JUNET aufgebaut hatte, das 1989 mit NSFNET verbunden war und die Verbreitung des Internets nach Asien markierte. Es veranstaltete das jährliche Treffen der Internet Society, INET'92, in Kobe. Singapur entwickelte 1990 TECHNET und Thailand gewann 1992 eine globale Internetverbindung zwischen der Chulalongkorn Universität und UUNET. Dennoch wurden in den späten 1980er und frühen 1990er Jahren Ingenieure, Organisationen und Nationen über die Frage polarisiert, welche Standard, das OSI-Modell oder die Internet-Protokoll-Suite zu den besten und robustesten Computernetzwerken führen würde. Die frühe globale "digitale Teilung" entsteht Während Entwicklungsländer mit technologischen Infrastrukturen ins Internet kamen, begannen Entwicklungsländer eine digitale Trennung zu erleben, die sie vom Internet trennte. Auf einer im Wesentlichen kontinentalen Basis bauen sie Organisationen für die Internet-Ressourcenverwaltung und teilen operative Erfahrungen, da immer mehr Übertragungseinrichtungen in Betrieb gehen. Afrika Zu Beginn der 1990er Jahre setzten sich afrikanische Länder auf X.25 IPSS und 2400 baud modem UUCP-Links für internationale und internetwork Computerkommunikation. Im August 1995, InfoMail Uganda, Ltd., ein privat geführtes Unternehmen in Kampala jetzt bekannt als InfoCom, und NSN Network Services von Avon, Colorado, verkauft 1997 und jetzt bekannt als Clear Channel Satellite, etablierte Afrikas erste native TCP/IP High-Speed-Satelliten-Internet-Dienste. Die Datenverbindung wurde ursprünglich von einem russischen C-Band RSCC-Satelliten getragen, der die Kampala-Büros von InfoMail direkt mit dem MAE-West-Präsenzpunkt von NSN über ein privates Netzwerk von NSN's gemietete Bodenstation in New Jersey verbunden hat. Die erste Satellitenverbindung von InfoCom war nur 64 kbit/s und diente einem Sun-Host-Computer und zwölf US-amerikanischen Robotics-Diams. 1996 begann ein von der USAID gefördertes Projekt, die Leland Initiative, mit der Entwicklung der vollen Internetverbindung für den Kontinent. Guinea, Mosambik, Madagaskar und Ruanda gewannen 1997 Satelliten-Erdstationen, gefolgt von Elfenbeinküste und Benin 1998. Afrika baut eine Internet-Infrastruktur. AFRINIC mit Sitz in Mauritius verwaltet die IP-Adressenzuweisung für den Kontinent. Wie auch die anderen Internet-Regionen gibt es ein operationelles Forum, die Internet-Gemeinschaft der Operational Networking Spezialisten. Es gibt viele Programme, um eine leistungsstarke Übertragungsanlage zu bieten, und die westlichen und südlichen Küsten haben Unterwasser-Optikkabel. Hochgeschwindigkeitskabel verbinden Nordafrika und das Horn von Afrika zu interkontinentalen Kabelsystemen. Die Kabelentwicklung von Undersea ist für Ostafrika langsamer; die ursprüngliche gemeinsame Anstrengung zwischen der Neuen Partnerschaft für Afrikas Entwicklung (NEPAD) und dem Submarinesystem Ostafrikas (Eassy) ist abgebrochen und kann zu zwei Anstrengungen werden. Asien und OzeanienDas Asia Pacific Network Information Centre (APNIC) mit Sitz in Australien verwaltet IP-Adressenzuweisung für den Kontinent. APNIC sponsert ein operationelles Forum, die Asia-Pacific Regional Internet Conference on Operational Technologies (APRICOT). Das erste Internetsystem Südkoreas, das System Development Network (SDN) begann am 15. Mai 1982.SDN wurde im August 1983 mit UUCP (Unixto-Unix-Copy;) verbunden mit CSNET im Dezember 1984 und formell mit dem US-Internet im Jahr 1990 verbunden. 1991 sah die Volksrepublik China ihr erstes TCP/IP-Collegenetzwerk, TUNET der Tsinghua University. Die PRC hat 1994 ihre erste globale Internetverbindung zwischen der Beijing Electro-Spectrometer Collaboration und dem Linear Accelerator Center der Stanford University hergestellt. China ging jedoch fort, seine eigene digitale Teilung durch die Umsetzung eines landesweiten Inhaltsfilters umzusetzen. Lateinamerika Wie bei den anderen Regionen verwaltet das lateinamerikanische und karibische Internet-Adressenregister (LACNIC) den IP-Adressenraum und andere Ressourcen für seinen Bereich. LACNIC, Hauptsitz in Uruguay, betreibt DNS-Root, reverse DNS und andere Schlüsseldienste. 1989–2004: Rise of the global Internet, Web 1.0 Zunächst, wie mit seinen Vorgängernetzwerken, war das System, das sich in das Internet entwickeln würde, in erster Linie für die staatliche und staatliche Körpernutzung. Das Interesse an der kommerziellen Nutzung des Internets wurde jedoch schnell zu einem häufig diskutierten Thema. Obwohl die kommerzielle Nutzung verboten war, war die genaue Definition der kommerziellen Nutzung unklar und subjektiv. UUCPNet und die X.25 IPSS hatten keine solchen Einschränkungen, die schließlich den offiziellen Barring der UUCPNet Nutzung von ARPANET- und NSFNET-Verbindungen sehen würden. (Einige UUCP-Links blieben jedoch immer noch in Verbindung mit diesen Netzwerken, da Administratoren ein blindes Auge auf ihre Operation werfen.) Infolgedessen wurden in den späten 1980er Jahren die ersten Internet-Dienstleister (ISP) Firmen gegründet. Unternehmen wie PSINet, UUNET, Netcom und Portal Software wurden gegründet, um den regionalen Forschungsnetzwerken Service zu bieten und alternative Netzwerkzugriffe, UUCP-basierte E-Mails und Usenet News an die Öffentlichkeit zu bieten. Die erste kommerzielle Wahl ISP in den Vereinigten Staaten war The World, die 1989 eröffnet wurde. 1992 verabschiedete der US-Kongress das Scientific and Advanced-Technology Act, 42 U.S.C § 1862(g), der es NSF erlaubte, den Zugang der Forschungs- und Bildungsgemeinschaften zu Computernetzwerken zu unterstützen, die nicht ausschließlich für Forschungs- und Bildungszwecke verwendet wurden, so dass NSFNET sich mit kommerziellen Netzwerken verbinden konnte. Dies verursachte Kontroversen innerhalb der Forschungs- und Bildungsgemeinschaft, die sich um die kommerzielle Nutzung des Netzes kümmerten, könnte zu einem Internet führen, das weniger auf ihre Bedürfnisse reagierte, und innerhalb der Gemeinschaft von kommerziellen Netzanbietern, die glaubten, dass staatliche Subventionen einigen Organisationen einen unfairen Vorteil verschafften. Die Ziele von ARPANET waren bis 1990 erfüllt und neue Vernetzungstechnologien übertrafen den ursprünglichen Umfang und das Projekt kam zu Ende. Neue Netzwerk-Dienstleister, darunter PSINet, Alternet, CERFNet, ANS CO+RE und viele andere bieten Netzwerk-Zugang zu kommerziellen Kunden. NSFNET war nicht mehr das de facto Backbone und Tauschpunkt des Internets. Die kommerziellen Internet eXchange (CIX,) Metropolitan Area Exchanges (MAEs) und später Network Access Points (NAPs) wurden zu den primären Verbindungen zwischen vielen Netzwerken. Die endgültigen Beschränkungen für die Beförderung von kommerziellem Verkehr endeten am 30. April 1995, als die National Science Foundation ihr Sponsoring des NSFNET Backbone Service beendete und der Service beendete. Die NSF unterstützte zunächst die NAP und die Interimsunterstützung, um den regionalen Forschungs- und Bildungsnetzen den Übergang zu kommerziellen ISP zu erleichtern. NSF unterstützte auch den sehr schnellen Backbone Network Service (vBNS), der weiterhin Unterstützung für die Supercomputing-Zentren und Forschung und Bildung in den Vereinigten Staaten bietet. World Wide Web und Einführung von Browsern Das World Wide Web (manchmal abgekürzt www oder W3) ist ein Informationsraum, in dem Dokumente und andere Web-Ressourcen von URIs identifiziert werden, durch Hypertext-Links verlinkt, und kann über das Internet über einen Webbrowser und (vor kurzem) Web-basierte Anwendungen zugreifen. Es ist einfach als "das Web" bekannt geworden. Seit den 2010er Jahren ist das World Wide Web das primäre Tool Milliarden nutzen, um im Internet zu interagieren, und es hat das Leben der Menschen unermesslich verändert. Vorläufer des Web-Browsers entstanden in Form von Hyperlink-Anwendungen in der Mitte und Ende der 1980er-Jahre (das bloße Konzept der Hyperlinkung hatte dann seit einigen Jahrzehnten bestanden). Im Anschluss daran wird Tim Berners-Lee mit der Erfinderschaft des World Wide Web im Jahr 1989 und der Entwicklung im Jahr 1990 sowohl der erste Webserver, als auch der erste Webbrowser namens WorldWideWeb (keine Leerzeichen) und später Nexus umbenannt.Viele andere wurden bald entwickelt, mit Marc Andreessens 1993 Mosaic (später Netscape), besonders einfach zu bedienen und zu installieren, und oft mit dem Funken des Internet-Booms der 1990er Jahre gutgeschrieben. Andere große Web-Browser waren Internet Explorer, Firefox, Google Chrome, Microsoft Edge, Opera und Safari.NCSA Mosaic war ein grafischer Browser, der auf mehreren beliebten Büro- und Heimcomputern lief. Es wird mit dem ersten bringen Multimedia-Inhalte an nicht-technische Benutzer durch die Aufnahme von Bildern und Text auf der gleichen Seite, im Gegensatz zu früheren Browser-Designs; Marc Andreessen, sein Schöpfer, auch das Unternehmen gegründet, dass im Jahr 1994 veröffentlicht Netscape Navigator, was zu einem der frühen Browser-Kriege führte, als es endete in einem Wettbewerb für Dominanz (die es verloren) mit Microsoft Windows-Recht Explorer, die mit Windows Corporation gebündelt wurde, die wiederum führte. Das Web begann im Jahr 1993-4 allgemein zu verwenden, als Webseiten für den täglichen Gebrauch begannen, verfügbar zu werden. 1995 wurden die Beschränkungen der kommerziellen Nutzung aufgehoben. In den USA bot der Online-Service America Online (AOL) ihren Nutzern eine Verbindung zum Internet über ihren eigenen internen Browser unter Verwendung einer Internetverbindung. Schnellere Breitband-Internetverbindungen haben seit Anfang der 2000er Jahre viele Verbindungsverbindungen ersetzt. Nutzung in der breiteren Gesellschaft Während der ersten zehn Jahre oder so des öffentlichen Internets waren die immensen Veränderungen, die es in den 2000er Jahren schließlich ermöglichen würde, noch nascent. In Bezug auf die Bereitstellung von Kontext für diese Zeit wurden mobile zellulare Geräte (Smartphones und andere zellulare Geräte), die heute einen Nah-Universal-Zugang bieten, für Unternehmen verwendet und nicht eine routinemäßige Haushaltsartikel im Besitz von Eltern und Kindern weltweit. Die sozialen Medien im modernen Sinne mussten noch ins Leben kommen, Laptops waren sperrig und die meisten Haushalte hatten keine Computer. Datenraten waren langsam und die meisten Menschen fehlten Mittel, um Video oder digitalisieren; Medienspeicher wechselte langsam von analogem Band zu digitalen optischen Discs (DVD und in einem Ausmaß noch, Diskette auf CD). Enabling-Technologien aus den frühen 2000er Jahren wie PHP, moderne JavaScript und Java, Technologien wie AJAX, HTML 4 (und seine Betonung auf CSS) und verschiedene Software-Frameworks, die die Geschwindigkeit der Web-Entwicklung ermöglichten und vereinfachten, weitgehend erwartete Erfindung und ihre schließlich weit verbreitete Annahme. Das Internet wurde weit verbreitet für Mailing-Listen, E-Mails, E-Commerce und frühen beliebten Online-Shopping (Amazon und eBay zum Beispiel,) Online-Foren und Bulletin-Boards, und persönliche Websites und Blogs, und die Verwendung wurde schnell zu wachsen, aber durch modernere Standards die verwendeten Systeme waren statische und nicht weit verbreitete soziale Engagement. Es erwartete in den frühen 2000er Jahren eine Reihe von Ereignissen, um sich von einer Kommunikationstechnologie zu ändern, um sich allmählich zu einem wesentlichen Teil der Infrastruktur der globalen Gesellschaft zu entwickeln. Typische Gestaltungselemente dieser Webseiten der "Web 1.0"-Ära enthalten: Statische Seiten statt dynamisches HTML; Inhalte aus Dateisystemen anstelle von relationalen Datenbanken; Seiten, die mit Server-Seite erstellt wurden Includes oder CGI anstelle einer in einer dynamischen Programmiersprache geschriebenen Web-Anwendung; HTML 3.2-era-Strukturen wie Frames und Tabellen, um Seitenlayouts zu erstellen; Online-Gästebücher; Übernutzung von GIF-Buttons und ähnliche kleine Grafiken, die bestimmte Elemente fördern; und HTML-Formen per E-Mail gesendet. (Support für Server-Seiten-Skripting war selten auf gemeinsamen Servern, so dass der übliche Feedback-Mechanismus per E-Mail, mit E-Mail-Formularen und ihrem E-Mail-Programm. Während des Zeitraums 1997 bis 2001 fand die erste spekulative Investitionsblase im Internet statt, bei der Unternehmen (entsprechend der von Unternehmen genutzten .com-Top-Level-Domain) zu überaus hohen Bewertungen angetrieben wurden, da Investoren schnell gehandelte Aktienwerte, gefolgt von einem Marktunfall; die erste Punkt-Com-Blase. Dies verlangsamte jedoch nur vorübergehend Begeisterung und Wachstum, die sich schnell erholten und weiter zu wachsen. Mit dem Aufruf an Web 2.0 wurde der Zeitraum des Internets bis 2004–2005 rückwirkend benannt und von einigen als Web 1.0 2005–präsent beschrieben: Web 2.0, globale ubiquity, Social Media Die Veränderungen, die das Internet als soziales System in seinen Platz bringen würden, fanden in einem relativ kurzen Zeitraum von höchstens fünf Jahren von 2005 bis 2010 statt. Sie enthalten: Der Aufruf zu "Web 2.0" im Jahr 2004 (erster Vorschlag 1999) Beschleunigung der Adoption und Kommoditierung unter den Haushalten und der Vertrautheit mit der notwendigen Hardware (wie Computern).Beschleunigung der Speichertechnologie und Datenzugriffsgeschwindigkeiten – Festplatten entstanden, übernahmen von weit kleineren, langsameren Disketten und wuchsen von Megabyte auf Gigabyte (und um 2010, Terabytes) RAM von Hunderten von Kilobyte zu Gigabyte als typische Mengen auf einem System, und Ethernet, die Aktivierungstechnologie für TCP/IP, bewegte sich von gemeinsamen Geschwindigkeiten von Kilobits zu zehn Megabyte. High-Speed-Internet und breitere Erfassung von Datenverbindungen, zu niedrigeren Preisen, ermöglicht größere Verkehrsraten, zuverlässiger einfacherer Verkehr und Verkehr von mehr Standorten, Die allmählich beschleunigende Wahrnehmung der Fähigkeit von Computern, neue Mittel und Ansätze für die Kommunikation zu schaffen, das Auftauchen von sozialen Medien und Websites wie Twitter und Facebook zu ihrer späteren Bedeutung, und globale Kooperationen wie Wikipedia (die vor, aber prominenz als Ergebnis bestanden haben), die mobile Gesellschaft, die den Zugang bot, Nichtflüchtige RAM wuchs schnell in Größe und Zuverlässigkeit und verringerte sich im Preis, zu einer Ware, die in der Lage ist, hohe Rechenaktivität auf diesen kleinen Handgeräten sowie Solid-State-Laufwerke (SSD) zu ermöglichen. Ein Schwerpunkt auf leistungseffizientem Prozessor- und Gerätedesign und nicht auf rein hoher Verarbeitungsleistung; einer der Begünstigten davon war ARM, ein britisches Unternehmen, das sich seit den 1980er Jahren auf leistungsfähige, aber kostengünstige einfache Mikroprozessoren konzentrierte. ARM-Architektur erlangte im Markt für mobile und eingebettete Geräte schnell Dominanz. Der Begriff "Web 2.0" beschreibt Webseiten, die nutzergenerierte Inhalte (einschließlich User-to-User-Interaktion) Usability und Interoperabilität hervorheben. Es erschien zuerst in einem Januar 1999 Artikel namens "Fragmented Future" geschrieben von Darcy DiNucci, ein Berater für elektronische Informationsdesign, wo sie schrieb: "Das Web, das wir jetzt kennen, das in ein Browser-Fenster in im Wesentlichen statischen Drehbüchern lädt, ist nur ein Embryo des Web zu kommen. Die ersten Glierungen von Web 2.0 beginnen zu erscheinen, und wir beginnen nur zu sehen, wie sich dieser Embryo entwickeln könnte. Das Web wird nicht als Screenful von Text und Grafik verstanden, sondern als Transportmechanismus, der Ether, durch den Interaktivität geschieht. Es wird [...] auf Ihrem Computer-Bildschirm erscheinen, [...] auf Ihrem TV-Set [...] Ihr Auto Dashboard [...] Ihr Handy [...] Handheld-Spielmaschinen [...] vielleicht sogar Ihre Mikrowelle. " Der in den Jahren 2002–2004 wiederaufgetretene Begriff wurde Ende 2004 nach Präsentationen von Tim O'Reilly und Dale Dougherty auf der ersten Web 2.0 Konferenz ins Leben gerufen. John Battelle und Tim O'Reilly skizzierten in ihren Eröffnungsrezensionen ihre Definition des "Web as Platform", wo Software-Anwendungen auf dem Web im Gegensatz zum Desktop gebaut werden. Der einzigartige Aspekt dieser Migration, argumentierten sie, ist, dass "Kunden bauen Ihr Geschäft für Sie". Sie argumentierten, dass die Aktivitäten von Nutzern, die Inhalte generieren (in Form von Ideen, Text, Videos oder Bildern), zur Wertschöpfung genutzt werden könnten. Web 2.0 bezieht sich nicht auf ein Update auf eine technische Spezifikation, sondern auf kumulative Änderungen in der Art und Weise, wie Webseiten gemacht und verwendet werden. Web 2.0 beschreibt einen Ansatz, bei dem sich Websites im Wesentlichen darauf konzentrieren, Nutzer in einem Social-Media-Dialog als Schöpfer von nutzergenerierten Inhalten in einer virtuellen Gemeinschaft miteinander zu interagieren und zu kooperieren, im Gegensatz zu Webseiten, auf denen die Menschen auf die passive Darstellung von Inhalten beschränkt sind. Beispiele für Web 2.0 sind Social Networking-Dienste, Blogs, Wikis, Folksonomien, Video-Sharing-Seiten, Hosted Services, Web-Anwendungen und Maschups. Terry Flew, in seiner 3. Ausgabe von New Media beschrieben, was er glaubte, die Unterschiede zwischen Web 1.0 und Web 2.0 zu charakterisieren: ["The] bewegen sich von persönlichen Websites zu Blogs und Blog-Site-Aggregation, von der Veröffentlichung bis zur Teilnahme, von Webinhalten als Ergebnis von großen Vor-Ort-Investitionen zu einem laufenden und interaktiven Prozess, und von Content Management Systemen zu Links basierend auf Tagsging (folksonomy)". In dieser Ära wurden mehrere Haushaltsnamen durch ihre gemeinschaftsorientierte Operation an Bedeutung gewonnen – YouTube, Twitter, Facebook, Reddit und Wikipedia sind einige Beispiele. Die mobile Revolution Der Veränderungsprozess, der im Allgemeinen mit "Web 2.0" zusammenfiel, war selbst stark beschleunigt und erst kurze Zeit später durch das zunehmende Wachstum mobiler Geräte transformiert.Diese mobile Revolution bedeutete, dass Computer in Form von Smartphones zu etwas wurden, das viele Menschen verwendet, mit ihnen überall genommen, mit ihnen kommuniziert, für Fotos und Videos verwendet, die sie sofort geteilt oder zu kaufen oder suchen Informationen "auf dem Umzug" - und verwendet sozial, im Gegensatz zu Gegenständen auf einem Schreibtisch zu Hause oder nur für die Arbeit verwendet. Standortbasierte Dienstleistungen, Dienstleistungen mit Standort- und anderen Sensorinformationen sowie Crowdsourcing (häufig, aber nicht immer standortbasierte) wurden gemeinsam, mit Beiträgen, die von Standort verschlagwortet werden, oder Webseiten und Dienstleistungen, die sich der Lage bewusst werden. Mobile-targeted Websites (wie m.website.com) wurden häufig, speziell für die neuen Geräte entwickelt. Netbooks, Ultrabooks, weit verbreitete 4G und Wi-Fi, und mobile Chips, die in fast der Macht der Desktops aus nicht vielen Jahren vor der weitaus geringeren Stromnutzung fähig oder laufen, wurden zu Enabler dieser Phase der Internet-Entwicklung, und der Begriff App entstanden (kurz für "Anwendungsprogramm" oder Programm) wie der "App Store". Diese "mobile Revolution" hat es den Menschen erlaubt, eine fast unbegrenzte Menge an Informationen an ihren Fingerspitzen zu haben. Mit der Fähigkeit, das Internet von Handys zu erreichen, kam eine Veränderung in der Art, wie wir Medien konsumieren. In der Tat, Blick auf Medienkonsum Statistiken, über die Hälfte des Medienkonsums zwischen denen im Alter von 18 und 34 wurden mit einem Smartphone. Vernetzung im Außenraum Am 22. Januar 2010 wurde der erste Internet-Link in Low Earth Orbit gegründet, als Astronaut T. J. Creamer das erste ununterstützte Update auf seinen Twitter-Account von der International Space Station veröffentlichte und die Erweiterung des Internets ins All markierte. (Astronauten auf der ISS hatten zuvor E-Mail und Twitter verwendet, aber diese Nachrichten wurden über einen NASA-Datenlink an den Boden weitergeleitet, bevor sie von einem menschlichen Proxy veröffentlicht wurden.) Dieser persönliche Web-Zugang, den die NASA das Crew Support LAN nennt, nutzt den High-Speed-Ku-Band-Mikrowelle-Link der Raumstation. Um das Internet zu surfen, können Astronauten einen Laptop-Rechner auf der Erde mit einem Computer auf dem PC steuern und mit ihren Familien und Freunden auf der Erde über Voice over IP-Geräte sprechen. Die Kommunikation mit Raumfahrzeugen über Erdorbit hinaus war traditionell über Punkt-zu-Punkt-Verbindungen durch das Deep Space Network. Jede solche Datenverbindung muss manuell geplant und konfiguriert werden. In den späten 1990er Jahren begannen NASA und Google an einem neuen Netzwerkprotokoll zu arbeiten, Delay-tolerante Vernetzung (DTN), die diesen Prozess automatisiert, ermöglicht die Vernetzung von raumgestützten Übertragungsknoten und trägt der Tatsache Rechnung, dass Raumfahrzeug vorübergehend Kontakt verlieren kann, weil sie sich hinter dem Mond oder Planeten bewegen, oder weil Raumwetter die Verbindung stört. Unter diesen Bedingungen sendet DTN Datenpakete zurück, anstatt sie fallen zu lassen, wie das Standard TCP/IP Internet Protocol tut. Die NASA führte im November 2008 den ersten Feldtest dessen durch, was sie das "Deep Space Internet" nennt. Die Prüfung der DTN-basierten Kommunikation zwischen der Internationalen Raumstation und der Erde (jetzt Disruption-Tolerant Networking genannt) ist seit März 2009 angelaufen und wird bis März 2014 fortgesetzt. Diese Netzwerktechnologie soll letztendlich Missionen ermöglichen, die mehrere Raumfahrzeuge umfassen, in denen eine zuverlässige inter-vessel-Kommunikation Vorrang vor Schiffs-zu-Erde-Downlinks haben könnte. Nach einer Erklärung von Googles Vint Cerf im Februar 2011 wurden die sogenannten "Bundle-Protokolle" auf die NASA-Missionsraumfahrzeuge der EPOXI (die sich um die Sonne herum befindet) hochgeladen und die Kommunikation mit der Erde in einer Entfernung von etwa 80 Lichtsekunden getestet. Internet-Government Als global verteiltes Netz von freiwillig vernetzten autonomen Netzwerken arbeitet das Internet ohne zentrale Leitungsorgane. Jedes konstituierende Netzwerk wählt die Technologien und Protokolle aus den technischen Standards, die von der Internet Engineering Task Force (IETF) entwickelt werden. Eine erfolgreiche Vernetzung vieler Netzwerke erfordert jedoch bestimmte Parameter, die im gesamten Netzwerk gemeinsam sein müssen. Für die Verwaltung dieser Parameter überwacht die Internetzuweisungsnummernbehörde (IANA) die Zuordnung und Zuordnung verschiedener technischer Kennungen. Darüber hinaus bietet die Internet Corporation for Assigned Names and Numbers (ICANN) Aufsicht und Koordination für die beiden wichtigsten Namensräume im Internet, den Internet Protocol Adressraum und das Domain Name System. NIC, InterNIC, IANA und ICANN Die IANA-Funktion wurde ursprünglich vom USC Information Sciences Institute (ISI) durchgeführt und delegierte Teile dieser Verantwortung in Bezug auf numerische Netzwerk- und autonome Systemkennzeichen an das Network Information Center (NIC) am Stanford Research Institute (SRI International) in Menlo Park, Kalifornien.ISI Jonathan Postel leitete die IANA, diente als RFC-Editor und spielte andere Schlüsselrollen bis zu seinem vorzeitigen Tod im Jahr 1998. Als das frühe ARPANET wuchs, wurden Hosts mit Namen und einem HOSTS bezeichnet. Die TXT-Datei würde von SRI International auf jeden Host im Netzwerk verteilt werden. Als das Netzwerk wuchs, wurde dies umständlich. Eine technische Lösung kam in Form des Domain Name Systems, erstellt von ISI Paul Mockapetris 1983. Das Defense Data Network – Network Information Center (DDN-NIC) bei SRI behandelte alle Registrierungsdienste, einschließlich der Top-Level-Domains (TLDs) von .mil, .gov, .edu, .org, .net, .comand .us, Root Nameserver Administration und Internet-Nummer Aufgaben unter einem United States Department of Defense Contract. 1991 hat die Defense Information Systems Agency (DISA) die Verwaltung und Wartung von DDN-NIC (managed by SRI bis zu diesem Punkt) an die Regierung Systems, Inc. vergeben, die sie an die kleinen privaten Netzwerklösungen,Inc. Die zunehmende kulturelle Vielfalt des Internets stellte auch administrative Herausforderungen für die zentralisierte Verwaltung der IP-Adressen. Im Oktober 1992 veröffentlichte die Internet Engineering Task Force (IETF) RFC 1366, die das "Wach des Internets und seine zunehmende Globalisierung" beschreibt und die Grundlage für eine Entwicklung des IP-Registrierungsprozesses auf Basis eines regional verteilten Registrierungsmodells bildete. Dieses Dokument betonte, dass in jedem geografischen Gebiet der Welt eine einzige Internet-Nummern-Registrierung existiert (die "kontinentale Dimensionen"). Kanzler würden innerhalb ihrer Region "unvoreingenommen und von Netzanbietern und Abonnenten allgemein anerkannt" werden. Das RIPE Network Coordination Centre (RIPE NCC) wurde im Mai 1992 als erster RIR gegründet. Das zweite RIR, das Asia Pacific Network Information Centre (APNIC), wurde 1993 in Tokio als Pilotprojekt der Asia Pacific Networking Group gegründet. Da zu diesem Zeitpunkt in der Geschichte die meisten Wachstum im Internet aus nicht-militärischen Quellen kam, wurde beschlossen, dass das Verteidigungsministerium keine Registrierungsdienste außerhalb der .mil TLD mehr finanzieren würde. 1993 hat die U.S National Science Foundation nach einem wettbewerbsorientierten Bieterverfahren 1992 das InterNIC geschaffen, um die Zuordnungen von Adressen und Verwaltung der Adressdatenbanken zu verwalten und den Auftrag an drei Organisationen zu vergeben. Die Registrierungsdienste würden von Network Solutions erbracht; Verzeichnisse und Datenbankdienste würden von AT&T erbracht werden; und Informationsdienste würden von General Atomics bereitgestellt werden. Im Laufe der Zeit wurde nach Konsultation mit der IANA, der IETF, RIPE NCC, APNIC und dem Federal Networking Council (FNC) beschlossen, die Verwaltung von Domain-Namen von der Verwaltung von IP-Nummern zu trennen. Im Anschluss an die Beispiele von RIPE NCC und APNIC wurde empfohlen, dass die Verwaltung von IP-Adressenraum, die dann von der InterNIC verwaltet werden, unter der Kontrolle derjenigen sein sollte, die es verwenden, insbesondere die ISPs, Endbenutzerorganisationen, Unternehmen, Universitäten und Einzelpersonen. Das amerikanische Register für Internet-Nummern (ARIN) wurde im Dezember 1997 als unabhängige, gemeinnützige Gesellschaft durch die Leitung der National Science Foundation gegründet und wurde zum dritten regionalen Internetregister. Im Jahr 1998 wurden sowohl die IANA als auch die verbleibenden DNS-bezogenen InterNIC-Funktionen unter der Kontrolle von ICANN neu organisiert, einem von der United States Department of Commerce beauftragten kalifornischen Non-Profit-Unternehmen, um eine Reihe von Internet-bezogenen Aufgaben zu verwalten. Da es sich bei diesen Aufgaben um eine technische Koordinierung für zwei Haupt-Internet-Namensräume (DNS-Namen und IP-Adressen) des IETF handelte, unterzeichnete ICANN mit dem IAB auch ein Verständnis für die technische Arbeit, die von der Internet Assigned Numbers Authority durchgeführt werden soll. Die Verwaltung des Internet-Adressenraums blieb mit den regionalen Internet-Registern, die gemeinsam als unterstützende Organisation innerhalb der ICANN-Struktur definiert wurden. ICANN bietet eine zentrale Koordination für das DNS-System, einschließlich der politischen Koordination für das Split-Registrierungs- / Registrar-System, mit dem Wettbewerb zwischen den Registrierungsdienstleistern zu jeder Top-Level-Domain und mehreren konkurrierenden Registrars, die DNS-Dienste für Endbenutzer anbieten. Aufgabenbereich der Internettechnik Die Internet Engineering Task Force (IETF) ist die größte und sichtbarste von mehreren lose verwandten Ad-hoc-Gruppen, die technische Richtung für das Internet bieten, einschließlich der Internet Architecture Board (IAB), der Internet Engineering Steering Group (IESG,) und der Internet Research Task Force (IRTF.)Der IETF ist eine lose selbstorganisierte Gruppe internationaler Freiwilliger, die zur Entwicklung und Entwicklung von Internettechnologien beitragen. Es ist der Hauptkörper in der Entwicklung neuer Internet-Standard-Spezifikationen. Ein Großteil der Arbeit des IETF wird in Arbeitsgruppen organisiert. Die Standardisierungsbemühungen der Arbeitsgruppen werden oft von der Internet-Community übernommen, aber der IETF kontrolliert oder patrouilliert das Internet nicht. Der IETF wuchs von vierteljährlichen Treffen mit US-amerikanischen staatlich geförderten Forschern ab Januar 1986. Nichtregierungsvertreter wurden von der vierten IETF-Sitzung im Oktober 1986 eingeladen. Das Konzept der Arbeitsgruppen wurde auf der fünften Tagung im Februar 1987 vorgestellt. Das siebte Treffen im Juli 1987 war das erste Treffen mit mehr als hundert Teilnehmern. 1992 wurde die Internetgesellschaft, eine professionelle Mitgliedsgesellschaft, gegründet und die IETF begann unter ihr als unabhängige internationale Normstelle zu arbeiten. Das erste IETF-Treffen außerhalb der Vereinigten Staaten fand im Juli 1993 in Amsterdam, den Niederlanden, statt. Heute trifft der IETF dreimal pro Jahr und die Teilnahme ist so hoch wie ca. 2.000 Teilnehmer. Typischerweise finden in drei IETF-Treffen in Europa oder Asien statt. Die Zahl der Nicht-US-Besucher beträgt in der Regel ca. 50,% sogar bei Treffen in den Vereinigten Staaten. Der IETF ist keine juristische Person, hat keinen Vorstand, keine Mitglieder und keine Forderungen. Der nächste Status, der der Mitgliedschaft ähnelt, befindet sich auf einer Mailingliste der IETF- oder Arbeitsgruppen. IETF-Freiwillige kommen aus der ganzen Welt und aus vielen verschiedenen Teilen der Internet-Community. Der IETF arbeitet eng mit der Internet Engineering Steering Group (IESG) und dem Internet Architecture Board (IAB) zusammen. Die Internet Research Task Force (IRTF) und die Internet Research Steering Group (IRSG), die Aktivitäten der IETF und der IESG unter der allgemeinen Aufsicht des IAB, konzentrieren sich auf langfristige Forschungsfragen. Anfrage für Kommentare Anfrage (RFCs) sind die wichtigsten Unterlagen für die Arbeit der IAB, IESG, IETF und IRTF. RFC 1, "Host Software", geschrieben von Steve Crocker bei UCLA im April 1969, kurz bevor die IETF erstellt wurde. Ursprünglich waren sie technische Memos, die Aspekte der ARPANET-Entwicklung dokumentierten und von Jon Postel, dem ersten RFC-Editor, bearbeitet wurden. RFCs umfassen eine breite Palette von Informationen aus vorgeschlagenen Standards, Entwürfen von Standards, Vollstandards, Best Practices, experimentellen Protokollen, Geschichte und anderen Informationsthemen. RFCs können von Einzelpersonen oder informellen Gruppen von Einzelpersonen geschrieben werden, aber viele sind das Produkt einer formaleren Arbeitsgruppe. Die Entwürfe werden der IESG entweder von Einzelpersonen oder vom Vorsitzenden der Arbeitsgruppe vorgelegt. Ein RFC-Editor, der vom IAB ernannt wird, getrennt von IANA, und in Verbindung mit dem IESG arbeitet, erhält Entwürfe von der IESG und bearbeitet, Formate und veröffentlicht sie. Sobald ein RFC veröffentlicht wird, wird es nie überarbeitet. Wenn der Standard, den er beschreibt, Änderungen beschreibt oder seine Informationen veraltet werden, werden die überarbeiteten Standard- oder aktualisierten Informationen als neue RFC veröffentlicht, die das Original veraltet. Die Internet SocietyThe Internet Society (ISOC) ist eine internationale, gemeinnützige Organisation, die 1992 gegründet wurde, um die offene Entwicklung, Entwicklung und Nutzung des Internets zum Nutzen aller Menschen auf der ganzen Welt zu gewährleisten. Mit Niederlassungen in der Nähe von Washington, DC, USA und in Genf, Schweiz, hat ISOC eine Mitgliedschaftsbasis mit mehr als 80 organisatorischen und mehr als 50.000 Einzelmitgliedern. Die Mitglieder bilden auch Kapitel, die auf gemeinsamen geografischen Standorten oder besonderen Interessen beruhen. Es gibt derzeit mehr als 90 Kapitel auf der ganzen Welt. ISOC bietet finanzielle und organisatorische Unterstützung und fördert die Arbeit der Standards-Einstellungen Einrichtungen, für die es das Organisationsheim ist: die Internet Engineering Task Force (IETF,) das Internet Architecture Board (IAB), die Internet Engineering Steering Group (IESG,) und die Internet Research Task Force (IRTF). ISOC fördert auch das Verständnis und die Wertschätzung des Internetmodells offener, transparenter Prozesse und konsensbasierter Entscheidungsfindung. Globalisierung und Internet-Governance im 21. Jahrhundert Seit den 1990er Jahren ist die Governance und Organisation des Internets für Regierungen, Handel, Zivilgesellschaft und Einzelpersonen von globaler Bedeutung. Die Organisationen, die die Kontrolle über bestimmte technische Aspekte des Internets abhielten, waren die Nachfolger der alten ARPANET-Übersicht und die aktuellen Entscheidungsträger in den heutigen technischen Aspekten des Netzes.Obwohl sie als Verwalter bestimmter Aspekte des Internets anerkannt werden, sind ihre Rollen und ihre Entscheidungsbefugnisse begrenzt und unterliegen einer zunehmenden internationalen Kontrolle und zunehmenden Einwänden. Diese Einwände haben dazu geführt, dass sich die ICANN im Jahr 2000 aus Beziehungen mit der ersten Universität von Südkalifornien entfernte und im September 2009 die Autonomie der US-Regierung bis zum Ende ihrer langjährigen Vereinbarungen erlangte, obwohl einige vertragliche Verpflichtungen mit der US-Handelsabteilung weitergingen. Am 1. Oktober 2016 beendete die ICANN ihren Vertrag mit der United States Department of Commerce National Telecommunications and Information Administration (NTIA), wodurch die Aufsicht an die globale Internet-Community weitergegeben werden konnte. Der IETF, mit finanzieller und organisatorischer Unterstützung der Internet Society, dient weiterhin als Ad-hoc-Standards des Internets Körper und Probleme Anfrage für Kommentare. Im November 2005 forderte der Weltgipfel zur Informationsgesellschaft in Tunis ein Internet Governance Forum (IGF), das vom Generalsekretär der Vereinten Nationen einberufen wird. Die IGF eröffnete ein kontinuierliches, unverbindliches Gespräch zwischen Interessenvertretern, die Regierungen, den Privatsektor, die Zivilgesellschaft und die technischen und akademischen Gemeinschaften über die Zukunft der Internet-Governance vertreten. Das erste IGF-Treffen fand im Oktober/November 2006 mit anschließenden Treffen jährlich statt. Seit dem WSIS wurde der Begriff "Internet-Governance" über enge technische Anliegen hinaus erweitert, um ein breiteres Spektrum von Fragen im Internet zu beinhalten. Tim Berners-Lee, Erfinder des Internets, wurde besorgt über Bedrohungen für die Zukunft des Internets und im November 2009 auf der IGF in Washington DC startete die World Wide Web Foundation (WWWF), um das Internet zu einem sicheren und befähigenden Werkzeug für das Wohl der Menschheit mit Zugang zu allen zu machen. Im November 2019 starteten Berners-Lee und der WWWF auf der IGF in Berlin den Vertrag für das Internet, eine Kampagnen-Initiative, um Regierungen, Unternehmen und Bürger zu überzeugen, neun Prinzipien zu verpflichten, Missbrauch mit der Warnung zu stoppen "Wenn wir jetzt nicht handeln - und gemeinsam handeln -, um zu verhindern, dass das Internet von denen, die ausnutzen, teilen und untergraben wollen, wir sind in Gefahr von Schrecken" (als möglich). Politisierung des Internets Aufgrund seiner Vorherrschaft und Unmittelbarkeit als wirksames Mittel der Massenkommunikation ist das Internet auch politisiert geworden, wie es gewachsen ist. Dies hat wiederum zu Diskursen und Aktivitäten geführt, die einst auf andere Weise stattgefunden hätten, um von Internet vermittelt zu werden. Beispiele sind politische Aktivitäten wie öffentliche Proteste und Unterstützungs- und Abstimmungsunterlagen, aber auch: Die Verbreitung von Ideen und Meinungen; Rekrutierung von Anhängern und "zusammenkommen" von Mitgliedern der Öffentlichkeit, für Ideen, Produkte und Ursachen; Bereitstellung und Verbreitung und Weitergabe von Informationen, die als sensibel angesehen werden oder sich auf Whistleblowing (und Bemühungen von bestimmten Ländern, dies durch Zensur zu verhindern;) Straffung und Terrorismus (und daraus resultierende Rechtsvollstreckung Verwendung, zusammen mit seiner Erleichterung durch Massenüberwachung;) Politische Fake-Aktion Nettoneutralität Am 23. April 2014 wurde die Federal Communications Commission (FCC) gemeldet, um eine neue Regel in Betracht zu ziehen, die es Internet-Dienstleistern ermöglichen würde, Content Providern einen schnelleren Weg zu bieten, um Inhalte zu senden und damit ihre frühere Nettoneutralitätsposition umzukehren. Eine mögliche Lösung für Netzneutralitätsbedenken kann das kommunale Breitband sein, so Professor Susan Crawford, Rechts- und Technologieexperte der Harvard Law School. Am 15. Mai 2014 beschloss die FCC, zwei Optionen in Bezug auf Internet-Dienste zu prüfen: erstens erlauben schnelle und langsame Breitband-Lanen, wodurch die Nettoneutralität beeinträchtigt wird; zweitens, klassifizieren Breitband als Telekommunikationsdienst, wodurch die Netzneutralität erhalten bleibt. Am 10. November 2014 empfahl Präsident Obama den FCC, Breitband-Internetdienst als Telekommunikationsdienst neu einzustufen, um die Nettoneutralität zu erhalten. Am 16. Januar 2015 präsentierten die Republikaner Rechtsvorschriften in Form eines US-Kongresses HR-Diskussionsentwurfs, der Konzessionen zur Nettoneutralität macht, aber die FCC verbietet, das Ziel zu erreichen oder weitere Regelungen zu erlassen, die Internet-Dienstleister (ISPs) betreffen. Am 31. Januar 2015 berichtete AP News, dass die FCC den Begriff der Anwendung ("mit einigen Höhlen")Title II (gemeinsamer Träger) des Kommunikationsgesetzes von 1934 in einer am 26. Februar 2015 erwarteten Abstimmung auf das Internet präsentieren wird.Die Annahme dieses Begriffs würde den Internet-Service von einer der Informationen auf eine der Telekommunikation neu klassifizieren und nach Tom Wheeler, Vorsitzender des FCC, die Nettoneutralität gewährleisten. Die FCC wird erwartet, dass die Nettoneutralität in ihrer Abstimmung durchgesetzt wird, so The New York Times. Am 26. Februar 2015 regierte die FCC zugunsten der Nettoneutralität durch die Anwendung von Titel II (gemeinsamer Träger) des Kommunikationsgesetzes von 1934 und Abschnitt 706 des Telekommunikationsgesetzes von 1996 auf das Internet. Der FCC-Vorsitzende, Tom Wheeler, sagte: "Dies ist kein Plan mehr, das Internet zu regulieren, als der erste Änderungsantrag ein Plan zur Regulierung der freien Rede ist. Beide stehen für dasselbe Konzept. "Am 12. März 2015 veröffentlichte die FCC die spezifischen Details der Nettoneutralitätsregeln. Am 13. April 2015 veröffentlichte die FCC die endgültige Regel über ihre neuen "Net Neutrality"-Verordnungen. Am 14. Dezember 2017 hat die FCC ihre Entscheidung vom 12. März 2015 mit einer 3–2 Stimme über die Nettoneutralitätsregeln aufgehoben. Verwendung und Kultur E-Mail und Usenet E-Mail wurde oft als Killer-Anwendung des Internets bezeichnet. Es predatet das Internet, und war ein entscheidendes Werkzeug, um es zu schaffen. E-Mail begann im Jahr 1965 als Weg für mehrere Benutzer eines Zeit-Sharing-Mainframe-Computer zu kommunizieren. Obwohl die Geschichte unbeschädigt ist, waren unter den ersten Systemen eine solche Anlage die System Development Corporation (SDC) Q32 und das kompatible Time-Sharing System (CTSS) am MIT. Das ARPANET-Computernetzwerk leistete einen großen Beitrag zur Entwicklung der elektronischen Post. Ein experimentelles Inter-System überwies kurz nach seiner Erstellung auf das ARPANET. 1971 erstellte Ray Tomlinson, was das Standardformat für elektronische E-Mail-Adressen im Internet werden sollte, mit dem @-Zeichen, um Mailbox-Namen von Hostnamen zu trennen. Es wurden mehrere Protokolle entwickelt, um Nachrichten zwischen Gruppen von Zeitaustausch-Computern über alternative Übertragungssysteme wie UUCP und IBMs VNET-E-Mail-System zu liefern. E-Mail könnte auf diese Weise zwischen einer Reihe von Netzwerken, einschließlich ARPANET, BITNET und NSFNET, sowie Hosts direkt mit anderen Websites über UUCP übertragen werden. Siehe die Geschichte des SMTP-Protokolls. Darüber hinaus erlaubte UUCP die Veröffentlichung von Textdateien, die von vielen anderen gelesen werden könnten. Die von Steve Daniel und Tom Truscott im Jahr 1979 entwickelte News-Software wurde verwendet, um Nachrichten und schussähnliche Nachrichten zu verbreiten. Dies wuchs schnell in Diskussionsgruppen, die als Newsgroups bekannt sind, zu einer Vielzahl von Themen. Auf ARPANET und NSFNET würden sich ähnliche Diskussionsgruppen über Mailing-Listen bilden, wobei sowohl technische Fragen als auch kulturell fokussierte Themen diskutiert werden (z.B. Science Fiction, diskutiert über die Sflovers Mailingliste). In den frühen Jahren des Internets waren E-Mails und ähnliche Mechanismen auch von grundlegender Bedeutung, um den Menschen den Zugang zu Ressourcen zu ermöglichen, die aufgrund der fehlenden Online-Konnektivität nicht verfügbar waren. UUCP wurde häufig verwendet, um Dateien mit den alt.binären Gruppen zu verteilen. Auch FTP-E-Mail-Gateways erlaubte Menschen, die außerhalb der USA und Europa lebten, Dateien mit ftp-Befehle, die innerhalb von E-Mail-Nachrichten geschrieben herunterladen. Die Datei wurde kodiert, in Stücke gebrochen und per E-Mail gesendet; der Empfänger musste es später wieder zusammenbauen und decodieren, und es war der einzige Weg für Menschen, die in Übersee leben, um Objekte wie die früheren Linux-Versionen mit den langsamen Dia-up-Verbindungen, die zur Zeit zur Verfügung. Nach der Popularisierung des Web und des HTTP-Protokolls wurden solche Tools langsam aufgegeben. Von Gopher zum WWW Als das Internet in den 1980er und frühen 1990er Jahren wuchs, erkannten viele Menschen die zunehmende Notwendigkeit, Dateien und Informationen zu finden und zu organisieren. Projekte wie Archie, Gopher, WAIS und die FTP-Archivliste versuchten, Wege zu schaffen, verteilte Daten zu organisieren. Anfang der 1990er Jahre bot Gopher, erfunden von Mark P. McCahill, eine tragfähige Alternative zum World Wide Web. 1993 sah das World Wide Web jedoch viele Fortschritte bei der Indexierung und dem einfachen Zugang durch Suchmaschinen, die Gopher und Gopherspace oft vernachlässigten. Da die Popularität durch einfache Nutzung zugenommen hat, wuchsen auch Investitionsanreize bis Mitte 1994 die Popularität der WWW an der Spitze. Dann wurde klar, dass Gopher und die anderen Projekte dazu verdammt waren, kurz zu fallen. Eines der vielversprechendsten Benutzeroberfläche Paradigmen während dieser Zeit war Hypertext. Die Technologie wurde von Vannevar Bushs Memex inspiriert und durch Ted Nelsons Forschung zum Projekt Xanadu, Douglas Engelbarts Forschung zu NLS und Augment und Andries van Dams Forschung von HES im Jahr 1968, durch FRESS, Intermedia und andere entwickelt.Auch viele kleine, selbstbehaftete Hypertextsysteme wurden erstellt, wie etwa die HyperCard von Apple Computer (1987). Gopher wurde die erste häufig verwendete Hypertext-Schnittstelle im Internet. Während Gopher Menüpunkte Beispiele für Hypertext waren, wurden sie auf diese Weise nicht allgemein wahrgenommen. Im Jahr 1989 erfand Tim Berners-Lee bei der Arbeit am CERN eine netzbasierte Implementierung des Hypertext-Konzepts. Durch die Freilassung seiner Erfindung zur öffentlichen Nutzung ermutigte er eine weit verbreitete Nutzung. Für seine Arbeit bei der Entwicklung des World Wide Web erhielt Berners-Lee 2004 den Millennium-Technologiepreis. Ein früher populärer Webbrowser, nach HyperCard modeled, war ViolaWWW. Ein Wendepunkt für das World Wide Web begann mit der Einführung des Mosaic Webbrowsers im Jahr 1993, einem grafischen Browser, der von einem Team am National Center for Supercomputing Applications an der Universität Illinois an Urbana-Champaign (NCSA-UIUC) unter der Leitung von Marc Andreessen entwickelt wurde. Die Förderung von Mosaic stammte aus der High-Performance Computing and Communications Initiative, einem Förderprogramm, das durch das High Performance Computing and Communication Act von 1991 initiiert wurde, auch bekannt als "Gore Bill". Mosaics grafische Schnittstelle wurde bald populärer als Gopher, die damals hauptsächlich textbasiert war, und die WWW wurde die bevorzugte Schnittstelle für den Internetzugang. (Gore's Hinweis auf seine Rolle bei der "Erstellung des Internets", wurde jedoch in seinem Präsidentschaftswahlkampf lächerlich gemacht. Siehe den vollständigen Artikel Al Gore und Informationstechnologie). Mosaic wurde 1994 von Andreessens Netscape Navigator überholt, der Mosaic als den beliebtesten Browser der Welt ersetzte. Während es diesen Titel für einige Zeit, schließlich Wettbewerb von Internet Explorer und eine Vielzahl von anderen Browsern fast komplett verschoben es. Eine weitere wichtige Veranstaltung fand am 11. Januar 1994 statt, war der SuperHighway Summit in der Royce Hall der UCLA. Dies war die "erste öffentliche Konferenz, die alle großen Industrie-, Regierungs- und akademischen Führer auf diesem Gebiet zusammenbringt [und] begann auch den nationalen Dialog über die Informations-Super-Highway und ihre Auswirkungen. " 24Hours in Cyberspace, "das größte eintägige Online-Event" (8. Februar 1996) bis zu diesem Datum, fand auf der dann-aktiven Website, cyber24.com. Es wurde von dem Fotografen Rick Smolan geleitet. Am 23. Januar 1997 wurde eine fotografische Ausstellung im Nationalmuseum der amerikanischen Geschichte der Smithsonian Institution mit 70 Fotos aus dem Projekt vorgestellt. Suchmaschinen Noch vor dem World Wide Web gab es Suchmaschinen, die versuchten, das Internet zu organisieren. Die erste davon war die Archie Suchmaschine der McGill University im Jahr 1990, gefolgt von WAIS und Gopher. Alle drei dieser Systeme predierten die Erfindung des World Wide Web, aber alle fuhren fort, das Web und den Rest des Internets für mehrere Jahre nach dem Web erschienen. Es gibt noch Gopher Server seit 2006, obwohl es eine Vielzahl von mehr Webservern gibt. Als das Web wuchs, wurden Suchmaschinen und Web-Verzeichnisse erstellt, um Seiten im Web zu verfolgen und Menschen zu ermöglichen, Dinge zu finden. Die erste Volltext-Web-Suchmaschine war WebCrawler 1994. Vor WebCrawler wurden nur Web-Seitentitel durchsucht. Eine weitere frühe Suchmaschine, Lycos, wurde 1993 als Universitätsprojekt gegründet und war die erste kommerzielle Erfolg zu erzielen. In den späten 1990er Jahren waren sowohl Web-Verzeichnisse als auch Web-Suchmaschinen beliebt –Yahoo!(gegründet 1994) und Altavista (gegründet 1995) waren die jeweiligen Branchenführer. Bis August 2001 hatte das Verzeichnismodell begonnen, den Suchmaschinen einen Weg zu geben und den Aufstieg von Google (gegründet 1998) zu verfolgen, der neue Ansätze zur Relevanz-Ranking entwickelt hatte. Verzeichnis-Features, obwohl immer noch allgemein verfügbar, wurden nach-Thoughts für Suchmaschinen. Die Datenbankgröße, die in den frühen 2000er Jahren ein bedeutendes Marketing-Feature war, wurde ebenfalls durch die Betonung der Relevanz-Ranking verschoben, die Methoden, mit denen Suchmaschinen versuchen, die besten Ergebnisse zuerst zu sortieren. Relevanz Ranking wurde zunächst ein wichtiges Thema um 1996, als es offensichtlich wurde, dass es unpraktisch war, vollständige Liste der Ergebnisse zu überprüfen. Infolgedessen haben sich Algorithmen zur Relevanzrangierung kontinuierlich verbessert. Die PageRank-Methode von Google zur Bestellung der Ergebnisse hat die meisten Presse erhalten, aber alle großen Suchmaschinen verfeinern kontinuierlich ihre Ranking-Methoden mit dem Ziel, die Reihenfolge der Ergebnisse zu verbessern. Seit 2006 sind Suchmaschinen-Rankings wichtiger als je zuvor, so dass eine Industrie entwickelt hat ("Suchmaschinen-Optimatoren", oder SEO), um Web-Entwickler zu helfen, ihre Such-Ranking zu verbessern, und eine ganze Körper des Fallrechts hat sich um Angelegenheiten entwickelt, die Suchmaschinen-Rankings beeinflussen, wie die Verwendung von Marken in Metatags.Der Verkauf von Suchlisten durch einige Suchmaschinen hat auch Kontroversen zwischen Bibliothekaren und Verbrauchervertretern geschaffen. Am 3. Juni 2009 startete Microsoft seine neue Suchmaschine, Bing. Im folgenden Monat Microsoft und Yahoo! gab einen Deal bekannt, in dem Bing Yahoo!Search treiben würde. Heute hat Google versucht, die Suchmaschinen-Erfahrung für Benutzer zu transformieren. Mit Googles Hinzufügung der Google-Wissens-Graphen hat sich das Internet als Ganzes erheblich ausgewirkt und möglicherweise sogar bestimmte Webseiten-Verkehr, einschließlich Wikipedia, eingeschränkt. Durch das Ziehen von Informationen aus Wikipedia und die Präsentation auf Googles Seite, einige argumentieren, dass es kann negativ beeinflussen Wikipedia und andere Websites. Es gab jedoch keine unmittelbaren Bedenken zwischen Wikipedia und Wissensdiagramm. File-Sharing Resource oder Datei-Sharing war eine wichtige Aktivität auf Computer-Netzwerken von weit vor der Gründung des Internets und wurde in einer Vielzahl von Möglichkeiten unterstützt, einschließlich Bulletin Board-Systeme (1978,) Usenet (1980,) Kermit (1981,) und viele andere. Das File Transfer Protocol (FTP) für die Nutzung im Internet wurde 1985 standardisiert und ist heute noch im Einsatz. Eine Vielzahl von Tools wurden entwickelt, um die Verwendung von FTP zu unterstützen, indem Nutzern helfen, Dateien zu entdecken, die sie übertragen möchten, einschließlich der Wide Area Information Server (WAIS) in 1991, Gopher in 1991, Archie in 1991, Veronica in 1992, Jughead in 1993, Internet Relay Chat (IRC) in 1988, und schließlich das World Wide Web (WW) in 1991 mit Web-Verzeichnern und Web-Suchmaschinen. 1999 wurde Napster zum ersten Peer-to-Peer-Datei-Sharing-System. Napster benutzte einen zentralen Server zur Indexierung und Peer Discovery, aber die Speicherung und Übertragung von Dateien wurde dezentralisiert. Eine Vielzahl von Peer-to-Peer-Datei-Sharing-Programme und -Dienste mit verschiedenen Ebenen der Dezentralisierung und Anonymität folgten, darunter: Gnutella, eDonkey2000 und Freenet im Jahr 2000, FastTrack, Kasa, Limewire und BitTorrent im Jahr 2001 und Poisoned im Jahr 2003. Alle diese Tools sind allgemeiner Zweck und können verwendet werden, um eine Vielzahl von Inhalten zu teilen, aber der Austausch von Musikdateien, Software und später Filme und Videos sind wichtige Anwendungen. Und während einige dieser Teilen legal sind, sind große Teile nicht. Klagen und andere rechtliche Maßnahmen verursachten Napster im Jahr 2001, eDonkey2000 im Jahr 2005, Kasa im Jahr 2006 und Limewire im Jahr 2010, um ihre Bemühungen abzuschalten oder neu zu fokussieren. Die 2003 in Schweden gegründete Pirate Bay setzt trotz eines Prozesses und Appells in den Jahren 2009 und 2010 fort, was zu Gefängnisbedingungen und großen Geldstrafen für einige ihrer Gründer führte. Die Dateifreigabe bleibt inhaltlich und kontrovers mit Anklagen der Diebstahl des geistigen Eigentums einerseits und Anklagen der Zensur andererseits. Dot-com Blase Plötzlich der niedrige Preis, Millionen weltweit zu erreichen, und die Möglichkeit, von diesen Menschen zu verkaufen oder zu hören, zum gleichen Zeitpunkt, als sie erreicht wurden, versprach, etablierte Business Dogma in Werbung, Post-Order-Verkäufe, Kundenbeziehungsmanagement und viele weitere Bereiche umzukehren. Das Web war eine neue Killer-App – es könnte unabhängige Käufer und Verkäufer auf nahtlose und kostengünstige Weise zusammenbringen. Unternehmer auf der ganzen Welt entwickelten neue Geschäftsmodelle und liefen zu ihrem nächsten Risikokapitalisten. Während einige der neuen Unternehmer Erfahrung in Wirtschaft und Wirtschaft hatten, war die Mehrheit einfach Menschen mit Ideen, und nicht das Kapital influx vorsichtig verwalten. Darüber hinaus wurden viele Dot-com-Geschäftspläne unter der Annahme, dass sie durch die Nutzung des Internets die Vertriebskanäle bestehender Unternehmen umgehen und daher nicht mit ihnen konkurrieren müssen; wenn die etablierten Unternehmen mit starken bestehenden Marken ihre eigene Internet-Präsenz entwickelt haben, wurden diese Hoffnungen zerstreut, und die Neuankömmlinge ließen versuchen, in Märkte zu brechen, die von größeren, etablierteren Unternehmen dominiert wurden. Viele hatten nicht die Fähigkeit, dies zu tun. Die Dot-com-Bubble Burst im März 2000, mit der Technologie schwere NASDAQ Composite Index Spitze auf 5,048.62 am 10. März (5,132.52 intraday,) mehr als verdoppeln seinen Wert nur ein Jahr zuvor. Bis 2001 lief die Ablenkung der Blase mit voller Geschwindigkeit. Eine Mehrheit der Punkt-Coms hatte den Handel eingestellt, nachdem sie durch ihr Risikokapital und IPO-Kapital verbrannt, oft ohne jemals einen Gewinn zu machen. Trotzdem wächst das Internet weiter, getrieben von Handel, immer größere Mengen an Online-Informationen und Wissen und Social Networking. Handys und das InternetDas erste Mobiltelefon mit Internet-Konnektivität war der Nokia 9000 Communicator, der 1996 in Finnland gestartet wurde.Die Lebensfähigkeit des Internet-Dienste-Zugang auf Mobiltelefonen war begrenzt, bis die Preise aus diesem Modell kamen, und Netzwerk-Anbieter begannen, Systeme und Dienste bequem auf Telefonen zugänglich zu entwickeln. NTT DoCoMo in Japan startete den ersten mobilen Internet-Service, i-mode, im Jahr 1999, und dies gilt als die Geburt der Handy-Internet-Dienste. 2001 wurde das Mobilfunk-E-Mail-System von Research in Motion (heute BlackBerry Limited) für ihr BlackBerry-Produkt in Amerika gestartet. Um einen effizienten Einsatz des kleinen Bildschirms und der winzigen Tastatur und eines für Mobiltelefone typischen Einhandbetriebs zu ermöglichen, wurde für mobile Geräte ein spezifisches Dokument- und Netzwerkmodell erstellt, das Wireless Application Protocol (WAP). Die meisten mobilen Geräte Internet-Dienste arbeiten mit WAP. Das Wachstum der Mobilfunkdienste war zunächst ein in erster Linie asiatisches Phänomen mit Japan, Südkorea und Taiwan alle bald die Mehrheit ihrer Internetnutzer auf Ressourcen per Telefon und nicht per PC zugreifen. Entwickelte Länder folgten mit Indien, Südafrika, Kenia, den Philippinen und Pakistan alle, dass die Mehrheit ihrer Inlandsnutzer das Internet von einem Mobiltelefon und nicht von einem PC aufgegriffen. Die europäische und nordamerikanische Nutzung des Internets wurde durch eine große installierte Basis von Personal Computern beeinflusst, und das Wachstum des Mobilfunk-Internetzugangs war gradueller, aber in den meisten westlichen Ländern erreicht nationale Penetrationsniveaus von 20–30%. Das Cross-over trat 2008 auf, als mehr Internet-Zugangsgeräte Mobiltelefone waren als persönliche Computer. In vielen Teilen der Entwicklungswelt ist das Verhältnis so viel wie 10 Handy-Benutzer zu einem PC-Benutzer. File Hosting Services File Hosting ermöglicht es Menschen, die Festplatten ihres Computers zu erweitern und ihre Dateien auf einem Server zu hosten. Die meisten Datei-Hosting-Dienste bieten kostenlosen Speicher sowie einen größeren Speicherbetrag gegen eine Gebühr. Diese Dienstleistungen haben das Internet für den geschäftlichen und persönlichen Gebrauch stark erweitert. Google Drive, gestartet am 24. April 2012 ist der beliebteste Datei-Hosting-Service geworden. Google Drive ermöglicht es Benutzern, Dateien mit sich selbst und anderen Benutzern zu speichern, zu bearbeiten und zu teilen. Diese Anwendung erlaubt nicht nur die Dateibearbeitung, das Hosting und das Teilen. Es fungiert auch als Googles eigene Frei-zu-Zugriff-Büro-Programme, wie Google Docs, Google Slides und Google Sheets. Diese Anwendung diente als nützliches Werkzeug für Universitätsprofessoren und Studenten sowie diejenigen, die Cloud-Speicher benötigen. Dropbox, veröffentlicht im Juni 2007 ist ein ähnlicher Datei-Hosting-Service, der es Benutzern ermöglicht, alle ihre Dateien in einem Ordner auf ihrem Computer zu halten, der mit Dropbox-Servern synchronisiert wird. Dies unterscheidet sich von Google Drive, da es nicht web-browser basiert. Jetzt, Dropbox arbeitet, um Arbeiter und Dateien synchron und effizient zu halten. Mega, mit mehr als 200 Millionen Nutzern, ist ein verschlüsseltes Speicher- und Kommunikationssystem, das Benutzern kostenlose und bezahlte Speicherung bietet, mit einem Schwerpunkt auf Privatsphäre. Als drei der größten Datei-Hosting-Dienste, Google Drive, Dropbox und Mega stellen alle die Kernideen und Werte dieser Dienste dar. Online Piraterie Die früheste Form der Online-Piraterie begann mit einem P2P (Peer to peer) Musik-Sharing-Service namens Napster, gestartet 1999. Seiten wie LimeWire, The Pirate Bay und BitTorrent erlaubt es jedem, sich an Online-Piraterie zu beteiligen, indem er durch die Medienindustrie Rippchen schickt. Mit Online-Piraterie kam eine Veränderung in der Medienbranche insgesamt. Webtechnologien Webseiten wurden zunächst als strukturierte Dokumente auf Basis von Hypertext Markup Language (HTML) konzipiert, die den Zugriff auf Bilder, Videos und andere Inhalte ermöglichen können. Hyperlinks auf der Seite ermöglichen es Benutzern, auf andere Seiten zu navigieren. In den frühesten Browsern werden Bilder in einer separaten Helper-Anwendung geöffnet. Marc Andreessens 1993 Mosaic und 1994 Netscape führte gemischte Texte und Bilder für nicht-technische Nutzer ein. HTML entwickelte sich in den 1990er Jahren, was zu HTML 4 führte, die große Elemente der CSS-Styling und, später, Erweiterungen, um Browser-Code, um Anrufe zu machen und um Inhalte von Servern auf strukturierte Weise (AJAX.) Geschichte Es gibt fast unüberwindliche Probleme bei der Bereitstellung einer Historiographie der Entwicklung des Internets. Der Digitalisierungsprozess stellt eine zweifache Herausforderung sowohl für die Historiographie im Allgemeinen als auch insbesondere für die historische Kommunikationsforschung dar. Ein Gefühl der Schwierigkeit, frühe Entwicklungen zu dokumentieren, die zum Internet führte, kann aus dem Zitat entnommen werden: "Die Arpanet-Periode ist etwas gut dokumentiert, weil das verantwortliche Unternehmen – BBN – einen physischen Rekord hinterlassen hat.Ein Übergang in die NSFNET-Ära wurde ein außerordentlich dezentraler Prozess. Die Platte existiert in den Kellern der Menschen, in Schränken.... So viel von dem, was geschah, wurde verbal und auf der Grundlage des individuellen Vertrauens getan." Siehe auch Referenzen Bibliographie Gillies, James; Cailliau, Robert (2000). Wie das Web geboren wurde: The Story of the World Wide Web.New York: Oxford University Press. ISBN 0-19-286207-3.Hafner, Katie; Lyon, Matthew (1998) [1996]. Wo Wizards bleiben Late: The Origins Of The Internet.New York: Touchstone.ISBN 978-0-684-83267-8. Lesen Sie weiter Abbate, Janet (1999). Erfindet das Internet. Cambridge, Massachusetts: MIT Drücken. ISBN 978-0262011723.Cerf, Vinton (1993). Wie das Internet kam zu Be.Ryan, Johnny (2010). Eine Geschichte des Internets und die digitale Zukunft. London, England: Reaktion Books. ISBN 978-1861897770. Thomas Greene; Larry James Landweber; George Strawn (2003)."Eine kurze Geschichte von NSF und dem Internet". National Science Foundation. Externe Links Internet History Timeline – Computer History Museum Histories of the Internet – Internet Society Hobbes' Internet Timeline 12 Geschichte des Internets, ein kurzer Animationsfilm (2009) Geschichte des Internets bei Curlie