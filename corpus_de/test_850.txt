Ein wiederkehrendes neuronales Netz (RNN) ist eine Klasse künstlicher neuronaler Netze, bei der Verbindungen zwischen Knoten entlang einer zeitlichen Sequenz einen gerichteten Graph bilden. Dies ermöglicht es, zeitliches dynamisches Verhalten zu zeigen. Von zukunftsweisenden neuronalen Netzen abgeleitet, können RNNs ihren internen Zustand (Speicher) verwenden, um variable Längensequenzen von Eingängen zu verarbeiten. Dies macht sie auf Aufgaben wie unsegmentierte, vernetzte Handschrifterkennung oder Spracherkennung anwendbar. Der Begriff "recurrent neural network" wird indiskriminierend verwendet, um auf zwei breite Klassen von Netzwerken mit einer ähnlichen allgemeinen Struktur zu verweisen, wobei einer endlicher Impuls ist und der andere unendlicher Impuls ist. Beide Netzwerkklassen zeigen zeitliches dynamisches Verhalten. Ein endliches Impuls-Rezidiv-Netzwerk ist ein gerichtetes azyklisches Diagramm, das von einem streng nachspeisenden neuronalen Netz abgeschrieben und ersetzt werden kann, während ein unendliches Impuls-Rezidiv-Netzwerk ein gerichtetes zyklisches Diagramm ist, das nicht abgerollt werden kann. Sowohl endliche Impulse als auch unendliche Impulswiederkehrende Netzwerke können zusätzliche gespeicherte Zustände aufweisen und die Speicherung kann durch das neuronale Netz direkt gesteuert werden. Der Speicher kann auch durch ein anderes Netzwerk oder Graph ersetzt werden, wenn dieser Zeitverzögerungen enthält oder Feedbackschleifen aufweist. Solche gesteuerten Zustände werden als gated state oder gated Memory bezeichnet und sind Teil von langen Kurzzeitspeichernetzwerken (LSTMs) und gated recurrent units. Dies wird auch Feedback Neural Network (FNN) genannt. In typischen Bibliotheken wie PyTorch spielt Just-in-time-Compilation eine wichtige Rolle, um wiederkehrende neuronale Netzwerke effizient umzusetzen. History Recurrent neural networks basierten 1986 auf David Rumelharts Arbeit. Hopfield-Netzwerke – eine besondere Art von RNN – wurden 1982 von John Hopfield entdeckt. 1993 löste ein neuronales Geschichtskompressorsystem eine "Sehr tiefes Lernen"-Aufgabe, die mehr als 1000 nachfolgende Schichten in einem RNN in der Zeit benötigte. LSTM Langer Kurzzeitspeicher (LSTM)-Netzwerke wurden 1997 von Hochreiter und Schmidhuber erfunden und in mehreren Anwendungsdomänen Genauigkeitsprotokolle gesetzt. Um 2007 begann LSTM, die Spracherkennung zu revolutionieren und traditionelle Modelle in bestimmten Sprachanwendungen zu übertreffen. Im Jahr 2009 war ein Verbindungsist Temporal Classification (CTC)-trainiertes LSTM-Netzwerk die erste RNN, um Mustererkennung Wettbewerbe zu gewinnen, wenn es mehrere Wettbewerbe in vernetzter Handschrifterkennung gewonnen. Im Jahr 2014 nutzte das chinesische Unternehmen Baidu CTC-trainierte RNs, um den 2S09 Switchboard Hub5'00 Spracherkennungsdatensatz Benchmark zu brechen, ohne herkömmliche Sprachverarbeitungsmethoden zu verwenden. LSTM verbesserte auch die Spracherkennung und Text-zu-Sprach-Synthese im Großformat und wurde in Google Android verwendet. Im Jahr 2015 erlebte Googles Spracherkennung einen dramatischen Leistungssprung von 49 % durch CTC-trained LSTM. LSTM brach Datensätze für verbesserte maschinelle Übersetzung, Sprachmodellierung und mehrsprachige Sprachverarbeitung. LSTM kombiniert mit konvolutionalen neuronalen Netzwerken (CNNs) verbesserte automatische Bildunterschrift. Architekturen RNNs kommen in vielen Varianten. Vollständig wiederkehrende neuronale Netze (FRN) verbinden die Ausgänge aller Neuronen mit den Eingängen aller Neuronen. Dies ist die allgemeinste neuronale Netzwerktopologie, da alle anderen Topologien durch die Einstellung einiger Verbindungsgewichte auf Null dargestellt werden können, um die fehlenden Verbindungen zwischen diesen Neuronen zu simulieren. Die Abbildung nach rechts kann für viele irreführend sein, weil praktische neuronale Netzwerktopologien häufig in Schichten organisiert werden und die Zeichnung dieses Aussehens verleiht. Was jedoch Schichten zu sein scheint, sind in der Tat verschiedene Schritte in der Zeit des gleichen vollständig wiederkehrenden neuronalen Netzes. Der in der Darstellung linke Teil zeigt die wiederkehrenden Verbindungen, wie der Bogen mit v bezeichnet ist. Es wird in der Zeit entfaltet, um das Aussehen von Schichten zu erzeugen. Elman-Netzwerke und Jordan-Netzwerke Ein Elman-Netzwerk ist ein dreischichtiges Netzwerk (in der Abbildung horizontal wie x, y und z angeordnet) unter Zusatz eines Satzes von Kontexteinheiten (u in der Abbildung). Die mittlere (geschirmte) Schicht ist mit diesen mit einem Gewicht von einem fixierten Kontexteinheiten verbunden. Zu jedem Zeitschritt wird die Eingabe vorgeschaltet und eine Lernregel angewendet. Die fixierten Rückschaltungen speichern eine Kopie der vorherigen Werte der versteckten Einheiten in den Kontexteinheiten (da sie sich vor der Anwendung der Lernregel über die Verbindungen propagieren). So kann das Netzwerk eine Art Zustand halten, so dass es solche Aufgaben wie Sequenz-Prädiktion ausführen kann, die über die Leistung eines Standard-Mehrschicht-Perceptron liegen. Jordan Netzwerke sind ähnlich wie Elman Netzwerke. Die Kontexteinheiten werden anstelle der versteckten Schicht aus der Ausgangsschicht zugeführt. Die Kontexteinheiten in einem Jordannetz werden auch als Zustandsschicht bezeichnet. Sie haben eine wiederkehrende Verbindung zu sich. Elman und Jordan Netzwerke sind auch als “Einfache wiederkehrende Netzwerke” (SRN) bekannt. Elman-Netzwerk h t = σ h ( W h x t + U h t - 1 + b h) y t = σ y ( W y h t + b y ) {\displaystyle begin{align}h_{t}&=\sigma (W_{h}x_{t}+U_{h}h_{t-1}+b_{h}\y_{t}&=\sigma y}(W_{y}h_{t}+b_{y})\end{ausgerichtet Jordannetzwerk h t = σ h ( W h x t + U h y t - 1 + b h) y t = σ y ( W y h t + b y ) {\displaystyle begin{align}h_{t}&=\sigma (W_{h}x_{t}+U_{h}y_{t-1}+b_{h}\y_{t}&=\sigma y}(W_{y}h_{t}+b_{y})\end{ausgerichtet Variablen und Funktionen x t {\displaystyle x_{t} : Eingabevektor h t {\displaystyle h_{t} : versteckter Schichtvektor y t {\displaystyle y_{t} : Ausgangsvektor W {\displaystyle W}, U {\displaystyle U} und b {\displaystyle b} : Parametermatrizen und Vektor σ h {\displaystyle \sigma {_h} und σ y {\displaystyle \sigma {_y} : Aktivierungsfunktionen Hopfield Das Hopfield-Netzwerk ist ein RNN, in dem alle Verbindungen über Schichten gleich groß sind. Es erfordert stationäre Eingänge und ist somit kein allgemeiner RNN, da er keine Musterfolgen verarbeitet. Sie garantiert jedoch, dass sie sich konvergieren wird. Wenn die Verbindungen mit hebbianischem Lernen trainiert werden, kann das Hopfield-Netzwerk als robuster, inhaltlich adressierbarer Speicher ausgeführt werden, der gegen Verbindungsänderung beständig ist. Bidirektionaler assoziativer Speicher Introduziert von Bart Kosko, ein bidirektionales assoziatives Speicher (BAM) Netzwerk ist eine Variante eines Hopfield-Netzwerks, das assoziative Daten als Vektor speichert. Die Bi-Richtungalität kommt von der Weitergabe von Informationen durch eine Matrix und deren Transpose. Typischerweise ist die bipolare Kodierung bevorzugt zur binären Kodierung der assoziativen Paare. In jüngster Zeit wurden stochastische BAM-Modelle mit Markov Stepping für eine erhöhte Netzwerkstabilität und Relevanz für reale Anwendungen optimiert. Ein BAM-Netzwerk weist zwei Schichten auf, von denen entweder als Eingang angesteuert werden kann, um eine Assoziation zu erinnern und einen Ausgang auf der anderen Schicht zu erzeugen. Echozustand Das Echozustandsnetzwerk (ESN) weist eine dünn verbundene zufällige verdeckte Schicht auf. Die Gewichte der Ausgangsneuronen sind der einzige Teil des Netzes, der sich ändern kann ( trainiert werden). ESNs sind gut bei der Wiedergabe bestimmter Zeitreihen. Eine Variante für spikende Neuronen ist als flüssige Zustandsmaschine bekannt. Unabhängig von RNN (IndRNN) Das unabhängig wiederkehrende neuronale Netz (IndRNN) richtet sich an den Gradienten, der Probleme im traditionellen, vollständig vernetzten RNN verschwindet und austreibt. Jedes Neuron in einer Schicht erhält nur seinen eigenen Vergangenheitszustand als Kontextinformationen (anstelle der vollständigen Konnektivität zu allen anderen Neuronen in dieser Schicht) und damit Neuronen sind unabhängig von der Geschichte des anderen. Die Gradienten-Backpropagation kann geregelt werden, um zu vermeiden, dass Gradienten verschwinden und explodieren, um lange oder kurzfristige Speicher zu halten. Die Cross-neuron-Information wird in den nächsten Schichten untersucht. IndRNN kann mit den nicht gesättigten nichtlinearen Funktionen wie ReLU robust trainiert werden. Mit Sprungverbindungen können tiefe Netzwerke trainiert werden. Rekursiv Ein rekursives neuronales Netz wird erzeugt, indem dieselbe Gewichtsmenge rekursiv über eine differenzierbare graphische Struktur aufgebracht wird, indem die Struktur in topologischer Reihenfolge durchläuft. Solche Netzwerke werden typischerweise auch durch den umgekehrten Modus der automatischen Differenzierung trainiert. Sie können verteilte Darstellungen von Struktur, wie logische Begriffe verarbeiten. Ein besonderer Fall von rekursiven neuronalen Netzen ist der RNN, dessen Struktur einer linearen Kette entspricht. Rekursive neuronale Netze wurden auf die natürliche Sprachverarbeitung angewendet. Das Recursive Neural Tensor Network nutzt eine tensorbasierte Kompositionsfunktion für alle Knoten im Baum. Neural History Kompressor Der neurale Geschichtsverdichter ist ein unübertroffener Stapel von RNNs. Auf der Eingabeebene lernt es, seinen nächsten Eingang von den vorherigen Eingängen vorherzusagen. Nur unvorhersehbare Inputs einiger RNN in der Hierarchie werden Inputs auf die nächsthöhere Ebene RNN, die daher nur selten ihren inneren Zustand rechnet. Jeder höhere Level RNN untersucht somit eine komprimierte Darstellung der Informationen im folgenden RNN. Dies geschieht so, dass die Eingabesequenz aus der Darstellung auf höchstem Niveau genau rekonstruiert werden kann. Das System minimiert effektiv die Beschreibungslänge oder den negativen Logarithmus der Wahrscheinlichkeit der Daten. Bei einer Vielzahl von erlernbaren Vorhersagen in der eingehenden Datensequenz kann die höchste Ebene RNN beaufsichtigtes Lernen verwenden, um auch tiefe Sequenzen mit langen Intervallen zwischen wichtigen Ereignissen leicht einzuordnen. Es ist möglich, die RNN-Hierarchie in zwei RNNs zu destillieren: der bewusste Zerkleinerer (höhere Ebene) und der unterbewusste Automatisator (untere Ebene). Sobald der Zerkleinerer gelernt hat, Eingaben vorherzusagen und zu komprimieren, die durch den Automatisierer nicht vorhersehbar sind, kann der Automatisierer in der nächsten Lernphase gezwungen werden, die versteckten Einheiten des langsamer wechselnden Zerkleinerers vorherzusagen oder nachzuahmen. Dies erleichtert es dem Automatisierungsgerät, über lange Zeit hinweg geeignete, selten wechselnde Erinnerungen zu lernen. Dies wiederum hilft dem Automatisierungsgerät, viele seiner einst unvorhersehbaren Inputs vorhersehbar zu machen, so dass sich der Zerkleinerer auf die übrigen unvorhersehbaren Ereignisse konzentrieren kann. Ein generatives Modell übertraf das verschwindende Gradientenproblem der automatischen Differenzierung oder Rückverbreitung in neuronalen Netzen 1992 teilweise. 1993 löste ein solches System eine "Sehr tiefes Lernen"-Aufgabe, die mehr als 1000 spätere Schichten in einem sich in der Zeit entfalteten RN benötigte. Zweite Ordnung RNNs Zweite Ordnung RNNs verwenden höhere Ordnungsgewichte w i j k {\displaystyle w{}_{ijk anstelle der Standard w i j {\displaystyle w{}_{ij Gewichte, und Zustände können ein Produkt sein. Dies ermöglicht eine direkte Zuordnung zu einer endlichen Maschine sowohl in der Ausbildung, Stabilität und Darstellung. Langer Kurzzeitgedächtnis ist ein Beispiel dafür, hat aber keine solchen formalen Kartierungen oder Stabilitätsnachweise. Langer Kurzzeitspeicher Langer Kurzzeitspeicher (LSTM) ist ein tiefes Lernsystem, das das verschwindende Gradientenproblem vermeidet. LSTM wird in der Regel durch wiederkehrende Gates namens "Vergessen-Gate" erweitert. LSTM verhindert, dass rückverschobene Fehler verschwinden oder explodieren. Stattdessen können Fehler durch unbegrenzte Anzahl virtueller Schichten, die sich im Raum entfaltet, rückwärts strömen. Das heißt, LSTM kann Aufgaben lernen, die Erinnerungen an Ereignisse erfordern, die tausende oder sogar Millionen von diskreten Zeitschritten früher geschehen. Problemspezifische LSTM-ähnliche Topologien können weiterentwickelt werden. LSTM arbeitet auch bei langen Verzögerungen zwischen bedeutenden Ereignissen und kann Signale verarbeiten, die niedrige und hochfrequente Komponenten mischen. Viele Anwendungen verwenden Stacks von LSTM RNs und trainieren sie von Connectionist Temporal Classification (CTC) um eine RNN-Gewichtsmatrix zu finden, die die Wahrscheinlichkeit der Labelsequenzen in einem Trainingsset bei den entsprechenden Eingabesequenzen maximiert. CTC erreicht sowohl Ausrichtung als auch Erkennung. LSTM kann lernen, kontextsensitive Sprachen im Gegensatz zu früheren Modellen basierend auf versteckten Markov-Modellen (HMM) und ähnlichen Konzepten zu erkennen. Gated recurrent unit Gated recurrent units (GRUs) sind ein Gating-Mechanismus in wiederkehrenden neuronalen Netzwerken, die 2014 eingeführt wurden. Sie werden in voller Form und mehreren vereinfachten Varianten eingesetzt. Ihre Performance auf polyphoner Musikmodellierung und Sprachsignalmodellierung war ähnlich wie die des langen Kurzzeitgedächtnisses. Sie haben weniger Parameter als LSTM, da sie kein Ausgangsgate haben. Bi-direktional bi-direktional RNNs verwenden eine endliche Sequenz, um jedes Element der Sequenz anhand der Vergangenheit und zukünftigen Kontexte des Elements vorherzusagen oder zu kennzeichnen. Dies geschieht, indem man die Ausgänge von zwei RNs konkatiert, wobei eine die Reihenfolge von links nach rechts, die andere von rechts nach links, verarbeitet. Die kombinierten Ausgänge sind die Vorhersagen der lehrerspezifischen Zielsignale. Diese Technik hat sich als besonders nützlich erwiesen, wenn sie mit LSTM RNs kombiniert wird. Dauerzeit Ein kontinuierlich wiederkehrendes neuronales Netz (CTRN) verwendet ein System von gewöhnlichen Differentialgleichungen, um die Auswirkungen auf ein Neuron der eingehenden Eingänge zu modellieren. Für ein Neuron i {\displaystyle i} im Netzwerk mit Aktivierung y i {\displaystyle y_{i} wird die Aktivierungsgeschwindigkeit durch: τ i y ̇ i = - angegeben. y i + Σ j = 1 n w i σ ( y j - Θ j) + I i ( t ) **___________ *****_______ (y_{j}-\Theta j})+I_{i}(t) Wo: τ i {\displaystyle \tau {_i} : Zeitkonstante des postynaptischen Knotens y i {\displaystyle y_{i} : Aktivierung des postynaptischen Knotens y ̇ i {\displaystyle {\dot y}_{i : Änderungsrate der Aktivierung des Postsynaptischen Knotens w i {\displaystyle w{}_{ji : Gewicht der Verbindung von Pre zu Postsynaptic node σ ( x ) {\displaystyle \sigma (x}) : Sigmoid von x z.B. σ ( x ) = 1 / ( 1 + e - x ) {\displaystyle \sigma x)=1/(1+e^{-x} . y j {\displaystyle y_{j} : Aktivierung des vorsynaptischen Knotens Θ j {\displaystyle \Theta {_j} : Bias of presynaptic node I i (t ) {\displaystyle I_{i}(t) : Input (falls vorhanden) to nodeCTRNs wurden auf evolutionäre Robotik angewendet, wo sie verwendet wurden, um Vision, Zusammenarbeit und minimales kognitives Verhalten zu adressieren.Beachten Sie, dass durch das Shannon-Sampling-Theorem diskrete Zeit wiederkehrende neuronale Netzwerke als fortlaufende wiederkehrende neuronale Netzwerke angesehen werden können, in denen die Differentialgleichungen in äquivalente Differenzgleichungen transformiert sind. Diese Transformation kann als erfolgt betrachtet werden, nachdem die post-synaptischen Knoten-Aktivierungsfunktionen y i (t ) {\displaystyle y_{i}(t) tiefpassgefiltert, aber vor der Abtastung. Hierarchische Hierarchische RNNs verbinden ihre Neuronen auf verschiedene Weise, hierarchisches Verhalten in nützliche Unterprogramme zu zersetzen. Solche hierarchischen Strukturen der Kognition sind in Theorien des Gedächtnisses vorhanden, die der Philosoph Henri Bergson präsentiert hat, dessen philosophische Ansichten hierarchische Modelle inspiriert haben. Wiederkehrendes mehrschichtiges Perceptron-Netzwerk In der Regel besteht ein wiederkehrendes mehrschichtiges Perceptron-Netzwerk (RMLP) aus kaskadierten Subnetzen, von denen jede mehrere Schichten von Knoten enthält. Jedes dieser Teilnetze ist bis auf die letzte Schicht nach vorne gespeist, die Rückkopplungsverbindungen aufweisen kann. Jedes dieser Teilnetze ist nur durch Zuleitungen verbunden. Mehrere Zeitskala Modell Ein mehrfaches Zeitskala wiederkehrendes neuronales Netzwerk (MTRNN) ist ein neuralbasiertes Rechenmodell, das die funktionelle Hierarchie des Gehirns durch Selbstorganisation simulieren kann, die von der räumlichen Verbindung zwischen Neuronen und von verschiedenen Arten von neuronen Aktivitäten abhängt, die jeweils mit unterschiedlichen Zeiteigenschaften. Mit solchen abwechslungsreichen neuronalen Aktivitäten werden fortlaufende Sequenzen beliebiger Verhaltensweisen zu wiederverwendbaren Primitiven segmentiert, die wiederum flexibel in unterschiedliche sequentielle Verhaltensweisen integriert sind. Die biologische Genehmigung einer solchen Hierarchie wurde in der Gedächtnis-Prädiktionstheorie der Gehirnfunktion von Hawkins in seinem Buch On Intelligence diskutiert. Eine solche Hierarchie stimmt auch mit Theorien des Gedächtnisses überein, die der Philosoph Henri Bergson in ein MTRNN-Modell integriert hat. Neurale Turing-Maschinen Neural Turing-Maschinen (NTMs) sind ein Verfahren zur Erweiterung wiederkehrender neuronaler Netzwerke, indem sie an externe Speicherressourcen gekoppelt werden, mit denen sie durch aufmerksame Prozesse interagieren können. Das kombinierte System ist analog zu einer Turing-Maschine oder von Neumann-Architektur, ist aber differenzierbar, so dass es effizient mit Gradientenabstieg trainiert werden kann. Differenzierbare neuronale Computer Differenzierbare neuronale Computer (DNCs) sind eine Erweiterung von Neural Turing-Maschinen, die die Verwendung von fuzzy Mengen jeder Speicheradresse und einen Datensatz von Chronologie ermöglicht. Neurale Netzwerk-Pushdown-Automaten Neural Netzwerk-Pushdown-Automata (NNPDA) sind ähnlich NTMs, aber Bänder werden durch analoge Stapel ersetzt, die differenzierbar sind und ausgebildet sind. Auf diese Weise sind sie ähnlich in der Komplexität zu erkennenden Kontext freie Grammatik (CFGs.) Memristive Networks Greg Snider von HP Labs beschreibt ein System des kortikalen Computing mit memristiven Nanodevices. Die Memristoren (Memory-Widerstände) werden durch dünne Filmmaterialien realisiert, bei denen der Widerstand über den Transport von Ionen oder Sauerstoff-Vakanzen innerhalb der Folie elektrisch abgestimmt wird. Das SyNAPSE-Projekt von DARPA hat IBM Research und HP Labs in Zusammenarbeit mit der Boston University Department of Cognitive and Neural Systems (CNS) gefördert, um neuromorphe Architekturen zu entwickeln, die auf memristiven Systemen basieren können. Memristive Netzwerke sind eine bestimmte Art von physikalischen neuronalen Netzwerken, die sehr ähnliche Eigenschaften wie (Little-)Hopfield-Netzwerke haben, da sie eine kontinuierliche Dynamik haben, eine begrenzte Speicherkapazität und sie natürliche Entspannung durch die Minimierung einer Funktion, die aymptotisch für das Ising-Modell ist. In diesem Sinne hat die Dynamik einer memristiven Schaltung den Vorteil gegenüber einem Resistor-Capacitor-Netzwerk, um ein interessanteres nichtlineares Verhalten zu haben. Aus dieser Sicht führt die Konstruktion einer analogen memristiven Netzwerke zu einer eigenartigen neuromorphen Technik, bei der das Geräteverhalten von der Schaltungsverdrahtung oder Topologie abhängt. Training Gradient descent Gradient Abstieg ist ein iterativer Optimierungsalgorithmus erster Ordnung, um das Minimum einer Funktion zu finden. In neuronalen Netzen kann die Fehlerdauer dadurch minimiert werden, dass jedes Gewicht im Verhältnis zur Ableitung des Fehlers gegenüber diesem Gewicht verändert wird, sofern die nichtlinearen Aktivierungsfunktionen differenzierbar sind. Verschiedene Methoden dafür wurden in den 1980er und frühen 1990er Jahren von Werbos, Williams, Robinson, Schmidhuber, Hochreiter, Pearlmutter und anderen entwickelt. Die Standardmethode wird als "Backpropagation durch Zeit" oder BPTT bezeichnet und ist eine Verallgemeinerung der Back-Propagation für Feed-Forward-Netzwerke. Wie diese Methode ist es ein Beispiel einer automatischen Differenzierung im umgekehrten Akkumulationsmodus des Minimumprinzips von Pontryagin. Eine rechnerisch aufwendigere Online-Variante wird als "Real-Time Recurrent Learning" oder RTRL bezeichnet, was eine automatische Differenzierung im Vorwärtsakkumulationsmodus mit gestapelten Tangentenvektoren ist. Im Gegensatz zu BPTT ist dieser Algorithmus lokal in der Zeit, aber nicht lokal im Raum. In diesem Zusammenhang bedeutet lokal im Raum, dass der Gewichtsvektor einer Einheit mit nur in den angeschlossenen Einheiten gespeicherten Informationen und der Einheit selbst so aktualisiert werden kann, dass die Update-Komplexität einer einzigen Einheit in der Dimensionalität des Gewichtsvektors linear ist. Lokal in time bedeutet, dass die Updates laufend (on-line) stattfinden und nur vom letzten Zeitschritt anstatt von mehreren Zeitschritten innerhalb eines bestimmten Zeithorizons wie in BPTT abhängen. Biologische neuronale Netzwerke scheinen bezüglich Zeit und Raum lokal zu sein. Zur rekursiven Berechnung der Teilderivate hat RTRL eine Zeitkomplexität von O(Anzahl der versteckten x-Anzahl der Gewichte) pro Zeitschritt zur Berechnung der Jacobian-Matrizen, während BPTT nur O(Anzahl der Gewichte) pro Zeitschritt benötigt, um alle Vorwärtsaktivierungen innerhalb des vorgegebenen Zeithorizons zu speichern. Es besteht ein Online-Hybrid zwischen BPTT und RTRL mit intermediärer Komplexität sowie Varianten für Dauerzeit. Ein großes Problem mit Gradientenabstieg für Standard-RNN-Architekturen ist, dass Fehlergradienten mit der Zeitverzögerung zwischen wichtigen Ereignissen exponentiell schnell verschwinden. LSTM kombiniert mit einer BPTT/RTRL-Hybrid-Learning-Methode versucht, diese Probleme zu überwinden. Dieses Problem wird auch im unabhängig wiederkehrenden neuronalen Netzwerk (IndRNN) gelöst, indem der Kontext eines Neurons in seinen eigenen Vergangenheitszustand reduziert und die Cross-neuron-Information dann in den folgenden Schichten untersucht werden kann. Memories verschiedener Reichweite, einschließlich Langzeitspeicher, können gelernt werden, ohne dass der Gradient verschwinden und explodieren Problem. Der on-line-Algorithmus, genannt Kausal rekursive Backpropagation (CRBP), implementiert und kombiniert BPTT und RTRL Paradigmen für lokal wiederkehrende Netzwerke. Es funktioniert mit den allgemeinsten lokal wiederkehrenden Netzwerken. Der CRBP-Algorithmus kann den globalen Fehlerterm minimieren. Diese Tatsache verbessert die Stabilität des Algorithmus und bietet eine einheitliche Sicht auf Gradientenberechnungstechniken für wiederkehrende Netzwerke mit lokalem Feedback. Ein Ansatz zur Berechnung von Gradienteninformationen in RNs mit willkürlichen Architekturen basiert auf Signalflussdiagrammen, die schematisch ableiten. Es verwendet den BPTT Batch-Algorithmus, basierend auf Lees Theorem für Netzempfindlichkeit Berechnungen. Es wurde von Wan und Beaufays vorgeschlagen, während seine schnelle Online-Version von Campolucci, Uncini und Piazza vorgeschlagen wurde. Globale Optimierungsmethoden Das Training der Gewichte in einem neuronalen Netzwerk kann als nichtlineares globales Optimierungsproblem modelliert werden. Eine Zielfunktion kann gebildet werden, um die Fitness oder den Fehler eines bestimmten Gewichtsvektors wie folgt auszuwerten: Zunächst werden die Gewichte im Netzwerk entsprechend dem Gewichtsvektor eingestellt. Anschließend wird das Netzwerk gegen die Trainingssequenz ausgewertet. Typischerweise wird zur Darstellung des Fehlers des aktuellen Gewichtsvektors die Summenquadratdifferenz zwischen den Vorhersagen und den in der Trainingssequenz angegebenen Zielwerten herangezogen. Arbiträre globale Optimierungstechniken können dann verwendet werden, um diese Zielfunktion zu minimieren. Die häufigste globale Optimierungsmethode für das Training von RNNs ist genetische Algorithmen, insbesondere in unstrukturierten Netzwerken. Zunächst wird der genetische Algorithmus mit den neuronalen Netzgewichten vordefiniert kodiert, wobei ein Gen im Chromosom eine Gewichtsverbindung darstellt. Das gesamte Netzwerk ist als ein einziges Chromosom dargestellt. Die Fitnessfunktion wird wie folgt ausgewertet: Jedes im Chromosom kodierte Gewicht ist der jeweiligen Gewichtsverbindung des Netzes zugeordnet. Der Trainingssatz wird dem Netzwerk vorgestellt, das die Eingangssignale weitergibt. Der Mittelquared-Ror wird in die Fitness-Funktion zurückgegeben. Diese Funktion treibt den genetischen Selektionsprozess an. Viele Chromosomen bilden die Bevölkerung; daher werden viele verschiedene neuronale Netzwerke entwickelt, bis ein Stoppkriterium erfüllt ist. Ein gemeinsames Stoppsystem ist: Wenn das neuronale Netz einen bestimmten Prozentsatz der Trainingsdaten erlernt hat oder wenn der Mindestwert des Mittel-Quadrat-Fehlers erfüllt ist oder wenn die maximale Anzahl der Trainingsgenerationen erreicht ist. Das Stoppkriterium wird von der Fitness-Funktion ausgewertet, da es während des Trainings die Gegenseitigkeit des Mittel-Quadrat-Fehlers aus jedem Netzwerk erhält. Ziel des Genalgorithmus ist es daher, die Fitnessfunktion zu maximieren, wodurch der Mittelquared-Ror reduziert wird. Andere globale (und/oder evolutionäre) Optimierungstechniken können verwendet werden, um eine gute Gewichtsmenge, wie simulierte Glühung oder Partikelschwarmoptimierung zu suchen. Verwandte Felder und Modelle RNNs können sich chaotisch verhalten. In solchen Fällen kann die dynamische Systemtheorie zur Analyse verwendet werden. Sie sind in der Tat rekursive neuronale Netze mit einer bestimmten Struktur: die einer linearen Kette. Während rekursive neuronale Netzwerke auf jeder hierarchischen Struktur arbeiten, Kinderdarstellungen in Elterndarstellungen kombinieren, arbeiten wiederkehrende neuronale Netzwerke am linearen Verlauf der Zeit, die den vorherigen Zeitschritt und eine versteckte Darstellung in die Darstellung für den aktuellen Zeitschritt kombinieren. Insbesondere können RNNs als nichtlineare Versionen von endlicher Impulsantwort und unendlichen Impulsantwortfiltern sowie als nichtlineares autoregressives exogenes Modell (NARX) erscheinen. Bibliotheken Apache Singa Caffe: Erstellt vom Berkeley Vision and Learning Center (BVLC). Es unterstützt sowohl CPU als auch GPU. Entwickelt in C+, und hat Python und MATLAB Wrappers. Kettenrad: Die erste stabile Deep-Learning-Bibliothek, die dynamische, definierbare neuronale Netzwerke unterstützt. Vollständig in Python, Produktionsunterstützung für CPU, GPU, verteiltes Training. Deeplearning4j:Deep Learning in Java und Scala auf multi-GPU-fähigen Spark. Eine universelle Deep-Learning-Bibliothek für den JVM-Produktionsstapel, der auf einem C+-Wissenschaftsrechner läuft. Ermöglicht die Erstellung von benutzerdefinierten Schichten. Integriert mit Hadoop und Kafka. Flux: enthält Schnittstellen für RNNs, einschließlich GRUs und LSTMs, geschrieben in Julia. Keras: High-Level, einfach zu bedienen API, bietet eine Wrapper für viele andere Deep Learning-Bibliotheken. Microsoft Cognitive Toolkit MXNet: ein modernes Open-Source-Tief-Learning-Framework, das verwendet wird, um tiefe neuronale Netzwerke zu trainieren und einzusetzen. PyTorch: Tensors und dynamische neuronale Netzwerke in Python mit starker GPU-Beschleunigung. TensorFlow: Apache 2.0-lizenziert Theano-ähnliche Bibliothek mit Unterstützung für CPU, GPU und Googles proprietäre TPU, mobile Theano: Die Referenz-TiefLearning-Bibliothek für Python mit einer API weitgehend kompatibel mit der beliebten NumPy-Bibliothek. Erlaubt dem Benutzer, symbolische mathematische Ausdrücke zu schreiben, erzeugt dann automatisch ihre Derivate, indem er den Benutzer davor bewahrt, Gradienten oder Rückverbreitung zu kodieren. Diese symbolischen Ausdrücke werden automatisch auf CUDA-Code für eine schnelle, on-the-GPU-Implementierung kompiliert. Fackel (www.torch.ch): Ein wissenschaftlicher Rechenrahmen mit breiter Unterstützung für maschinelle Lernalgorithmen, geschrieben in C und lua. Der Hauptautor ist Ronan Collobert und wird nun bei Facebook AI Research und Twitter verwendet. Anwendungen Anwendungen von wiederkehrenden neuronalen Netzwerken umfassen: Machine Translation Robot control Zeitreihen Vorhersage Spracherkennung Sprachsynthese Brain-Computer-Schnittstellen Zeitreihen-Anomalie-Erkennung Rhythm lernen Musikkomposition Grammar lernen Handwriting-Erkennung Human Aktion Erkennung Protein Homology Erkennung Predicting subzelluläre Lokalisierung von Proteinen Mehrere Vorhersageaufgaben im Bereich Business Process Management Prediction in medizinischen Pflegewegen Referenzen Weiter lesen Mandic, Danilo P. & Chambers, Jonathon A. (2001). Recurrent Neural Networks for Prediction: Algorithmen, Architekturen und Stabilität lernen. Wiley.ISBN 978-0-471-49517-8. Externe Links Seq2SeqSharpLSTM/BiLSTM/Transformer rezidivierende neuronale Netzwerke, die auf CPUs und GPUs für Sequenz-to-Sequence-Aufgaben (C#, .NET)RNNSharp CRFs basierend auf wiederkehrenden neuronalen Netzwerken (C#, .NET)Recurrent Neural Networks mit über 60 RNN-Papiern von Jürgen Neural Networks für die Intelligenzgruppe von Dalle Molific