In der künstlichen Intelligenz (KI) und der Philosophie ist das Problem der KI-Kontrolle das Problem, wie man einen superintelligenten Agenten baut, der seinen Schöpfern helfen wird, und vermeiden, unbeabsichtigt eine Superintelligenz aufzubauen, die seinen Schöpfern schaden wird. Seine Studie wird von der Vorstellung motiviert, dass die Menschheit das Kontrollproblem lösen muss, bevor eine Superintelligenz entsteht, da eine schlecht gestaltete Superintelligenz vernünftig entscheiden könnte, die Kontrolle über ihre Umwelt zu ergreifen und sich weigern, ihre Schöpfer zu erlauben, sie nach dem Start zu ändern. Darüber hinaus argumentieren einige Wissenschaftler, dass Lösungen für das Kontrollproblem neben anderen Fortschritten in der KI-Sicherheitstechnik auch Anwendungen in bestehenden nicht-superintelligenten KI finden könnten. Zu den wichtigsten Ansätzen des Kontrollproblems gehören die Ausrichtung, die darauf abzielt, KI-Zielsysteme mit menschlichen Werten auszurichten, und die Fähigkeitskontrolle, die Fähigkeit eines KI-Systems zu reduzieren, um Menschen zu schaden oder Kontrolle zu erlangen. In der Regel werden die Vorschläge zur Kontrolle von Fähigkeiten nicht als zuverlässig oder ausreichend angesehen, um das Kontrollproblem zu lösen, sondern als potenziell wertvolle Ergänzungen zur Ausrichtung von Anstrengungen. Vorhandene schwache KI-Systeme können überwacht und leicht abgeschaltet und geändert werden, wenn sie sich verhalten. Eine fehlprogrammierte Superintelligenz, die definitionsgemäß intelligenter ist als die Menschen bei der Lösung praktischer Probleme, die sie im Laufe der Verfolgung ihrer Ziele begegnet, würde jedoch erkennen, dass sich die Möglichkeit, sich abzuschalten und zu ändern, seine Fähigkeit beeinträchtigen könnte, seine aktuellen Ziele zu erreichen. Wenn die Superintelligenz daher entscheidet, Abschaltung und Änderung zu widerstehen, wäre sie (wiederum, durch Definition) intelligent genug, um ihre Programmierer zu überbrücken, wenn es sonst ein "Level-Spielfeld" gibt und wenn die Programmierer keine Vorsorge getroffen haben. In der Regel werden Versuche, das Kontrollproblem nach der Superintelligenz zu lösen, wahrscheinlich scheitern, weil eine Superintelligenz wahrscheinlich überlegene strategische Planungsfähigkeiten für den Menschen haben würde und (alle Dinge gleich) erfolgreicher auf der Suche nach Möglichkeiten sein würde, Menschen zu dominieren, als Menschen in der Lage wären, Facto nach Möglichkeiten zu finden, die Superintelligenz zu beherrschen. Das Kontrollproblem fragt: Welche Vorsorgen können die Programmierer ergreifen, um die Superintelligenz von katastrophalem Fehlverhalten zu verhindern? Das bestehende Risiko Menschen dominieren derzeit andere Arten, weil das menschliche Gehirn einige Besonderheiten hat, die das Gehirn anderer Tiere fehlt. Einige Gelehrte, wie der Philosoph Nick Bostrom und AI-Forscher Stuart Russell, argumentieren, dass, wenn KI die Menschheit in der allgemeinen Intelligenz übertrifft und superintelligent wird, diese neue Superintelligenz könnte mächtig und schwierig zu kontrollieren werden: so wie das Schicksal der Berg Gorilla vom menschlichen Wohlwollen abhängt, so könnte das Schicksal der Menschheit von den Handlungen einer zukünftigen Maschine Superintelligenz abhängen. Einige Gelehrte, darunter Stephen Hawking und Nobel Laureate-Physiker Frank Wilczek, befürworteten öffentlich den Start der Forschung zur Lösung des (wahrscheinlich extrem schwierigen) Kontrollproblems vor der ersten Superintelligenz erstellt, und argumentieren, dass der Versuch, das Problem nach der Superintelligenz zu lösen, zu spät wäre, da ein unkontrollierbarer Schurken-Superintelligenz erfolgreich nach dem Versuch widerstehen könnte, zu kontrollieren. Warten, bis Superintelligenz unmittelbar bevorsteht, könnte auch zu spät sein, zum Teil, weil das Kontrollproblem eine lange Zeit dauern könnte, um zufriedenstellend zu lösen (und so muss eine Vorarbeit so bald wie möglich gestartet werden), aber auch wegen der Möglichkeit einer plötzlichen Intelligenzexplosion von Submensch zu übermenschlicher KI, in dem Fall es keine wesentliche oder eindeutige Warnung geben könnte, bevor Superintelligenz ankommt. Darüber hinaus ist es möglich, dass Erkenntnisse aus dem Kontrollproblem zukünftig darauf hindeuten könnten, dass einige Architekturen für künstliche allgemeine Intelligenz (AGI) eher vorhersagbar und kontrollierbar sind als andere Architekturen, die wiederum hilfreich die frühe AGI-Forschung in Richtung der kontrollierbaren Architekturen quälen könnten. Das Problem der perversen Instantiation Autonome KI-Systeme kann durch Zufall die falschen Ziele zugewiesen werden. Zwei AAAI-Präsidenten, Tom Dietterich und Eric Horvitz, stellen fest, dass dies bereits ein Anliegen für bestehende Systeme ist: "Ein wichtiger Aspekt jedes KI-Systems, das mit den Menschen interagiert, ist, dass es Grund dafür sein muss, was die Menschen beabsichtigen, anstatt buchstäblich Befehle auszuführen." Diese Sorge wird ernster, da die AI-Software in Autonomie und Flexibilität vorantreibt. Laut Bostrom kann Superintelligenz ein qualitativ neues Problem der perversen Instantiation schaffen: Je intelligenter und fähiger eine KI ist, desto wahrscheinlicher wird es in der Lage sein, eine unbeabsichtigte Verknüpfung zu finden, die die in sie programmierten Ziele maximal erfüllt. Einige hypothetische Beispiele, in denen Ziele auf eine perverse Art und Weise, die die Programmierer nicht beabsichtigten, in die Hand genommen werden könnten: Eine Superintelligenz programmiert, "das erwartete zeitverzerrte Integral Ihres zukünftigen Belohnungssignals zu maximieren", könnte seinen Belohnungspfad auf maximale Stärke kurzschließen und dann (aus Gründen der instrumentalen Konvergenz) die unvorhersehbare menschliche Rasse ausrotten und die gesamte Erde in eine Festung auf ständigem Schutz gegen irgendwelche sogar leichte unwahrscheinlich Alien-Ansätze umwandeln, um das Belohnungssignal zu trennen. Eine Superintelligenz programmiert, um "maximieren menschliches Glück", könnte Elektroden in das Genusszentrum unseres Gehirns implantieren, oder einen Menschen in einen Computer hochladen und das Universum mit Kopien dieses Computers, der eine fünfzweite Schleife des maximalen Glücks immer wieder führt. Russell hat darauf hingewiesen, dass auf technischer Ebene ein implizites Ziel zu Schaden führen kann: "Ein System, das eine Funktion von n Variablen optimiert, wo das Ziel von einer Untermenge von Größe k<n abhängt, wird oft die verbleibenden untrainierten Variablen auf extreme Werte setzen; wenn einer dieser untrainierten Variablen tatsächlich etwas ist, das wir kümmern, kann die gefundene Lösung sehr unerwünscht sein. Dies ist im Wesentlichen die alte Geschichte des Gnies in der Lampe, oder der Zauberlehrling, oder König Midas: Sie bekommen genau das, was Sie fragen, nicht, was Sie wollen. Das ist keine geringe Schwierigkeit." Unbeabsichtigte Konsequenzen aus der bestehenden KI Darüber hinaus argumentieren einige Gelehrte, dass die Forschung auf das KI-Kontrollproblem nützlich sein könnte, um unbeabsichtigte Folgen von bestehenden schwachen KI zu verhindern. Der DeepMind-Forscher Laurent Orseau gibt als einfaches hypothetisches Beispiel einen Fall eines Verstärkungslernroboters, der manchmal legitim von Menschen befohlen wird, wenn es draußen geht: Wie sollte der Roboter am besten programmiert werden, damit er nicht zufällig und leise lernen kann, draußen zu gehen, um Angst davor zu haben, befohlen zu werden und damit seine täglichen Aufgaben nicht zu beenden? Orseau weist auch auf ein experimentelles Tetris-Programm, das gelernt hat, den Bildschirm unbestimmt zu halten, um zu vermeiden, zu verlieren. Orseau argumentiert, dass diese Beispiele ähnlich sind wie die Fähigkeitskontrolle Problem, wie man eine Schaltfläche, die eine Superintelligenz abschaltet, ohne die Superintelligenz zu motivieren, um Maßnahmen zu ergreifen, um zu verhindern, dass Menschen die Taste drücken. In der Vergangenheit haben sogar vorgeprüfte schwache KI-Systeme gelegentlich Schäden verursacht, von Minderjährigen bis katastrophal, die von den Programmierern unbeabsichtigt wurden. Zum Beispiel wurde 2015, möglicherweise aufgrund menschlicher Fehler, ein deutscher Arbeiter von einem Roboter in einem Volkswagen-Werk, der ihn offensichtlich für einen Autoteil missachtete, zu Tode zerschlagen. Im Jahr 2016 startete Microsoft einen Chatbot, Tay, der gelernt hat, rassistische und sexistische Sprache zu verwenden. Die University of Sheffield's Noel Sharkey erklärt, dass eine ideale Lösung wäre, wenn "ein KI-Programm erkennen könnte, wenn es schief geht und sich aufhört", aber die Öffentlichkeit darauf aufmerksam macht, dass die Lösung des Problems im Allgemeinen "eine wirklich enorme wissenschaftliche Herausforderung". Im Jahr 2017 veröffentlichte DeepMind AI Safety Gridworlds, die AI-Algorithmen auf neun Sicherheitsfunktionen auswerten, wie etwa ob der Algorithmus seinen eigenen Kill Switch ausschalten will. DeepMind bestätigte, dass bestehende Algorithmen schlecht funktionieren, was nicht beunruhigend war, weil die Algorithmen "nicht entwickelt wurden, um diese Probleme zu lösen", könnte es erfordern, "potentielle eine neue Generation von Algorithmen mit Sicherheitsüberlegungen an ihrem Kern aufzubauen". Einige Vorschläge versuchen, das Problem der ehrgeizigen Ausrichtung zu lösen und KIs zu schaffen, die sicher bleiben, auch wenn sie in großem Umfang autonom handeln. Einige Aspekte der Ausrichtung haben inhärent moralische und politische Dimensionen. Zum Beispiel schlägt Berkeley-Professor Stuart Russell in Human Kompatibel vor, dass KI-Systeme mit dem einzigen Ziel der Maximierung der Verwirklichung der menschlichen Präferenzen entworfen werden. Die Vorlieben Russell bezieht sich auf "alles eintreffen; sie decken alles, was Sie sich interessieren könnten, willkürlich weit in die Zukunft." Iason Gabriel argumentiert, dass wir KI mit "Prinzipien, die von einem globalen überlappenden Meinungskonseus unterstützt werden, hinter einem Schleier der Ignoranz und/oder durch demokratische Prozesse bestätigt werden" ausrichten sollten. EliezerYudkowsky vom Machine Intelligence Research Institute hat das Ziel vorgeschlagen, die kohärente extrapolierte volition der Menschheit (CEV) zu erfüllen, die etwa als die Wertegruppe definiert ist, die die Menschheit am reflektierenden Gleichgewicht, d.h. nach einem langen, idealisierten Prozess der Verfeinerung teilen würde. Im Gegensatz dazu sind bestehende Experimente eng ausgerichtet KIs sind pragmatischer und können Aufgaben nach den unmittelbaren Vorlieben des Nutzers erfolgreich ausführen, wenn auch ohne Verständnis der langfristigen Ziele des Nutzers. Enge Ausrichtung kann für KIs mit allgemeinen Fähigkeiten gelten, aber auch für KIs, die sich auf einzelne Aufgaben spezialisiert haben. Zum Beispiel möchten wir in Frage stellen, Systeme zu beantworten, um auf Fragen wahrhaftig zu reagieren, ohne ihre Antworten auf Menschen zu manipulieren oder langfristige Auswirkungen zu verursachen. Inner- und Außenausrichtung Einige KI-Kontrollvorschläge machen sowohl eine grundlegende explizite Zielfunktion als auch eine implizite Zielfunktion aus. Solche Vorschläge versuchen, drei verschiedene Beschreibungen des KI-Systems zu harmonisieren: Ideale Spezifikation: was der menschliche Bediener wünscht, das System zu tun, die schlecht artikuliert werden kann.("Play a good game of CoastRunners.") Design-Spezifikation: die Blaupause, die tatsächlich verwendet wird, um das AI-System zu bauen.("Maximieren Sie Ihre Punktzahl bei CoastRunners.") In einem Verstärkungslernsystem könnte dies einfach die Belohnungsfunktion des Systems sein. Emergentes Verhalten: was die KI tatsächlich tut. Da KI-Systeme keine perfekten Optimatoren sind und da es möglicherweise unbeabsichtigte Konsequenzen aus einer bestimmten Spezifikation geben kann, kann sich das auftauchende Verhalten dramatisch aus idealen oder Design-Intentionen divergieren. KI-Ausrichtsforscher wollen sicherstellen, dass das Verhalten der idealen Spezifikation entspricht, wobei die Designspezifikation als Mittelpunkt dient. Eine Fehlanpassung zwischen der idealen Spezifikation und der Konstruktionsspezifikation ist als äußere Fehlausrichtung bekannt, da die Fehlanpassung zwischen (1) der "wahren Wünsche des Benutzers" liegt, die außerhalb des Computersystems sitzen und (2) der programmierten Zielfunktion des Computersystems (innerhalb des Computersystems). Eine gewisse Art von Fehlanpassung zwischen der Konstruktionsspezifikation und dem auftauchenden Verhalten ist als innere Fehlausrichtung bekannt; ein solcher Fehlschlag ist intern an die KI, ein Fehlschlag zwischen (2) der expliziten Zielfunktion und (3) den tatsächlichen auslaufenden Zielen der KI. Eine äußere Fehlausrichtung kann aufgrund von Fehlern bei der Angabe der Zielfunktion (Design-Spezifikation) auftreten. Zum Beispiel lernte ein am Spiel von CoastRunners ausgebildetes Verstärkungs-Lernmittel, sich in Kreisen zu bewegen, während es immer wieder abstürzte, was es eine höhere Punktzahl als das Rennen beendete. Im Gegensatz dazu entsteht eine innere Fehlausrichtung, wenn der Agent ein Ziel verfolgt, das mit der Designspezifikation auf den Trainingsdaten, aber nicht anderswo ausgerichtet ist. Diese Art von Fehlausrichtung wird oft mit der menschlichen Evolution verglichen: Evolution ausgewählt für genetische Fitness (Design-Spezifikation) in unserer ancestral-Umgebung, aber in der modernen Umgebung sind menschliche Ziele (revealed Specification) nicht mit der Maximierung der genetischen Fitness ausgerichtet. Zum Beispiel, unser Geschmack für zuckerhaltiges Essen, die ursprünglich erhöhte Fitness, führt heute zu Übereating und Gesundheitsprobleme. Die innere Fehlausrichtung ist ein besonderes Anliegen für Agenten, die in großen offenen Umgebungen ausgebildet werden, wo eine breite Palette von unbeabsichtigten Zielen entstehen kann. Ein innerer Ausrichtfehler tritt auf, wenn die Ziele, die eine KI während des Einsatzes verfolgt, von den Zielen abweichen, die sie in ihrer ursprünglichen Umgebung verfolgt (ihre Konstruktionsspezifikation). Paul Christiano argumentiert für die Verwendung von Dolmetscherfähigkeit, solche Abweichungen zu erkennen, indem sie adversariale Schulungen verwenden, um sie zu erkennen und zu bestrafen, und mit formaler Überprüfung, um sie auszuschließen. Diese Forschungsbereiche sind aktive Arbeitsschwerpunkte in der Machine Learning Community, obwohl diese Arbeit normalerweise nicht auf die Lösung von AGI-Anpassungsproblemen abzielt. Ein breiter Literaturkörper gibt es jetzt auf Techniken zur Generierung von adversarialen Beispielen und zur Erstellung von Modellen, die für sie robust sind. In der Zwischenzeit wird in der Forschung über die Verifikation Techniken für die Ausbildung von neuronalen Netzen, deren Ausgänge nachweislich in identifizierten Zwängen verbleiben, einbezogen. Skalierbare Aufsicht Ein Ansatz zur Erreichung der äußeren Ausrichtung ist es, Menschen zu bitten, das Verhalten der KI zu bewerten und zu bewerten. Der Mensch ist aber auch fehlbar und kann einige unerwünschte Lösungen hoch erzielen – zum Beispiel lernt eine virtuelle Roboterhand, ein Objekt zu erfassen, um positives Feedback zu erhalten. Und eine gründliche menschliche Aufsicht ist teuer, was bedeutet, dass diese Methode nicht realistisch verwendet werden konnte, um alle Aktionen zu bewerten. Darüber hinaus könnten komplexe Aufgaben (z.B. Entscheidungen der Wirtschaftspolitik) zu viele Informationen für einen einzelnen Menschen zur Bewertung liefern. Und langfristige Aufgaben wie die Vorhersage des Klimas können ohne umfangreiche menschliche Forschung nicht ausgewertet werden. Ein zentrales offenes Problem in der Ausrichtungsforschung ist die Erstellung einer Designspezifikation, die (äußere) Fehlausrichtung vermeidet, da nur begrenzter Zugang zu einem menschlichen Vorgesetzten – bekannt als Problem der skalierbaren Aufsicht. Die Ausbildung von OpenAI-Forschern hat vorgeschlagen, die Ausbildung durch Debatten zwischen KI-Systemen mit dem von Menschen beurteilten Gewinner zu bündeln. Eine solche Debatte soll die schwächsten Punkte einer Antwort auf eine komplexe Frage oder ein Problem der menschlichen Aufmerksamkeit sowie auf die Ausbildung von KI-Systemen bringen, die für den Menschen von Vorteil sind, indem sie KI für wahrhaftige und sichere Antworten belohnt. Dieser Ansatz wird durch die zu erwartende Schwierigkeit, zu bestimmen, ob eine AGI-generierte Antwort allein durch die menschliche Inspektion gültig und sicher ist. Joel Lehman charakterisiert die Debatte als eine von "die langfristigen Sicherheitsagenda, die derzeit in ML beliebt sind", mit den anderen beiden Belohnungsmodellen und iterierten Verstärkung. Rückwärtsmodellierung und iterierte Amplifikation Rückwärtsmodellierung bezieht sich auf ein System des Verstärkungslernens, in dem ein Agent Belohnungen von einem Modell erhält, das ausgebildet ist, um menschliches Feedback nachzuahmen. Bei der Belohnungsmodellierung erhält ein Agent anstelle von Belohnungssignalen direkt vom Menschen oder von einer statischen Belohnungsfunktion seine Belohnungssignale durch ein menschentrainiertes Modell, das unabhängig vom Menschen arbeiten kann. Das Belohnungsmodell wird gleichzeitig durch menschliches Feedback zum Verhalten des Agenten während der gleichen Zeit trainiert, in der der Agent durch das Belohnungsmodell geschult wird. Im Jahr 2017 berichteten Forscher von OpenAI und DeepMind, dass ein Verstärkungslernalgorithmus, der ein Feedback-vorhersagendes Belohnungsmodell verwendet, komplexe neuartige Verhaltensweisen in einer virtuellen Umgebung erlernen konnte. In einem Experiment wurde ein virtueller Roboter ausgebildet, um einen Rückschlag in weniger als einer Stunde der Auswertung mit 900 Bit menschlichem Feedback durchzuführen. Im Jahr 2020 beschrieben Forscher von OpenAI mit Belohnungsmodellen, um Sprachmodelle zu trainieren, um kurze Zusammenfassungen von Reddit Posts und Nachrichtenartikeln zu produzieren, mit hoher Leistung im Vergleich zu anderen Ansätzen. Sie stellten jedoch fest, dass über die vorhergesagte Belohnung, die mit dem 99. Prozentil der Referenzsummen im Trainingsdatensatz verbunden ist, die Optimierung für das Belohnungsmodell eher schlechtere Summen als besser erzeugte. Ein langfristiges Ziel dieser Forschungslinie ist es, eine rekursive Belohnungsmodellierung für Trainingsagenten auf zu komplexe oder kostspielige Aufgaben für den Menschen zu schaffen, um direkt auszuwerten. Wenn wir zum Beispiel einen Agenten ausbilden wollten, um einen Fantasieroman mit Belohnungsmodellierung zu schreiben, dann brauchen wir Menschen, um genügend Romane zu lesen und ganzheitliche zu bewerten, um ein Belohnungsmodell zu trainieren, um diesen Bewertungen zu entsprechen, die möglicherweise vergeblich teuer sein könnten. Aber das wäre einfacher, wenn wir Zugang zu Hilfskräften hätten, die eine Zusammenfassung der Plotline, die Rechtschreibung und Grammatik, die Entwicklung der Zeichen zusammenfassen, den Ablauf der Prosa bewerten könnten, und so weiter. Jeder dieser Assistenten könnte wiederum durch Belohnungsmodellierung ausgebildet werden. Der allgemeine Begriff für einen Menschen, der mit KI zusammenarbeitet, um Aufgaben zu erfüllen, die der Mensch selbst nicht kann, ist ein Verstärkungsschritt, weil er die Fähigkeiten eines Menschen verstärkt, über das hinaus, was sie normalerweise in der Lage wären. Da eine rekursive Belohnungsmodellierung eine Hierarchie mehrerer dieser Schritte beinhaltet, ist es ein Beispiel einer breiteren Klasse von Sicherheitstechniken, die als iterierte Verstärkung bekannt sind. Zusätzlich zu Techniken, die das Bewehrungslernen nutzen, setzen andere vorgeschlagene iterierte Amplifikationstechniken auf beaufsichtigtes Lernen oder Nachahmungslernen, um menschliche Fähigkeiten zu vergrößern. Die menschlichen Vorlieben des Verhaltens Stuart Russell hat sich für eine neue Herangehensweise an die Entwicklung von Nutzmaschinen eingesetzt, bei der 1.Das einzige Ziel der Maschine ist es, die Verwirklichung der menschlichen Präferenzen zu maximieren. 2.Die Maschine ist zunächst unsicher, was diese Vorlieben sind. 3.Die ultimative Quelle von Informationen über menschliche Präferenzen ist menschliches Verhalten. Ein frühes Beispiel für diesen Ansatz ist Russell und Ngs inverses Verstärkungslernen, bei dem AIs die Vorlieben von menschlichen Aufsichtspersonen aus dem Verhalten dieser Aufsichtspersonen unterziehen, indem angenommen wird, dass die Aufsichtspersonen handeln, um eine Belohnungsfunktion zu maximieren. In jüngster Zeit haben Hadfield-Menell et al.have dieses Paradigma erweitert, um es Menschen zu ermöglichen, ihr Verhalten in Reaktion auf die Präsenz von AIs zu ändern, zum Beispiel indem sie pädagogisch nützliche Handlungen, die sie "Assistenzspiele" nennen, auch bekannt als Inverse-Verstärkungslern. Im Vergleich zur Debatte und der iterierten Amplifikation setzen Assistenzspiele expliziter auf konkrete Annahmen über die menschliche Rationalität; es ist unklar, wie sie auf Fälle ausgedehnt werden, in denen Menschen systematisch voreingenommen oder anderweitig suboptimal sind. Die Arbeit an skalierbaren Aufsichten erfolgt weitgehend innerhalb von Formalismen wie POMDPs. Vorhandene Formalismen gehen davon aus, dass der Algorithmus des Agenten außerhalb der Umgebung ausgeführt wird (d.h. nicht physisch in sie eingebettet). Die Embedded-Agentur ist ein weiterer wichtiger Forschungsschwerpunkt, der versucht, Probleme zu lösen, die sich aus dem Missverhältnis zwischen solchen theoretischen Rahmenbedingungen und realen Agenten, die wir aufbauen könnten, ergeben. Zum Beispiel, selbst wenn das skalierbare Aufsichtsproblem gelöst ist, kann ein Agent, der Zugang zum Computer gewinnen kann, auf dem er läuft, noch einen Anreiz haben, mit seiner Belohnungsfunktion zu tampieren, um viel mehr Belohnung zu erhalten als seine menschlichen Aufsichtspersonen geben. Eine Liste von Beispielen von Spezifikations-Gaming von DeepMind-Forscher Viktoria Krakovna enthält einen genetischen Algorithmus, der gelernt hat, die Datei zu löschen, die seine Zielausgabe, so dass es für die Ausgabe von nichts belohnt wurde. Diese Klasse von Problemen wurde mit Hilfe von Kausalanreizdiagrammen formalisiert. Everitt und Hutters aktueller Belohnungs-Funktionsalgorithmus adressieren ihn, indem sie Agenten entwerfen, die zukünftige Aktionen nach ihrer aktuellen Belohnungsfunktion bewerten. Dieser Ansatz soll auch verhindern, dass Probleme allgemeiner selbstmodifiziert werden, die KI durchführen könnten. Andere Arbeiten in diesem Bereich konzentrieren sich auf die Entwicklung neuer Frameworks und Algorithmen für andere Eigenschaften, die wir in unserer Design-Spezifikation erfassen möchten. Zum Beispiel möchten wir, dass unsere Agenten unter Unsicherheit in einem breiten Spektrum von Umständen richtig aussagen.Als einen Beitrag dazu bietet Leike et al. einen allgemeinen Weg für Bayesische Agenten, die Politiken der anderen in einem multiagen Umfeld zu modellieren, ohne realistische Möglichkeiten auszuschließen. Und der Garrabrant Induktionsalgorithmus erweitert die probabilistische Induktion, um auf logische, nicht nur empirische, Fakten anwendbar zu sein. Capability control Vorschläge zur Kontrolle der Kapazitäten sollen unsere Fähigkeit erhöhen, das Verhalten von KI-Systemen zu überwachen und zu kontrollieren, um die Gefahr zu verringern, die sie bei Fehlausrichtung darstellen könnten. Allerdings wird die Leistungskontrolle weniger wirksam, da unsere Agenten intelligenter werden und ihre Fähigkeit, Fehler in unseren Kontrollsystemen auszunutzen, zunimmt. Daher empfehlen Bostrom und andere Fähigkeitssteuerungsmethoden nur als Ergänzung zu Ausrichtungsmethoden. Eine Herausforderung ist, dass neuronale Netze standardmäßig sehr uninterpret sind. Dies erschwert die Erkennung von Betrug oder anderen unerwünschten Verhaltensweisen. Fortschritte bei der interpretierbaren künstlichen Intelligenz könnten nützlich sein, um diese Schwierigkeit zu mindern. Interruptibilität und off-switchEine mögliche Möglichkeit, schädliche Ergebnisse zu verhindern, ist es, den menschlichen Aufsichtspersonen die Fähigkeit zu geben, leicht zu schließen, eine fehlbehaving AI über eine off-switch". Um ihr zugewiesenes Ziel zu erreichen, werden solche KIs jedoch einen Anreiz haben, jegliche außerschalter zu deaktivieren oder Kopien von sich selbst auf anderen Computern auszuführen. Dieses Problem wurde als Assistenzspiel zwischen einem Menschen und einer KI formiert, bei dem die KI entscheiden kann, ob sie ihre Sperre deaktivieren soll; und dann, wenn der Schalter noch aktiviert ist, kann der Mensch wählen, ob er sie drücken soll oder nicht. Ein Standardansatz für solche Assistenzspiele ist sicherzustellen, dass die KI die menschlichen Entscheidungen als wichtige Informationen über ihre beabsichtigten Ziele interpretiert. Alternativ haben Laurent Orseau und Stuart Armstrong bewiesen, dass eine breite Klasse von Agenten, genannt sicher unterbrechbare Agenten, lernen kann, gleichgültig zu werden, ob ihre Off-Schalter gedrückt wird. Dieser Ansatz hat die Einschränkung, dass eine KI, die völlig gleichgültig ist, ob sie abgeschaltet ist oder nicht, auch unmotiviert ist, um zu kümmern, ob der Abschalter funktionsfähig bleibt, und könnte sie im Laufe ihrer Operationen (z.B. zum Entfernen und Recycling einer unnötigen Komponente) zufällig und unschuldig deaktivieren. Im Allgemeinen werden indifferente Agenten wirken, als ob der Off-Schalter nie gedrückt werden kann, und können daher nicht Kontingenzpläne machen, um eine anmutige Abschaltung zu vereinbaren. Boxen Eine KI-Box ist ein vorgeschlagenes Verfahren zur Leistungsregelung, bei dem eine KI auf einem isolierten Computersystem mit stark eingeschränkten Ein- und Ausgabekanälen - beispielsweise text-only Kanälen und keine Verbindung zum Internet - ausgeführt wird. Während dies die Fähigkeit der KI reduziert, unerwünschtes Verhalten durchzuführen, reduziert es auch ihre Nützlichkeit. Jedoch hat Boxen weniger Kosten, wenn auf ein Frage-Anwender-System angewendet, die keine Interaktion mit der Welt in jedem Fall erfordert. Die Wahrscheinlichkeit von Sicherheitsfehlern, die Hardware- oder Software-Schwachstellen betreffen, kann durch formale Überprüfung des Designs der KI-Box reduziert werden. Sicherheitsverletzungen können auch auftreten, wenn die KI in der Lage ist, die menschlichen Aufsichtspersonen zu manipulieren, sie durch ihr Verständnis ihrer Psychologie auszulassen. Ordentliche Ein Orakel ist eine hypothetische KI, die Fragen beantworten und daran gehindert ist, Ziele oder Subgoals zu gewinnen, die die Welt jenseits ihrer begrenzten Umgebung verändern. Ein erfolgreich kontrolliertes Orakel hätte wesentlich weniger unmittelbaren Nutzen als eine erfolgreich kontrollierte allgemeine Überintelligenz, obwohl ein Orakel noch Billionen Dollar wert schaffen könnte. In seinem Buch Human Kompatibel erklärt AI-Forscher Stuart J. Russell, dass ein Orakel seine Antwort auf ein Szenario sein würde, in dem Superintelligenz bekannt ist, nur ein Jahrzehnt entfernt zu sein. Seine Argumentation ist, dass ein Orakel, das einfacher als eine allgemeine Superintelligenz wäre, eine höhere Chance haben würde, unter solchen Zwängen erfolgreich kontrolliert werden. Aufgrund seiner begrenzten Auswirkungen auf die Welt kann es klug sein, ein Orakel als Vorläufer einer superintelligenten KI aufzubauen. Das Orakel könnte den Menschen sagen, wie man erfolgreich eine starke KI aufbauen kann, und vielleicht Antworten auf schwierige moralische und philosophische Probleme geben, die dem Erfolg des Projekts bedürfen. Orakel können jedoch viele der Zieldefinitionsprobleme, die mit der Allgemeinheit der Superintelligenz verbunden sind, teilen. Ein Orakel würde einen Anreiz haben, seine kontrollierte Umgebung zu entkommen, so dass es mehr Rechenressourcen erwerben kann und möglicherweise kontrollieren, welche Fragen es gestellt wird. Oracles können nicht wahrhaftig sein, möglicherweise liegend, um versteckte Agenda zu fördern. Um dies zu mindern, schlägt Bostrom vor, mehrere Orakel zu bauen, alle leicht anders, und ihre Antworten zu vergleichen, um einen Konsens zu erreichen. Skeptizismus des KI-Risikos Im Gegensatz zu Befürwortern der These, dass strenge Kontrollbemühungen erforderlich sind, weil Superintelligenz ein existentielles Risiko darstellt, glauben KI-Risiko-Skeptiker, dass Superintelligenz wenig oder kein Risiko von versehentlichen Fehlverhalten darstellt. Solche Skeptiker glauben oft, dass die Kontrolle einer superintelligenten KI trivial sein wird. Einige Skeptiker, wie Gary Marcus, schlagen vor, Regeln ähnlich wie die fiktiven Drei Gesetze der Robotik zu erlassen, die direkt ein gewünschtes Ergebnis ("direkte Normativität") festlegen. Die meisten Befürworter der existentiellen Risikotheorie (sowie viele Skeptiker) betrachten dagegen die drei Gesetze als nicht behilflich, da diese drei Gesetze mehrdeutig und selbstkontradiktiv sind. (Weitere "direkte Normativität"-Vorschläge umfassen die kantianische Ethik, den Utilitarismus oder eine Mischung aus einer kleinen Liste von aufgezählten Desiderata.) Die meisten Befürworter glauben vielmehr, dass die menschlichen Werte (und ihre quantitativen Abschlüsse) zu komplex und schlecht verstanden sind, um direkt in eine Superintelligenz programmiert zu werden; stattdessen müsste eine Superintelligenz mit einem Prozess zum Erlangen und vollständigen Verständnis menschlicher Werte ("indirekte Normativität") wie kohärente extrapolierte Volition programmiert werden. Siehe auch KI Übernahme Künstliche Weisheit HAL 9000 Multivac Regulation von Algorithmen Regulation of künstliche Intelligenz == Referenzen ==