Normale Quadrate (OLS) sind eine Art linearer, mindestens Quadrate Methode zur Schätzung der unbekannten Parameter in einem linearen Regressionsmodell. OLS wählt die Parameter einer linearen Funktion einer Reihe von erläuternden Variablen nach dem Prinzip von mindestens Quadratmetern aus: Minimierung der Summe der Quadrate der beobachteten Abhängigkeitsvariablen (Werte der zu beobachtenden variablen Variablen) in der angegebenen Datenset und der von der linearen Funktion der unabhängigen Variablen vorhergesagten Parameter. geometrischerweise wird dies als Summe der Quadratkilometer parallel zur Achse der abhängigen Variablen zwischen jedem Datenpunkt im Set und dem entsprechenden Punkt auf der Regressionsoberfläche gesehen – die kleineren Unterschiede, das Modell entspricht den Daten. Der daraus resultierende Estimator kann durch eine einfache Formel ausgedrückt werden, insbesondere im Falle einer einfachen linearen Regression, in der ein einziger Regressor auf der rechten Seite der Regressionsgedanke besteht. Der OLS-Stimator ist konsistent, wenn die Regressoren exogen sind und – durch die Gaus-Markov-Theoremoptimal – in der Klasse linearer, unvoreingenommener Estimatoren, wenn die Fehler Homosivität und regelwidrige Unkorrekte sind. Unter diesen Bedingungen bietet die Methode der OLS eine minimale Varianz der durchschnittlichen Schätzung, wenn die Fehler finite Sorten aufweisen. Unter der zusätzlichen Annahme, dass die Fehler normalerweise verteilt werden, ist OLS die höchstwahrscheinliche Schätzung. Lineares Modell Löschung der Daten besteht aus n JPYdisplaydisplaystyle n} Beobachtungen { x i , y i } i = 1 n {\displaystyle linksbf\log {x} i},y_{i}\_{i=1}^{n .Each-Beobachtung i {\displaystyle i} beinhaltet eine scalar Antwort y i WELLdisplaystyle y_{i} und einen Spaltenvektor x i {\displaystyle \ Mathematik 1998 {x} {_i} von p {\displaystyle p} Parameter (Regressoren), d. h. x i = [ x i 1 , x i 2, ... , ... x i ] T KINGstyle \ Mathematik {x} i}=\left [x_{i1},x_{i2},\dots ,x_{ip}\right]^{\sf {T} .In einem linearen Regressionsmodell, der Reaktionsvariable, y i KINGstyle y_{i} ist eine lineare Funktion der Regressoren: y i = β 1 x i 1 + β 2 x i 2 +  + + β p x i p +  i i , {\displaystyle y_{i}=\beta {_1\} x_{i1}+\beta {_2s x_{i2}+\cdots +\beta {_p\} x_{ip}+\varepsilon {_i,} oder in Vektorform, y i = x i T β +  i i , {\displaystyle y_{i}=\ Mathematik {x} T iprint Symbol Memebeta +\}varepsilon {_i, where, wo x i {\displaystyle \ Mathematik {x} {_i}, wie bereits eingeführt, ist ein Spaltenvektor der i {\displaystyle i} -th Beobachtung aller erläuternden Variablen; β Memestyle Memestyle Memebeta . ist ein p × 1 KING p\times 1} unbekannter Parameter; und cala s istyle istyle istylerstyle {rstyle {T vare }.  i i {\displaystyle \varepsilon {_i} trägt den Einflüssen auf die Reaktionen y i {\displaystyle y_{i} aus anderen Quellen als den Explanatoren x i {\displaystyle \ Mathematik {x} {x} {x} {_i} . Dieses Modell kann auch in der Matrix nicht als y = X ε, Memestyle \logy} \ Mathematik {X} {X} {X} {x} {x} {x} {X-Marke-Markta} geschrieben werden. {+s s   {\,\,}, wo y {\displaystyle \ Mathematik {y} } und ε displaystyle olivarepsilon × × n × 1 {\displaystyle n\times 1} Vektor der Reaktionsvariablen und Fehler der n varie n}, und X \rm \rm } {X} ist ein × 1 n s s n\times 1}. {T} und enthält die i {\displaydisplaystyle i} - Bemerkungen zu allen erläuternden Variablen. In der Regel ist der konstante Begriff immer in den Satz der Regressoren X dampfstyle \ Mathematikrm {X} } enthalten, indem man x i 1 = 1 Glühbirne x_{i1}=1 für alle i = 1 , ... n Memestyle i=1,\dots ,n} .Der Koeffizient 1 ß \beta \beta {1} entspricht diesem Regressor. Regressors müssen nicht unabhängig sein: Es kann jede gewünschte Beziehung zwischen den Regressoren geben (so lange es sich nicht um eine lineare Beziehung handelt). Man kann beispielsweise verdächtigen, dass die Antwort linear von einem Wert und seinem Quadrat abhängt; in welchem Fall wir einen Regressor umfassen, dessen Wert nur dem Quadrat eines anderen Regressor entspricht. In diesem Fall wäre das Modell quadratisch im zweiten Regressor, aber noch kein lineares Modell, weil das Modell in den Parametern noch linear ist ( β {\displaystyle fasersymbol HANAbeta     ). Matrix/vector Formulierung erwägen Sie ein überfälliges System  j j = 1 p X i j β j = y i , i = 1 , 2 , ... n ) , Memestyle \sum j=1pp}X_{ijsbeta j}=y_{i,\ (i=1,2\dots n)} n n } lineare n} in p fistyle, β, β, 1 {d, s,  ..., s, s, s, s, s (Anmerkung: für ein lineares Modell, wie oben, enthält nicht alle Elemente in X KINGstyle \ Mathematikrm {X} } Informationen zu den Datenpunkten. Die erste Spalte ist mit denen bevölkert, X i 1 = 1 {\displaystyle X_{i1}=1 .Nur die anderen Spalten enthalten tatsächliche Daten. So ist die Zahl der Regressoren plus eines gleich.) Dieses Formular kann in Matrixform als X β = y , KINGstyle \ Mathematikrm geschrieben werden {X} faser-Logo =  {y} ,}, wo X = [ X 11 X 12  X X 21 X 22  X X 2 p  X  X X n 1 X  X X n 2  X X n p ] , β = [ β 1 β 2 ⋮ β p ] , y = [ y 1 y 2   n ] ] . 7.8displaystyle \ Mathematikrm {X} =begin{b Matrix}X_{11} &X_{12} X_{1pXX_{21} &X_{22} &\cdots &X_{2pvvdots &\vdots &\ddots &\vdots X_{n1} &X_{n2} &\cdots &X_{np}\end{bmatrix}},\qquad  Handelsmarke olibeta =Begin{bmatrix}\beta {_1}\\\beta {_2vvdots \beta _p}\end{b Matrix}},\qquad \ Mathematik {y} =begin{b Matrix}y_{1}\\y_{2vvdots y_{n.end{bmatrix. Ein solches System hat in der Regel keine genaue Lösung, so dass das Ziel stattdessen darin besteht, die Koeffizienten β dampfdisplaystyle HANAbeta }}} zu finden, die am besten geeignet sind, im Sinne der Lösung des quadratischen Minimisierungsproblems β ^ = ein r g i n β S ( β ) , Memedisplaystyle SSOhat  Binnenmarktbeta ·)underset  Binnenmarktzeichen 7.8beta )operator {arg\,min} ,}S(Eckezeichen Memebeta }}),}, wo die objektive Funktion S HANAdisplaystyle S} von S ( β ) =  i i = 1 n i −  j j = 1 p X i j β j; 2 = ) y  - X β ‖ 2 . . Memedisplaystyle S(Getreta )=\}sum _i=1}^{n}{\biggl {i}-\sum j=1}^{p}X_{ij}\beta _j}{\biggr ^2}=grobigl \8.5} -\ Mathematikrm {X} fasersymbol Memebeta {\}bigr | Rechtfertigung für die Auswahl dieses Kriteriums ist in der nachstehenden Eigenschaft enthalten. Dieses Minimisierungsproblem hat eine einzigartige Lösung, sofern die Spalten der Matrix X Memestyle \ Mathematikrm {X} linear voneinander unabhängig sind, da die normalen Gleichungen ( X T X ) β ^ = X T y gelöst werden. 7.8displaystyle (\ Mathematikrm {X} )sf {T) Mathematikrm {X} 574 574 fasersymbol SSObeta {X} . Mathematiksf {T. Mathematik {y} \ \ } Kennzeichnend für die Matrix X T X X X Memedisplaystyle {X} . Mathematikrm {X} } ist die Grammatrix und die Matrix X T y WELLdisplaystyle \rm {X} {^\sf {T. Mathematik {T. Mathematik {X] {T. Mathematik {X} } ist als Moment der Matrix von Regressoren bekannt. Letztlich ist β ^ HANAdisplaystyle Memehat HANA-Mark Symbol 7.8beta  coefficient der Schlüsselfaktor des am wenigsten frequentierten Hyperflugzeugs, ausgedrückt als β ^ = X T X ) - 1 X T y . 7.8displaystyle SSOhat faserzeichen HANAbeta = Link(\ Mathematikrm) {X} Mathematiksf {T}}\rm {X} rechts)-1-1-1 {X} Mathematikf {T}}\9.5 {y} .} Estimation Suppos b ist ein Kandidat für den Parametervektor β. Die Menge yi ‐ xiTb, die als Rest für die i-th-Beobachtung bezeichnet wird, misst die vertikale Entfernung zwischen dem Datenpunkt (xi, yi) und dem Hyperflugzeug y =xTb und bewertet somit den Grad der Anpassung zwischen den tatsächlichen Daten und dem Modell. Die Summe der Quadratkilometer (SSR) (auch als Fehlerquote von Quadraten (ESS) oder Restbeträge von Quadraten (RSS) bezeichnet) ist eine Maßnahme des Gesamtmodells: S (b ) =  i i = 1 n ( y i − x i T b) 2 = ( y − X b ) T ( y − X b ) , {\displaystyle S(b)=\sum i=1}^{n}(y_{i}-x_{i}^{\rm {T} b)^{2=(y-Xb)^{\ma.rm {T} (y-Xb), wo T Ts die Matrix umsetzt und die Werte aller unabhängigen Variablen, die mit einem bestimmten Wert verbunden sind, verleugnet werden. Der Wert von b, der diese Summe minimiert, ist der OLS-Stimator für β.Die Funktion S(b) quadratisch in b mit positivem Hesssian und damit verfügt diese Funktion über ein einzigartiges globales Mindestmaß an b = β ^ 7.8displaystyle b= Finanzbeta }}} . . . .  X  X  X  X  X  X S (T) X  1 1 y  y  y  y  X  X  X  X  X. KINGstyle = atorname {argmin} {_b\in \bb {R} p)S(b)=(X^{\rm Mathematik) {T} X)-1-1}X) Mathematik {T} } Das Produkt N=XT X ist eine Grammatrix und seine umgekehrt, Q=N–1, ist die Koeffiziente Matrix von β, die eng mit ihrer Kovarianzmatrix, Cβ.The Matrix (XT X)–1 Tub=Q, wird als die Moore-Penrose pseudoinverse Matrix von X bezeichnet. In dieser Formulierung wird der Punkt hervorgehoben, dass die Schätzung vorgenommen werden kann, wenn und nur, wenn es keine perfekte Multicollinearität zwischen den erläuternden Variablen gibt (die die Gramm-Matrix nicht umgekehrt haben würde). Nach schätzungsweise β werden die eingebauten Werte (oder vorhergesagte Werte) aus der Regression y ^ = X β ^ = P y , Memestyle Memehat y}}= XILLAhat HANA =Py, wo P = X(XTX) -1XT die Projektionsmatrix auf den Raum V, der von den Spalten von X bedeckt ist. Diese Matrix P wird manchmal auch die Hassmatrix genannt, weil sie "ein Hass" auf den variablen y anwendet. Eine andere Matrix, die eng mit P verknüpft ist, ist die annihilatormatrix M = In − P; dies ist eine Projektionsmatrix auf dem Raum oder dem Roten Kreuz bis V. Sowohl matrices P als auch M sind symmetrisch und idempotent (das bedeutet, dass P2 = P und M2 = M2) und beziehen sich auf die Datenmatrix X über Identität PX = X und MX = 0. M schafft die Reste der Regression:   ^ = y − y ^ = y − X β ^ = M y = M ( X β + ε ) = ( M X ) β + M  = = M . KINGstyle SSOhat HANAvare =} y==y-XILLAhat livbeta My=M(X\beta +\varepsilon (=MX)\beta+M\varepsilon = M\varepsilon .} Mit diesen Resten können wir den Wert von  2 2 anhand der reduzierten chi-Quadrat-Statistiken schätzen: s 2 = ^ ^ ^ n − p = ( M y ) T M y n − p = y T M T M T M y n n − p = y T M y T M y n n − p = S ( β ^ ) n  = 2 = n n n n n n n s 2 n n n n sstyle HOR2 {T} n-p== Finanzfrac (My))rm {T} My}{n-p==ggiofrac y^{\ Mathematik {T} {T} My}{n-p==ggiofrac y^{\ Mathematik {T} My}{n-p==ggiofrac S(Gethat olibeta ) {n-p,,\qquad Memehat sigma ^2} n-p;n;;s^{2 n-p ist der statistische Grad der Freiheit. Die erste Menge, s2, ist die OLS-Schätzung für  for2, während die zweite,  2 ^ 2 Memestyle \skriptstyle Memehat 7.8sigma {^2 , die MLE Schätzung für σ2. Die beiden Ester sind in großen Proben recht ähnlich; der erste Estimator ist immer unvoreingenommen, während der zweite Estimator einseitig ist, aber mit einem kleineren durchschnittlichen Fehler. In der Praxis wird s2 häufiger verwendet, da es für die Hypothesis-Tests bequemer ist. Die Quadratwurzel von s2 wird als Regressionsstandardfehler, Standardfehler der Regression oder Standardfehler der Gleichung bezeichnet. Es ist üblich, den Nutzen der OLS-Regression zu bewerten, indem man vergleicht, wie viel die anfängliche Variante der Probe durch eine Regressierung auf X verringert werden kann. R2 wird als Verhältnis der erklärten Varianz der Gesamtvarianz der abhängigen variablen y definiert, in den Fällen, in denen die Regressionssumme der Quadrate die Summe der Reste entspricht: R 2 = ) ( y ^ i − y ̄   ) 2 ) ( y i - y ̄ ) 2 = y T P T L P y T L y = 1 − y T M y T L y = 1 − R S S S S S S S R22} = {(\hat y}}_{i}-Baroverline y}}2{\sum (y_{i}-Baroverline y}}){\2== {T} {T} LPy}{y^{\rm {T} y^{\ Mathematik {T} My}{y^{\rm {T} Ly==1-Getfrac Memerm RSS RSSrm {TSS}, wo TSS die Gesamtsumme der Quadrate für die abhängige variable L = I n − 1 n J n ILLAtext L=I_{n}-7.8frac 1JnJJ_{n , und J n {\textstyle J_{n} ist eine n×n Matrix von denen(. L {\displaystyle L} ist eine Konzentrationsmatrix, die einer ständigen Regression gleichkommt; sie untergräbt einfach den Mittelwert einer variablen Variablen.) Um R2 sinnvoll zu sein, muss die Matrix X der Daten über Regressoren einen Spaltenvektoren enthalten, um den konstanten Regressionsabschuss zu vertreten. In diesem Fall wird R2 immer eine Reihe von 0 bis 1 sein, wobei Werte in der Nähe von 1 zu einem guten Grad an Eignung angegeben sind. Die Varianz der unabhängigen Variablen als Funktion der abhängigen Variablen wird in Artikel Polynomial mindestens Quadrate angegeben. Einfache lineare Regression Liegt die Datenmatrix X nur zwei Variablen, ein konstantes und ein scalar Regressor xi, so wird dies als "einfaches Regressionsmodell" bezeichnet. Dieser Fall wird oft in den Klassen der Chip-Statistiken betrachtet, da es viel einfachere Formeln bietet, die sogar für die manuelle Berechnung geeignet sind. Die Parameter werden häufig als (α, β): y i = . + β x i +  i i . KINGstyle y_{i}=\alpha +\beta x_{i} {_i}. In diesem Fall werden die am wenigsten geschätzten Quadrate durch einfache Formeln β ^ = n  i x i y i y i  y x i 2  ( x i 2  i  2  i  i  i s  i  =  i s s s s s s s s s s ^ s , s s {\ {\ {\ {\ {\ {\ }{\ s }{\ }{\ s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s  } &=1,0frac {n}\sum x_{i}y_{i}}-\sum x_{i}}\sum {i}\n}\sum x_{i}^{2(-(\sum x_{i))^{2 hat livalpha } &=ñoverline y}}-Porthat ,{\overline {x\} ,\end{aligned} Alternative Ursachen In der vorherigen Sektion wurden die am wenigsten Quadrate Estimator β ^ RARdisplaystyle Memebeta . als Wert gewonnen, der die Summe der Quadratkilometer des Modells minimiert. Es ist jedoch auch möglich, den gleichen Erreger aus anderen Ansätzen zu entziehen. In allen Fällen bleibt die Formel für OLS Estimator unverändert:  (= (=XTX) -1XT; der einzige Unterschied ist in der Auslegung dieses Ergebnisses. Projekt OLS ist eine ungefähre Lösung für ein überfälliges System linearer Gleichungen. Xβ ≈ y, wo β das unbekannt ist. Wenn das System nicht genau gelöst werden kann (die Zahl der n ist viel größer als die Zahl der unbekannten P), suchen wir eine Lösung, die die kleinste Diskrepanz zwischen den richtigen und linken Seiten bieten könnte. Mit anderen Worten suchen wir die Lösung, die β ^ = ein r g min ‖ y − X β   {\ {\ {\ ={\}rm {arg{\min WELL_beta ,}lVerty-X\beta \rVert }, wo · die Standard-L2-Norm im ndimensionalen Euclidean-Raum Rn.Die vorhergesagte Menge Xβ ist nur eine gewisse lineare Kombination der Vektor von Regressoren. So ist der Rest Vektor y − Xβ wird die kleinste Länge haben, wenn y auf den linearen Teilraum, der von den Spalten von X bedeckt ist, projiziert wird. In diesem Fall kann der OLS estimator β ^ {\displaystyle Memehat ggiobeta . so ausgelegt werden wie die Parameter der Vektorablagerung von ^y =Py auf der Grundlage von X. In anderen Worten können die Unterschiede auf dem Mindestmaß geschrieben werden wie: ( y ‐ X ^ ) T X = 0. Memedisplaystyle (\ Mathematik {y} -X cuhat liv faserzeichen Memebeta )^\rm{T.X=0. Eine geometrische Auslegung dieser Gleichungen ist, dass der Vektor der Reste, y ‐ X β ^ Memestyle \ Mathematik 1998 {y} -X cuhat Stempel Symbol Memebeta  the ist orthogonal für den Spaltenraum von X, da das Punktprodukt ( y β ^ ^) ) X v {\displaystyle (\ Mathematik {y} -XTONhat faserzeichen faserbeta ))\cdot X\ Mathematik {v} } ist für alle Konformitätsvektoren, v. Dies bedeutet, dass y ‐ X β ^ {\displaystyle \ Mathematik {y} -X Steuerzeichen Memehat Memebeta  of die kürzeste aller möglichen Vektor y − X β {\displaystyle \ Mathematik {y} -Xholprint Symbol HANAbeta ., das ist die Varianz der Reste das Minimum. Dies zeigt sich am richtigen. Einführung von ^ ^ WELLdisplaystyle Memehat 574gamma }}}} und eine Matrix K mit der Annahme, dass eine Matrix [ X K ] [X\ K]} ist nicht-singular und KT X = 0(vgl.orthogonale Projektionen), der Restvektor sollte die folgende Formel erfüllen: r ^  y y β ^ = K . ^ = K . ^ . KINGstyle SSOhat {r}  triwinkelq \ Mathematik {y} -XTONhat faserzeichen Glühbirnebeta =}KILLAhat faser 7.8gamma ..} Die Gleichung und Lösung linearer mindestens Quadrate werden daher wie folgt beschrieben: y = [ X K ] ( β ^ ^ ^ ) , Memedisplaystyle \ Mathematik\\\\phy}=beginb Matrix}X &Kend\{b Matrix}}{\beginp Matrix}{\beginp Matrix}{\beginp Matrix hat faserzeichen helmbeta {\benta \{endp Matrix, β ^ ) ^ ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )  =  =  =  =  = ) ) ) ) ) ) ) ) ) ) ) ) ) ) X K ] − 1 y = [ ( X T X ) − 1 X T (K T K ) − 1 K T ] y . 7.8displaystyle beginnt {p Matrix hat olibeta {\}hat {\hat {\hat {\hat {\hat {\benta {\hat {\ {\ {\ {\ {\ {\ {\ {\ amma amma amma amma {\ = = TXX)-1-1}X^{\rm T((K)rm T}}K)-1-1}K Trm T Tend{b Matrix}}\ Mathematik {y} Eine weitere Möglichkeit, zu prüfen, ob die Regressionslinie ein gewichteter Durchschnitt der Linien ist, die über die Kombination von zwei Punkten im Datenset abgewickelt werden. Obwohl diese Berechnungsmethode rechnerisch teurer ist, bietet sie eine bessere Ausbildung an OLS. Maximale Wahrscheinlichkeit OLS Estimator ist identisch mit der höchstwahrscheinlichen Estimator (MLE) unter der normalen Annahme der Fehlerbedingungen.[sicher] Diese Annahme hat historische Bedeutung, da sie die Grundlage für die frühe Arbeit in linearer Regressionsanalyse von Yule und Pearson lieferte. Wir können von den Eigenschaften von MLE abweichen, dass der OLS estimator asymmetrisch effizient ist (im Sinne des Erreichens der Cramér-Rao-Konzentration für Varianz), wenn die Normalität erfüllt ist. Allgemeine Methode der Momente In iid-Fall kann der OLS-Stimator auch als GMM-Stimator angesehen werden, der sich aus den Momentbedingungen E [ x i ( y i − x i T β ) ] ergibt. 0. KINGstyle \rm {E} WELLbig ,[x_{i}(y_{i}-x_{iTT beta ) Haushalts,big =]0. In diesem Moment ist festzustellen, dass die Regressoren mit den Fehlern nicht in Zusammenhang stehen sollten. Da xi ein p-vector ist, entspricht die Anzahl der Momentbedingungen der Dimension des Parametervektors β und damit genau das System. Dies ist der sogenannte klassische GMM-Fall, wenn der Estimator nicht von der Wahl der Gewichtungsmatrix abhängt. Hinweis darauf, dass die ursprüngliche strenge exogene Annahme E[.i[ xi] = 0 einen weit reichenren Satz von Momentbedingungen bedeutet als oben angegeben. Insbesondere bedeutet diese Annahme, dass für alle vektorfunktionellen  the die derzeitige Bedingung E[[(xi)xii] = 0 halten wird. Es kann jedoch unter Verwendung der Gauss-Markov-Theorem gezeigt werden, dass die optimale Wahl der Funktion   (x)= x, die in der vorübergehenden Formel führt. Eigenschaften Es gibt mehrere verschiedene Rahmen, in denen das lineare Regressionsmodell zum Einsatz kommen kann, um die OLS-Technik anzuwenden. Jede dieser Einstellungen produziert dieselben Formeln und gleiche Ergebnisse. Der einzige Unterschied ist die Auslegung und die Annahmen, die erforderlich sind, um die Methode sinnvoll zu nutzen. Die Wahl des anwendbaren Rahmens hängt vor allem von der Art der Daten und der zu erfüllenden Aufgabe ab. Eine der unterschiedlichen Auslegungslinien ist, ob die Regressoren als Zufallsvariablen oder als vordefinierte konstanten behandelt werden. Im ersten Fall (random design) werden die Regressors xi randomisiert und gemeinsam mit den yi's von einigen Bevölkerungsgruppen getestet, wie in einer Beobachtungsstudie. Dieser Ansatz ermöglicht eine weitere natürliche Untersuchung der asymptotischen Eigenschaften der Ester. In der anderen Auslegung (fixiertes Design) werden die Regressoren X als bekannte Kontingente behandelt, die von einem Design festgelegt werden, und y wird an die Werte von X als Experiment gebunden. Konkret ist diese Unterscheidung oft unwichtig, da die Schätzung und die Gleichgültigkeit während der Lagerung auf X durchgeführt werden. Alle in diesem Artikel genannten Ergebnisse liegen im Zufallskonzept. Klassische lineare Regression Das klassische Modell konzentriert sich auf die Schätzung und Ausgewogenheit von "finite Proben", so dass die Anzahl der Beobachtungen n feststeht. In diesem Gegensatz zu den anderen Ansätzen, die das asymptotische Verhalten von OLS untersuchen, und in denen die Anzahl der Beobachtungen zu Unvermögen wachsen darf. Berichtigungsspezifikation. Die lineare funktionelle Form muss mit der Form des tatsächlichen Datenerhebungsprozesses übereinstimmen. Strenge Exogenität. Fehler bei der Regression sollten bedingt sein Null: E  in [   X ] = 0. KINGstyle \operatorname {E} ,\[varepsilon \mid X\,]=0.} unmittelbare Folge der exogenitätsbedingten Annahme ist, dass die Fehler Null haben: E[ and] = 0 und dass die Regressoren nicht mit den Fehlern in Zusammenhang stehen: E[XT of] =0.Die exogene Annahme ist für die OLS Theorie von entscheidender Bedeutung. Kommt es dann, werden die Regressor-variablen exogeniert. Kommt es nicht, so werden die Regressoren, die mit der Fehlerfrist korreliert werden, als endogen bezeichnet, und dann werden die OLS-Schätzungen ungültig. In diesem Fall kann die Methode der maßgeblichen Variablen verwendet werden, um die Gleichgültigkeit zu erfüllen. Keine lineare Abhängigkeit. Man muss alle linear unabhängig sein. Maisch bedeutet dies, dass die Matrix X die gesamte Spalte nahezu sicher haben muss: [ Rang ) ( X ) = p ] = 1. Memedisplaystyle \Pr ggio!big ,\[}operatorname {rank} (X)=p\, cubig =]1. In der Regel wird auch davon ausgegangen, dass die Regressoren bis zu mindestens dem zweiten Moment finite Momente haben. Dann ist die Matrix Qxx = E[XTX / n] finite und positive Halbdefinite. Wenn diese Annahme verletzt wird, werden die Regressoren linear abhängig oder vollkommen multicollinear genannt. In diesem Fall kann der Wert des Regressionskoeffizienten β nicht gezogen werden, obwohl die Vorhersage von y-Werten für neue Werte der Regressoren, die im gleichen linear abhängigen Teilraum liegen, noch möglich ist. Spherische Fehler: Var   [    X X ] =  2 2 I n , JPYstyle \operatorname {Var} ,\[varepsilon \mid X\,]=\sigma ^2}I_{n, wo Ist die Identitätsmatrix in der Dimension n, und σ2 ist ein Parameter, der die Varianz der einzelnen Beobachtungen bestimmt. Diese .2 gilt als Nusance-Parameter im Modell, obwohl sie in der Regel ebenfalls geschätzt wird. Wenn diese Annahme verletzt wird, sind die OLS-Schätzungen noch gültig, aber nicht mehr effizient. Es ist üblich, diese Annahme in zwei Teile aufzuteilen: Homoscedasticity: E[ ii2] = .2, was bedeutet, dass die Fehlerfrist bei jeder Beobachtung die gleiche Varianz aufweist. Wenn dieses Erfordernis verletzt wird, wird dies als Heterogenität bezeichnet, in einem solchen Fall würde ein effizienterer Ester mindestens Quadrate gewichtet. Wenn die Fehler unbegrenzt sind, werden die OLS-Schätzungen auch unendliche Unterschiede aufweisen (obwohl das Gesetz der großen Zahl, die sie dennoch in der Regel auf die wahren Werte, solange die Fehler Null aufweisen). In diesem Fall werden robuste Schätzungstechniken empfohlen. Keine Autokorrelation: Die Fehler sind untrennbar mit Beobachtungen verbunden: E[ ii |j | X ] = 0 für i  j j. Diese Annahme kann im Zusammenhang mit Zeitreihendaten, Paneldaten, Clusterproben, hierarchischen Daten, wiederholten Maßnahmendaten, Längsschnittdaten und anderen Daten mit Abhängigkeiten verletzt werden. In solchen Fällen bieten die meisten Quadrate eine bessere Alternative als die OLS. Ein weiterer Ausdruck für die autocorrelation ist eine serielle Korrelation. Normalität. Es wird manchmal auch davon ausgegangen, dass die Fehler die normale Verteilung von den Regressoren abhängig machen:   X  N N ( 0 ,  2 2 I n ) . 7.8displaystyle \varepsilon \mid X\sim fasercal {N).(0,\sigma ^2}I_{n) Diese Annahme ist für die Gültigkeit der OLS-Methode nicht erforderlich, obwohl bestimmte zusätzliche Floheigenschaften im Falle der Prüfung (insbesondere im Bereich der Hypothesen) festgelegt werden können. Auch wenn die Fehler normal sind, entspricht der OLS-Stimator dem höchstwahrscheinlichen Ester (MLE) und ist daher in der Klasse aller regelmäßigen Ester asymmetrisch effizient. Wichtig ist, dass die Normalität nur für die Fehlerbedingungen gilt; im Gegensatz zu einem populären Missverständnis ist die Reaktion (unabhängig) nicht erforderlich, um normalerweise zu verteilen. Unabhängige und identisch verteilte (iid)In einigen Anwendungen, insbesondere mit Querschnittsdaten, wird eine zusätzliche Annahme eingeführt – dass alle Beobachtungen unabhängig und identisch verteilt sind. Dies bedeutet, dass alle Beobachtungen aus einer Zufallsstichprobe entnommen werden, die alle oben genannten Annahmen einfacher und einfacher auszulegen. In diesem Rahmen können auch die asymptotischen Ergebnisse (wie die Stichprobengröße n → .) angegeben werden, die als theoretische Möglichkeit verstanden werden, neue unabhängige Beobachtungen aus dem Datenerhebungsprozess einzuholen. Die Liste der Annahmen in diesem Fall ist: iid Beobachtungen: (xi, yi) ist unabhängig von und hat die gleiche Verteilung wie (xj, yj) für alle i  j j; keine perfekte Multicollinearität: Qxx =E[ xiT ] ist eine positive Matrix; exogenität: E[ ii ] = 0; Homoscedity: Var[ ii[ xi ] = σ2. {xi, yi} ist stationärer und ergodischen Prozess; wenn {xi, yi} nichtstationär ist, sind die Ergebnisse der OLS oft spärlich, es sei denn, {xi, yi} ist zusammengratuliert. Die Regressoren sind vorab festgelegt: E[xiεi] = 0 für alle i = 1, ..., n; n; die p×p Matrix Qxx =E[ xiT ] ist vollrangig und daher positiv; {xiεi} ist eine Martingale Differenzsequenz mit einer finite Matrix der zweiten Momente Qxxε2=E[ ii2 xiT xiT ].] Finite Stichprobeneigenschaften Erstens, unter strenger Exogenitäts Annahme, dass die OLS estimatoren β ^ WELLdisplaystyle \skriptstyle Memehat 7.8beta }}} und s2 unvoreingenommen sind, was bedeutet, dass ihre erwarteten Werte mit den wahren Werten der Parameter übereinstimmen:[sicher] E  of [ β ^  X X ] = β ,, E , [ s 2  X X ] =  2 2 . 7.8displaystyle \operatorname {E} , cu[hat ggiobeta \}mid X\,]=\beta ,\quad \operatornameatorname {E},[s^{2 Xmid X\,]=\sigma {^2}. Liegt die strenge Exogenität nicht (wie bei vielen Zeitreihenmodellen, wo Exogenität nur in Bezug auf die früheren Schocks, aber nicht die zukünftigen) übernommen wird), werden diese Erreger in Finkenproben verdrängt. Die Varianzmatrix (oder schlichte Kovarianzmatrix) von β ^ KINGstyle \skriptstyle Memehat Memebeta  Var entspricht Var  to [ β ^  X X ] =  2 2 ( X T X ) − 1 =  2 2 Q . KINGstyle \operatorname {Var} WELL[hat Memebeta \}mid X\,]=\sigma 2}(X^{T}X)-1-1}=\sigma {^2}. Insbesondere der Standardfehler der einzelnen Koeffizienten β ^ j Memedisplaystyle \skriptstyle Memehat Memebeta {_j ist der Quadratwurzel des j-th diagonalen Elements dieser Matrix. Die Schätzung dieses Standardfehlers wird dadurch erzielt, dass die unbekannte Menge  its2 durch ihre Schätzung ersetzt wird2. s . e. . ^ ( β ^ j ) = s 2 ( X T X ) j j − 1 {\displaystyle Memestr {s.\!e} {(\}hat 7.8beta _j})= sqrt s22}(XTT} Man kann auch leicht nachweisen, dass der Estimator β ^ displaydisplaystyle \skriptstyle Memebeta s nicht mit den Resten des Modells in Zusammenhang steht: Cov ⁡ [ β ^ , ε ^  X X ] 0. KINGstyle \operatorname {Cov} , cu[hat Memebeta ,{\}hat Memevarepsilon \}mid X\,]=0,} Die Gauss-Markov-Theorem stellt fest, dass die Fehler unter der Annahme spherischer Fehler (d. h. die Fehler sollten unkorrespondiert und Homoscedastic) des Estimator β ^ RARstyle \skriptstyle HANAhat SSObeta . in der Klasse linearer unvorhergesehener Ester effizient sind.Dies ist der beste lineare unbiased estimator (BLUE). Effizienz sollte verstanden werden, wenn wir einige andere Estimator β ~ KINGstyle \skriptstyle Memetilde SSObeta }}} finden sollten, die linear in y und unbiased wären, dann Var  [ [ β ~ ∣ X ]  Var Var  Var [ β ^  X X ] ≥ 0 {\displaystyle \operatorname {Var} , cu[tilde Memebeta }midX\,]-\operatornameatorname {Var} , cu[hat Memebeta \}mid X\,]\geq 0} im Sinne, dass dies eine nicht-definite Matrix ist. Nur in der Klasse linearer, unvorhergesehener Estimatoren, die sehr restriktiv ist, ist dies der Fall. Je nach Verteilung der Fehlerbedingungen können andere nichtlineare Ester bessere Ergebnisse erzielen als OLS. Annahme der Normalität Die bisher aufgeführten Eigenschaften gelten unabhängig von der zugrunde liegenden Verteilung der Fehlerbedingungen. Wenn Sie jedoch bereit sind, zu übernehmen, dass die Normalitäts Annahme (d. h., dass   ~ N(0, .2In), dann können zusätzliche Eigenschaften der OLS-Stimatoren angegeben werden. In der Regel wird der Ester β ^ {\displaydisplaystyle \skriptstyle  cuhat ggiobeta ) verteilt, wobei der Mittelwert und die Varianz, wie vor: β ^  N N ( β ,  X 2 ( X T X ) − 1 ) Memestyle 7.8beta ) \sim \ chinocal N}}{\big (\}beta ,\ \sigma 2} (X. Mathematikrm {T} X)-1-1}{\big ) where, wo Q die Ko-Faktormatrix ist. Dieser Estimator erreicht den Cramér-Rao-Anteil an dem Modell und ist somit optimal in der Klasse aller unvorhergesehenen Ester. Hinweis darauf, dass im Gegensatz zu den Gausss-Markov-Theorem die optimale Wirkung sowohl linearer als auch nicht-linearer Ester, aber nur im Fall normaler Fehlerbedingungen festgelegt wird. Der Estimator s2 wird im Verhältnis zum Scho-Grenzwert stehen: s 2   2 n − p χ n n n − p 2 {\displaystyle s^{2\} \sim \ Memefrac ggiosigma 2pn-p}}\cdot\chi_{n-p22 Die Varianz dieser Estimator entspricht 2 24/(n − p), die den Cramér-Rao-Anteil von 2σ4/n nicht erreicht. Es wurde jedoch gezeigt, dass es keine unvoreingenommenen Estimatoren von σ2 gibt, die geringer sind als die des Estimators2. Wenn wir bereit sind, unvoreingenommene Estimatoren zuzulassen und die Klasse der Estimatoren zu berücksichtigen, die proportional zur Summe der flächendeckenden Reste des Modells sind, dann das beste (im Sinne des durchschnittlichen gestreiften Fehlers) Estimator in dieser Klasse wird ~σ2 =SSR / (n ‐ p + 2) sein, was sogar den Cram-Rao im Falle eines einzigen Regressorsor (p = 1). Darüber hinaus sind die Estimatoren β ^ WELLdisplaystyle \skriptstyle Memehat WELLbeta . und s2 unabhängig, was beim Aufbau der T- und F-Tests für die Regression nützlich ist. Grippende Beobachtungen Wie bereits erwähnt, ist der Estimator β ^ KINGstyle Memehat ggiobeta . linear in y, was eine lineare Kombination der abhängigen Variablen yi darstellt. Die Gewichte dieser linearen Kombination sind Funktionen der Regressoren X und sind in der Regel ungleich. Die Beobachtungen mit hohen Gewichten werden als einflussreich bezeichnet, weil sie einen stärkeren Einfluss auf den Wert des Esters haben. Um zu analysieren, welche Beobachtungen einflussreich sind, entfernen wir eine spezifische j-th-Beobachtung und überlegen, wie viel die geschätzten Mengen sich ändern werden (ähnlich mit der Methode der Narknife). Man kann nachweisen, dass die Änderung des OLS-Emulators für β ^ ( j) − β ^ = − 1 − h j ( X T X ) − 1 x j T ε ^ j {\   {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\  be  be  be  be  be {\ {\ {\ {\ {\ {\ {\ }- 1j1-h_{j((X^{\ Mathematikrm {T} X)-1-1}x_{j}^{\rm {T} {\hat 574 {_j,\, wo hj = xjT (XTX) -1xj ist das dritte diagonale Element der Hassmatrix P und xj ist der Vektor von Regressoren, die der j-th-beobachtung entsprechen. Gleiches gilt für die Änderung des vorhergesagten Wertes für die j-th-Beobachtung, die sich aus der Annahme ergeben, dass die Beobachtung durch den Datenset mit y ^ j ( j ) − y ^ j = x j T β ^ ( j ) − x j T β ^ = h  j ^  j  j  j ^ j 7.8hat y{\_{j((j{\)}-Barhat y__{j} {T} )hat HANAbeta (j)}-x_{jTT hat ggiobeta }- h_{j1-1-h_{j}}}\, cuhat ggiovarepsilon {_j Von den Eigenschaften der Hassmatrix, 0 ≤ hj ≤ 1, und sie zahlen bis zu p, so dass im Durchschnitt hj . p/n. Diese Mengen hj werden als Hebel bezeichnet und Beobachtungen mit hohem hj werden als Hebelpunkte bezeichnet. In der Regel sollten die Beobachtungen mit hoher Hebelwirkung sorgfältiger geprüft werden, falls sie fehlerhaft sind oder in irgendeiner Weise atypischer der restlichen Datenset sind. Manchmal können die Variablen und die entsprechenden Parameter in der Regression logischerweise in zwei Gruppen aufgeteilt werden, so dass die Regression in Form y = X 1 β 1 + X 2 β 2 + {\ , HANAstyle y=X_{1}\beta_{1}+X_{2}\beta {_2}+\varepsilon ,} wo X1 und X2 Abmessungen n×p1, n×p2, und β1, β2 sind p1×1 und p2×1 Vektoren, mit p1 + p2 =p.The Frisch-Waugh-Lovell theorem stellt fest, dass in dieser Regression die Reste s ^ KING KING var  O  O  O  O und OLS β 2 η s s s  O  O  O  O  O  O  O  O 2 s s s  O  O s  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  2 s  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O s  O s  O  O  O  O  O  O  M_{1}y=M_{1}X_{2}\beta_{2}+\eta \,}, wo M1 die anihilatormatrix für Regressors X1. Derorem kann verwendet werden, um eine Reihe theoretischer Ergebnisse zu erstellen. Beispielsweise ist eine Regression mit einem konstanten und anderen Regressor gleichwertig, um die Mittel von der abhängigen Variablen und dem Regressor aufzustocken und dann die Regression für die de-meanierten Variablen, aber ohne Dauer. kontrainierte Schätzung Kennzeichnend dafür ist, dass die Koeffizienten in der Regression ein System linearer Gleichungen A erfüllen: Q T β = c, Memestyle A\colon \quad QTT =beta =c, where, wo Q eine P×q-matrix ist, und c ist ein q×1 Vektor bekannter Konstanten, wo q · In diesem Fall ist die Schätzung von mindestens Quadratmetern geeignet, die Summe der Quadratkilometer des Modells unter dem Druck A.Die kontrainierten mindestens Quadrate (CLS)-Schätzer kann mit einer expliziten Formel versehen werden: β ^ c = β ^ − ( X T X ) − 1 Q ( Q T ( X T) − 1 Q ) − 1 ( Q T β ^ − c ) . 7.8displaystyle SSOhat HANAbeta ^c} (XTT}X)-1-1} Big QTT}(XTT}X)-1-1}QRARBig )-1}(QTT hat Memebeta -c). Dieser Ausdruck ist gültig, solange die Matrix XTX unvertierbar ist. Es wurde von Anfang an angenommen, dass diese Matrix vollrangig ist, und es wurde festgestellt, dass β bei Ausfall der Rangfolge nicht erkennbar ist. Es kann jedoch vorkommen, dass die Aufnahme der Einschränkung A β identifizierbar ist, in der ein Fall die Formel für den Estimator finden möchte. β ^ c = R (R T X T X R) − 1 R T X T y + ( I p  – R ( R T X R ) − 1 R T X T X T X T X ) Q ( Q T ) − 1 c , Memedisplaystyle ggiobeta c}=R(RTT}X^{T}XR-1-1}RTT}X^{T}y+ggio Mehr I_{p}-R(RTT}X^{T}X-1T}X^{T}X{\T}X{\Big Q(Q}T}Q)-1-1}c, wo R eine P×(p − q) Matrix ist, so dass die Matrix [Q R] nicht singular ist und RTQ = 0.Such eine Matrix immer gefunden werden kann, obwohl sie nicht einzigartig ist. Die zweite Formel fällt mit dem ersten, wenn XTX unvertierbar ist. Große Stichprobeneigenschaften Laut Schätzungen der linearen Regressionsparameter β. Wie immer wollen wir auch wissen, wie nah diese Schätzungen an den wahren Parametern der Parameter liegen könnten. Mit anderen Worten wollen wir die Intervallvoranschläge erstellen. Da wir keine Annahme über die Verteilung des Fehlerbegriffs gemacht haben ii, ist es unmöglich, die Verteilung der Estimatoren β ^ Memedisplaystyle Memehat  Fibeta }}} und ^ ^ 2 {\displaystyle Memehat ggiohat sigma {^2 .Nevertheless, wir können den zentralen Grenzgrenzwert für ihre asymptotischen Eigenschaften anwenden, da nfinity geht. Obwohl die Stichprobengröße notwendigerweise finite ist, ist es üblich, zu übernehmen, dass n "groß genug" ist, damit die tatsächliche Verteilung des OLS-Erätzers in der Nähe seiner asymptotischen Grenze liegt. Wir können zeigen, dass unter den Modellannahmen der am wenigsten Quadrate Ester für β konsistent ist (das ist β ^ displaystyle Memebeta }}} in der Wahrscheinlichkeit, β) und asymmetrische Normal:[sicher] ( β ^ − ) → d ( 0 ,  2 2 Q x − 1 ) , 7.8style {hat {\ {\ {\ {\ {\ {\                     {d  JPY N Nbig (}0,\;\sigma 2}Q_{xx-1-1}{\big )}, wo Q x x = X X X . . Q_{xx}=X^{T}X IntervalsUsing this asymptotic Distribution, ungefähr zweiseitige Vertrauensintervalle für die j-th Komponente des Vektors β ^ HANAdisplaystyle SSOhat {\ können als β j ∈ gebaut werden [ β ^ j ± q 1 −  2 2 N ( 0 , 1 ) ) 2 [ Q x x − 1 ] j ] \beta {_j)in ggiobigg [in] 7.8hat {_jpmpm q_{1-9.5frac ggioalpha 2 Ncal N((0,1)!! ZARhat WELLsigma 2}\left [Q_{xx-1-1}\right]_{j Memebigg ] at auf der 1 − . vertrauenspegel, in der die quantile Funktion der normalen Verteilung und [·]jjj ist das dritte diagonale Element einer Matrix. Gleiches gilt auch für die kleinsten Quadrate, die für die 22 gelten, als konsistent und asymmetrische Normalität (sofern der vierte Moment der )i existiert) mit Begrenzung der Verteilung (σ ^ 2 −  2 2 )→ d N ( 0 , E  [ [  i i 4 ]  4 4 ) . 7.8displaystyle {(\hat sigma 2}-\sigma{\2) 7.8xrightarrow {d  {N).left(0,\;\operatorname {E} links[\varepsilon _ipsilon4right]-\sigma {^4).right). Diese asymptotischen Vertriebene können für Vorhersage, Testhypothesen, Bau anderer Ester usw. verwendet werden. Als Beispiel betrachtet man das Problem der Vorhersage. X 0 {\displaystyle x_{0} ist ein Punkt im Bereich der Verteilung der Regressoren, und man möchte wissen, was die Reaktionsvariable an diesem Punkt gewesen wäre. Die mittlere Antwort ist die Menge y 0 = x 0 T β ISINstyle y_{0}=x_{0}^{\ Mathematikrm {T} \}beta } , während die vorhergesagte Antwort auf y ^ 0 = x 0 T β ^ {\displaystyle  y__{0} {T}  hat 7.8beta   . Klar die vorhergesagte Antwort ist eine Zufallsvariable, deren Verteilung aus dem β ^ {\displaystyle  cuhat 7.8beta : abgeleitet werden kann: ( y ^ 0 − y 0 ) → d N ( 0 ,  2 2 x 0 T Q x x x x 0 − 1 x 0 ) , KINGstyle links(Kaffeehat y}}_0}-y_{0}\right {d\}right ] 7.8 Mathematik {N}}\left(0,\;\sigma 2}x_{0}^{\ Mathematikrm {T} Q_{xx-1-1}x_{0 beright), die das Vertrauen in die richtige Antwort y 0 {\displaystyle y_{0} aufbauen lassen: y 0 ) [ x 0 T β ^ ± q 1 −  2 2 N ( 0 , 1 ) ) ^ 2 x 0 T Q x − 1 x 0 ] KINGstyle y_{0}\in links [ x_{0}^{\rm {T}  pm_{1-Barfrac Memealpha 2 2 mathematische N((0,1)!! 2}x_{0}^{\rm {T} Q_{xx-1-1}x_{0\ \right}] auf der 1 − . vertrauensniveau. Hypothesis-Tests Zwei Hypothesentests werden besonders weit verbreitet. Erstens möchte man wissen, ob die geschätzte Regressionsgedanke besser ist als nur vorhersagen, dass alle Werte der Antwort variabler sind als ihre Stichprobe (wenn nicht, es wird gesagt, keine erläuternde Kraft zu haben). Die Nullhypothese der geschätzten Regression wird anhand eines F-Tests getestet. Kommt der berechnete F-Wert zu groß genug, um seinen kritischen Wert für die Vor-Schokosen-Werte zu überschreiten, wird die Nullhypothese abgelehnt und die alternative Hypothese, dass die Regression eine erläuternde Kraft hat, akzeptiert. Andernfalls wird die Nullhypothese ohne Begründungskraft akzeptiert. Zweitens möchte man wissen, ob der geschätzte Koeffizient erheblich von Null - d. h. ob diese besondere erläuternde variable Größe tatsächlich eine beanstandende Kraft bei der Vorhersage der Reaktionsvariable hat. Hier ist die Nullhypothese, dass der wahre Koeffizient Null ist. Diese Hypothese wird durch die Berechnung der T-Statistic des Koeffizienten getestet, da das Verhältnis des Koeffizienten-Schätzwertes zu seinem Standardfehler liegt. Wenn die T-statistic größer ist als ein vorherrschender Wert, wird die Nullhypothese abgelehnt, und die Differenz wird festgestellt, dass sie eine erläuternde Kraft haben, wobei ihr Koeffizient erheblich von Null unterscheidet. Andernfalls wird die Nullhypothese eines Nullwertes des wahren Koeffizienten akzeptiert. Darüber hinaus wird der Chow-Test verwendet, um zu prüfen, ob zwei Subsamples beide die gleichen Grundwerte aufweisen. Die Summe der flächendeckenden Reste der Regressionen auf den einzelnen Teilsets und auf den kombinierten Daten wird mit einem F-Statistische verglichen; wenn dies einen kritischen Wert übersteigt, wird die Nullhypothese von keinem Unterschied zwischen den beiden Teilsets abgelehnt; ansonsten wird sie akzeptiert. Beispiel für echte Daten Die folgenden Daten geben durchschnittliche Höhe und Gewichte für amerikanische Frauen im Alter von 30 bis 39 Jahren (Quelle: World Almanac und Buch von Fakten, 1975). Wenn nur eine abhängige variable Formel entwickelt wird, wird eine Streuplot die Form und die Stärke der Beziehung zwischen den abhängigen variablen und Regressoren vorschlagen. Man kann auch Ausbrüche, Heterogenität und andere Aspekte der Daten, die die Auslegung eines eingebauten Regressionsmodells erschweren könnten, erkennen. Die Zersplitterung deutet darauf hin, dass die Beziehung stark ist und als quadratische Funktion angeglichen werden kann. OLS kann nicht-lineare Beziehungen durch Einführung des Regressor HEIGHT2 bewältigen. Das Regressionsmodell wird dann zu einem multiplen linearen Modell: w i = β 1 + β 2 h i + β 3 h i 2 + . i . KINGstyle w_{i} {_1}+\beta 2}h_{i}+\beta 3} {_i}. Je nach Ergebnis der beliebtesten statistischen Pakete wird dies ähnlich aussehen: In dieser Tabelle: Die Wertesäule enthält die Schätzungen der Parameter βj Die Std- Fehlersäule mit Standardfehlern der einzelnen Koeffizienten:   ^ j = (   ^ 2 [ Q x x x − 1 ] j ] j ) 1 2 7.8displaystyle Memehat 7.8sigma _j}=\left(Gethat ggiosigma 2}\left [Q_{xx-1-1}\right]_{j}\right)^{\frac 1}{2 Die Spalten des T-Statistics und des p-Werts testen, ob jeder der Koeffizienten gleich Null sein könnte. Man berechnet einfach als t = β ^ j / ^ ^ j displaystyle t= 7.8beta _j} . Wenn die Fehler . einem normalen Vertrieb folgen, folgt t einem Studentenvertrieb. Unter schwächeren Bedingungen ist t asymmetrisch normal. Große Werte von t zeigen, dass die Nullhypothese abgelehnt werden kann und dass der entsprechende Koeffizient nicht Null ist. Die zweite Spalte, p-Wert, zeigt die Ergebnisse des Hypothesis-Tests als wichtiges Niveau. Konventiv werden p-Werte kleiner als 0,05 als Beweis dafür, dass der Bevölkerungskoeffizient nichtzero ist. R-Quadrat ist der Entschlossenheitskoeffizient, der die Wirksamkeit der Regression zeigt. Diese Statistiken werden gleich sein, wenn sie perfekt sind, und auf Null bei Regressoren. X hat keine erläuternde Macht. Dies ist eine einseitige Schätzung der Bevölkerung R-Quadrat und wird nie sinken, wenn zusätzliche Regressoren hinzugefügt werden, auch wenn sie irrelevant sind. angepasste R-Quadrat ist eine leicht geänderte Version von R 2 {\displaystyle R^{2}, die für die überschüssige Zahl von Regressoren, die nicht zur Begründung der Regression beitragen, bestraft werden soll. Diese Statistik ist immer kleiner als R 2 WELLdisplaystyle R^{2}, kann abnehmen, da neue Regressoren hinzugefügt werden und sogar negativ für arme Modelle sind: R  2 2 = 1 − n − 1 n − p ( 1 − R 2 ) Kaffeestyle 7.8overline R}}^{2} n-1}{n-p((1-R2)2) Log-likelihood wird anhand der Annahme berechnet, dass Fehler den normalen Vertrieb verfolgen. Obwohl die Annahme nicht sehr vernünftig ist, kann diese Statistik ihre Verwendung bei der Durchführung von LR-Tests noch finden. Durbin-Watson Statistiktests, ob es Beweise für eine serielle Korrelation zwischen den Resten gibt. In der Regel des Daumens wird der Wert kleiner als 2 eine positive Korrelation sein. Akaike Informationskriterium und Schwarz-Kriterium werden sowohl für die Modellauswahl verwendet. Im Allgemeinen werden bei der Vergleich zwei alternative Modelle kleinere Werte eines dieser Kriterien ein besseres Modell angeben. Standardfehler der Regression ist eine Schätzung der ., Standardfehler der Fehlerfrist. Gesamtzahl der Quadrate, Modellsumme der Quadrate und Restsumme der Quadrate sagen uns, wie viel der anfänglichen Variationen der Probe durch die Regression erklärt wurde. F-statistic versucht, die Hypothese zu testen, dass alle Koeffizienten (ausgenommen das Abfangen) Null aufweisen. Diese Statistik hat F(p–1,n-p)-Vertrieb unter der Nullhypothese und der Normalität, und ihr p-Wert zeigt die Wahrscheinlichkeit, dass die Hypothese tatsächlich wahr ist. Hinweis darauf, dass Fehler nicht normal sind, dass diese Statistiken ungültig werden, und andere Tests wie Waldtest oder LR-Test sollten verwendet werden. Ordinary Low Quadrats Analyse umfasst häufig die Verwendung von Diagnoseflächen, die den Abgang der Daten aus der angenommenen Form des Modells erkennen sollen. Es handelt sich um einige der gemeinsamen Diagnoseflächen: Rückstände gegen die erläuternden Variablen im Modell. Eine nichtlineare Beziehung zwischen diesen Variablen deutet darauf hin, dass die Linearität der bedingten durchschnittlichen Funktion nicht bestehen kann. Unterschiede in den Resten für unterschiedliche Ebenen der erläuternden Variablen lassen eine mögliche Heterogenität erkennen. Rückstände gegen erläuternde Variablen nicht im Modell. Jegliche Beziehung der Reste dieser Variablen würde vorschlagen, diese Variablen für die Aufnahme in das Modell in Betracht zu ziehen. Rückstände gegen die montierten Werte, y ^ Memestyle Memehat {y} .Residials gegen die vorherige Reste. In diesem Grundstück können serielle Korrelationen in den Resten ermittelt werden. Eine wichtige Berücksichtigung bei der Durchführung statistischer Unterschiede bei Regressionsmodellen ist, wie die Daten getestet wurden. In diesem Beispiel sind die Daten eher durchschnittlich als Messungen an einzelnen Frauen. Das Modell ist sehr gut, aber dies bedeutet nicht, dass das Gewicht einer einzelnen Frau mit einer hohen Genauigkeit vorhergesagt werden kann, die nur auf ihrer Höhe beruht. Empfindlichkeit zur Rundung Dieses Beispiel zeigt auch, dass die durch diese Berechnungen ermittelten Koeffizienten empfindlich sind, um die Daten vorzubereiten. Die Höhen wurden ursprünglich auf die nächsthöchste Tinte gerundet und wurden in den nächsten Centi Meter umgestaltet und abgerundet. Da der Umrechnungsfaktor eine Größe von 2,54 cm ist, ist dies keine genaue Umrechnung. Der ursprüngliche Zoll kann durch die Runde (x/0.0254) zurückgefordert werden und dann ohne Rundung auf die Parameter umgerechnet werden. Wenn dies geschehen ist, werden die Ergebnisse: Je nach Gewicht von 5' 6" (1.6764 m) gibt Frau ähnliche Werte: 62,94 kg mit Rundung gegenüber 62,98 kg ohne Rundung. Eine scheinbar kleine Variation der Daten hat eine echte Wirkung auf die Koeffizienten, aber eine geringe Wirkung auf die Ergebnisse der Gleichung. In der Mitte des Datenspektrums könnte dies zu einem erheblichen Teil der Extremen oder im Falle, in dem das installierte Modell für das Projekt außerhalb des Datenbereichs verwendet wird (extrapolation). In diesem Zusammenhang wird ein gemeinsamer Fehler hervorgehoben: Dieses Beispiel ist ein Missbrauch von OLS, der in der Regel verlangt, dass die Fehler in der unabhängigen Variablen (in diesem Fall) null oder zumindest vernachlässigbar sind. Die erste Runde auf die nächste Tinte und etwaige tatsächliche Messfehler stellen einen finite und nicht-negbaren Fehler dar. Infolgedessen sind die eingebauten Parameter nicht die besten Schätzungen, die sie als wahrscheinlich betrachten. Obwohl der Fehler in der Schätzung nicht vollständig zerstreut wird, hängt von der relativen Größe der x und y-fehler ab. weiteres Beispiel für weniger echte Datenproblemerklärung Wir können den am wenigsten Quadrat-Mechanismus verwenden, um die Gleichung eines zwei Körpers in Polarbasiskoordnen aufzuzeigen. In der Regel wird r (  ) = p 1 − e cos ⁡ (   ) {\displaystyle r(\theta ) cu=frac p}{1-e\cos(\theta )}}} verwendet, wo r (θ ) faserstyle r(\theta )} der Radius ist, wie weit das Objekt von einem der Organe ist. In der Gleichung werden die Parameter p HANAdisplaystyle p} und e HANAdisplaystyle e} verwendet, um den Weg des Umlaufs zu bestimmen. Wir haben die folgenden Daten gemessen. Wir müssen für die angegebenen Daten die am wenigsten geeignete Annäherung der e Memestyle e} und der p KINGstyle p} finden. Lösung Erstens müssen wir e und p in linearer Form vertreten. Wir werden also die Formel r (  ) {\displaydisplaystyle r(\theta )} als 1 r (θ ) = 1 p cos ⁡ (θ ) {\displaystyle 7.8frac 1 1r(\theta )}= 574 Jetzt können wir diese Form nutzen, um unsere Beobachtungsdaten wie: A T A ( x y ) = A T b HANAstyle ATT}ASSObinom x=y== ATT}b, wo x Memedisplaystyle x} 1 p RARdisplaystyle SSOfrac 1pp und y displaystyle y} ist e p {\displaystyle WELLfrac e{\p und A ggiostyle A} wird von der ersten Spalte als Koeffizient von 1 p Memedisplaystyle Memefrac 1 1p und die zweite Säule ist der Koeffizient von e p {\displaystyle Memefrac e}{p und b Memestyle b} die Werte für die jeweiligen 1 r (θ )7.8displaystyle Memefrac 1 1r(\theta ) A so A = [ 1 − 0,731354 1 − 0,707107 1 − 0,615661 1 0,02336 1 0.309017 1 0,438371 ] A=TONbegin{bmatrix}1 &-0.73135411 &-0.707107\\10.5233611 &0.30901711 &0.438371\end{bmatrix} und b = [ 0,21220 0,24741 0,45071 0,52883 0,56820 ] . b=Getbegin{bmatrix}0.21220\\0.219580.20.24741410.45071\\0.52883\\0.56820\end{b Matrix Lösung: ( x y ) = ( 0,43478 0.30435 ) KINGstyle SSObinom x=y== 0,43478 so435 so p = 1 x = 2,3000 {\displaystyle p= 1}{x==2.3000 und e = p ⋅ y = 0,70001 RARstyle e=p\cdot y=0,70001} Siehe auch Bayesische mindestens Quadrate Fama-MacBeth Regression Non-lineare, mindestens Quadrate Numerical-Methoden für lineare, nichtlineare SystemkennzeichnungsreferenzenFurther Dougherty, Christopher (2002). Einführung in Econometrics (2. New York: Oxford University Press.pp.48–113.ISBN 0-19-877643-8.Gujarati, Damodar N;. Porter, Dawn C. (2009). Grundwirtschaft (Fifth ed). Boston: McGraw-Hill Irwin.pp.55-96.ISBN K. (2004):0-07-337577-9.Heij, Christiaan; Boer, Paul; Franses, Philip H; Kloek, Germann; van Dijk. Wirtschaftlichkeit Methoden mit Anwendungen in Wirtschaft und Wirtschaft (1. Oxford: Oxford University Press.pp.76–115.ISBN K.:0-19-926801-6.H., R. Carter;s, William E; Lim, Guay C. (2008). Leitprinzipien der wirtschaftlichen Parameter (3rd ed). Hoboken, NJ: John Kuhn & Sons.pp.8–47.ISBN gegen0-471-72360-8.Wooldridge, Jeffrey (2008). "Das einfache Regressionsmodell". Einführung wirtschaftlicher Parameter: Ein modernes Konzept (4. ed). Mason, OH: Cengage Learning.pp.22-324-58162-1.