Eine eingeschränkte Boltzmann-Maschine (RBM) ist ein generatives stochastisches künstliches neuronales Netz, das eine Wahrscheinlichkeitsverteilung über seine Eingaben erlernen kann. RBMs wurden zunächst unter dem Namen Harmonium von Paul Smolensky im Jahr 1986 erfunden und stiegen nach Geoffrey Hinton und Mitarbeitern, die Mitte 2000 schnelle Lernalgorithmen für sie erfanden. RBMs haben Anwendungen in der Dimensionsreduktion, Klassifizierung, Collaborative Filterung, Feature-Learning, Top-Modellierung und sogar viele Körperquantenmechanik gefunden. Sie können je nach Aufgabe auf beaufsichtigte oder unbeaufsichtigte Weise ausgebildet werden. Wie ihr Name schon sagt, sind RBMs eine Variante von Boltzmann-Maschinen, mit der Einschränkung, dass ihre Neuronen ein Bipartit-Diagramm bilden müssen: Ein Paar von Knoten aus jeder der beiden Gruppen von Einheiten (gemeinsam als sichtbare und versteckte Einheiten bezeichnet) kann eine symmetrische Verbindung zwischen ihnen haben; und es gibt keine Verbindungen zwischen Knoten innerhalb einer Gruppe. Dagegen können unbeschränkte Boltzmann-Maschinen Verbindungen zwischen versteckten Einheiten aufweisen. Diese Einschränkung ermöglicht effizientere Trainingsalgorithmen als für die allgemeine Klasse der Boltzmann-Maschinen, insbesondere den Gradienten-basierten Kontrastdivergenzalgorithmus, zur Verfügung stehen. Eingeschränkte Boltzmann-Maschinen können auch in tiefen Lernnetzwerken eingesetzt werden. Insbesondere können tiefe Glaubensnetzwerke durch Stapeln von RBMs und gegebenenfalls Feinabstimmung des resultierenden tiefen Netzwerks mit Gradientenabstieg und Rückverbreitung gebildet werden. Struktur Der Standardtyp von RBM hat binärwertige (Boolean) versteckte und sichtbare Einheiten, und besteht aus einer Matrix von Gewichten W \{displaystyle W} der Größe m × n \{displaystyle m\times n} . Jedes Gewichtselement (w i, j ) \{displaystyle (w_{i,j)} der Matrix ist mit der Verbindung zwischen der sichtbaren (Eingabe) Einheit v i Darüber hinaus gibt es Biasgewichte (offsets) a i \{displaystyle a_{i} für v i \{displaystyle v_{i} und b j \{displaystyle b_{j} für h j \{displaystyle h_{j} .Die Energie einer Konfiguration (Paar der booleschen Vektoren) (v,h) wird bei den Gewichten und Bias als E (v, h) = - Σ i a i v i - Σ j b j h ,,,............... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ) ... j}v_{i}w_{i,j}h_{j oder, in Matrixnotation, E (v , h ) = - a T v - b T h - v T W h . \{displaystyle E(v,h)=-a^{\mathrm (T) (T) Diese Energiefunktion ist analog zu der eines Hopfield-Netzwerks. Wie bei den allgemeinen Boltzmann-Maschinen wird die gemeinsame Wahrscheinlichkeitsverteilung für die sichtbaren und versteckten Vektoren in Bezug auf die Energiefunktion wie folgt definiert, P (v, h) = 1 Z e - E (v, h) \{displaystyle P(v,h)={\frac 1}{Z}e^{-E(v,h) wobei Z \{displaystyle Z} eine als Summenfunktion definiert ist Die Randwahrscheinlichkeit eines sichtbaren Vektors ist die Summe von P (v, h) \{displaystyle P(v,h)} über alle möglichen verdeckten Schichtkonfigurationen, P (v) = 1 Z Σ {h } e - E (v, h) \{displaystyle P(v)={\frac 1}{Z}\sum _h\}e^{-E(v,h) Da die zugrunde liegende Graphenstruktur des RBM zweiteilig ist (d.h. es gibt keine Intra-Schicht-Verbindungen), sind die versteckten Einheiten-Aktivierungen bei den sichtbaren Einheiten-Aktivierungen gegenseitig unabhängig. Umgekehrt sind die sichtbaren Einheitenaktivierungen bei den versteckten Einheitenaktivierungen gegenseitig unabhängig. Das heißt, für m sichtbare Einheiten und n versteckte Einheiten ist die bedingte Wahrscheinlichkeit einer Konfiguration der sichtbaren Einheiten v, bei einer Konfiguration der versteckten Einheiten h, P (v  of h) = ∏ i = 1 m P (v i | h) \{displaystyle P(v|h)=\prod i=1}^{m}P(v_{i}}h) .Umgekehrt ist die bedingte Wahrscheinlichkeit von h angegeben v P (h | v ) = ∏ j = 1 n P (h j | v ) \{displaystyle P(h|v)=\prod}=1 1 m w i , j v i ) \{displaystyle P(h_{j}=1|v)=\sigma links(b_{j}+\sum i=1{m}w_{i,j}v_{i}\right) und P (v i = 1 | h ) = σ (a i + Σ j = 1 n w i, j h j ) \{displaystyle ,P(v_{i}=1hh)=\sigma links(a_{i}+\sum j=1}{n}" Die sichtbaren Einheiten der eingeschränkten Boltzmann-Maschine können multinom sein, obwohl die versteckten Einheiten Bernoulli sind. In diesem Fall wird die logistische Funktion für sichtbare Einheiten durch die Softmax-Funktion P (v i k = 1 | h ) = exp ≠ (a i k + Σ j) ersetzt. W i j k h j Σ k' = 1 K exp ermittelt (a i k' + Σ j ) Sigma ) Sigma k'=1 Sigma j}W_{ij}^{k'}h_{j) wobei K die Anzahl der diskreten Werte ist, die die sichtbaren Werte haben. Sie werden im Thema Modellierung und Empfehlungssysteme angewendet. Relation zu anderen Modellen Eingeschränkte Boltzmann Maschinen sind ein Sonderfall von Boltzmann Maschinen und Markov Zufallsfelder. Ihr grafisches Modell entspricht dem der Faktoranalyse. TrainingsalgorithmusEingeschränkte Boltzmann-Maschinen werden ausgebildet, um das Produkt von Wahrscheinlichkeiten zu maximieren, die einem Trainingssatz V \{displaystyle V} zugeordnet sind (eine Matrix, deren Zeile als sichtbarer Vektor v \{displaystyle v} ), arg ‡ v ε V P (v ) \ max{displaystyle \ max _{W}\prod _{v} Der Algorithmus, der am häufigsten verwendet wird, um RBMs zu trainieren, d.h. um den Gewichtsvektor W \{displaystyle W} zu optimieren, ist der kontrastierende Divergenz-Algorithmus aufgrund Hinton, der ursprünglich entwickelt wurde, um PoE (Produkt von Experten) Modelle zu trainieren. Der Algorithmus führt Gibbs-Sampling und wird innerhalb eines Gradientenabstiegs-Verfahrens (ähnlich der Art, wie Backpropagation in einem solchen Verfahren verwendet wird, wenn Training nach vorn neuronale Netze) Gewicht-Update berechnen. Die grundsätzliche, einstufige kontrastreiche Divergenz (CD-1)-Prozedur für eine einzelne Probe lässt sich wie folgt zusammenfassen: Nehmen Sie eine Trainingsprobe v, berechnen Sie die Wahrscheinlichkeiten der versteckten Einheiten und probieren Sie einen versteckten Aktivierungsvektor h von dieser Wahrscheinlichkeitsverteilung. Berechnen Sie das äußere Produkt von v und h und nennen Sie dies den positiven Gradienten. Aus h eine Rekonstruktion v' der sichtbaren Einheiten ausprobieren, dann die versteckten Aktivierungen h' daraus wiedergeben.(Gibbs-Probeschritt) Berechnen Sie das äußere Produkt von v' und h' und nennen Sie dies den negativen Gradienten. Lassen Sie das Update der Gewichtsmatrix W \{displaystyle W} ist der positive Gradient abzüglich des negativen Gradienten, mal einige Lernrate: Δ W = ε (v h T - v' h' t ) \{displaystyle \Delta W=\epsilon (vh^{\mathsf T}-v'h'{\\\mathsf {T)} .Aktualisieren Sie die Bias a und bstyle Siehe auch Autoencoder Helmholtz-Maschine Referenzen Externe Links Einführung in eingeschränkte Boltzmann-Maschinen. Edwin Chens Blog, 18. Juli 2011."Ein Leitfaden für Anfänger zu eingeschränkten Boltzmann Maschinen". Archiviert aus dem Original am 11. Februar 2017. Retrieved November 15, 2018.CS1 maint: bot: original URL Status unbekannt (link). Deeplearning4j Dokumentation "RBM verstehen". Archiviert aus dem Original am 20. September 2016. Retrieved December 29, 2014.Deeplearning4j Dokumentation Python Implementierung von Bernoulli RBM und Tutorial SimpleRBM ist ein sehr kleiner RBM-Code (24kB) nützlich für Sie, um zu erfahren, wie RBMs lernen und arbeiten.