In digitaler Bildgebung, einem Pixel, pel oder Bildelement ist ein kleinstes adressenpflichtiges Element in einem rasteren Bild oder das kleinste adressenpflichtige Element in einem alle auf dem Bildschirm angegebenen Anzeigegerät; so ist es das kleinste Kontrollelement eines auf dem Bildschirm vertretenen Bildes. Jedes Pixel ist eine Stichprobe eines Originalbildes; mehr Proben liefern in der Regel genauere Darstellungen des Originals. Die Intensität jedes Pixels ist variabler. In Farbgebungssystemen ist eine Farbe in der Regel durch drei oder vier Komponenten-Intensitäten wie rot, grün und blau, oder yan, gelb und schwarz vertreten. In einigen Kontexten (z.B. Beschreibungen von Kamerasensoren) bezieht sich Pixel auf einen einzigen scalar-Element einer Multi-Komponenten-Vertretung (ein Fotoit im Kamerasensor-Konspekt, obwohl sinnvolll manchmal verwendet wird), während in noch anderen Kontexten auf die Reihe von Komponentenintensitäten für eine räumliche Position verweist. Etymologie Das Wort Pixel ist eine Kombination aus pix (aus Bildern, verkürzt auf Pis) und el (für Element); ähnliche Bildungen mit el umfassen die Wörter Woxel und texel. Das Wort pix wurde in Variety Magazine Schlagzeilen im Jahr 1957 als Abkürzung für die Wortbilder im Bezug auf Filme erschienen. Im Jahr 1938 wurde pix auf immer noch Bilder von Fotojournalisten verwendet. Erst 1965 wurde das Wort Pixel von Frederic C. Billingsley von JPL veröffentlicht, um die Bildelemente der Bilder zu beschreiben, die aus Feldversuchen auf den Mond und Mars gescannt wurden. Billingsley hatte das Wort von Keith E. McFarland, in der Link Division der allgemeinen Präzision in Palo Alto gelernt, die wiederum nicht wissen, wo sie stammt. McFarland sagte einfach, es sei "in der Zeit" (circa 1963). Das Konzept eines "Bildelements" ist den frühesten Tagen des Fernsehens, z.B. als Bildpunkt (das deutsche Wort für Pixel, wörtlich "Bildpunkt") im 1888 deutschen Patent von Paul Nipkow. Laut verschiedenen etymologien war die frühe Veröffentlichung des Begriffsbildelements selbst im Jahr 1957 in Wireless World Magazine, obwohl es früher in verschiedenen US-Patenten verwendet wurde, die bereits seit dem Jahr 1916 eingereicht wurden. Manche Autoren erklären Pixel als Bildzelle schon 1972. Bild- und Videoverarbeitung wird oft anstelle von Pixel verwendet. IBM nutzte es zum Beispiel in ihrem technischen Referenzrahmen für den Original PC.Scanners, abgekürzt als px, sind ebenfalls eine Messeinheit, die häufig im grafischen und Webdesign verwendet wird, was etwa 1⁄96 inch (0,26 mm) entspricht. Diese Messung wird verwendet, um sicherzustellen, dass ein bestimmtes Element als gleiche Größe dargestellt wird, egal was die Bildschirm-Resolutionsauskunft darstellt. Pixilation, die mit einer zweiten i bezeichnet wird, ist eine nicht miteinander verknüpfte Filmtechnik, die den Beginn des Kinos abgibt, in der Live-Akteure durch Rahmen und Fotoaufnahmen zur Schaffung von Stop-motion Animationsfilmen dargestellt werden. Ein geschichtliches britisches Wort, das heißt "Präsen von Spirituosen" (Tabletten), wurde der Begriff seit den frühen 1950er Jahren verwendet; verschiedene Animatoren, darunter Norman McLaren und Grant Munro, werden mit der populären Darstellung gutgeschrieben. Technik Ein Pixel wird in der Regel als kleinstes Element eines digitalen Images betrachtet. Jedoch ist die Definition sehr kontextempfindlicher. Man kann beispielsweise in einer Seite "geschneiderte Pixel" oder durch elektronische Signale durchgeführte oder durch digitale Werte oder Pixel auf einem Anzeigegerät oder Pixel in einer digitalen Kamera (Fotosensor-Elemente) vertretene Pixel enthalten. Diese Liste ist nicht erschöpfend und, je nach Kontext, die Synonyme umfassen pel, Probe, Byte, Bit, Punkt und Ort. Pixel können als Maßeinheit verwendet werden, wie: 2400 Pixel pro Inch, 640 Pixel pro Linie oder flächendeckend 10 Pixel. Die Maßnahmenpunkte pro Tinten (dpi) und Pixel pro Tinte (ppi) werden manchmal austauschbar, haben aber unterschiedliche Bedeutung, insbesondere für Druckergeräte, wo dpi eine Maßnahme der Dichte des Druckers (z.B. Tintentropfen) ist. Beispielsweise kann ein hochwertiges Fotobild mit 600 ppi auf einem 1200 dpi Tintenjet Drucker bedruckt werden. Selbst höhere dpi-Nummern wie die 4800 dpi, die seit 2002 von Druckerherstellern zitiert wurden, bedeuten nicht viel in Bezug auf eine erreichbare Auflösung. Je mehr Pixel verwendet werden, um ein Bild zu machen, desto größer ist das Ergebnis. Die Zahl der Pixel in einem Bild wird manchmal als Entschließung bezeichnet, obwohl die Entschließung eine spezifischere Definition enthält. Pixel zählt zu einer einzigen Nummer, wie in einer drei-Mega-Digitalkamera, die eine nominal drei Millionen Pixel oder ein Paar von Nummern aufweist, wie in einer „640 von 480 Anzeige“, die 640 Pixel von Seitenseite und 480 von oben nach unten (wie in einer Display) hat und daher eine Gesamtzahl von 640 × 480 = 307,200 Pixel oder 0,3 Mega-Chips aufweist. Die Pixel, Farbproben, die ein digitalisiertes Bild bilden (wie ein auf einer Website benutztes IDE-Datei), können oder dürfen nicht in einem einzigen Brief mit Bildschirmen erfolgen, je nachdem, wie ein Computer ein Bild zeigt. In Computing ist ein Bild, das sich aus Pixeln zusammensetzt, als ein BitCardd Bild oder ein Radbild bekannt. Das Wort raster stammt aus Fernsehkameramustern und wurde häufig verwendet, um ähnliche halbtone Druck- und Lagertechniken zu beschreiben. Sampling Muster Für Komfort sind die Pixel normalerweise in einem regulären zweidimensionalen Netz untergebracht. Durch die Nutzung dieser Regelung können viele gemeinsame Operationen durch einheitliche Anwendung derselben Operation auf jeden Pixel unabhängig durchgeführt werden. Andere Modalitäten von Pixeln sind möglich, mit einigen Probenahmen, die sogar die Form (oder den Kern) jedes einzelnen Pixels über das Bild verändern. Aus diesem Grund muss bei dem Erwerb eines Bildes auf einem Gerät und seiner Anzeige auf einem anderen oder bei der Umwandlung von Bilddaten aus einem Pixelformat auf ein anderes geachtet werden. Beispiel: LCD-Bildschirme verwenden in der Regel ein stabiles Netz, bei dem die roten, grünen und blauen Komponenten an leicht unterschiedlichen Orten getestet werden. Subcement ist eine Technologie, die diese Unterschiede nutzt, um die Erstellung von Texten auf LCD-Bildschirmen zu verbessern. Die große Mehrheit der Farb-Digitalkameras nutzt einen Bayer-Filter, der zu einem regulären Netz von Pixeln führt, wo die Farbe jedes Pixels von seiner Position auf dem Netz abhängt. Ein Clipmap verwendet ein hierarchisches Probemuster, bei dem die Größe der Stützung jedes Pixels von seinem Standort innerhalb der Hierarchie abhängt. Warped Grids werden verwendet, wenn die zugrunde liegende Geometrie nicht planar ist, wie Bilder der Erde vom Weltraum. Nicht-uniforme Netze sind ein aktiver Forschungsraum, der versucht, die traditionelle Nyquist-Grenze zu umgehen. Pixels auf Computerbildschirmen sind in der Regel quadratisch (d. h. haben gleiche horizontale und vertikale Probenahme); Pixel in anderen Systemen sind oft rechteckig (d. h. haben ungleiche horizontale und vertikale Probenahme – oblong in Form), wie digitale Videoformate mit unterschiedlichen Aspekten, wie z.B. aamorphen Breitbildformaten des Rec.601 Digital Videostandard. Lösung von Computerbildschirmen kann Pixel verwenden, um ein Bild zu zeigen, oft ein abstraktes Bild, das eine GUI darstellt. Die Auflösung dieses Bildes ist die Anzeigelösung und wird durch die Videokarte des Computers bestimmt. LCD-Bildschirme verwenden auch Pixel, um ein Bild zu zeigen und eine einheimische Lösung zu haben. Jedes Pixel besteht aus Triads, wobei die Zahl dieser Trithesen zur Bestimmung der einheimischen Auflösung liegt. In einigen CRT-Monitoren kann der Abstrahlsatz festgelegt werden, was zu einer festen einheimischen Auflösung führt. Die meisten CRT-Monitore verfügen nicht über einen festen Abstrahlsatz, d. h. sie verfügen nicht über eine einheimische Lösung, sondern haben eine Reihe von Resolutionen, die gleichermaßen gut unterstützt werden. Um die scharfesten Bilder auf einer LCD zu erstellen, muss der Nutzer die Anzeigeauflösung des Computers mit der einheimischen Auflösung des Monitors gewährleisten. Entschließung von Teleskopen Die in der Astronomie verwendete Pixelskala ist die agulare Entfernung zwischen zwei Gegenständen auf dem Luftraum, die ein Pixel auf dem Detektor (CCD oder Infrarot-Chip) fallen. Die in radians gemessene Skala ist das Verhältnis des Pixels, der p und der Schwerpunktlänge f der vorherigen Optik, s=p/f.(Die Hauptlänge ist das Produkt der Konzentrationsverhältnisse durch den Durchmesser der zugehörigen Linse oder Spiegel). Da p normalerweise in Stücken von Bogensekten pro Pixel ausgedrückt wird, weil 1 Kaukasus 180/**360006206, 265s, und weil Durchmesser oft in Millimetern und Pixelgrößen in Mikrometern angegeben werden, die einen anderen Faktor von 1.000 bilden, wird häufig als s=206p/f zitiert. Bits pro Pixel Die Anzahl der verschiedenen Farben, die durch einen Pixel vertreten werden können, hängt von der Anzahl der Bits pro Pixel (bpp) ab. Ein 1-Bpp-Bilder verwendet 1-bit für jeden Pixel, so dass jeder Pixel entweder auf oder außerhalb sein kann. Jedes zusätzliche Bit verdoppelt die Anzahl der verfügbaren Farben, so dass ein 2 bpp-Bilder 4 Farben haben kann und ein 3 bpp-Bilder 8 Farben haben kann: 1 bpp, 21 = 2 Farben (Monochrome) 2 bpp, 22 = 4 Farben 3 bpp, 23 = 8 Farben 4 bpp, 24 = 16 Farben 8pp, 28 = 256 Farben 16 bpp, 216 = 65,536 Farben (Hartettfarben) 24 h, 224 = 1776,7162 Farben) Farbtiefe von 15 oder mehr Bits pro Pixel ist in der Regel die Summe der für jede der roten, grünen und blauen Komponenten zugeteilten Bits. Hohe Farbe, in der Regel 16 Bpp, verfügt in der Regel über fünf Bits für rot und blau und sechs Bits für Grün, da das menschliche Auge empfindlicher ist als in den anderen beiden Grundfarben. Für Anwendungen, die die Transparenz betreffen, können die 16 Bits jeweils in fünf Tranchen aufgeteilt werden, die jeweils rot, grün und blau sind, wobei ein wenig für Transparenz übrig bleibt. Eine 24bit Tiefe erlaubt 8 Bits pro Komponente. In einigen Systemen ist die 32-bit Tiefe verfügbar: Dies bedeutet, dass jeder 24-bit-Tablet einen zusätzlichen 8 Bits zur Beschreibung seiner MKS (für die Kombination mit einem anderen Bild) hat. Subsaat Viele Anzeige- und Bildaufnahmesysteme sind nicht in der Lage, die verschiedenen Farbkanäle an derselben Stelle zu zeigen oder zu erkennen. Das Pixel-Netz ist daher in einzelne Regionen aufgeteilt, die einen Beitrag zur angezeigten oder sinnvollen Farbe leisten, wenn es in einer Entfernung gesehen wird. In einigen Bildschirmen, wie LCD, LED und Plasmabildschirme, sind diese einzigen Farbregionen gesondert adressenpflichtige Elemente, die als Subsaat bekannt sind, vor allem Schokoladefarben. LCDs z.B. spalten in der Regel jedes Pixel vertikal in drei Teilbereiche. Liegt das Quadrat Pixel in drei Unterteile aus, so ist jeder Teil von iPhone notwendigerweise rechteckig. In der Anzeige der Industrieterminologie werden Partikel häufig als Pixel bezeichnet, da sie die grundlegenden adressenpflichtigen Elemente im Hinblick auf Hardware sind und daher Pixelkreise anstelle von Sub-Aluminium-Sendern verwendet werden. Die meisten Digitalkamera-Bilder verwenden einzelne Farbsensor-Regionen, zum Beispiel die Verwendung des Bayer-Filtermusters, und in der Kamerabranche sind diese als Pixel bekannt wie in der Display-Industrie, nicht unter Aluminium. Für Systeme mit Subsaat können zwei unterschiedliche Ansätze gewählt werden: Die Subdacs können außer Acht gelassen werden, wobei die vollfarbenen Pixel als kleinstes adressenpflichtiges Bildelement behandelt werden; oder die Sub-Aluminiums können in die Berechnungen einbezogen werden, die mehr Analyse- und Bearbeitungszeit erfordern, aber in einigen Fällen offensichtlich überlegene Bilder produzieren können. Letzterer Ansatz, der als Substitution bezeichnet wird, nutzt das Know-how der Pixel Geometrie, um die drei gefärbten Substylen getrennt zu manipulieren, was zu einer Erhöhung der offensichtlichen Auflösung der Farbbildschirme führt. CRT stellt zwar die Verwendung rot-grün-blau-blindiger Phosphorzonen vor, die von einem Netz namens Schattenmaske bestimmt werden, aber es würde einen schwierigen Kalibrierungsschritt erfordern, der an den angezeigten Pixel-Raster angeglichen werden soll, so dass CRTs derzeit keine Subsaat verwenden. Das Konzept von Subdacs ist mit Proben verbunden. Los Mega (MP) ist eine Million Pixel; der Begriff wird nicht nur für die Anzahl der Pixel in einem Bild verwendet, sondern auch für die Anzahl der Bildsensorelemente digitaler Kameras oder die Anzahl der Anzeigeelemente digitaler Bildschirme. Beispielsweise nutzt eine Kamera, die ein 2048 × 1536 Pixelbild (3,145,728 Endbild-Bilder) macht, in der Regel nur wenige Extra-Sequenzen und Spalten von Sensorelementen und wird allgemein gesagt, "3.2 Mega iPhones" oder "4,4 Mega iPhones" zu haben, je nachdem, ob die Zahl der gemeldeten Zahlen die tatsächliche oder die gesamte Pixelzahl ist. Pixel wird verwendet, um die Auflösung eines Fotos zu definieren. Photolösung wird berechnet, indem die Breite und Höhe eines Sensores in Pixel multipliziert werden. Digitalkameras verwenden Fotoempfindliche Elektronik, entweder abgabeunabhängiges Gerät (CCD) oder ergänzende Metall-Oxide-Semiconduktor (CMOS)-Bilder, bestehend aus einer Vielzahl von Einzelsensorelementen, die jeweils eine gemessene Intensität aufweisen. In den meisten digitalen Kameras wird das Sensorspektrum mit einem gemusterten Farbfilter mit roten, grünen und blauen Regionen in der Bayer-Filter-Vereinbarung abgedeckt, so dass jedes Sensorelement die Intensität einer einzigen primären Lichtfarbe erfassen kann. Durch einen Prozess namens Demosafigur werden die Farbinformationen der benachbarten Sensorelemente miteinander verknüpft, um das Endbild zu schaffen. Diese Sensorelemente werden oft als Pixel bezeichnet, obwohl sie nur 1 Kanal (nur rot oder grün oder blau) des letzten Farbbildes aufweisen. So müssen zwei der drei Farbkanäle für jeden Sensor interpoliert und eine so genannte N-Mega-Kamera, die ein N-Mega iPhone-Bilder produziert, nur ein Drittel der Informationen liefern, die ein Bild der gleichen Größe aus einem Scanner erhalten könnten. Manche Farbkontraste können also fuzzier als andere betrachten, je nach der Zuweisung der Grundfarben (grün hat doppelt so viele Elemente wie rot oder blau in der Bayer-Vereinbarung). DxO Labs hat den Perceptual Mega® (P-MPix) entwickelt, um die scharfe Wirkung zu messen, die eine Kamera bei einer bestimmten Linse produziert – gegenüber den MP-Herstellern für ein Kameraprodukt, das nur auf dem Kamerasensor basiert. Die neue P-MPix behauptet, ein genauerer und relevanter Wert für Fotografen zu sein, um zu erwägen, wann die Belastung durch die Kamera scharf ist. Mitte 2013 hat die Sigma 35 mm f/1.4 DG HSM-Linden auf einer MH D800 die höchste Messmenge P-MPix. Mehr als ein Drittel des 36.3en MP-Sensors von D800 entfernt sich jedoch mit einem Wert von 23 MP. August 2019 veröffentlicht. Redmi Note 8 Pro als erstes Smartphone der Welt mit 64 MP Kamera. Dezember 2019 Samsung hat Samsung A71 mit ebenfalls 64 MP Kamera veröffentlicht. Ende 2019 gab es mit 108MP 1/1.33-Zoll über Sensor das erste Kamera-Telefon. Der Sensor ist größer als die meisten Brückenkameras mit 1/2.3-Zoll über Sensor. Eine neue Methode zur Addition von Megapixels wurde in einer Micro Four Dritter Systemkamera eingeführt, die nur einen 16 MP-Sensor nutzt, kann aber ein 64 MP-Bilder (40 MP) mit zwei Expositionen herstellen und den Sensor um ein halbes Pixel zwischen ihnen verschieben. In einem einzigen 64 MP-Bild werden die Multiplen 16 MP-Bilder dann in ein einheitliches 64 MP-Bild eingebracht. Siehe auch Verweise Externe LinksA Pixel Is Not A Little Square: Microsoft Memo von Computergrafik Pionier Alvy Ray Smith. Video von Lyons Gespräch über Pixel Geschichte im Computer History Museum und Non-Square Pixels: Technische Informationen über Pixel-Elemente moderner Videonormen (480i, 576i, 1080i, 720p) sowie Software-Auswirkungen. 120 voestalpine ist hier: Es gibt viele Informationen über MegaScanner und Giga. Wie ein TV-Werk in langsamen Schritten - The langsam Mo Guys – YouTube Video von The langsam Mo Guys