k-means Clustering ist eine Methode der vektoriellen Quantisierung, die ursprünglich von der Signalverarbeitung bestimmt ist, um die Beobachtungen in k Cluster zu zerteilen, in denen jede Beobachtung zu dem Cluster mit dem nächstgelegenen Mittel (Cluster- oder Cluster-Cluster) gehört und als Prototyp des Clusters dient. Diese Ergebnisse führen zu einer Teilung des Datenraums in Voronoi Zellen.k-means Clustering minimiert innerhalb der Clustervariationen (kalierte Euclidean Entfernungen), aber nicht regelmäßige Euclidean-Entfernungen, die das schwierigere Weber-Problem darstellen: das bedeutet Optimierungen Quadratkilometer, während nur die geometrischen Medien die Euclidean-Abstände minimieren. Beispiel: bessere Lösungen für Euclidean finden Sie unter Verwendung von k-median und k-medoiden. Das Problem ist rechnerisch schwierig (NP-hard); aber effiziente hetourismusische Algorithmen sind schnell auf eine lokale Optimierung abgestimmt. Diese sind in der Regel ähnlich wie der erwartete Abbau-maximisierungsgorithmus für Mischungen von Gaussian-Vertrieben über einen iterativen Raffinerieansatz, der sowohl von k-means als auch von Gausssian-Gemischmodelling verwendet wird. Sie verwenden Clusterzentren, um die Daten zu modellieren; k-means Clustering ist jedoch tendenziell, Cluster vergleichbarer räumlicher Tragweite zu finden, während das Gausian-Mixmodell Clustern unterschiedliche Formen ermöglicht. Der unüberwachte k-means-Algorithmus hat eine lose Beziehung zu dem k-norest-Nachbarn-Klasserhalter, einer gängigen maschinellen Lerntechnik zur Einstufung, die oft mit k-means verwechselt ist. Anhand der 1-Nearest-Nachbarklasse an die von k-means gewonnenen Cluster-Zentren werden neue Daten in die bestehenden Cluster aufgenommen. Dies ist bekannt als der nächstgelegene Centroid-Klasser oder der Rocchio-Algorithmus. In Anbetracht einer Reihe von Beobachtungen (x1, x2, ..., xn), bei denen jede Beobachtung ein d-dimensionaler realer Vektor ist, zielt die k-means-Clusterierung darauf ab, die n Beobachtungen in k (≤ n) zu k(n) zu {S1, S2, ..., Sk} aufzuteilen, um die Summe der Quadrate (WCSS) (d. h. Variance) zu minimieren. Formell ist das Ziel, i = 1 k ∑ x   S i   x − μ i  2 2 = a r g i n S ∑ i = 1 k S i {\displaystyle liv {S} {\}operatorname {arg\,min} \}sum _i=1}^{k}\sum {x} \in S_{i-left\\\ Mathematik {x} {\print Symbol 7.8mu _i\right\|^{2} {S}  operator {arg\,min}  isum i=1}^{k} {Var} S_{i}, wo μi das Mittel der Punkte in Si ist. Dies entspricht der Minimierung der gewichtsmäßigen Abweichungen von Punkten im gleichen Cluster: a r g i n S  i i = 1 k 1 2 [  i x , y  S S i ‖ x  y x  - y {\ 2 {\displaystyle 574underset ggiolog {S} ,operatorname {arg\,min} ,sum _i=1}^{k{\ 1}{2|S_{i}|,\sum chino {x},\ Mathematik {y} \in S_{i\left\\\\ Mathematik {x} -\ Mathematik {y} rechts\|2} Die Gleichwertigkeit kann von der Identität  i x  i x  i x  i 2 = ∑ x  y x  S y  S S i ( x − μ i ) T ( μ i  - y ) {\displaystyle \sum \sum {x} \in\in S_{i-left\\\ Mathematik {x} {\print Symbol 7.8mu _i\right\|^{2} 7.8_ Mathematik {x} \neq \ Mathematik {y} \in S_{i((\ Mathematik {x} {-\print Symbol HANAmu _i})^{T}(Eheschneider Symbol 7.8mu {_i}-\ Mathematik {y} .Die Gesamtvarianz ist konstant, dies entspricht der Maximierung der Summe der flächendeckenden Abweichungen zwischen den Punkten in verschiedenen Clustern (zwischen-Cluster-Zahl von Quadraten, BCSS), die sich aus dem Gesetz der Gesamtvarianz ergibt. Geschichte Erst 1967 wurde der Begriff k-means von James MacQueen verwendet, obwohl die Idee im Jahr 1956 auf Hugo Steinhaus zurückgeht. Der Standardgorithmus wurde 1957 von Stuart Lloyd von Bell Labs als Technik für die Modulation von Puls vorgeschlagen, obwohl er bis 1982 nicht als Zeitungsartikel veröffentlicht wurde. Edward W. Forgy hat 1965 im Wesentlichen dieselbe Methode veröffentlicht, weshalb es manchmal als Lloyd-Forgy-Algorithmus bezeichnet wird. Algorithms Standard-Algorithmus (naive k-means) Der häufigste Algorithmus verwendet eine iterative Raffinerietechnik. Aufgrund seiner ubiquity wird es oft als "k-means-Algorithmus" bezeichnet; sie wird auch als Lloyd's-Algorithmus bezeichnet, vor allem in der Computerwissenschaft. Es wird manchmal auch als "naïve k-means" bezeichnet, weil es viel schnellere Alternativen gibt. In Anbetracht eines ersten Satzes von k · m1(1),...,mk(1)(siehe unten) führt der Algorithmus zu einem Wechsel zwischen zwei Schritten: Sendungsschritt: Als jede Beobachtung des Clusters mit dem nächstgelegenen bedeutet: dass mit der geringsten Entfernung von Euclidean. () Mathematisch bedeutet dies, dass die Beobachtungen nach dem von den Mitteln erstellten Voronoi- Diagramm aufgeteilt werden. S i ( t) = { x p : ‖ x p  - m i ( t) ) 2 ≤ ) x p − j ( t )  2 2  j  1 , 1 ≤ k } , {\displaystyle S_{i((t)}=\left\{x_{p}:\left\left\\\\\\\\\\|x_{p}-m_{i((t)|2|leq links\|x_{p}-m_{j((t)|right\|2 jforall j,1\leq j\leq k\right,}, wo jeder x p RARstyle x_{p} genau ein S ( t) Memestyle S S(t)} zugewiesen wird, auch wenn es zwei oder mehr von ihnen zugewiesen werden könnte. Aktualisierungsschritt: Neukalkulierung bedeutet (Konzentrationen) für Beobachtungen, die jedem Cluster zugewiesen werden. m i ( t + 1 ) = 1 [ S i ( t )];  j x j  i S i ( t ) x j {\displaystyle m_{i((t+1)}=1,0frac 1 12|(t)|(t)|(t)||||| x_{j Sin S_{i((t)}}x_ Der Algorithmus hat sich geeinigt, wenn sich die Zuteilungen nicht mehr ändern. Der Algorithmus ist nicht garantiert, um das beste zu finden. Der Algorithmus wird oft als zuweisende Objekte an das nächste Ferncluster präsentiert. Nutzung einer anderen Fernfunktion als (Grenzwert) Euclidean Entfernung kann verhindern, dass der Algorithmus konvergiert wird. Verschiedene Änderungen von k-means wie spherical k-means und k-medoids wurden vorgeschlagen, um andere Fernmaßnahmen zu ermöglichen. Initialisierungsmethoden, die allgemein angewandt werden, sind Forgy und Random Trennwand. Zufallsprinzip wählt k Beobachtungen vom Datenset aus und nutzt diese als erste Mittel. In der Random-Dimensionsmethode wird zunächst ein Cluster zu jeder Beobachtung zugewiesen und dann wird der aktualisierte Schritt weiterverfolgt, was bedeutet, dass das erste Mittel die Zentriken der zufällig zugewiesenen Cluster sein wird. Die Forgy-Methode setzt sich tendenziell aus, während zufällige Trennwände alle in der Nähe des Zentrums der Daten liegen. Laut Hamerly et al ist die Random-Distributionsmethode in der Regel vorzugsweise für Algorithmen wie die k-harmonischen Mittel und fuzzy k-means. Vorausschätzungen zur Maximierung und Standard-Kmeans-Algorithmen sind vorzugsweise die Forgy-Methode der Paraphierung. Jedoch ergab eine umfassende Studie von Prominenten und al, dass populäre anfängliche Methoden wie Forgy, zufällige Trennung und Maximin oft schlecht durchgeführt werden, während das Konzept von Bradford und Fayyad in "der besten Gruppe" konsequent funktioniert und k-means+ im Allgemeinen gut funktioniert. Demonstration des Standardgorithmus Der Algorithmus sorgt nicht für eine weltweite optimale Konvergenz. Das Ergebnis kann von den ersten Clustern abhängen. Da der Algorithmus in der Regel schnell ist, ist es üblich, ihn mehrfach mit unterschiedlichen Ausgangsbedingungen zu führen. Leider kann die Leistung des schlimmsten Falles langsam sein: Insbesondere bestimmte Punkte, auch in zwei Dimensionen, konvergieren sich in exponentieller Zeit, das ist 2 Ohm(n). Dieser Punkt scheint in der Praxis nicht zu kommen: Dies wird durch die Tatsache bestätigt, dass die reibungslose Betriebszeit von k-means Polynomial ist. Der Zuteilungsschritt wird als "Erwarteungsschritt" bezeichnet, während der "Aktualisierungsschritt" ein Maximierungsschritt ist, wodurch dieser Algorithmus eine Variante des allgemeinen Herabschätzungs-maximisierungsgorithmus darstellt. Komplexität Suche nach einer optimalen Lösung des k-means-Clusterproblems für Beobachtungen in d-Dimensionen: NP-hard im allgemeinen Euclidean-Raum (von d-Dimensionen) selbst für zwei Cluster, NP-hard für eine allgemeine Anzahl von Clustern k selbst im Flugzeug, wenn k und d (die Dimension) fixiert sind, kann das Problem in der Zeit (n d k + 1 ) Memestyle O Od+1} gelöst werden, wo n die Zahl der Einheiten für Cluster ist. So werden in der Regel verschiedene He touristische Algorithmen wie Lloyd's Algorithmus verwendet. Laufzeit des Lloyd's-Algorithmus (und die meisten Varianten) ist O ( k d i) Memestyle O(nkdi)}, wo: n ist die Zahl der d-dimensionalen Vektoren (zu Clustered) k die Zahl der Cluster, die bis zur Konvergenz erforderlich sind. Daten, die eine Clusterstruktur aufweisen, sind oft klein, und die Ergebnisse verbessern sich nur leicht nach dem ersten Dutzend. Lloyd's Algorithmus wird daher häufig als lineare Komplexität in der Praxis angesehen, obwohl es im schlimmsten Fall Supermonopole ist, wenn sie bis zur Konvergenz durchgeführt werden. Lloyd's Algorithmus benötigt i = 2 fluor ( ) n ) Memedisplaystyle i=2Omega® {(\sqrt {n)} Iterations, so dass die schlimmste Komplexität des Lloyd's Algorithmus Supermonopol ist. Lloyd's k-means Algorithmus hat eine polynomial reibungslose Laufzeit. In [ 0 , 1 ] d Memestyle [0,1]^{d} ist festzustellen, ob jeder Punkt unabhängig von einer normalen Verteilung mit einem Mittelwert von 0 und variance  2 2 Memestyle \sigma {^2} ist, dann ist die erwartete Laufzeit von k-means durch O (n 34 k 8 Log 4  6  6  6) /  O 6 O  O \sigma  1 2  O 2 \sigma {^2} gebunden. Zum Beispiel wird gezeigt, dass die Laufzeit des k-means-Algorithmus von O ( d n 4 M 2 ) Memedisplaystyle O(dn^{4}M2)2) für n Punkte in einer Vielzahl von Lattice { 1 , ... , M } d KINGstyle {1,\dots ,M\}^{d .Lloyd's Algorithmus ist der Standardansatz für dieses Problem. Jedoch gibt es viel Zeit, um die Entfernungen zwischen den einzelnen K-Clusterzentren und den n-Datenpunkten zu ermitteln. Da die Punkte in der Regel in denselben Clustern bleiben, so ist ein Großteil dieser Arbeit unnötig, was die naïve Umsetzung sehr ineffizient macht. Manche Umsetzungen verwenden Kaching und das Dreieck Ungleichbehandlung, um Bindungen zu schaffen und den Lloyd's Algorithmus zu beschleunigen. Variationen Jenks natürliche Optimierung: k-means, die auf unvariable Daten k-medians Clustering angewendet werden, verwendet die Medien in jeder Dimension anstelle des Mittels und minimiert so L 1 {\displaystyle L_{1} Norm (Flustabge). k-medoids (auch: Trennung rund Medoids, PAM) verwendet das Mittel anstelle des Mittels und so minimiert die Summe der Entfernungen für willkürliche Fernfunktionen. Fuzzy C-Means Clustering ist eine Soft-Version von k-means, in der jeder Datenpunkt einen fuzzen Grad an Zugehörigkeit zu jedem Cluster hat. Gaussian-Mixmodelle, die mit Erwartungs-maximisierungs-Algorithmus (EM-Algorithmus) ausgebildet wurden, halten probabilistische Zuteilungen an Cluster, statt deterministische Zuteilungen, und multivariate Gausssian-Vertrieb anstelle von Mitteln.k-means+ wählt zunächst Zentren auf eine Weise aus, die eine tragfähige Obergrenze für das Ziel des WCSS darstellt. Der Filtergorithmus verwendet kd-trees, um jeden k-means Schritt zu beschleunigen. Manche Methoden versuchen, jeden k-means Schritt mit dem Dreiecksungleich zu beschleunigen. Ausscheiden Sie lokale Optik durch Tauschpunkte zwischen Clustern. Der Spherical k-means Clustering-Algorithmus ist für Textdaten geeignet. Hierarchische Varianten wie Bisecting k-means, X-means Clustering und G-means Clustern, die eine Rangordnung aufbauen, wiederholt aufgeteilt werden und auch versuchen, die optimale Anzahl von Clustern in einem Datenset automatisch zu bestimmen. Interne Cluster-Bewertungsmaßnahmen wie Cluster-Kurve können bei der Bestimmung der Anzahl von Clustern hilfreich sein. Minkowski gewichtete k-means berechnet automatisch Cluster-spezifische Merkmalsgewichte und unterstützt die intuitive Idee, dass ein Merkmal in unterschiedlichen Merkmalen unterschiedliche Bedeutung haben kann. Diese Gewichte können auch verwendet werden, um ein bestimmtes Datensatz neu zu gestalten und die Wahrscheinlichkeit eines Index für die Gültigkeit von Clustern zu erhöhen, der bei der erwarteten Anzahl von Clustern optimiert werden soll. Mini-batch k-means: k-means Variationen mit "minie Charge"-Proben für Datensets, die nicht in den Gedächtnis passen. Otsus Methode Hartigan-Wong-Methode Hartigan und die Methode von Wong bieten eine Variante des k-means-Algorithmus, der auf ein lokales Minimum an Summen-Problem mit unterschiedlichen Lösungs-Aktualisierungen vorankommt. Die Methode ist eine lokale Suche, die versucht, eine Probe in ein anderes Cluster umzuwandeln, solange dieser Prozess die objektive Funktion verbessert.Wenn keine Probe in ein anderes Cluster mit einer Verbesserung des Ziels umgesiedelt werden kann, endet die Methode (in einem lokalen Minimum). In ähnlicher Weise wie die klassischen k-means ist der Ansatz nach wie vor ein touristischer Ansatz, da er nicht unbedingt sicherstellt, dass die endgültige Lösung weltweit optimal ist. Let  j (S j ) {\displaydisplaystyle \varphi (S_{j)} sind die individuellen Kosten von S j {\displaystyle S_{j}, definiert durch ∑ x  S S j ( x − μ j ) 2 Memestyle \sum {_x\in S_{j((x-\mu _j})^{2 , mit μ j \mu \muj {j } des Clusters. Sendungsschritt: Hartigan und die Methode von Wong beginnen, indem die Punkte in zufällige Cluster aufgeteilt werden { S j } j { { 1 ,  k k } S_{j__{j\in {1,\cdots k. . Aktualisierungsschritt: Künftig bestimmt sie die n , m   { 1 , ... , k } displaystyle n,m\in {1,\ldots ,k\} und x  S S n {\displaystyle x\in S_{n}, für die die folgende Funktion eine maximale Δ ( m , n , x) = ) ( S n ) + ) ( S  S n  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  S  . KINGstyle \Delta (m,x)=\varphi (S_{n})+\varphi (S_{m})-\varphi (S_{n}\smallsetminus {x\})-\varphi (S_{m}\ {x)}. n , n , m Memestyle x,n,m}, die diesen Höchstwert erreichen, x Memestyle x} bewegt sich aus dem Cluster S n Memestyle S_{n} auf den Cluster S m HANAdisplaystyle S_{m} .Termination: Der Algorithmus endet einmal Δ ( m , n , x ) JPYstyle \Delta (m,n,x)} ist weniger als Null für alle x , n , m {\displaystyle x,n,m} .Different Bewegungsannahmestrategien können verwendet werden. In einer ersten Verbesserungsstrategie kann jede Verbesserung der Umverteilung angewendet werden, während in einer Best-Verbesserungsstrategie alle möglichen Umsiedlungen auf iterative Weise getestet werden und nur das beste gilt für jede Erhöhung. Der frühere Ansatz befürwortet Geschwindigkeit, ob der letztere Ansatz generell die Lösungsqualität auf Kosten zusätzlicher Rechenzeit bevorzugt. Die Funktion {\ \Delta }, die zur Berechnung des Ergebnisses einer Umsiedlung verwendet wird, kann auch durch die Anwendung der Chancengleichheit ( x , n , m ) =  S S  S  S S  S  S  S  1  1  1  1  1  1  1 1 ⋅  1  2  1 2 .  S S m  S  S  S  1 + 1 μ μ  2  2  2  2  2 2  2 2 . . . . . . . . . 2 . . . . . . . . . . . 2 . . . . . . 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.8displaystyle \Delta (x,n,m)=Portfrac ggiomid S_{n}\mid  mid S_{n}\mid -1}}\cdot lVert\mu {_n}-x\r Vert ^2}-Barfrac Mememid S_{m}\mid  mid S_{m}\mid +1 +1cdot lVert\mu {_m}-x\r Vert {^2}. Globale Optimierung und Metahetourismus Der klassische k-means-Algorithmus und seine Differenzen sind bekannt, dass sich das als r g i n S  i i = 1 k ∑ x  to definierte Mindest-Risiko des Cluster-Risikos auf lokaler Ebene nur annähern lässt. S i . x − μ i  i 2 . 7.8displaystyle WELLunterset 7.8 Mathematik {S} {arg\,min} \}sum _i=1}^{k}\sum {x} \in S_{i-left\\\ Mathematik {x} {\print Symbol 7.8mu _i\\||2. Viele Studien haben versucht, das Konvergenzverhalten des Algorithmus zu verbessern und die Chancen zu maximieren, das globale optimale (oder zumindest lokale Minima besserer Qualität) zu erreichen. Initialisierungs- und Wiederaufnahmetechniken, die in den vorherigen Abschnitten erörtert werden, sind eine Alternative, um bessere Lösungen zu finden. Kürzlich haben mathematische Programmierungsalgorithmen auf der Grundlage von Branchen-und-bound- und Spaltenerzeugung 'verbessert' Lösungen für Datensets mit bis zu 2300 Unternehmen hergestellt. Wie erwartet, dass die Rechenzeit der optimalen Algorithmen für K-Mechanismen aufgrund der NP-Belastung des Subjacent Optimierungsproblems schnell über diese Größe hinaus ansteigt. optimale Lösungen für Klein- und Mittelausstattung sind nach wie vor wertvoll als Benchmark-Instrument, um die Qualität anderer Hetourismus zu bewerten. Um qualitativ hochwertige lokale Minima innerhalb einer kontrollierten Rechenzeit zu finden, aber ohne optimale Garantie haben andere Werke Metahetourismus und andere globale Optimierungstechniken untersucht, z.B. auf der Grundlage von inkrativen Ansätzen und der konvexen Optimierung, Zufallstausch (d. h. iterierte lokale Suche), variablen Nachbarschafts- und Genetikalgorithmen. Kennzeichnend ist, dass die Suche nach besserer lokalen Minima des minimalen Cluster-Problems zwischen Ausfall und Erfolg zur Wiedereinziehung von Clusterstrukturen in Merkmalsräumen von hoher Dimension. Diskussion Drei wesentliche Merkmale von k-means, die es effizient machen, werden häufig als die größten Rückschläge angesehen: Euclidean Entfernung wird als Maßeinheit und Varianz verwendet. Anzahl von Clustern k ist ein Inputparameter: Eine unangemessene Auswahl von k kann zu schlechten Ergebnissen führen. Daher ist es bei der Durchführung von k-means wichtig, Diagnosekontrollen durchzuführen, um die Anzahl der Cluster in den Daten festzulegen. Konvergenz auf ein lokales Minimum kann falsche (w) falsche Ergebnisse (siehe Beispiel in Figel’) liefern. K-means ist das Clustermodell. Das Konzept basiert auf spheralen Clustern, die separierbar sind, damit sich die Mittel an das Clusterzentrum annähern. Die Cluster werden voraussichtlich eine ähnliche Größe sein, so dass die Zuteilung an das nächste Clusterzentrum die richtige Aufgabe ist. Zum Beispiel die Anwendung von k-means mit einem Wert von k = 3 {\displaystyle k=3} auf den bekannten Markt Iris Blumendaten Set, das Ergebnis unterscheidet oft die drei Iris-Arten, die in der Datensammlung enthalten sind. Mit k = 2 {\displaystyle k=2} werden die beiden sichtbaren Cluster (eine mit zwei Arten) entdeckt, während mit k = 3 {\displaystyle k=3} eine der beiden Cluster in zwei Teile aufgeteilt werden. k = 2 Coladisplaystyle k=2} ist für diese Daten, trotz der Daten, die 3 Klassen enthalten, besser geeignet. Wie bei jedem anderen Cluster-Algorithmus ergibt die k-means Annahmen, dass die Daten bestimmte Kriterien erfüllen. Es funktioniert gut auf einigen Datensätzen und versagen andere. Das Ergebnis von k-means ist als die Voronoi-Zellen des Clusters zu sehen. Da die Daten zwischen Clustern aufgeteilt werden, kann dies zu suboptimalen Bruchteilen führen, die im Beispiel der Maus zu sehen sind. Die Gaussian-Modelle, die durch den erwarteten Abbau-maximisierungs-Algorithmus (vor allem eine allgemeine Einführung von k-means) verwendet werden, sind flexibler, da sie sowohl Varizen als auch Kovariationen aufweisen. Das Ergebnis des EM ist somit in der Lage, Cluster von variabler Größe viel besser als k-means und korrelierte Cluster (nicht in diesem Beispiel) zu berücksichtigen. EM erfordert die Optimierung einer größeren Anzahl freier Parameter und stellt aufgrund von Vancing Clustern oder schlecht abhängigen Kovarianz-Matrices einige methodische Probleme dar. K-means steht in engem Zusammenhang mit nichtparametrischen Bayesianmodelling. Anwendungen k-means Clustering ist eher leicht anwendbar für noch große Datensets, insbesondere bei der Nutzung von Hetourismus wie Lloyd's Algorithmus. Es wurde erfolgreich in Marktsegmentierung, Computervision und Astronomie in vielen anderen Bereichen genutzt. Es wird oft als Vorverarbeitungsschritt für andere Algorithmen verwendet, z.B. um eine Anfangsform zu finden. Vector quantization k-means stammt aus der Signalverarbeitung und findet immer noch in diesem Bereich Anwendung. In Computerbildern ist die Farbberechnung die Aufgabe, die Farbpalette eines Bildes auf eine feste Anzahl von Farben k zu reduzieren. Der k-means-Algorithmus kann für diese Aufgabe leicht genutzt werden und wettbewerbsfähige Ergebnisse hervorbringen. Ein Anwendungsfall für diesen Ansatz ist Bildsegmentierung. Andere Verwendungen der Vektorberechnung umfassen nicht-randomisierte Probenahmen, da k-means leicht genutzt werden können, um k verschiedene, aber modellische Objekte aus einer großen Datengruppe für weitere Analysen auszuwählen. Clusteranalyse In der Clusteranalyse kann der k-means-Algorithmus verwendet werden, um die in k-Distributionen (Cluster) gesetzten Inputdaten aufzuteilen. Jedoch ist der reine k-means-Algorithmus nicht sehr flexibel, und als solche ist ein begrenzter Einsatz (ausgenommen, wenn die Radikalisierung tatsächlich der gewünschte Einsatzfall ist). Insbesondere ist der Parameter k bekannt, dass er schwer wählen kann (wie oben diskutiert), wenn er nicht durch externe Zwänge gegeben wird. Eine weitere Beschränkung ist, dass sie nicht mit willkürlichen Fernfunktionen oder nicht-numerischen Daten verwendet werden kann. Für diese Verwendungsfälle sind viele andere Algorithmen überlegen. k-means Clustering wurde als ein Merkmalslehrgang (oder das Sprachenlernen) in entweder (semi-)beaufsichtigtem Lernen oder unüberwachtem Lernen genutzt. Der Grundansatz ist zunächst, um eine k-means-Clustervertretung mithilfe der Input-Ausbildungsdaten (die nicht gekennzeichnet werden müssen) auszubilden. Dann, um alle Eingaben in den neuen Platz zu projizieren, eine Programmierfunktion, wie das gegrenzte Matrix-Produkt des Datums mit den Zenroid-Standorten, berechnet die Entfernung von dem Datum bis zu jedem Centroid oder einfach eine Indikatorfunktion für die nächste Centroid oder eine reibungslose Umwandlung der Entfernung. Um die Probe-Clusterentfernung durch einen Gausssian RBF umzuwandeln, erhält die versteckte Schicht eines Radialfunktionsnetzes. Diese Nutzung von k-means wurde erfolgreich mit einfachen, linearen Klassenaggregaten für das halbgesteuerte Lernen in NLP (insbesondere für die Anerkennung von Unternehmen) und in der Computervision kombiniert. In einer objektiven Anerkennungsaufgabe wurde festgestellt, dass vergleichbare Leistungen mit anspruchsvolleren Lernkonzepte wie Autoencoders und eingeschränkten Poszmann-Maschinen hergestellt wurden. In der Regel sind jedoch mehr Daten für gleichwertige Leistung erforderlich, da jeder Datenpunkt nur zu einem Merkmal beiträgt. Zusammenhang mit anderen Algorithmen Gausssian-Gemischmodell Der langsame "Standard-Algorithmus" für k-means Clustering und sein damit verbundener Erwarteungs-maximisierungsgorithmus ist ein Sonderfall eines Gausischen Mischmodells, insbesondere die Begrenzung der Fälle, in denen alle Kovariationen zu diagonalen, gleichen und unbegrenzten Kleinvariationen gehören. statt kleiner Sorten kann auch eine harte Clusterzuteilung verwendet werden, um eine weitere Gleichwertigkeit von k-means Clustern zu einem besonderen Fall harter Gausssischer Mischmodelle zu zeigen. Dies bedeutet nicht, dass es effizient ist, Gausssian-Mix-Modellen zu verwenden, um k-means zu berechnen, aber nur, dass ein theoretisches Verhältnis besteht und dass Gausssische Mischungsmodelle als allgemeine Einführung von k-means ausgelegt werden können; im Gegenteil wurde vorgeschlagen, k-means Clustering zu verwenden, um Ausgangspunkte für Gausian-Gemischmodellierung auf schwierigen Daten zu finden. K-SVDAn Andere Generalisierung des k-means-Algorithmus ist der K-SVD-Algorithmus, der Datenpunkte als eine geringe lineare Kombination von "Codebook Vektors" aufk-means geschätzt, entspricht dem besonderen Fall der Verwendung eines einzigen Codebook Vektors mit einem Gewicht von 1. Kernelementanalyse Die entspannte Lösung von k-means Clustering, die in den Clusterindikatoren angegeben ist, wird durch die Hauptkomponentenanalyse (PCA) vorgenommen. Intuation ist, dass k-means sphertische (ballähnliche) Cluster beschreiben. Wenn die Daten 2 Cluster aufweisen, ist die Linie, die die beiden Centroide verbindet, die beste 1-dimensionale Projektionslinie, die auch die erste PCA-Leitung ist. Zerkleinert die Linie im Massenbereich die Cluster (dies ist die kontinuierliche Lockerung des gesonderten Clusterindikators). Wenn die Daten drei Cluster aufweisen, ist das von drei Clustern geteilte 2-dimensionale Flugzeug die beste 2-D-Prognose. Dieses Flugzeug wird auch von den ersten beiden PCA-Dimensionen definiert. Well-getrennte Cluster werden durch ballförmige Cluster effektiv modelliert und von k-means entdeckt. Nicht-ball-förmige Cluster sind schwer zu trennen, wenn sie geschlossen sind. z.B. zwei halbmoon-geschneiderte Cluster, die im Weltraum miteinander verbunden sind, trennen nicht gut, wenn sie auf den PCA-Unterraum projiziert werden. k-means sollten nicht auf diese Daten eingehen. Es ist einfach, widersprüchliche Beispiele für die Aussage zu erstellen, dass der Teilraum von Cluster Centroid von den wichtigsten Richtungen geteilt wird. Mean Schicht Clustering Grundlegendes bedeutet eine Verlagerung von Clustering-Algorithmen, die eine Reihe von Daten enthält, die die gleiche Größe wie die Ausgangsdaten. Anfangs wird dieses Set aus dem Eingangssatz kopiert. In diesem Fall wird dieser Satz durch den Mittelwert dieser Punkte in dem Set ersetzt, der in einer bestimmten Entfernung dieses Punkts liegt. k-means beschränkt diesen aktualisierten Satz auf k Punkte, die in der Regel viel weniger als die Anzahl der in den Eingabedaten festgesetzten Punkte sind, und ersetzt jeden Punkt in diesem Satz durch alle Punkte in den Inputs, die näher an diesem Punkt sind als jeder andere (z.B. innerhalb der Voronoi-Verteilung jeder Aktualisierung). Ein mittlerer Umstellungsgorithmus, der dann ähnlich ist wie k-means, so genannte Wahrscheinlichkeit, bedeutet eine Verlagerung, ersetzt das Set von Punkten, die durch alle Punkte im Eingangssatz ersetzt werden, die in einer bestimmten Entfernung des sich verändernden Set liegen. Einer der Vorteile einer Verlagerung über k-means ist, dass die Anzahl der Cluster nicht vorher spezifiziert ist, weil eine Verlagerung wahrscheinlich nur wenige Cluster finden wird, wenn nur eine kleine Zahl vorhanden ist. Man kann jedoch viel langsamer als k-means sein und erfordert immer noch eine Auswahl an Bandbreiten. Mean verlagert sich mit weichen Varianten. Analyse der unabhängigen Komponente Unter geringfügigen Annahmen und bei Vorabprozessen der Inputdaten mit der weißen Entwicklung stellt die k-means die Lösung der linearen unabhängigen Komponenteanalyse (ICA) her. Diese Beihilfen erklären die erfolgreiche Anwendung von k-means auf das Lernen. Bilaterale Filterung k-means implizit davon ausgehen, dass die Bestellung der Eingabedaten nicht geregelt ist. Der bilaterale Filter ähnelt den k-means und bedeutet, dass es eine Reihe von Daten, die durch Mittel ersetzt werden, aufhält. Jedoch beschränkt der bilaterale Filter die Berechnung des (kernel gewichteten) bedeutet, nur Punkte aufzunehmen, die bei der Bestellung der Eingangsdaten nahe liegen. Dies macht es auf Probleme wie Bildverweigerung anwendbar, wo die räumliche Anordnung von Pixeln in einem Bild von entscheidender Bedeutung ist. Ähnliche Probleme Die Zahl der flächendeckenden Fehler, die Clusterfunktionen minimieren, umfasst auch den k-medoids-Algorithmus, ein Ansatz, der den Schwerpunkt jedes Clusters bildet, um eines der tatsächlichen Punkte zu sein, d. h. es verwendet Medoide anstelle von Zenroiden. Software-Durchführung unterschiedliche Umsetzungen des Algorithmus zeigen Leistungsunterschiede, wobei die am schnellsten auf einem Testdatensatz von 10 Sekunden, der langsamsten Einnahme von 25,988 Sekunden (~7 Stunden) liegen. Die Unterschiede können auf die Umsetzungsqualität, die Sprache und die Zusammenstellung von Unterschieden, die unterschiedlichen Kündigungskriterien und die Präzisionsebene sowie die Verwendung von Indexen zur Beschleunigung zurückzuführen sein. Free Software/Open Source Folgende Umsetzungen sind im Rahmen von Free/Open Source Software-Lizenzen mit öffentlich zugänglichem Quellcode erhältlich. Vereinbarung.NET enthält C# Umsetzungen für k-means, k-means+ und k-modes. ALGLIB enthält Parallelisierte C+ und C# Umsetzungen für k-means und k-means++. AOSP enthält eine Java-Durchführung für k-means. KriminalitätStat führt zwei räumliche k-means-Algorithmen durch, die es dem Nutzer ermöglichen, die Ausgangsstandorte festzulegen. ELKI enthält k-means (mit Lloyd und MacQueen Iteration, zusammen mit verschiedenen anfänglichen Methoden wie k-means+ anfänglicheization) und verschiedenen fortgeschrittenen Cluster-Algorithmen. Smile enthält k-means und verschiedene andere Algorithmen und Ergebnisse (für sorg, kotlin und scala). Julia enthält eine k-means-Durchführung im JuliaStats Clustering-Paket. KNIME enthält Knoten für k-means und k-medoide. Mahout enthält einen Karte Rotuce basierende k-means. mlpack enthält eine C+-Durchführung von k-means. Octave enthält k-means. OpenCV enthält eine k-means-Durchführung. Orange umfasst eine Komponente für k-means Clustering mit automatischer Auswahl an k- und Cluster-Kurve. ZahlungsdienstleisterP enthält k-means, Der Befehl QUICK CLUSTER führt k-means Clustering an der Datenset. R enthält drei k-means Variationen. SciPy und Skikit-learn enthalten mehrere k-means-Durchführungen. Zünd MLlib setzt einen verteilten k-means-Algorithmus um. Torch enthält ein Paket, das k-means Clustering bietet. Wirka enthält k-means und x-means.Propriet Folgende Umsetzungen sind unter urheberrechtlich geschützten Lizenzbedingungen verfügbar und können nicht öffentlich verfügbar sein. Siehe auch BFRalgorithm Centroidal Voronoi tessellation Head/tail Breaks k q-site Kmeans++Linde-Buzo-Gray-Algorithmus Selbstorganisationskarte Referenzen =Wringe World ist das erste Studioalbum von American Rapper Tierra Whack. Es wurde am 30. Mai 2018 von Interkop Records veröffentlicht. Das Album wird hauptsächlich von Kenete Simms und Nick Verruto produziert und enthält andere Hersteller, darunter J Melodic, RicandThadeus,DJ Fly Guy und Scottstyle. Es wurde von Kenete Simms geteilt und von Chris Athen geleitet. Das Album-Werk – von einer Hubschrauber-Claw-Maschine – ist eine Skulptur, die von der Philadelphia Künstlerin Caroline Kunka entworfen wurde. Hintergrund WH war ein Kind, das in einer vorwiegend weißen Schule schwarz ist, die viel der "emotional work" inspirierte, die auf dem Album gemacht wurde. Mit jeder Songlänge, die eine Minute lang ist, hat Tierra Wringe ein 15-minütiges visuelles Album mit einem Musikvideo für jede Strecke veröffentlicht. WH sagt, dass sie ein visueller Lernender ist und die Bilder für WH World erlaubten, ihre Ideen zum Leben zu bringen und die Wahrheit in das Auge des Zuschauers zu rücken.“ In Bezug auf die vielen Veränderungen in ihrer Stimme sprach WH an Billboard: Ich bin mit meiner Stimme so lang. Ich habe begonnen, als ich eine Klasse war, und ich habe Spaß gemacht. Und es ist schlecht, weil manchmal ich es noch tut -- Ich höre jemand und sie haben eine seltsame Stimme, und ich habe es nicht. Ich muss aber allein [do it] sein und die Stimme so ausdrücken, und ich kann diese Stimme tun. Ich bin ein Schwämme, so habe ich diese Dinge gerade hören können. Kritische Aufnahme Das Album war kritisch und erhielt positive Bewertungen. Mallfork lobte das Album und verleiht ihm eine 8,3 von 10 Ratingen, was heißt: „Wringe Welt ist ein kleines Unternehmen von Minuten-langvignette, Tetering zwischen einem fantastischen Traum und einem unbefüllten Alptraum. Lieder teilen Doppelwerte mit der entsprechenden 15-minütigen visuelle Künftig mit dem Album, das noch mehr Dimension und Intrigue zu dem ehrgeizigen Projekt hinzufügt; Licht und Dunkel sind gezwungen, gemeinsam zu sein. Der Autor behauptete auch, dass das visuelle Album "vorverpackt für den optimalen sozialen Medienverbrauch ist; jeder kleine Stück befindet sich selbst, ohne das größere Bild zu verlieren. In seinem Kern, auch wenn das Gefühl von Trauer – die Ausdrucksweise einer Schwarzen Frau – eine Gelegenheit ist, einen Aspekt der Art zu feiern, der oft unlebiert ist, eine Chance für WH, sich selbst zu feiern.“ In einem Wirtschaftsstück über Frauen in der Musikindustrie im Jahr 2019 schrieb der Autor, dass Whack World daran arbeite, die populäre, derzeit kennzeichnende Musik zu vernichten. NPR Hip Hop Schriftsteller Rodney Carmichael lobte die Traumlogloglogik von Whack, die das visuelle Album kennzeichnet, und erklärte: „each Song vignette bietet ein tieferes Maß an Enthüllung in ihre Blauen. Liste der Teilnehmer auf dem Markt Links