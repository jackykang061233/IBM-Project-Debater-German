Nichtnegative Matrix-Faktorisierung (NMF oder NNMF) ist auch nichtnegative Matrix-Approximation eine Gruppe von Algorithmen in multivariater Analyse und linearer Algebra, wo eine Matrix V in (normalerweise) zwei Matrizen W und H faktorisiert wird, mit der Eigenschaft, dass alle drei Matrizen keine negativen Elemente aufweisen. Diese Nicht-Negativität macht die resultierenden Matrizen leichter zu kontrollieren. Auch bei Anwendungen wie der Verarbeitung von Audio-Spektrogrammen oder Muskelaktivität ist die Nicht-Negativität den betrachteten Daten inhärent. Da das Problem im allgemeinen nicht exakt lösbar ist, wird es häufig numerisch angenähert. NMF findet Anwendungen in Bereichen wie Astronomie, Computer Vision, Dokumenten-Clustering, fehlende Daten-Imputation, Chemometrie, Audiosignalverarbeitung, Empfehlungssysteme und Bioinformatik. Geschichte In der Chemometrie hat die nichtnegative Matrix Factorisierung eine lange Geschichte unter dem Namen "Selbstmodellierungskurvenauflösung". In diesem Rahmen sind die Vektoren in der richtigen Matrix kontinuierliche Kurven und nicht diskrete Vektoren. Auch die frühe Arbeit an nicht-negativen Matrixfaktoren wurde von einer finnischen Forschergruppe in den 1990er Jahren unter dem Namen positive Matrixfaktorisierung durchgeführt. Es wurde häufiger als nicht-negative Matrix Factorisierung bekannt, nachdem Lee und Seung die Eigenschaften des Algorithmus untersuchten und einige einfache und nützliche Algorithmen für zwei Arten von Factorisierungen veröffentlichten. Hintergrund Matrix V ist das Produkt der Matrizen W und H, V = W H . {\displaystyle \mathbf {V} =\mathbf {W} \mathbf {H} \,.} Matrixmultiplikation kann als Berechnung der Spaltenvektoren von V als lineare Kombinationen der Spaltenvektoren in W mit von Spalten von H gelieferten Koeffizienten realisiert werden. Das heißt, jede Spalte von V kann wie folgt berechnet werden: v i = W h i, {\displaystyle \mathbf {v} {_i}=\mathbf {W} \mathbf {h} {_i},\ wobei vi der i-te Spaltenvektor der Produktmatrix V und hi der i-te Spaltenvektor der Matrix H ist.Wenn Multipliziermatrizen, können die Dimensionen der Faktormatrizen deutlich niedriger sein als die der Produktmatrix und es ist diese Eigenschaft, die die Basis von NMF bildet. NMF erzeugt Faktoren mit deutlich reduzierten Abmessungen im Vergleich zur ursprünglichen Matrix. Ist V beispielsweise eine m × n-Matrix, W eine m × p-Matrix und H eine p × n-Matrix, so kann p deutlich kleiner als beide m und n sein. Hier ein Beispiel basierend auf einer Text-Mining-Anwendung: Die Eingabematrix (die zu faktorisierende Matrix) soll V mit 10000 Zeilen und 500 Spalten sein, in denen Wörter in Zeilen und Dokumenten in Spalten stehen. Das heißt, wir haben 500 Dokumente mit 10000 Wörtern indexiert. Es folgt, dass ein Spaltenvektor v in V ein Dokument darstellt. Nehmen wir an, dass der Algorithmus 10 Funktionen findet, um eine Features Matrix W mit 10000 Zeilen und 10 Spalten und eine Koeffizientenmatrix H mit 10 Zeilen und 500 Spalten zu erzeugen. Das Produkt von W und H ist eine Matrix mit 10000 Zeilen und 500 Spalten, die gleiche Form wie die Eingangsmatrix V und, wenn die Factorisierung funktioniert, ist es eine vernünftige Annäherung an die Eingangsmatrix V.Aus der Behandlung der Matrix-Multiplikation oben folgt, dass jede Spalte in der Produktmatrix WH eine lineare Kombination der 10 Spaltenvektoren in den Merkmalen Matrix W mit Koeffizienten, die von der Matrix H bereitgestellt werden, ist.Diese letztes Dokument ist die Basis von NMF gebaut. NMF erzeugt diese Funktionen. Es ist nützlich, an jede Funktion (Spaltenvektor) in den Merkmalen Matrix W als Dokumentbogentyp mit einem Satz von Wörtern zu denken, in denen der Zellenwert jedes Wort den Rang des Wortes in dem Merkmal definiert: Je höher der Zellenwert eines Wortes ist, desto höher ist der Rang des Wortes im Feature. Eine Spalte in der Koeffizientenmatrix H stellt ein Originaldokument mit einem Zellwert dar, der den Rang des Dokuments für ein Feature definiert. Wir können nun ein Dokument (Spaltenvektor) aus unserer Eingabematrix durch eine lineare Kombination unserer Funktionen (Spaltenvektoren in W) rekonstruieren, wobei jede Funktion durch den Zellenwert der Funktion aus der Spalte des Dokuments in H gewichtet wird. Clustering-Eigenschaft NMF hat eine inhärente Clustering-Eigenschaft, d.h. es bündelt automatisch die Spalten der Eingabedaten V = (v 1 , ..., v n ) {\displaystyle \mathbf {V} (=v_{1},\dots ,v_{n}) .Genauer gesagt, die Näherung von V {\displaystyle \mathbf {V} von V ≠ W H {\displaystyle \mathbf {V} \simeq \mathbf {W} \mathbf {H} } wird durch das Finden von W\displaystyle W} und H {\displaystyle H} erreicht, die die Fehlerfunktion minimieren V - W H W\geq 0, H\geq 0}. Legen wir darüber hinaus eine Orthogonalitätsbeschränkung auf H {\displaystyle \mathbf {H} , dh H T = I {\displaystyle \mathbf {H} \mathbf {H} {^T}=I , so ist die obige Minimierung mathematisch der Minimierung von K-Means-Clustern entspricht. Darüber hinaus gibt die berechnete H {\displaystyle H} die Cluster-Mitgliedschaft, d.h. wenn H k j > H i {\displaystyle \mathbf {H} {_kj}>\mathbf {H} {_ij} für alle i ≠ k, dies deutet darauf hin, dass die Eingangsdaten v j {\displaystyle v_{j} zu k {\displaystyle k} -th Cluster gehören. Die berechnete W {\displaystyle W} gibt den Cluster-Centroids, d.h. die k {\displaystyle k} -th Spalte gibt den Cluster-Centroid von k {\displaystyle k} -th Cluster. Die Darstellung dieser Schwerpunkte kann durch konvexes NMF deutlich verbessert werden. Wenn die Orthogonalität H H H T = I {\displaystyle \mathbf {H} \mathbf {H} {^T}=I nicht explizit auferlegt wird, hält die Orthogonalität weitgehend, und die Clustering Eigenschaft hält auch. Clustering ist das Hauptziel der meisten Data Mining-Anwendungen von NMF. Wenn die zu verwendende Fehlerfunktion Kullback–Leibler Divergenz ist, ist NMF identisch mit der probabilistischen latenten semantischen Analyse, einer beliebten Dokument-Clustering-Methode. Arten Ungefähre nicht-negative Matrix Factorisierung Üblicherweise werden die Anzahl der Spalten von W und die Anzahl der Zeilen von H in NMF so gewählt, dass das Produkt WH eine Annäherung an V wird. Die vollständige Zersetzung von V beträgt dann die beiden nicht-negativen Matrizen W und H sowie einen Rest U, so dass:V = WH + U. Die Elemente der Restmatrix können entweder negativ oder positiv sein. Wenn W und H kleiner als V sind, werden sie leichter zu speichern und zu manipulieren. Ein weiterer Grund zur Factorisierung V in kleinere Matrizen W und H, ist, dass, wenn man die Elemente von V durch deutlich weniger Daten annähernd darstellen kann, man eine latente Struktur in den Daten einfügen muss. Konvex nichtnegative Matrix Factorisierung In Standard NMF kann der Matrixfaktor W ∈ R + m × k, d.h. W alles in diesem Raum sein. Convex NMF beschränkt die Spalten von W auf konvexe Kombinationen der Eingangsdatenvektoren (v 1 , ..., v n ) {\displaystyle (v_{1},\dots ,v_{n}) .Dies verbessert die Qualität der Datendarstellung von W. Weiterhin wird der resultierende Matrixfaktor H sparsamer und orthogonaler. Nichtnegative Rangfaktorisierung Falls der nichtnegative Rang V gleich seinem tatsächlichen Rang ist, wird V = WH als nichtnegative Rang Factorisierung (NRF) bezeichnet.Das Problem, die NRF von V zu finden, ist bekanntermaßen NP-hart. Verschiedene Kostenfunktionen und Regularisierungen Es gibt verschiedene Arten von nicht-negativen Matrixfaktoren. Die unterschiedlichen Typen ergeben sich aus der Verwendung unterschiedlicher Kostenfunktionen zur Messung der Divergenz zwischen V und WH und gegebenenfalls durch Regularisierung der W- und/oder H-Matrizen. Zwei einfache Divergenzfunktionen, die von Lee und Seung untersucht werden, sind der quadratische Fehler (oder Frobenius-Norm) und eine Erweiterung der Kullback-Leibler Divergenz zu positiven Matrizen (die ursprüngliche Kullback-Leibler-Divergenz wird auf Wahrscheinlichkeitsverteilungen definiert). Jede Divergenz führt zu einem anderen NMF-Algorithmus, der die Divergenz mit iterativen Update-Regeln in der Regel minimiert. Das Faktorisierungsproblem in der quadratischen Fehlerversion von NMF kann angegeben werden als: Bei einer Matrix V {\displaystyle \mathbf {V} finden nichtnegative Matrizen W und H, die die Funktion F (W , H ) minimieren = V - W H , F 2 {\displaystyle F(\mathbf {W},\mathbf {H} =)left\|\\\mathbf {V} -\mathbf {WH} rechts\){F}{2} Eine andere Art von NMF für Bilder basiert auf der Gesamtvariationsnorm. Wenn dem NMF mit der mittleren quadratischen Fehlerkostenfunktion die L1-Regalisation (einsteigen in Lasso) hinzugefügt wird, kann das resultierende Problem aufgrund der Ähnlichkeit mit dem spärlichen Codierungsproblem als nicht-negative Sparse Codierung bezeichnet werden, obwohl es auch noch als NMF bezeichnet werden kann. Online NMFMany Standard NMF Algorithmen analysieren alle Daten zusammen, d.h. die ganze Matrix ist von Anfang an verfügbar. Dies kann bei Anwendungen, bei denen zu viele Daten in den Speicher passen oder bei denen die Daten Streaming bereitgestellt werden, unbefriedigend sein. Eine solche Anwendung ist für die kollaborative Filterung in Empfehlungssystemen, wo es viele Benutzer und viele Elemente zu empfehlen, und es wäre ineffizient, alles neu zu berechnen, wenn ein Benutzer oder ein Element zum System hinzugefügt wird. Die Kostenfunktion zur Optimierung in diesen Fällen kann oder darf nicht die gleiche sein wie bei Standard NMF, aber die Algorithmen müssen ziemlich unterschiedlich sein. Algorithmen Es gibt verschiedene Möglichkeiten, wie die W und H gefunden werden können: Lee und Seung war aufgrund der Einfachheit der Implementierung eine beliebte Methode. Dieser Algorithmus ist: initialize: W und H nicht negativ. Dann aktualisieren Sie die Werte in W und H, indem Sie die folgenden, mit n {\displaystyle n} als Index der Iteration. H [ i , j , n + 1 ← H [ i , j ] n ( W n ) T V ) [i , j ] (W n ) T W n n H n ) [ i , j ] [i][_i,j] [i,j] und W [i, j ] n + 1 ← W [i, j ] n (V (H n + 1 ) T ) [ i , j ] ( W n n + 1 (H n + 1 ) T ) [_i,j]}{n+1}\leftarrow \mathbf {W} [_i,j]}{n}{\frac {\mathbf {V} (\mathbf {H} n+1}}}{[i,j]}{(\mathbf} {\\\cH}\cH}\cH}\cH}\cH}\cH}\cH}\cH}\cH}\cH\cH}\c\cH}\cH}\cH\cH}\cH\cH}\cH}\cH}\cH\cH\cH\cH\cH\cH\cH\cH}\cH\cH\cH\cH\cH\cH}\cH\cH\cH\cH\cH\cH\cH\cH\cH\cH\cH\cH}\cH\cH\cH\cH\cH} (H) Bis W und H stabil sind. Beachten Sie, dass die Updates auf einem Element nach Element-Basis durchgeführt werden nicht Matrix-Multiplikation. Wir weisen darauf hin, dass die multiplikativen Faktoren für W und H, d.h. die W T V W T W H {\textstyle {\frac {\mathbf {W} {^\mathsf {T}\mathbf} {V} {\mathbf\W} {\mathf}\c}\mathbfstyle {W} {V} \mathbf {H} {^\mathsf T}}{\mathbf {W} \mathbf {H} \mathbf {H} {^\mathsf {T} Begriffe, sind Matrizen von einem, wenn V = W H {\displaystyle \mathbf {V} =\mathbf {W} \mathbf}f {f} Einige Ansätze basieren auf alternierenden, nicht-negativen, wenigsten Quadraten: in jedem Schritt eines solchen Algorithmus wird zuerst H fixiert und W gefunden durch einen nicht-negativen, wenigsten Quadrate-Löser, dann W fixiert und H wird analog gefunden.Die für W und H verwendeten Verfahren können gleich oder verschieden sein, da einige NMF-Varianten eine von W und H regulären. Spezifische Ansätze umfassen die projizierten Gradientenabstiegsmethoden, das aktive Set-Verfahren, das optimale Gradienten-Verfahren und das Blockhauptschwenkverfahren unter anderem. Aktuelle Algorithmen sind suboptimal, indem sie nur garantieren, ein lokales Minimum zu finden, anstatt ein globales Minimum der Kostenfunktion. Ein nachweislich optimaler Algorithmus ist in naher Zukunft unwahrscheinlich, da das Problem gezeigt wurde, das k-Means Clustering Problem zu verallgemeinern, das bekanntermaßen NP-komplete ist. Wie bei vielen anderen Datenbergbauanwendungen kann sich jedoch ein lokales Minimum als nützlich erweisen. Sequency NMF Der sequentielle Aufbau von NMF-Komponenten (W und H) wurde zunächst verwendet, um NMF mit der Hauptkomponentenanalyse (PCA) in der Astronomie zu beziehen. Der Beitrag der PCA-Komponenten wird durch die Größe ihrer entsprechenden Eigenwerte geordnet; für NMF können ihre Komponenten empirisch geordnet werden, wenn sie nach einem (Sequentiell) aufgebaut sind, d.h. die (n + 1 ) {\displaystyle (n+1}) -th-Komponente mit den ersten n {\displaystyle n}-Komponenten aufgebaut werden. Der Beitrag der sequentiellen NMF-Komponenten kann mit dem Karhunen-Loève Theorem, einer Anwendung von PCA, unter Verwendung des Eigenwerts verglichen werden. Eine typische Wahl der Anzahl der Komponenten mit PCA basiert auf dem Ellbogenpunkt, dann zeigt das Vorhandensein des flachen Plateau, dass PCA die Daten nicht effizient erfasst, und schließlich gibt es einen plötzlichen Tropfen, der die Erfassung von zufälligem Rauschen widerspiegelt und in das Regime des Überholens fällt. Bei sequentiellem NMF wird die Eigenwert-Plotte durch die Parzelle der fraktionierten Restvarianzkurven angenähert, wobei die Kurven kontinuierlich abnehmen und auf einen höheren Pegel als PCA konvergieren, was die Indikation einer weniger übergeordneten NMF ist. Exaktes NMF Genaue Lösungen für die Varianten von NMF können erwartet werden (in der Polynomzeit), wenn zusätzliche Zwänge für die Matrix V bestehen. Ein Polynomzeitalgorithmus zur Lösung nichtnegativer Ranking-Faktorisierung, wenn V eine monomiale Sub-Matrix des Ranks gleich seiner Range enthält, wurde 1981 von Campbell und Poole gegeben. Kalofolias und Gallopoulos (2012) lösten das symmetrische Gegenstück dieses Problems, wobei V symmetrisch ist und eine diagonale Hauptsubmatrix des Ranges r enthält.Ihr Algorithmus läuft in O(rm2) Zeit im dichten Fall. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) geben einen Polynomzeitalgorithmus für genau NMF, der für den Fall funktioniert, dass einer der Faktoren W eine Trennbarkeitsbedingung erfüllt. Beziehung zu anderen Techniken Beim Lernen der Teile von Objekten durch nicht-negative Matrix-Faktorisierung schlugen Lee und Seung NMF vor allem für teilebasierte Zersetzung von Bildern. Es vergleicht NMF mit Vektorquantisierung und Hauptkomponentenanalyse und zeigt, dass die drei Techniken zwar als Faktorisierungen geschrieben werden können, aber unterschiedliche Zwänge implementieren und daher unterschiedliche Ergebnisse erzielen. Es wurde später gezeigt, dass einige Arten von NMF eine Instanz eines allgemeineren probabilistischen Modells namens "multinomial PCA". Wenn NMF durch Minimierung der Kullback-Leibler Divergenz erhalten wird, ist es in der Tat gleichwertig mit einer anderen Instanz von multinomial PCA, probabilistische latente semantische Analyse, trainiert durch maximale Wahrscheinlichkeitsschätzung. Diese Methode wird häufig zur Analyse und Clusterung von Textdaten verwendet und ist auch mit dem latenten Klassenmodell verbunden. NMF mit dem kleinsten Quadratobjektiv entspricht einer entspannten Form der K-Means-Clusterung: Der Matrixfaktor W enthält Clusterschwerpunkte und H enthält Cluster-Mitgliedsindikatoren. Dies bietet eine theoretische Grundlage für die Verwendung von NMF zur Daten-Clusterung. k-Means erzwingt jedoch nicht die Nicht-Negativität auf ihre Schwerpunkte, so dass die nächste Analogie tatsächlich mit semi-NMF". NMF kann als zweischichtig gerichtetes graphisches Modell mit einer Schicht von beobachteten Zufallsvariablen und einer Schicht von versteckten Zufallsvariablen betrachtet werden. NMF erstreckt sich über Matrizen bis zu Zehner beliebiger Ordnung. Diese Erweiterung kann als nicht-negatives Gegenstück zum Beispiel dem PARAFAC-Modell angesehen werden. Andere Erweiterungen von NMF umfassen die gemeinsame Faktorisierung von mehreren Datenmatrizen und Tensors, wo einige Faktoren geteilt werden. Solche Modelle sind nützlich für Sensorfusion und relationales Lernen. NMF ist eine Instanz der nichtnegativen quadratischen Programmierung (NQP), wie die Stützvektormaschine (SVM.)SVM und NMF sind jedoch auf einem intimeren Niveau als NQP verwandt, was eine direkte Anwendung der für beide Methoden entwickelten Lösungsalgorithmen zu Problemen in beiden Domänen ermöglicht. Einzigartigkeit Die Factorisierung ist nicht einzigartig: Mit einer Matrix und deren Inverse können die beiden Factorisierungsmatrizen um z.B. W H = W B - transformiert werden. {\displaystyle \mathbf {WH} =\mathbf WBB}{-1}\mathbf {H} Wenn die beiden neuen Matrizen W ~ = W B {\displaystyle \mathbf {\tilde {W}=WB } und H ~ = B − 1 H {\displaystyle \mathbf {\tilde {H} =\mathbf {B} {-^1}\mathbf {H} sind nichtnegativ, sie bilden eine weitere Parametrisierung der Factorisierung. Die Nicht-Negativität von W ~ {\displaystyle \mathbf {\tilde {W} und H ~ {\displaystyle \mathbf {\tilde {H}}} gilt zumindest, wenn B eine nicht-negative monomiale Matrix ist. In diesem einfachen Fall wird es nur einer Skalierung und einer Permutation entsprechen. Eine stärkere Kontrolle über die Nichteinstimmung von NMF wird mit Sparzwangen erreicht. Anwendungen Astronomie In der Astronomie ist NMF eine vielversprechende Methode zur Dimensionsreduktion in dem Sinne, dass astrophysische Signale nicht-negativ sind. NMF wurde auf die spektroskopischen Beobachtungen und die direkten bildgebenden Beobachtungen angewendet, um die gemeinsamen Eigenschaften von astronomischen Objekten zu untersuchen und die astronomischen Beobachtungen nachzuverarbeiten. Die Fortschritte in den spektroskopischen Beobachtungen von Blanton & Roweis (2007) berücksichtigen die Unsicherheiten der astronomischen Beobachtungen, die später durch Zhu (2016) verbessert werden, wo auch fehlende Daten berücksichtigt werden und ein paralleles Computing ermöglicht wird. Ihr Verfahren wird dann von Ren et al.(2018) als eine der Methoden zur Detektion von Exoplaneten, insbesondere zur direkten Abbildung von umstellaren Platten, auf das direkte Bildgebungsfeld übernommen. Ren et al.(2018) sind in der Lage, die Stabilität von NMF-Komponenten zu beweisen, wenn sie sequentiell (d.h. nach einem) aufgebaut sind, was die Linearität des NMF-Modellierungsprozesses ermöglicht; die Linearitätseigenschaft wird verwendet, um das stellare Licht und das von den Exoplaneten und umstellaren Scheiben gestreute Licht zu trennen. In der direkten Abbildung, um die schwachen Exoplaneten und umstellaren Scheiben von hell die umgebenden stellaren Lichter, die einen typischen Kontrast von 105 bis 1010 hat, sind verschiedene statistische Methoden angenommen worden, aber das Licht der Exoplaneten oder umstellaren Scheiben sind in der Regel über-fitted, wo die Vorwärtsmodellierung angenommen werden muss, um den wahren Fluss wiederherzustellen. Vorwärtsmodellierung ist derzeit für Punktquellen optimiert, jedoch nicht für erweiterte Quellen, insbesondere für unregelmäßig geformte Strukturen, wie etwa umstellare Scheiben. In dieser Situation war NMF ein ausgezeichnetes Verfahren, das weniger im Sinne der Nicht-Negativität und Sparsität der NMF-Modellierungskoeffizienten übertrifft, so dass die Vorwärtsmodellierung mit einigen Skalierungsfaktoren statt einer rechnerisch intensiven Datenreduktion auf generierten Modellen durchgeführt werden kann. Dateneingabe Um fehlende Daten in der Statistik zu erfassen, kann NMF fehlende Daten bei der Minimierung seiner Kostenfunktion annehmen, anstatt diese fehlenden Daten als Nullen zu behandeln. Dies macht es zu einem mathematisch bewährten Verfahren zur Datenberechnung in der Statistik. Indem man zunächst beweist, dass die fehlenden Daten in der Kostenfunktion ignoriert werden, beweist man, dass die Auswirkungen von fehlenden Daten so gering wie ein zweiter Ordnungseffekt sein können, untersuchte und angewandte Ren et al.(2020) einen solchen Ansatz für das Gebiet der Astronomie. Ihre Arbeit konzentriert sich auf zweidimensionale Matrizen, speziell, es umfasst mathematische Ableitung, simulierte Dateneingabe und Anwendung auf On-Sky-Daten.Das Dateneingabeverfahren mit NMF kann aus zwei Schritten zusammengesetzt sein. Als erstes, wenn die NMF-Komponenten bekannt sind, hat Ren et al.(2020) bewiesen, dass die Auswirkungen von fehlenden Daten während der Dateneingabe ("Zielmodellierung" in ihrer Studie) ein zweiter Ordnungseffekt ist. Zweitens, wenn die NMF-Komponenten unbekannt sind, haben die Autoren bewiesen, dass die Auswirkungen von fehlenden Daten bei der Bauteilkonstruktion ein erster Ordnungseffekt ist. Je nachdem, wie die NMF-Komponenten erhalten werden, kann der frühere Schritt oben entweder unabhängig oder von letzteren abhängig sein. Darüber hinaus kann die Imputationsqualität bei Verwendung von mehr NMF-Komponenten erhöht werden, siehe Abbildung 4 von Ren et al.(2020) für ihre Darstellung. Textbergbau NMF kann für Textbergbauanwendungen verwendet werden. Dabei wird eine Dokument-Zeitmatrix mit den Gewichten verschiedener Begriffe (typischerweise gewichtete Wortfrequenzinformationen) aus einem Satz von Dokumenten aufgebaut. Diese Matrix wird in eine Term-Feature und eine Merkmals-Dokumentationsmatrix faktorisiert. Die Merkmale werden aus dem Inhalt der Dokumente abgeleitet, und die Feature-Dokument-Matrix beschreibt Datencluster von verwandten Dokumenten. Eine spezielle Anwendung verwendet hierarchische NMF auf einer kleinen Teilmenge von wissenschaftlichen Abstracts von PubMed. Eine weitere Forschungsgruppe hat Teile des Enron-E-Mail-Datensatzes mit 65.033 Nachrichten und 91.133 Termen in 50 Cluster zusammengefasst. NMF wurde auch auf Zitate-Daten angewendet, mit einem Beispiel Clustering Englisch Wikipedia-Artikel und wissenschaftliche Zeitschriften auf der Grundlage der ausgelagerten wissenschaftlichen Zitate in englischer Wikipedia. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) haben Polynom-Zeit-Algorithmen gegeben, um Themenmodelle mit NMF zu lernen. Der Algorithmus geht davon aus, dass die Themamatrix eine Trennbarkeitsbedingung erfüllt, die in diesen Einstellungen oft zu halten ist. Hassani, Iranmanesh und Mansouri (2019) schlugen eine Merkmals-Agglomerationsmethode für Begriff-Dokument-Matrizen vor, die mit NMF arbeitet. Der Algorithmus reduziert die Term-Dokumentationsmatrix in eine kleinere Matrix, die für die Text-Clusterung besser geeignet ist. Die Spektraldatenanalyse NMF dient auch zur Analyse von Spektraldaten; eine solche Verwendung ist die Klassifizierung von Raumobjekten und Trümmern. Skalierbare Internet-Distanzvorhersage NMF wird in skalierbarem Internet-Distanz (Rundzeit) Vorhersage angewendet. Für ein Netzwerk mit N {\displaystyle N}-Hosts können mit Hilfe von NMF die Abstände aller N 2 {\displaystyle N^{2}-End-Links nach nur O (N ) {\displaystyle O(N})-Messungen vorhergesagt werden. Diese Art von Methode wurde zunächst im Internet-Distanzschätzung Service (IDES) eingeführt. Anschließend wird als voll dezentraler Ansatz das Phoenix-Netzwerkkoordinatensystem vorgeschlagen. Durch die Einführung des Gewichtskonzepts erreicht es eine bessere Gesamtprädiktionsgenauigkeit. Nicht-stationäres Sprechen Denoising Speech Denoising war ein lang anhaltendes Problem in der Audiosignalverarbeitung. Es gibt viele Algorithmen zum Entlüften, wenn das Rauschen stationär ist. Beispielsweise eignet sich der Wiener Filter für additive Gausssche Geräusche. Ist das Rauschen jedoch nicht stationär, haben die klassischen Denoising-Algorithmen in der Regel eine schlechte Leistung, da die statistischen Informationen des unstationären Rauschens schwierig zu schätzen sind. Schmidt et al.use NMF zu sprechen, die unter unstationärem Rauschen geleugnet, was sich völlig von klassischen statistischen Ansätzen unterscheidet. Die Schlüsselidee ist, dass sauberes Sprachsignal sparsam durch ein Sprachwörterbuch repräsentiert werden kann, aber nicht-stationäres Rauschen kann nicht. Ebenso kann unstationäres Rauschen auch sparsam durch ein Rauschwörterbuch repräsentiert werden, aber Sprache kann nicht. Der Algorithmus für NMF-Verleugnung geht wie folgt. Zwei Wörterbücher, eine für Sprache und eine für Lärm, müssen offline trainiert werden. Sobald eine laute Rede vorliegt, berechnen wir zunächst die Größe der Short-Time-Fourier-Transform. Zweitens, in zwei Teile über NMF zu trennen, kann man sparsam durch das Sprachwörterbuch repräsentiert werden, und der andere Teil kann sparsam durch das Rauschwörterbuch dargestellt werden. Drittens wird der Teil, der durch das Sprachwörterbuch repräsentiert wird, die geschätzte saubere Rede sein. Population Genetics Sparse NMF wird in Populationsgenetik verwendet, um einzelne Beimischungskoeffizienten zu schätzen, genetische Cluster von Individuen in einer Populationsprobe zu erkennen oder genetische Beimischung in genomen Proben zu bewerten. In der humanen genetischen Clustering liefern NMF-Algorithmen ähnliche Schätzungen wie die des Computerprogramms STRUCTURE, aber die Algorithmen sind effizienter rechnerisch und ermöglichen die Analyse von großen Populationen genomischer Datensätze.Bioinformatics NMF wurde erfolgreich in der Bioinformatik zur Clusterung von Genexpressions- und DNA-Methylierungsdaten angewendet und die Gene, die am repräsentativsten für die Cluster sind, gefunden. Bei der Analyse von Krebsmutationen wurde verwendet, um gemeinsame Muster von Mutationen zu identifizieren, die bei vielen Krebserkrankungen auftreten und die wahrscheinlich verschiedene Ursachen haben. NMF-Techniken können Variationsquellen wie Zelltypen, Krankheitssubtypen, Bevölkerungsschichtung, Gewebezusammensetzung und Tumorklonalität identifizieren. Kernbildende NMF, auch in diesem Bereich als Faktoranalyse bezeichnet, wurde seit den 1980er Jahren verwendet, um Sequenzen von Bildern in SPECT und PET dynamische medizinische Bildgebung zu analysieren. Die Nichteinigkeit des NMF wurde mit Sparzwängen behandelt. Aktuelle Forschung Aktuelle Forschung (seit 2010) in der nichtnegativen Matrix Factorization umfasst, aber nicht beschränkt auf, Algorithmik: Suche nach globalen Minima der Faktoren und Faktor Initialisierung. Skalierbarkeit: wie man Million-by-billion-Matrizen, die in Web-Skala-Daten Bergbau häufig sind, z.B. siehe Distributed Nonnegative Matrix Factorization (DNMF,) Scalable Nonnegative Matrix Factorization (ScalableNMF,) Distributed Stochastic Singular Value Decomposition. Online: Wie man die Factorisierung aktualisiert, wenn neue Daten eintreffen, ohne von Grund auf zu recomputieren, z.B. Online CNSC Collective (Joint) Factorization: Factorizing multiple inter related matrices for Multiple-View-Learning, z.B. Multi-View-Clustering, siehe CoNMF und MultiNMF Cohen und Rothblum 1993 Problem: ob eine rationale Matrix immer einen NMF von minimaler innerer Dimension hat, deren Faktoren, der auch rational sind. In letzter Zeit wurde dieses Problem negativ beantwortet. Siehe auch Multilinear Algebra Multilineares Subraum-Erlernen Tensor Tensor Zersetzung Tensor Software Quellen und externe Links Anmerkungen = Andere ==