Density-basierte räumliche Clustering von Anwendungen mit Rauschen (DBSCAN) ist ein Daten-Cluster-Algorithmus, der von Martin Ester, Hans-Peter Kriegel, Jörg Sander und Xiaowei Xu 1996 vorgeschlagen wurde. Es ist eine Dichte-basierte Clustering nicht-parametrischen Algorithmus: Bei einer Reihe von Punkten in einigen Raum, es Gruppen zusammen Punkte, die eng zusammengepackt (Punkte mit vielen nahe gelegenen Nachbarn,) Markierung als Ausreißer Punkte, die allein in Low-Density-Regionen liegen (die nächstgelegenen Nachbarn sind zu weit entfernt). DBSCAN ist einer der häufigsten Clustering Algorithmen und auch am meisten in der wissenschaftlichen Literatur zitiert. Im Jahr 2014 wurde der Algorithmus auf der führenden Data Mining-Konferenz, ACM SIGKDD, den Test des Zeitpreises (eine Auszeichnung an Algorithmen, die in Theorie und Praxis beachtliche Aufmerksamkeit erhalten haben) verliehen. Ab Juli 2020 erscheint das Nachfolgepapier "DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN" in der Liste der 8 am häufigsten heruntergeladenen Artikel der renommierten ACM Transactions on Database Systems (TODS) Zeitschrift. Geschichte 1972 veröffentlichte Robert F. Ling einen eng verwandten Algorithmus in "Theory and Construction of k-Clusters" im Computer Journal mit einer geschätzten Laufzeitkomplexität von O(n3). DBSCAN hat einen schlimmsten Fall von O(n2,) und die datenbankorientierte Range-Quecky-Formulierung von DBSCAN ermöglicht eine Indexbeschleunigung. Die Algorithmen unterscheiden sich in ihrem Umgang mit Grenzpunkten leicht. Vorläufige Betrachten Sie eine Reihe von Punkten in einem Raum, um gebündelt werden. Lassen Sie ε ein Parameter sein, der den Radius einer Nachbarschaft zu einem bestimmten Punkt angibt. Für die Zwecke der DBSCAN-Clusterung werden die Punkte als Kernpunkte, (Density-) erreichbare Punkte und Ausreißer, wie folgt klassifiziert: Ein Punkt p ist ein Kernpunkt, wenn mindestens minPts Punkte im Abstand ε davon (einschließlich p) liegen. Von p ist ein Punkt q unmittelbar erreichbar, wenn der Punkt q im Abstand ε vom Kernpunkt p liegt. Punkte sollen nur von Kernpunkten direkt erreichbar sein. Ein Punkt q ist von p erreichbar, wenn es einen Pfad p1, ..., pn mit p1 = p und pn = q gibt, wobei jeder Pi+1 direkt von pi erreichbar ist. Beachten Sie, dass dies impliziert, dass der Anfangspunkt und alle Punkte auf dem Pfad Kernpunkte sein müssen, mit der möglichen Ausnahme von q. Alle von einem anderen Punkt nicht erreichbaren Punkte sind Ausreißer oder Geräuschpunkte. Wenn p ein Kernpunkt ist, dann bildet es zusammen mit allen Punkten (Kern oder Nicht-Kern) einen Cluster, der von ihm erreichbar ist. Jeder Cluster enthält mindestens einen Kernpunkt; Nicht-Kernpunkte können Teil eines Clusters sein, aber sie bilden seinen Rand, da sie nicht verwendet werden können, um mehr Punkte zu erreichen. Erreichbarkeit ist keine symmetrische Beziehung: definitionsgemäß können nur Kernpunkte nicht Kernpunkte erreichen. Das Gegenteil ist nicht wahr, so dass ein Nicht-Kernpunkt erreichbar sein kann, aber nichts davon kann erreicht werden. Es ist daher ein weiterer Begriff der Verknüpfung erforderlich, um das Ausmaß der von DBSCAN gefundenen Cluster formal zu definieren. Zwei Punkte p und q sind dichteverbunden, wenn ein Punkt o so liegt, daß sowohl p als auch q von o erreichbar sind. Dichte-Verbindung ist symmetrisch. Ein Cluster erfüllt dann zwei Eigenschaften: Alle Punkte innerhalb des Clusters sind miteinander dicht verbunden. Wenn ein Punkt von einem bestimmten Punkt des Clusters abdichtet erreichbar ist, ist er auch Teil des Clusters. Algorithm Original-Abfrage-basierter Algorithmus DBSCAN benötigt zwei Parameter: ε (eps) und die minimale Anzahl von Punkten, die zur Bildung einer dichten Region (minPts) erforderlich sind. Es beginnt mit einem beliebigen Ausgangspunkt, der nicht besucht wurde. Dieser Punkt ist ε-Nachbarschaft abgerufen, und wenn er genügend viele Punkte enthält, wird ein Cluster gestartet. Ansonsten wird der Punkt als Rausch bezeichnet. Beachten Sie, dass dieser Punkt später in einer ausreichend großen ε-Umgebung eines anderen Punktes zu finden ist und daher Teil eines Clusters sein könnte. Wird ein Punkt als dichter Teil eines Clusters gefunden, so ist seine ε-Nachbarschaft auch Teil dieses Clusters. Daher werden alle Punkte, die innerhalb der ε-Nachbarschaft gefunden werden, hinzugefügt, wie ihre eigene ε-Nachbarschaft, wenn sie auch dicht sind. Dieser Vorgang wird solange fortgesetzt, bis das dicht verbundene Cluster vollständig gefunden ist. Dann wird ein neuer unbesuchter Punkt abgerufen und verarbeitet, was zur Entdeckung eines weiteren Clusters oder Rauschens führt. DBSCAN benötigt auch eine Distanzfunktion (sowie Ähnlichkeitsfunktionen oder andere Prädikate). Die Abstandsfunktion (dist) kann daher als zusätzlicher Parameter betrachtet werden. Der Algorithmus kann in Pseudocode wie folgt ausgedrückt werden:DBSCAN(DB, distFunc, eps, minPts) 0 /* Cluster-Zähler */ für jeden Punkt P in der Datenbank DB { if label(P) ≠ undefiniert dann fortfahren /* Vorher in der inneren Schleife */Nachbarn N =:RangeQuery(DB, distFunc, P, eps) /* Finden Sie Nachbarn */ wenn N <minPts dann { /* Dichteprüfung */ Label(P) := Noise /* Label as Noise */ Continue } C:= C + 1 /* nächster Clusterlabel */label(P) := C /* Label initial point */ SeedSet S := N \ {P} /* Nachbarn, um */ für jeden Punkt Q in S { /* Verarbeiten Sie jeden Saatpunkt Q */ wenn Label(Q) = Lärm dann Etikett(Q) := C /* Ändern Sie Noise an den Grenzpunkt */ wenn Label(Q)≠ undefiniert dann fortfahren /* Vorher bearbeitet (z.B. Grenzpunkt) */ Label(Q):= C /* Label Nachbar */Nachbarn N := RangeQuery(DB, distFunc, Q, eps) /* Finden Sie Nachbarn */ wenn N ≥min Pts dann { /* Dichteprüfung (wenn Q ein Kernpunkt/* ist) S := S ∪ N /* Fügen Sie neue Nachbarn zum Saatgut-Set */ } } hinzu, wo RangeQuery mit einem Datenbank-Index für bessere Leistung implementiert werden kann, oder mit einem langsamen linearen Scan: RangeQuery(DB, distFunc, Q, eps) { Nachbarn N := leere Liste für jeden Punkt P in der Datenbank DB { /* Scannen Sie alle Punkte in der Datenbank */ wenn distFunc(Q, P) ≤ eps dann { /* Entfernung berechnen und epsilon überprüfen */ N := NICHT {P} /*Add zum Ergebnis */ } } zurück N } Zusammenfassung Algorithmus Der DBSCAN-Algorithmus kann in die folgenden Schritte abstrahiert werden: Finden Sie die Punkte in der ε (eps) Nachbarschaft jeden Punkt, und identifizieren Sie die Kernpunkte mit mehr als minPts Nachbarn. Finden Sie die angeschlossenen Komponenten von Kernpunkten auf dem Nachbardiagramm, ignorieren Sie alle nicht-Kernpunkte. Jeden Nicht-Kernpunkt einem nahe gelegenen Cluster zuordnen, wenn der Cluster ein ε (eps) Nachbar ist, sonst dem Rauschen zuordnen. Eine naive Umsetzung hierzu erfordert die Speicherung der Nachbarschaften in Schritt 1, wodurch ein wesentlicher Speicher erforderlich ist. Der ursprüngliche DBSCAN-Algorithmus erfordert dies nicht, indem er diese Schritte für einen Zeitpunkt durchführt. Komplexität DBSCAN besucht jeden Punkt der Datenbank, möglicherweise mehrmals (z.B. als Kandidaten für verschiedene Cluster). Für praktische Überlegungen richtet sich die Zeitkomplexität jedoch meist nach der Anzahl der RegionQuery invocations. DBSCAN führt für jeden Punkt genau eine solche Abfrage aus, und wenn eine Indexierungsstruktur verwendet wird, die eine Nachbarschaftsabfrage in O(log n) ausführt, wird eine Gesamtmittellaufzeitkomplexität von O(n log n) erhalten (wenn der Parameter ε sinnvoll gewählt wird, d.h. dass im Durchschnitt nur O(log n) Punkte zurückgegeben werden). Ohne die Verwendung einer beschleunigenden Indexstruktur oder auf degenerierten Daten (z.B. alle Punkte innerhalb einer Distanz kleiner als ε) bleibt die schlimmste Laufzeitkomplexität O(n2). Die Abstandsmatrix der Größe (n2-n)/2 kann zur Vermeidung von Abstandsrekomputationen materialisiert werden, dieser benötigt jedoch O(n2)-Speicher, während eine nicht-matrixbasierte Implementierung von DBSCAN nur O(n)-Speicher benötigt. Vorteile DBSCAN erfordert nicht, dass man die Anzahl der Cluster in den Daten a priori, im Gegensatz zu k-Means. DBSCAN kann beliebig geformte Cluster finden. Es kann sogar einen Cluster finden, der vollständig von (aber nicht mit) einem anderen Cluster umgeben ist. Durch den MinPts-Parameter wird der sogenannte Single-Link-Effekt (verschiedene Cluster, die durch eine dünne Punktlinie verbunden sind) reduziert. DBSCAN hat einen Begriff des Rauschens und ist robust für Ausreißer. DBSCAN benötigt nur zwei Parameter und ist meist unempfindlich gegen die Reihenfolge der Punkte in der Datenbank. (Die Punkte, die am Rande von zwei verschiedenen Clustern sitzen, können jedoch die Cluster-Mitgliedschaft wechseln, wenn die Reihenfolge der Punkte geändert wird, und die Cluster-Zuordnung ist einzigartig nur bis zu Isomorphismus.) DBSCAN ist für die Verwendung mit Datenbanken konzipiert, die Regionsabfragen beschleunigen können, z.B. mit einem R*-Baum. Die Parameter minPts und ε können durch einen Domänenexperten eingestellt werden, wenn die Daten gut verstanden werden. Nachteile DBSCAN ist nicht ganz deterministisch: Grenzpunkte, die von mehr als einem Cluster erreichbar sind, können Teil eines Clusters sein, je nach Auftrag werden die Daten verarbeitet. Für die meisten Datensätze und Domänen entsteht diese Situation nicht oft und hat wenig Einfluss auf das Clustering-Ergebnis: Sowohl auf Kernpunkte als auch auf Rauschpunkte ist DBSCAN deterministisch. DBSCAN* ist eine Variation, die Grenzpunkte als Rausch behandelt und so ein voll deterministisches Ergebnis sowie eine konsequentere statistische Interpretation von dichteverbundenen Komponenten erreicht. Die Qualität von DBSCAN hängt von der in der FunktionsregionQuery(P,ε) verwendeten Entfernungsmaßnahme ab. Die am häufigsten verwendete Distanz Metrik ist Euclidean Entfernung. Insbesondere für hochdimensionale Daten kann diese Metrik aufgrund des sogenannten "Curse of dimensionsality" nahezu nutzlos gemacht werden, wodurch es schwierig ist, einen geeigneten Wert für ε zu finden. Dieser Effekt ist jedoch auch in jedem anderen Algorithmus auf Basis Euclidean Abstand vorhanden. DBSCAN kann Datensätze mit großen Dichteunterschieden nicht gut bündeln, da die minPts-ε-Kombination dann nicht für alle Cluster geeignet gewählt werden kann. Sind Daten und Skalen nicht gut verstanden, kann die Wahl einer aussagekräftigen Distanzschwelle ε schwierig sein. Siehe unten den Abschnitt über Erweiterungen für algorithmische Modifikationen, um diese Probleme zu bewältigen. Parameterschätzung Jede Data Mining Aufgabe hat das Problem der Parameter. Jeder Parameter beeinflusst den Algorithmus auf bestimmte Weise. Für DBSCAN werden die Parameter ε und minPts benötigt. Die Parameter müssen vom Benutzer angegeben werden. Idealerweise wird der Wert von ε durch das Problem gegeben, um (z.B. einen physikalischen Abstand), und minPts ist dann die gewünschte minimale Clustergröße. MinPts: In der Regel des Daumens kann ein Minimum minPts aus der Anzahl der Abmessungen D im Datensatz abgeleitet werden, als minPts ≥ D + 1.Der niedrige Wert von minPts = 1 ergibt keinen Sinn, da dann jeder Punkt ein Kernpunkt nach Definition ist. Bei minPts ≤ 2 wird das Ergebnis gleich der hierarchischen Clusterung mit der einzelnen Gliedermetrie sein, wobei das Dendrogramm in Höhe ε geschnitten wird. Daher müssen minPts mindestens 3 gewählt werden. Allerdings sind größere Werte in der Regel besser für Datensätze mit Rauschen und werden mehr signifikante Cluster liefern. Als Faustregel können minPts = 2·dim verwendet werden, es kann jedoch erforderlich sein, größere Werte für sehr große Daten, für laute Daten oder für Daten zu wählen, die viele Duplikate enthalten.ε: Der Wert für ε kann dann mit einem k-Abstandsdiagramm gewählt werden, das den Abstand zum k = minPts-1 nächstgelegenen Nachbar von dem größten auf den kleinsten Wert aufgetragen. Gute Werte von ε sind, wo dieses Diagramm einen Ellbogen zeigt: Wenn ε viel zu klein gewählt wird, wird ein großer Teil der Daten nicht gebündelt, während für einen zu hohen Wert von ε Cluster zusammengeführt werden und die Mehrheit der Objekte im gleichen Cluster sein wird. Im allgemeinen sind kleine Werte von ε bevorzugt, und in der Regel sollte nur ein kleiner Anteil von Punkten innerhalb dieses Abstandes zueinander liegen. Alternativ kann ein OPTICS-Plot verwendet werden, um ε zu wählen, aber dann kann der OPTICS-Algorithmus selbst zum Clustern der Daten verwendet werden. Entfernungsfunktion: Die Wahl der Distanzfunktion ist eng mit der Wahl von ε gekoppelt und hat einen großen Einfluss auf die Ergebnisse. Generell muss zunächst ein vernünftiges Maß an Ähnlichkeit für den Datensatz ermittelt werden, bevor der Parameter ε gewählt werden kann. Für diesen Parameter gibt es keine Schätzung, aber die Entfernungsfunktionen müssen für den Datensatz entsprechend gewählt werden. So ist beispielsweise bei geographischen Daten der großkreisige Abstand oft eine gute Wahl. OPTICS kann als Verallgemeinerung von DBSCAN betrachtet werden, die den ε-Parameter durch einen maximalen Wert ersetzt, der die Leistung meist beeinflusst. MinPts wird dann im Wesentlichen die minimale Clustergröße zu finden. Während der Algorithmus viel einfacher zu parametrieren ist als DBSCAN, sind die Ergebnisse etwas schwieriger zu verwenden, da es in der Regel eine hierarchische Clustering anstelle der einfachen Datenpartitionierung, die DBSCAN produziert. Kürzlich hat einer der Originalautoren von DBSCAN DBSCAN und OPTICS neu besucht und eine verfeinerte Version der hierarchischen DBSCAN (HDBSCAN,*) veröffentlicht, die nicht mehr den Begriff der Grenzpunkte hat. Stattdessen bilden nur die Kernpunkte den Cluster. Beziehung zum spektralen Clustering DBSCAN kann als spezielle (effiziente) Variante der spektralen Clustering betrachtet werden: Verbundene Komponenten entsprechen optimalen spektralen Clustern (keine Kantenschnitte – spektrale Clustering versucht, die Daten mit einem minimalen Schnitt zu trennen); DBSCAN findet vernetzte Komponenten auf dem (asymmetrischen) Erreichbarkeitsdiagramm. Die spektrale Clusterung kann jedoch rechnerisch intensiv sein (bis zu O (n 3 ) {\displaystyle O(n^{3}) ohne Näherung und weitere Annahmen), und man muss die Anzahl der Cluster k {\displaystyle k} für die Anzahl der zu wählenden Eigenvektoren und die Anzahl der Cluster mit k-Meanen auf der spektralen Einbettung wählen. Aus Leistungsgründen bleibt also der ursprüngliche DBSCAN-Algorithmus einer spektralen Umsetzung bevorzugt, und diese Beziehung ist bisher nur von theoretischem Interesse. Erweiterungen Generalized DBSCAN (GDBSCAN) ist eine Verallgemeinerung durch die gleichen Autoren in willkürliche Nachbarschaft und dichte Prädikate. Die Parameter ε und minPts werden aus dem ursprünglichen Algorithmus entfernt und zu den Prädikaten bewegt. Beispielsweise könnte die Nachbarschaft auf Polygondaten jedes sich kreuzende Polygon sein, während die Dichtevorgabe die Polygonbereiche anstelle nur der Objektzahl verwendet. Es wurden verschiedene Erweiterungen zum DBSCAN-Algorithmus vorgeschlagen, einschließlich Methoden zur Parallelisierung, Parameterschätzung und Unterstützung von unsicheren Daten. Die Grundidee wurde durch den OPTICS-Algorithmus auf hierarchische Clustering erweitert. DBSCAN wird auch als Teil von Subspace Clustering Algorithmen wie PreDeCon und SUBCLU verwendet. HDBSCAN ist eine hierarchische Version von DBSCAN, die auch schneller als OPTICS ist, aus der eine flache Partition aus den prominentesten Clustern aus der Hierarchie extrahiert werden kann. Verfügbarkeit Verschiedene Implementierungen desselben Algorithmus wurden gefunden, um enorme Leistungsunterschiede zu zeigen, mit dem schnellsten auf einem Testdatensatz, der in 1,4 Sekunden endet, die langsamste dauerte 13803 Sekunden. Die Unterschiede können auf Implementierungsqualität, Sprache und Compiler-Differenzen und die Verwendung von Indexen zur Beschleunigung zurückgeführt werden. Apache Commons Math enthält eine Java-Implementierung des Algorithmus, der in quadratischer Zeit läuft. ELKI bietet eine Implementierung von DBSCAN sowie GDBSCAN und anderen Varianten. Diese Implementierung kann verschiedene Indexstrukturen für die subquadratische Laufzeit verwenden und unterstützt willkürliche Abstandsfunktionen und beliebige Datentypen, kann aber durch Low-Level optimierte (und spezialisierte) Implementierungen auf kleinen Datensätzen übertroffen werden. mlpack enthält eine Implementierung von DBSCAN beschleunigt mit dual-tree-Range-Suchtechniken. PostGIS enthält ST_ClusterDBSCAN – eine 2D-Implementierung von DBSCAN, die R-tree-Index verwendet. Jeder Geometrietyp wird unterstützt, z.B. Point, LineString, Polygon usw. R enthält Implementierungen von DBSCAN in den Paketen dbscan und fpc. Beide Pakete unterstützen willkürliche Distanzfunktionen über Distanzmatrizen. Das Paket fpc hat keine Indexunterstützung (und hat somit quadratische Laufzeit und Speicherkomplexität) und ist aufgrund des R-Interpreters ziemlich langsam. Das Paket dbscan bietet eine schnelle C+-Implementierung mit k-d Bäumen (nur für Euclidean Entfernung) und beinhaltet auch Implementierungen von DBSCAN,* HDBSCAN,* OPTICS, OPTICSXi und anderen verwandten Methoden. scikit-learn enthält eine Python-Implementierung von DBSCAN für willkürliche Minkowski-Metriken, die mit k-d Bäumen und Ballbäumen beschleunigt werden kann, aber das schlimmste quadratische Gedächtnis verwendet. Ein Beitrag zu scikit-learn bietet eine Implementierung des HDBSCAN* Algorithmus. Die pyclustering-Bibliothek umfasst eine Python- und C+-Implementierung von DBSCAN für Euclidean-Distanz sowie OPTICS-Algorithmus. SPMF beinhaltet eine Implementierung des DBSCAN-Algorithmus mit k-d-Baum-Unterstützung nur für Euclidean Entfernung. Weka enthält (als optionales Paket in neuesten Versionen) eine grundlegende Implementierung von DBSCAN, die in quadratischer Zeit und linearem Speicher läuft. Anmerkungen == Referenzen ==