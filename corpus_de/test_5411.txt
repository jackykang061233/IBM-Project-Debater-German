Eine grafische Verarbeitungseinheit (GPU) ist eine spezialisierte elektronische Schaltung, die dazu ausgelegt ist, den Speicher schnell zu manipulieren und zu verändern, um die Erstellung von Bildern in einem Rahmenpuffer zu beschleunigen, der für die Ausgabe an ein Anzeigegerät bestimmt ist. GPUs werden in Embedded Systemen, Mobiltelefonen, Personal Computern, Workstations und Spielkonsolen verwendet. Moderne GPUs sind bei der Manipulation von Computergrafiken und Bildverarbeitung sehr effizient. Ihre hochparallele Struktur macht sie effizienter als allgemeine zentrale Verarbeitungseinheiten (CPUs) für Algorithmen, die große Datenblöcke parallel verarbeiten. In einem persönlichen Computer kann eine GPU auf einer Videokarte vorhanden oder auf dem Motherboard eingebettet sein. In bestimmten CPUs sind sie auf der CPU-Diät eingebettet. In den 1970er Jahren stand der Begriff GPU ursprünglich für Grafikprozessoreinheit und beschreibt eine programmierbare Verarbeitungseinheit, die unabhängig von der CPU arbeitet und für Grafikmanipulation und -ausgabe verantwortlich ist. Später, 1994, verwendet Sony den Begriff (jetzt stehend für Grafik-Verarbeitungseinheit) in Bezug auf die Toshiba-designed Sony GPU der PlayStation-Konsole im Jahr 1994. Der Begriff wurde 1999 von Nvidia populär, der die GeForce 256 als "die weltweit erste GPU" vermarktete. Es wurde als "Single-Chip-Prozessor mit integrierter Transformation, Beleuchtung, Dreieck-Setup/Clipping und Rendering-Engines" vorgestellt. Rivale ATI Technologien prägen den Begriff "visuelle Verarbeitungseinheit" oder VPU mit der Freilassung des Radeon 9700 im Jahr 2002. Geschichte 1970er Jahre Arcade-System-Boards haben seit den 1970er Jahren spezielle Grafik-Schaltungen verwendet. In der frühen Videospiel-Hardware war der RAM für Frame-Puffer teuer, so dass Video-Chips zusammengesetzte Daten zusammen, wie das Display auf dem Monitor ausgescannt wurde. Eine spezialisierte Barrel Shifter-Schaltung wurde verwendet, um der CPU zu helfen, die Framebuffer-Grafik für verschiedene 1970er Jahre Arcade-Spiele von Midway und Taito, wie Gun Fight (1975,) Sea Wolf (1976) und Space Invaders (1978) anzuimieren. Das Namco Galaxian Arcade-System im Jahr 1979 verwendet spezialisierte Grafik-Hardware unterstützt RGB-Farbe, mehrfarbige Sprite und Kachelkarten-Hintergründe. Die galaxische Hardware wurde im goldenen Zeitalter der Arcade-Videospiele weit verbreitet, von Spielfirmen wie Namco, Centuri, Gremlin, Irem, Konami, Midway, Nichibutsu, Sega und Taito. Im heimischen Markt benutzte die Atari 2600 1977 einen Videoschieber namens Fernsehinterface Adaptor. Die Atari 8-Bit-Computer (1979) hatten ANTIC, einen Videoprozessor, der Anleitungen interpretierte, die eine "Displayliste" beschreiben – die Art, wie die Scanzeilen auf bestimmte Bitmap- oder Zeichenmodi abbilden und wo der Speicher gespeichert ist (so gab es keinen zusammenhängenden Framepuffer).6502 Maschinencode-Unterprogramme konnten auf Scanzeilen durch ein Bit auf einer Displayliste-Anweisung ausgelöst werden. ANTIC unterstützte auch reibungslose vertikale und horizontale Scrolling unabhängig von der CPU. 1980sDie NEC μPD7220 war die erste Implementierung eines PC-Grafikdisplay-Prozessors als ein einziger integrierter Schaltungschip für große Skalenintegration (LSI), der das Design kostengünstiger, leistungsstarker Video-Grafikkarten wie die von Number Nine Visual Technology ermöglichte. Es wurde die bekannteste GPU bis Mitte der 1980er Jahre. Es war der erste voll integrierte VLSI (sehr große Integration) Metalloxid-Halbleiter (NMOS) Grafik-Display-Prozessor für PCs, unterstützt bis zu 1024x1024 Auflösung und legte die Grundlagen für den aufstrebenden PC-Grafikenmarkt. Es wurde in einer Reihe von Grafikkarten verwendet und für Klone wie die Intel 82720, die erste der Intel Grafikverarbeitungseinheiten lizenziert. Die Williams Electronics Arcade-Spiele Robotron 2084, Joust, Sinistar und Bubbles, alle 1982 veröffentlicht, enthalten benutzerdefinierte Blitter-Chips für den Betrieb auf 16-farbigen Bitmaps. 1984 veröffentlichte Hitachi ARTC HD63484, der erste große CMOS Grafikprozessor für PC. Das ARTC war in der Lage, bis zu 4K Auflösung bei monochromem Modus anzuzeigen, und es wurde in einer Reihe von PC-Grafikkarten und Terminals in den späten 1980er Jahren verwendet. Im Jahr 1985 verfügte der Commodore Amiga über einen benutzerdefinierten Grafik-Chip, mit einer blitter-Einheit beschleunigt Bitmap Manipulation, Linienzug und Bereich füllen Funktionen. Ebenfalls enthalten ist ein Coprozessor mit einem eigenen einfachen Befehlssatz, der in der Lage ist, Graphik-Hardware-Register synchron mit dem Videostrahl (z.B. für Per-Scanline-Palette-Schalter, Sprit-Multiplexing und Hardware-Fensterung) zu manipulieren oder den Blitter zu fahren. 1986 veröffentlichte Texas Instruments den TMS34010, den ersten voll programmierbaren Grafikprozessor. Es könnte allgemeiner Code laufen, aber es hatte eine grafikorientierte Anleitung. Während 1990-1992 wurde dieser Chip die Basis der Texas Instruments Graphics Architecture (TIGA) Windows-Beschleunigerkarten. Im Jahr 1987 wurde das IBM 8514-Grafikensystem als eine der ersten Videokarten für IBM PC-kompatible zur Implementierung von 2D-Festfunktionen in elektronischer Hardware veröffentlicht. Sharp's X68000, veröffentlicht im Jahr 1987, verwendet eine benutzerdefinierte Grafik-Chipsatz mit einer 65,536 Farbpalette und Hardware-Unterstützung für Sprite, Scrolling und mehrere Playfields, schließlich dienen als Entwicklungsmaschine für Capcom CP System Arcade Board. Fujitsu trat später mit dem FM Towns Computer, veröffentlicht 1989 mit Unterstützung für eine volle 16,777,216 Farbpalette. 1988 wurden mit dem Namco System 21 und dem Taito Air System die ersten dedizierten polygonalen 3D-Grafikkarten in Arkaden eingeführt. Der proprietäre Video Graphics Array (VGA) Displaystandard von IBM wurde 1987 eingeführt, mit einer maximalen Auflösung von 640×480 Pixeln. Im November 1988, NEC Home Electronics kündigte die Gründung der Video Electronics Standards Association (VESA) an, einen Super VGA (SVGA) Computer-Display-Standard als Nachfolger von IBMs proprietären VGA-Display-Standard zu entwickeln und zu fördern. Super VGA ermöglichte Grafik-Display-Auflösungen bis zu 800×600 Pixel, eine Steigerung um 36%. 1990sIm Jahre 1991 stellte S3 Graphics den S3 86C911 vor, den seine Designer nach dem Porsche 911 als Zeichen der Leistungssteigerung benannten. Die 86C911 spawnte eine Vielzahl von Imitatoren: bis 1995 hatten alle großen PC-Grafik-Chip-Hersteller 2D-Beschleunigungsunterstützung zu ihren Chips hinzugefügt. Bis zu diesem Zeitpunkt hatten Festfunktionen Windows-Beschleuniger teure Universal-Grafik-Koprozessoren in Windows-Performance übertroffen, und diese Coprozessoren verblassten vom PC-Markt. In den 1990er Jahren hat sich die 2D GUI-Beschleunigung weiter entwickelt. Da sich die Fertigungsfähigkeiten verbessert haben, hat sich auch die Integration von Grafikchips verbessert. Zusätzliche Anwendungs-Programmierschnittstellen (APIs) kamen für eine Vielzahl von Aufgaben, wie Microsofts WinG-Grafikenbibliothek für Windows 3.x, und ihre spätere DirectDraw-Schnittstelle zur Hardwarebeschleunigung von 2D-Spielen innerhalb von Windows 95 und später. In den frühen und Mitte der 1990er Jahre wurden Echtzeit-3D-Grafik in Arcade-, Computer- und Konsolenspielen immer häufiger verbreitet, was zu einer steigenden öffentlichen Nachfrage nach hardwarebeschleunigten 3D-Grafiken führte. In Arcade-Systemboards wie dem Sega Model 1, Namco System 22 und dem Sega Model 2 und den Videospielkonsolen der fünften Generation wie Saturn, PlayStation und Nintendo 64 finden sich frühe Beispiele für Massenmarkt-3D-Grafik-Hardware. Arcade-Systeme wie der Sega Model 2 und Namco Magic Edge Hornet Simulator im Jahr 1993 waren in der Lage, Hardware T&L (Transform, Clipping und Beleuchtung) Jahre, bevor in Verbraucher-Grafikkarten erscheinen. Einige Systeme nutzten DSPs, um Transformationen zu beschleunigen. Fujitsu, der an dem Arcade-System Sega Model 2 arbeitete, begann mit der Integration von T&L in eine einzige LSI-Lösung für den Einsatz in Heimcomputern 1995; der Fujitsu Pinolite, der erste 3D-Geometrieprozessor für Personalcomputer, der 1997 veröffentlicht wurde. Die erste Hardware T&L GPU auf Heimvideo-Spielkonsolen war der 1996 veröffentlichte Reality Coprozessor von Nintendo 64. 1997 veröffentlichte Mitsubishi die 3Dpro/2MP, eine voll ausgestattete GPU, die in der Lage ist, Transformation und Beleuchtung, für Workstations und Windows NT Desktops; ATi nutzte sie für ihre FireGL 4000 Grafikkarte, veröffentlicht 1997. Der Begriff GPU wurde von Sony in Bezug auf die 32-Bit Sony GPU (entworfen von Toshiba) in der PlayStation Videospielkonsole, veröffentlicht 1994. In der PC-Welt waren die S3 ViRGE, ATI Rage und Matrox Mystique die ersten Versuche für kostengünstige 3D-Grafikchips. Diese Chips waren im Wesentlichen 2D-Beschleuniger der vorherigen Generation mit 3D-Features aufgeschraubt. Viele waren sogar pin-kompatibel mit den früheren Generation Chips für einfache Implementierung und minimale Kosten. Zunächst waren Performance 3D-Grafiken nur mit diskreten Boards möglich, die 3D-Funktionen (und ohne 2D GUI-Beschleunigung ganz) wie dem PowerVR und dem 3dfx Voodoo beschleunigen. Da jedoch die Fertigungstechnologie weiter voranging, wurden Video, 2D GUI Beschleunigung und 3D Funktionalität in einen Chip integriert. Rendition's Verite Chipsets gehörten zu den ersten, die das gut genug tun, um würdig zu sein. Im Jahr 1997 ging Rendition einen Schritt weiter, indem er mit Hercules und Fujitsu auf einem Projekt "Thriller Conspiracy" zusammenarbeitete, das einen Fujitsu FXG-1 Pinolite Geometrieprozessor mit einem Vérité V2200 Kern kombinierte, um eine Grafikkarte mit einer vollen T&L-Engine zu erstellen, die Jahre vor Nvidias GeForce 256. Diese Karte, entworfen, um die Belastung auf die CPU des Systems zu reduzieren, nie machte es auf den Markt. OpenGL erschien in den frühen 90er Jahren als professionelle Grafik-API, aber ursprünglich litt unter Performance-Problemen, die die Glide API erlaubte, einzutreten und zu einer dominanten Kraft auf dem PC in den späten 90er Jahren. Allerdings wurden diese Probleme schnell überwunden und die Glide API fiel am Rande. Software-Implementierungen von Open GL waren in dieser Zeit üblich, obwohl der Einfluss von OpenGL schließlich zu einer weit verbreiteten Hardwareunterstützung führte. Im Laufe der Zeit entstand eine Parität zwischen den Funktionen in der Hardware und den in OpenGL angebotenen. DirectX wurde während der späten 90er Jahre bei Windows-Spielentwicklern beliebt. Im Gegensatz zu OpenGL bestand Microsoft darauf, strenge One-to-one-Unterstützung der Hardware bereitzustellen. Der Ansatz direkt gemacht X weniger beliebt als Standalone-Grafik-API zunächst, da viele GPUs ihre eigenen spezifischen Funktionen zur Verfügung gestellt, von denen bestehende OpenGL-Anwendungen bereits profitieren konnten, so dass DirectX oft eine Generation zurück.(Siehe: Vergleich von OpenGL und Direct3D.) Im Laufe der Zeit begann Microsoft enger mit Hardware-Entwicklern zu arbeiten, und begann, die Releases von DirectX anzusprechen, um mit denen der unterstützenden Grafik-Hardware zusammenzufallen. Direkt3D 5.0 war die erste Version der Burgeoning API, um eine weit verbreitete Adoption auf dem Gaming-Markt zu gewinnen, und es trat direkt mit vielen hardwarespezifischen, oft proprietären Grafikbibliotheken, während OpenGL eine starke Nachfolge. Direct3D 7.0 führte Unterstützung für hardwarebeschleunigte Transformation und Beleuchtung (T&L) für Direct3D ein, während OpenGL diese Fähigkeit bereits von seiner Inception belichtet hatte.3D-Beschleunigerkarten bewegten sich über einfach nur Rasterizer, um eine weitere signifikante Hardware-Stufe in der 3D-Rendering-Pipeline hinzuzufügen. Die Nvidia GeForce 256 (auch bekannt als NV10) war die erste auf dem Markt veröffentlichte Verbraucher-Level-Karte mit hardwarebeschleunigten T&L, während professionelle 3D-Karten bereits diese Fähigkeit hatten. Hardware-Transformation und -Beleuchtung, beide bereits vorhandenen Features von OpenGL, kamen in den 90er Jahren auf verbraucher-Level-Hardware und setzen den Präzedenzfall für spätere Pixel-Töner und Vertex-Tönereinheiten, die weit flexibler und programmierbar waren. 2000 bis 2010 wurde Nvidia zunächst einen Chip produziert, der programmierbar ist; die GeForce 3 (Code NV20). Jeder Pixel könnte nun durch ein kurzes Programm bearbeitet werden, das zusätzliche Bildtexturen als Eingaben umfassen könnte, und jeder geometrische Scheitel konnte ebenfalls durch ein kurzes Programm bearbeitet werden, bevor er auf den Bildschirm projiziert wurde. In der Xbox-Konsole verwendet, konkurrieren sie mit der PlayStation 2, die eine benutzerdefinierte Vektoreinheit für die hardwarebeschleunigte Vertex-Verarbeitung (gemeinsam VU0/VU1) verwendet. Die frühesten Inkarnationen von in Xbox verwendeten Shader-Ausführungsmotoren waren nicht generell und konnten keinen beliebigen Pixelcode ausführen. Vertices und Pixel wurden von verschiedenen Einheiten verarbeitet, die ihre eigenen Ressourcen mit Pixel-Töndern mit viel engeren Zwängen (wie sie bei viel höheren Frequenzen ausgeführt werden als bei Vertices). Pixel-Shading-Motoren waren eigentlich eher an einen hoch anpassbaren Funktionsblock und nicht wirklich ein Programm ausgeführt. Viele dieser Unterschiede zwischen Vertex und Pixelschattung wurden erst sehr später mit dem Unified Shader Model angesprochen. Bis Oktober 2002 konnte mit der Einführung des ATI Radeon 9700 (auch bekannt als R300) der weltweit erste Direct3D 9.0 Beschleuniger, Pixel und Scheiteltöner Schleifen und langwierige Floating-Point-Math implementieren und schnell so flexibel wie CPUs, aber Größenordnungen schneller für Bild-Array-Betriebe. Pixel Shading wird oft für Stoßkartierungen verwendet, die Textur hinzufügen, um ein Objekt aussehen glänzend, matt, rau oder sogar rund oder extrudiert. Mit der Einführung der Nvidia GeForce 8 Serie, und dann neue generische Stream-Verarbeitungseinheit GPUs wurde zu einem allgemeineren Rechengerät. Heute parallel GPUs haben begonnen, rechnerische Inroads gegen die CPU zu machen, und ein Teilgebiet der Forschung, gegraben GPU Computing oder GPGPU für General Purpose Computing auf GPU, hat seinen Weg in Felder so vielfältig wie maschinelles Lernen, Ölexploration, wissenschaftliche Bildverarbeitung, lineare Algebra, Statistiken, 3D Rekonstruktion und sogar Aktienoptionen Preisbestimmung gefunden. GPGPU damals war die Vorstufe zu dem, was jetzt als Compute-Shader (z.B. CUDA, OpenCL, DirectCompute) bezeichnet wird und die Hardware in einem Maße tatsächlich missbraucht, indem die an Algorithmen übergebenen Daten als Texturkarten und Ausführen von Algorithmen durch Zeichnen eines Dreiecks oder Quads mit einem entsprechenden Pixel-Shader behandelt werden. Dies beinhaltet offensichtlich einige Overheads, da Einheiten wie der Scan Converter involviert sind, wo sie nicht wirklich benötigt werden (nor sind Dreiecksmanipulationen sogar eine Sorge - außer den Pixel-Töner zu rufen). Die 2007 vorgestellte CUDA-Plattform von Nvidia war das früheste, weit verbreitete Programmiermodell für GPU Computing. Vor kurzem wurde OpenCL breit unterstützt. OpenCL ist ein von der Khronos Group definierter offener Standard, der die Entwicklung von Code für GPUs und CPUs mit einem Schwerpunkt auf Portabilität ermöglicht. OpenCL-Lösungen werden von Intel, AMD, Nvidia und ARM unterstützt, und nach einem kürzlichen Bericht von Evans Daten ist OpenCL die GPGPU-Entwicklungsplattform, die am häufigsten von Entwicklern in den USA und Asien-Pazifik genutzt wird. 2010 zu präsentieren In 2010 begann Nvidia eine Partnerschaft mit Audi, um die Armaturenbretter ihrer Autos zu treiben, indem die Tegra GPUs eine erhöhte Funktionalität für die Navigations- und Unterhaltungssysteme der Autos bieten. Fortschritte in der GPU-Technologie in Autos haben dazu beigetragen, selbstfahrende Technologie. AMD's Radeon HD 6000 Serienkarten wurden 2010 veröffentlicht und 2011 veröffentlichte AMD ihre 6000M Serie diskrete GPUs, die in mobilen Geräten verwendet werden sollen. Die Kepler-Grafikkarten von Nvidia kamen 2012 heraus und wurden in den 600 und 700 Serienkarten von Nvidia verwendet. Eine Funktion in dieser neuen GPU-Mikroarchitecture beinhaltete GPU-Boost, eine Technologie, die die Taktgeschwindigkeit einer Videokarte anpasst, um sie entsprechend ihrer Stromabnahme zu erhöhen oder zu verringern. Die Kepler-Mikroarchitektur wurde auf dem 28 nm-Verfahren hergestellt. Die PS4 und Xbox Eine wurde 2013 veröffentlicht, beide verwenden GPUs auf Basis von AMD's Radeon HD 7850 und 7790. Nvidias Kepler-Linie von GPUs folgte der Maxwell-Linie, die auf demselben Prozess hergestellt wurde. 28 nm Chips von Nvidia wurden von TSMC, der Taiwan Semiconductor Manufacturing Company, hergestellt, die damals mit dem 28 nm-Verfahren hergestellt wurde. Im Vergleich zu der 40 nm-Technologie aus der Vergangenheit ermöglichte dieser neue Herstellungsprozess eine Leistungssteigerung von 20 Prozent bei weniger Strom. Virtual Reality Headsets haben sehr hohe Systemanforderungen. VR-Headset-Hersteller empfohlen die GTX 970 und die R9 290X oder besser zum Zeitpunkt ihrer Veröffentlichung. Pascal ist die nächste Generation von Consumer-Grafikkarten von Nvidia veröffentlicht im Jahr 2016. Die GeForce 10 Kartenserie ist unter dieser Generation von Grafikkarten. Sie werden mit dem Herstellungsprozess von 16 nm hergestellt, der sich bei früheren Mikroarchitekten verbessert. Nvidia hat eine Non-Consumer-Karte unter der neuen Volta-Architektur veröffentlicht, die Titan V. Änderungen von der Titan XP, Pascal High-End-Karte, umfassen eine Erhöhung der Anzahl der CUDA-Kerne, die Zugabe von Tensor-Kerne, und HBM2. Tensor-Kerne sind Kerne speziell für tiefes Lernen konzipiert, während High-Bandbreite-Speicher ist on-die, gestapelt, untertakteten Speicher, der einen extrem breiten Speicherbus bietet, der für den beabsichtigten Zweck des Titan V nützlich ist. Um hervorzuheben, dass der Titan V keine Spielkarte ist, entfernte Nvidia den "GeForce GTX"-Suffix, den er zu Verbraucher-Gaming-Karten hinzufügt. Am 20. August 2018 startete Nvidia die GPUs der RTX 20-Serie, die GPUs mit Strahltracing-Kernen ergänzen und ihre Leistung auf Lichteffekte verbessern. Polaris 11 und Polaris 10 GPUs von AMD werden durch ein 14-Nanometer-Verfahren hergestellt. Ihre Freisetzung führt zu einer erheblichen Steigerung der Leistung pro Watt von AMD-Videokarten. AMD hat auch die Vega GPUs Serie für den High-End-Markt als Wettbewerber für Nvidias High-End Pascal Karten veröffentlicht, auch mit HBM2 wie der Titan V. Im Jahr 2019 veröffentlichte AMD den Nachfolger ihres Graphics Core Next (GCN) Mikroarchitecture/Instruction Sets. Als RDNA wurde die erste Produkt-Lineup mit der ersten Generation von RDNA die Radeon RX 5000-Serie von Videokarten vorgestellt, die später am 7. Juli 2019 gestartet wurde. Später kündigte das Unternehmen an, dass der Nachfolger der RDNA-Mikroarchitektur eine Erfrischung wäre. Als RDNA 2 wurde die neue Mikroarchitektur gemeldet, die für die Veröffentlichung im vierten Quartal 2020 geplant war. AMD enthüllte die Radeon RX 6000 Serie, ihre nächsten RDNA 2 Grafikkarten mit Unterstützung der hardwarebeschleunigten Strahlverfolgung auf einem Online-Event am 28. Oktober 2020. Das Lineup besteht zunächst aus RX 6800, RX 6800 XT und RX 6900 XT. Die RX 6800 und 6800 XT starteten am 18. November 2020, wobei die RX 6900 XT am 8. Dezember 2020 veröffentlicht wurde. Die RX 6700 und RX 6700 XT-Varianten, die auf Navi 22 basiert, sollen in der ersten Hälfte von 2021 starten. Die PlayStation 5 und Xbox Serie X und Serie S wurden im Jahr 2020 veröffentlicht, beide verwenden GPUs basierend auf der RDNA 2 Mikroarchitektur mit proprietären Tweaks und verschiedenen GPU-Konfigurationen in der Implementierung jedes Systems. GPU-Unternehmen Viele Unternehmen haben GPUs unter einer Reihe von Markennamen produziert. Im Jahr 2009 waren Intel, Nvidia und AMD/ATI mit 49,4,% 27,8% bzw. 20,6% Marktanteil. Diese Zahlen beinhalten jedoch Intels integrierte Grafiklösungen als GPUs. Nicht zählen diese, Nvidia und AMD steuern fast 100% des Marktes ab 2018. Ihre jeweiligen Marktanteile betragen 66% und 33%. Außerdem produzieren Matrox GPUs. Moderne Smartphones nutzen auch meist Adreno GPUs von Qualcomm, PowerVR GPUs von Imagination Technologies und Mali GPUs von ARM. Moderne GPUs verwenden die meisten ihrer Transistoren, um Berechnungen im Zusammenhang mit 3D-Computergrafiken zu machen. Neben der 3D-Hardware umfassen die heutigen GPUs grundlegende 2D-Beschleunigungs- und Framebuffer-Funktionen (in der Regel mit einem VGA-Kompatibilitätsmodus). Neue Karten wie AMD/ATI HD5000-HD7000 fehlen sogar 2D Beschleunigung; es muss durch 3D Hardware emuliert werden. GPUs wurden zunächst verwendet, um die speicherintensive Arbeit von Textur-Mapping und Rendering-Polygonen zu beschleunigen, später Einheiten hinzufügen, um geometrische Berechnungen wie Rotation und Translation von Vertices in verschiedene Koordinatensysteme zu beschleunigen. Zu den jüngsten Entwicklungen in GPUs zählen die Unterstützung von programmierbaren Shadern, die Vertiken und Texturen mit vielen der gleichen Operationen, die von CPUs unterstützt werden, übersampling und Interpolation Techniken, um Aliasing zu reduzieren, und sehr hochpräzise Farbräume. Da die meisten dieser Berechnungen Matrix- und Vektoroperationen beinhalten, haben Ingenieure und Wissenschaftler zunehmend die Verwendung von GPUs für nicht-graphische Berechnungen untersucht; sie eignen sich besonders für andere peinlich parallele Probleme. Mehrere Faktoren der GPU-Konstruktion treten in die Leistung der Karte für Echtzeit-Rendering ein.Häufige Faktoren können die Größe der Verbindungswege in der Halbleiterbaugruppe, die Taktsignalfrequenz und die Anzahl und Größe verschiedener On-Chip-Speichercaches umfassen. Zusätzlich die Anzahl der Streaming Multiprozessoren (SM) für NVidia GPUs, oder Compute Units (CU) für AMD GPUs, die die Anzahl der Kern-On-Silizium-Prozessoreinheiten innerhalb des GPU-Chips beschreiben, die die Kernberechnungen durchführen, typischerweise parallel zu anderen SM/CUs auf der GPU arbeiten. Die Leistung von GPUs wird typischerweise in Floating Point Operationen pro Sekunde oder FLOPS gemessen, wobei GPUs in den 2010s und 2020s typischerweise die Leistung in Teraflops (TFLOPS) liefern. Dies ist ein geschätztes Leistungsmaß, da andere Faktoren die tatsächliche Anzeigerate beeinflussen können. Mit dem Entstehen des tiefen Lernens hat sich die Bedeutung von GPUs erhöht. In der Forschung von Indigo wurde festgestellt, dass GPUs während der Ausbildung von tiefen neuronalen Netzwerken 250 mal schneller als CPUs sein können. Es gab in diesem Bereich einige Wettbewerbsebene mit ASICs, vor allem die Tensor Processing Unit (TPU) von Google. ASICs erfordern jedoch Änderungen an bestehenden Code und GPUs sind immer noch sehr beliebt. GPU beschleunigte Videodekodierung und Codierung Die meisten GPUs seit 1995 unterstützen den YUV-Farbraum und Hardware-Overlays, wichtig für die digitale Videowiedergabe, und viele GPUs seit 2000 unterstützen auch MPEG-Primitive wie Bewegungskompensation und iDCT. Dieser Prozess der hardwarebeschleunigten Videodecodierung, bei dem Teile des Videodekodierungsprozesses und der Videonachbearbeitung auf die GPU-Hardware abgeladen werden, wird häufig als "GPU beschleunigte Videodecodierung" bezeichnet "GPU unterstützte Videodecodierung", "GPU-Hardware beschleunigte Videodecodierung" oder "GPU-Hardware unterstützte Videodecodierung". Neuere Grafikkarten dekodieren sogar High-Definition-Videos auf der Karte und laden die zentrale Verarbeitungseinheit aus. Die häufigsten APIs für die GPU beschleunigte Videodekodierung sind DxVA für Microsoft Windows Betriebssystem und VDPAU, VAAPI, XvMC und XvBA für Linux-basierte und UNIX-ähnliche Betriebssysteme. Alle außer XvMC sind in der Lage, mit MPEG-1, MPEG-2, MPEG-4 ASP (MPEG-4 Teil 2,) MPEG-4 AVC (H.264 / DivX 6,) VC-1, WMV3/WMV9, Xvid / OpenDivX (DivX 4,) und DivX 5 Codecs kodierte Videos zu dekodieren, während XvMC nur MPEG-1 und MPEG-2 dekodieren kann. Es gibt mehrere dedizierte Hardware-Video-Decodierung und Codierung Lösungen. Videodekodierungsverfahren, die beschleunigt werden können Die Video-Dekodierprozesse, die durch die heutige moderne GPU Hardware beschleunigt werden können, sind: Bewegungskompensation (mocomp) Inverse diskrete Cosinus-Transformation (iDCT) Inverse Telecine 3:2 und 2:2 Pull-Down-Korrektur Inverse modifizierte diskrete Cosin-Transformation (iMDCT)In-Loop-Deblocking-Filter Intra-Frame-Prädiktion Inverse-Quantisierung (IQ)Variable-Länge-Dekodierung (VLD,) allgemeiner als Slice-Level-Level-Beschleunigung bezeichnet Die obigen Operationen haben auch Anwendungen in der Videobearbeitung, Kodierung und Transcodierung GPU-Formulare Terminologie In Personal Computern gibt es zwei Hauptformen von GPUs. Jeder hat viele Synonyme: Dedizierte Grafikkarte - auch diskret genannt. Integrierte Grafik - auch genannt: gemeinsame Grafiklösungen, integrierte Grafikprozessoren (IGP) oder einheitliche Speicherarchitektur (UMA.) Die meisten GPUs sind für eine bestimmte Nutzung, Echtzeit-3D-Grafik oder andere Massenberechnungen ausgelegt: Gaming GeForce GTX, RTX Nvidia Titan Radeon HD, R5, R7, R9, RX, Vega und Navi Serie Radeon VII Cloud Gaming Nvidia GRID Radeon Sky Workstation Nvidia Quadro AMD FirePro AMD Radeon Pro Cloud Workstation NvidiaTesla AMD FireStream Artificial Intelligence Training und Cloud Nvidia Tesla AMD Radetin Dedizierte Grafikkarten Die GPUs der mächtigsten Klasse typischerweise Schnittstelle mit dem Motherboard mittels eines Erweiterungsschlitzes wie PCI Express (PCIe) oder Accelerated Graphics Port (AGP) und kann in der Regel durch relativ einfache ersetzt oder aktualisiert werden, vorausgesetzt, das Motherboard ist in der Lage, das Upgrade zu unterstützen. Einige Grafikkarten verwenden immer noch Peripheral Component Interconnect (PCI), aber ihre Bandbreite ist so begrenzt, dass sie im Allgemeinen nur dann verwendet werden, wenn ein PCIe- oder AGP-Slot nicht verfügbar ist. Eine dedizierte GPU ist nicht notwendigerweise abnehmbar, auch nicht notwendigerweise Schnittstelle mit dem Motherboard in Standard-Mode. Der Begriff dediziert bezieht sich auf die Tatsache, dass dedizierte Grafikkarten RAM haben, die der Verwendung der Karte gewidmet ist, nicht auf die Tatsache, dass die meisten dedizierten GPUs abnehmbar sind. Darüber hinaus wird dieser RAM in der Regel speziell für die erwartete serielle Workload der Grafikkarte ausgewählt (siehe GDDR). Manchmal wurden Systeme mit dedizierten, diskreten GPUs als DIS-Systeme im Gegensatz zu UMA-Systemen bezeichnet (siehe nächste Sektion). Dedizierte GPUs für tragbare Computer werden am häufigsten durch einen nicht standardmäßigen und oft proprietären Slot aufgrund von Größe und Gewichtsbeschränkungen verbunden. Solche Ports können noch als PCIe oder AGP in Bezug auf ihre logische Host-Schnittstelle betrachtet werden, auch wenn sie mit ihren Pendants nicht physikalisch austauschbar sind. Technologien wie SLI und NVLink von Nvidia und CrossFire von AMD ermöglichen es mehreren GPUs, Bilder gleichzeitig für einen einzigen Bildschirm zu zeichnen und die für Grafiken verfügbare Verarbeitungsleistung zu erhöhen. Diese Technologien sind jedoch immer häufiger, da die meisten Spiele nicht vollständig mehrere GPUs nutzen, da die meisten Benutzer sie nicht leisten können. mehrere GPUs werden immer noch auf Supercomputern (wie auf Summit) auf Workstations verwendet, um Video (Aufbereitung mehrerer Videos auf einmal) und 3D-Rendering, für VFX und für Simulationen zu beschleunigen, und in AI, um Schulungen zu beschleunigen, wie dies bei Nvidias Aufstellung von DGX Workstations und Servern und Tesla GPUs und Intels anstehenden Ponte Vecchio GPUs der Fall ist. Integrierte Grafik-Verarbeitungseinheit Integrierte Grafik-Verarbeitungseinheit (IGPU,) Integrierte Grafiken, gemeinsame Grafiklösungen, integrierte Grafikprozessoren (IGP) oder einheitliche Speicherarchitektur (UMA) verwenden einen Teil des Computer-Systems RAM anstatt dedizierte Grafikspeicher. IGPs können als Teil des (northbridge) Chipsatzes oder auf dem gleichen Die (integrierte Schaltung) mit der CPU (wie AMD APU oder Intel HD Graphics) integriert werden. Auf bestimmten Mainboards können die IGPs von AMD dedizierte Sideport-Speicher verwenden. Dies ist ein separater fester Block des Hochleistungsspeichers, der für den Einsatz durch die GPU bestimmt ist. Anfang 2007 machen Computer mit integrierter Grafik rund 90% aller PC-Versandungen aus. Sie sind weniger kostspielig zu implementieren als dedizierte Grafikverarbeitung, aber neigen dazu, weniger fähig zu sein. Historisch betrachtet wurde die integrierte Verarbeitung als untauglich, um 3D-Spiele zu spielen oder grafisch intensive Programme auszuführen, konnte aber weniger intensive Programme wie Adobe Flash ausführen. Beispiele für solche IGPs wären Angebote von SiS und VIA um 2004. Moderne integrierte Grafikprozessoren wie AMD Accelerated Processing Unit und Intel HD Graphics sind jedoch mehr als in der Lage, 2D-Grafiken oder 3D-Grafiken mit geringem Stress zu verarbeiten. Da die GPU-Rechnungen extrem speicherintensive sind, kann sich die integrierte Verarbeitung mit der CPU für das relativ langsame System RAM konkurrieren, da sie einen minimalen oder keinen dedizierten Videospeicher aufweist. IGPs können bis zu 29.856 GB/s Speicherbandbreite aus System RAM haben, während eine Grafikkarte bis zu 264 GB/s Bandbreite zwischen RAM und GPU-Kern haben kann. Diese Speicherbus-Bandbreite kann die Leistung der GPU begrenzen, obwohl Multi-Kanal-Speicher kann diesen Mangel mildern. Ältere integrierte Grafik-Chipsätze fehlen Hardware-Transformation und Beleuchtung, aber neuere enthalten es. Hybride grafische Verarbeitung Diese neuere Klasse von GPUs steht im Wettbewerb mit integrierten Grafiken in den Low-End-Desktop- und Notebookmärkten. Die häufigsten Implementierungen davon sind ATIs HyperMemory und Nvidias TurboCache. Hybride Grafikkarten sind etwas teurer als integrierte Grafiken, aber viel weniger teuer als dedizierte Grafikkarten. Diese teilen Speicher mit dem System und haben einen kleinen dedizierten Speicher Cache, um für die hohe Latenz des Systems RAM. Technologien innerhalb der PCI Express kann dies ermöglichen. Während diese Lösungen manchmal als mit 768MB RAM beworben werden, bezieht sich dies auf, wie viel mit dem Systemspeicher geteilt werden kann. Stream-Verarbeitung und allgemeiner Zweck GPUs (GPGPU)Es wird immer häufiger, eine allgemeine Zweck-Grafiken-Verarbeitungseinheit (GPGPU) als modifizierte Form von Stream-Prozessor (oder Vektor-Prozessor) verwenden, die Rechenkerne ausführen. Dieses Konzept verwandelt die massive Rechenleistung einer modernen Graphikbeschleuniger-Pipeline in die universelle Rechenleistung, im Gegensatz zu nur hart verdrahtet zu grafischen Operationen. Bei bestimmten Anwendungen, die massive Vektoroperationen erfordern, kann dies mehrere Größenordnungen höherer Leistung liefern als eine herkömmliche CPU. Die zwei größten diskreten (siehe "Dedicated Graphics Karten" oben) GPU-Designer, AMD und Nvidia, beginnen diesen Ansatz mit einer Reihe von Anwendungen zu verfolgen. Sowohl Nvidia als auch AMD haben sich mit der Stanford University zusammengetan, um einen GPU-basierten Client für das Folding@home-Distributed Computing-Projekt zu schaffen, für Protein-Faltenberechnungen. Unter Umständen berechnet die GPU vierzigmal schneller als die von solchen Anwendungen traditionell verwendeten CPUs. GPGPU kann für viele Arten von peinlich parallelen Aufgaben verwendet werden, einschließlich der Strahlverfolgung. Sie eignen sich in der Regel für hochdurchsatzartige Berechnungen, die Datenparallelität aufweisen, um die breite Vektorbreite SIMD-Architektur der GPU auszunutzen. Darüber hinaus spielen GPU-basierte Hochleistungsrechner eine wichtige Rolle bei der Großmodellierung. Drei der 10 mächtigsten Supercomputer der Welt nutzen die GPU-Beschleunigung. GPUs unterstützen API-Erweiterungen zur C-Programmiersprache wie OpenCL und OpenMP. Darüber hinaus hat jeder GPU-Anbieter seine eigene API eingeführt, die nur mit seinen Karten, AMD APP SDK und CUDA von AMD bzw. Nvidia funktioniert. Diese Technologien ermöglichen spezifizierte Funktionen, die als Rechenkerne aus einem normalen C-Programm bezeichnet werden, um auf den Streamprozessoren der GPU zu laufen. Dadurch ist es möglich, dass C-Programme die Möglichkeit nutzen, auf großen Puffern parallel zu arbeiten, wobei die CPU gegebenenfalls noch verwendet wird. CUDA ist auch die erste API, mit der CPU-basierte Anwendungen direkt auf die Ressourcen einer GPU für allgemeinere Zwecke Computing zugreifen können, ohne die Einschränkungen der Verwendung einer Grafik-API. Seit 2005 besteht Interesse daran, die von GPUs angebotenen Leistungen für evolutionäre Berechnungen allgemein zu nutzen und insbesondere die Eignungsbewertung in der genetischen Programmierung zu beschleunigen. Die meisten Ansätze kompilieren lineare oder Baumprogramme auf dem Host-PC und übertragen das ausführbare auf die zu betreibende GPU. Typischerweise wird der Leistungsvorteil nur dadurch erreicht, dass das einzelne aktive Programm gleichzeitig auf vielen Beispielproblemen parallel unter Verwendung der SIMD-Architektur der GPU läuft. Eine erhebliche Beschleunigung kann jedoch auch dadurch erreicht werden, daß die Programme nicht kompiliert werden, sondern sie auf die dort zu interpretierende GPU übertragen werden. Die Beschleunigung kann dann erreicht werden, indem mehrere Programme gleichzeitig interpretiert werden, wobei gleichzeitig mehrere Beispielprobleme oder Kombinationen von beiden laufen. Eine moderne GPU kann leicht gleichzeitig Hunderttausende von sehr kleinen Programmen interpretieren. Einige moderne Workstation-GPUs, wie die Nvidia Quadro-Workstationskarten mit den Volta- und Turing-Architekturen, zeigen Verarbeitungskerne für Tensor-basierte Deep Learning-Anwendungen. In der aktuellen GPU-Serie von Nvidia werden diese Kerne Tensor Cores genannt. Diese GPUs haben in der Regel signifikante FLOPS-Leistungssteigerungen, unter Verwendung von 4x4 Matrix Multiplikation und Division, wodurch Hardware-Performance bis zu 128 TFLOPS in einigen Anwendungen. Diese Tensor-Kerne sollen auch in Verbraucherkarten mit der Turing-Architektur und möglicherweise in der Navi-Serie von Verbraucherkarten von AMD erscheinen. Externe GPU (eGPU) Eine externe GPU ist ein Grafikprozessor außerhalb des Gehäuses des Computers, ähnlich einer großen externen Festplatte. Externe Grafikprozessoren werden manchmal mit Laptop-Computern verwendet. Laptops haben möglicherweise eine beträchtliche Menge an RAM und eine ausreichend leistungsfähige zentrale Verarbeitungseinheit (CPU), aber oft fehlt ein leistungsfähiger Grafikprozessor und haben stattdessen einen weniger leistungsfähigen, aber energieeffizienteren On-Board-Grafikchip. On-Board-Grafikchips sind oft nicht leistungsfähig genug, um Videospiele zu spielen, oder für andere grafisch intensive Aufgaben, wie das Editieren von Video oder 3D-Animation/Rendering. Daher ist es wünschenswert, eine GPU an einem externen Bus eines Notebooks befestigen zu können. PCI Express ist der einzige Bus für diesen Zweck. Der Port kann beispielsweise ein ExpressCard- oder mPCIe-Port (PCIe ×1, bis zu 5 bzw. 2,5 Gbit/s) oder ein Thunderbolt 1, 2 bzw. 3 Port (PCIe ×4, bis zu 10, 20 bzw. 40 Gbit/s) sein. Diese Ports sind nur auf bestimmten Notebook-Systemen verfügbar.e GPU-Gehäuse umfassen ihre eigene Stromversorgung (PSU), da leistungsstarke GPUs leicht Hunderte von Watt verbrauchen können. Die offizielle Anbieter-Unterstützung für externe GPUs hat in letzter Zeit eine Traktion gewonnen. Ein bemerkenswerter Meilenstein war die Entscheidung von Apple, externe GPUs offiziell mit MacOS High Sierra 10.13.4 zu unterstützen. Es gibt auch mehrere große Hardware-Anbieter (HP, Alienware, Razer) Freigabe Thunderbolt 3 eGPU-Gehäuse. Diese Unterstützung hat die eGPU-Implementierungen von Enthusiasten weiter vorangetrieben. Im Jahr 2013 wurden 438,3 Millionen GPUs weltweit ausgeliefert und die Prognose für 2014 betrug 414.2 Millionen. Siehe auch Hardware-Vergleich von AMD-Grafik-Verarbeitungseinheiten Vergleich von Nvidia-Grafik-Verarbeitungseinheiten Vergleich von Intel-Grafik-VerarbeitungseinheitenIntel GMA Larrabee Nvidia PureVideo - die Bit-stream-Technologie von Nvidia in ihren Grafikchips verwendet, um die Videodekodierung auf Hardware GPU mit DXVA zu beschleunigen. SoC UVD (Unified Video Decoder) – die Video-Dekodierung Bit-Stream-Technologie von ATI zur Unterstützung von Hardware (GPU) Decodierung mit DXVA APIs Applikationen GPU Cluster Mathematica – beinhaltet integrierte Unterstützung für CUDA und OpenCL GPU Ausführung Molekulare Modellierung auf GPU Deeplearning4j – Open-Source, verteiltes Deep Learning für Java Referenzen ==Externale Links ==