Computerleistung ist der Wert der nützlichen Arbeit eines Computersystems. außerhalb bestimmter Kontexte wird die Leistung von Computern auf Genauigkeit, Effizienz und Geschwindigkeit der Ausführung von Computerprogrammen geschätzt. In Bezug auf hohe Computerleistung könnten eine oder mehrere der folgenden Faktoren einbezogen werden: kurze Antwortzeit für ein bestimmtes Arbeitsstück. Hoher Durchsatz (Verarbeitung). geringe Nutzung von Rechenressourcen(s). Schnell (oder sehr kompakte) Datenkompression und Dekompression. hohe Verfügbarkeit des Rechensystems oder der Anwendung. Hohe Bandbreite. kurze Datenübermittlungszeit. Technische und nichttechnische Definitionen Die Leistung eines Computersystems kann anhand eines oder mehrerer der oben aufgeführten Parameter in messbaren, technischen Bedingungen bewertet werden. Wie kann die Leistung im Vergleich zu anderen Systemen oder dem gleichen System vor/nach Änderungen verglichen werden absolut gesehen, z.B. für die Erfüllung einer vertraglichen VerpflichtungWährst die vorstehende Definition betrifft einen wissenschaftlichen, technischen Ansatz, die folgende Definition von Arnold Allen wäre für ein nicht-technisches Publikum nützlich: Das Wortergebnis in der Computerleistung bedeutet, dass Leistung in anderen Kontexten, d. h. es bedeutet, dass „Wie gut ist der Computer, der die Arbeit erbringt?" Als Aspekt der Software-Qualität Computer-Softwareleistung, insbesondere der Software-Anwendungszeit, ist ein Aspekt der Softwarequalität, der für Mensch-Computer-Interaktionen wichtig ist. Leistungs-Engineering im System-Engineering umfasst die Reihe von Rollen, Fähigkeiten, Tätigkeiten, Praktiken, Werkzeuge und Leistungsfähigkeiten, die in jeder Phase des Systementwicklungszyklus angewandt werden, die gewährleisten, dass eine Lösung entwickelt, umgesetzt und operationell unterstützt wird, um die für die Lösung festgelegten Leistungsanforderungen zu erfüllen. Leistungs-Engineering befasst sich kontinuierlich mit den Abschlägen zwischen Leistungsarten. Gelegentlich kann ein CPU-Designer einen Weg finden, um eine CPU mit einer besseren Gesamtleistung zu erzielen, indem er eine der Leistungsaspekte verbessert, die unten dargestellt werden, ohne die Leistungsfähigkeit der CPU in anderen Bereichen zu verlieren. z.B. den Aufbau der CPU aus besseren, schnelleren Transistoren. manchmal eine Art von Leistung zu einem Extrem führt zu einer CPU mit schlechterer Gesamtleistung, da andere wichtige Aspekte geopfert wurden, um beispielsweise eine beeindruckende Zahl zu erhalten (siehe den Megahertz Mythos). Leistungs-Engineering Performance Engineering (APE) ist eine spezifische Methode im Leistungstechnikbereich, die die mit der Anwendung verbundenen Herausforderungen in zunehmend verteilten mobilen, Cloud- und terrestrischen IT-Umgebungen bewältigen soll. Sie umfasst die Rollen, Fähigkeiten, Aktivitäten, Praktiken, Werkzeuge und Bereitstellungsmöglichkeiten, die in jeder Phase des Anwendungszyklus angewendet werden, um einen Antrag zu erstellen, umzusetzen und operationell zu unterstützen, um nicht funktionsfähige Leistungsanforderungen zu erfüllen. Leistungsindikatoren (Maßnahmen) umfassen Verfügbarkeit, Reaktionszeit, Kanalkapazität, verspätete Fertigstellung, Servicezeit, Bandbreite, Durchsatz, relative Effizienz, Skalierbarkeit, Leistung pro Kilowatt, Kompressionskoeffizient, Lehrpfadlänge und Geschwindigkeit. CPU Benchmarks sind verfügbar. Verfügbarkeit eines Systems wird in der Regel als Faktor seiner Zuverlässigkeit gemessen, da die Zuverlässigkeit erhöht, so dass die Verfügbarkeit (d. h. weniger Zeit) gewährleistet ist. Verfügbarkeit eines Systems kann auch durch die Strategie erhöht werden, sich auf die Verbesserung der Testfähigkeit und der Aufrechterhaltungsfähigkeit zu konzentrieren und nicht auf die Zuverlässigkeit. Verbesserung der Erhaltungsfähigkeit ist im Allgemeinen einfacher als Zuverlässigkeit. Nachhaltigkeitsvoranschläge (Verbesserungsraten) sind ebenfalls allgemein genauer. Da die Unsicherheiten in den Zuverlässigkeitsvoranschlägen in den meisten Fällen sehr groß sind, ist es wahrscheinlich, das Problem der Verfügbarkeit (Versorgungssicherheit) zu beherrschen, auch wenn die Erhaltungsniveaus sehr hoch sind. Antwortzeit ist der Gesamtbetrag der Zeit, die er auf einen Dienstleistungsersuchen trifft. In Computing kann dieser Dienst jede Arbeitseinheit von einer einfachen Scheibe IO sein, um eine komplexe Webseite zu beladen. Antwortzeit ist die Summe von drei Zahlen: Service-Zeit - Wie lange dauert es, die gewünschten Arbeiten durchzuführen. Wartezeit - Wie lange der Antrag auf Warteschlangen warten muss, bevor er läuft. Sendezeit – Wie lange dauert es, den Antrag an den Computer zu stellen, der die Arbeit und die Antwort wieder an den Anmelder geht. Verarbeitungsgeschwindigkeit Die meisten Verbraucher holen eine Computerarchitektur (normalerweise Intel IA32 Architektur) ein, die eine große Basis vorbeugender Software betreiben kann. relativ uninfiziert auf Computer-Benchmarks, einige von ihnen holen eine bestimmte CPU auf der Grundlage der Betriebsfrequenz (siehe Megahertz Myth). Manche Systemdesigner bauen Parallelcomputer auf CPUs auf der Geschwindigkeit pro Dollar. Kanalkapazität ist die engste Obergrenze für die Informationsrate, die zuverlässig über einen Kommunikationskanal übertragen werden können. Durch den lauteren Kanalcode desorem ist die Kanalkapazität eines bestimmten Kanals die Begrenzung der Informationsrate (in Einheiten von Informationen pro Einheitszeit), die mit willkürlicher Fehlerwahrscheinlichkeit erreicht werden kann. Informationstheorie, die von Claude E. Shannon während des Zweiten Weltkriegs entwickelt wurde, definiert den Begriff der Kanalkapazität und stellt ein mathematisches Modell vor, mit dem man ihn berechnen kann. Kernergebnis ist, dass die Kapazität des Kanals, wie oben definiert, durch die maximalen gegenseitigen Informationen zwischen der Eingabe und dem Output des Kanals, wo die Maximierung im Hinblick auf die Inputverteilung erfolgt, gewährleistet ist. Letztes Ende ist eine Verzögerung zwischen der Ursache und der Wirkung einiger physischer Veränderungen im System. Missstand ist das Ergebnis der begrenzten Geschwindigkeit, mit der jede physische Interaktion stattfinden kann. Diese Geschwindigkeit ist immer niedriger oder entspricht der Lichtgeschwindigkeit. Jedes physische System, das räumliche Dimensionen von Null hat, wird daher eine Art spät erleben. Die genaue Definition von verspäteter Wirkung hängt von dem beobachteten System und der Art der Stimulierung ab. In der Kommunikation wird der niedrigere Grenzwert durch das Medium für Kommunikation bestimmt. In zuverlässigen zweispurigen Kommunikationssystemen beschränkt sich die Verspätung auf den maximalen Satz, den die Informationen übermittelt werden können, da es häufig eine Begrenzung der in einem Moment befindlichen Informationen gibt. Im Bereich der Mensch-Maschine-Interaktion hat sich die Zufriedenheit der Nutzer und die Benutzerfreundlichkeit erheblich verändert. Computer setzen Anweisungen ein, die als Prozess bezeichnet werden. In Betriebssystemen kann die Durchführung des Verfahrens verschoben werden, wenn auch andere Prozesse durchgeführt werden. Darüber hinaus kann das Betriebssystem bei der Durchführung der Maßnahme, die der Prozess verfolgt, planbar sein. Man braucht beispielsweise einen Prozess, der vorschreibt, dass die Spannungsleistung einer Computerkarte hoch niedrig und so auf einer Rate von 1000 Hz festgesetzt wird. Das Betriebssystem kann sich für die Anpassung der Planungspläne für jeden Übergang (hoch- oder niedrig) entscheiden, die auf einer internen Uhr basieren. Spätestens ist die Verzögerung zwischen der Prozessanweisung, die den Übergang und die Hardware beleben, die die Spannung von hoher bis niedrigem oder niedrigem bis hohen Niveau tatsächlich überführt. System-Designer bauen Echtzeit-.-Systeme wollen eine schlechte Antwort garantieren. Das ist leichter zu tun, wenn die CPU verspätet unterbrecht hat und wann sie eine deterministische Antwort hat. Bandbreite In Computernetzen ist Bandbreite eine Messung der Bitrate der verfügbaren oder verbrauchten Datenkommunikationsressourcen, ausgedrückt in Bits pro Sekunde oder mehrfach (bit/s, kbit/s, Mbit/s, Gbit/s usw.). Bandbreite definiert manchmal den Netto-Grenzsatz (aka.peak Bitrate, Informationsrate oder sinnvolle physikalische Schicht), Kanalkapazität oder den maximalen Durchsatz eines logischen oder physischen Kommunikationswegs in einem digitalen Kommunikationssystem. Bandbreitentests messen beispielsweise den maximalen Durchsatz eines Computernetzes. Grund für diese Nutzung ist, dass nach dem Recht von Hartley die maximale Datenquote eines physischen Kommunikationsnetzes proportional zu ihrer Bandbreite in Hertz ist, die manchmal Frequenzbandbreite, spektrale Bandbreite, RF Bandbreite, Signalbreite oder analoge Bandbreite genannt wird. Byput Insgesamt ist der Durchsatz der Produktionsrate oder der Satz, bei dem etwas verarbeitet werden kann. In Kommunikationsnetzen ist der Durchsatz im Wesentlichen Synonym für den digitalen Bandbreitenverbrauch. In drahtlosen Netzen oder zellulären Kommunikationsnetzen ist das System spektrale Effizienz in Bit/s/Hz/Gebietseinheit, Bit/s/site oder Bit/s/Hz/cell der maximale Systemdurchsatz (Aggregatedurchsatz) geteilt durch die analoge Bandbreite und eine gewisse Messung des Systemabdeckungsbereichs. In integrierten Schaltkreisen hat oft ein Block in einem Datenflussdiagramm einen einzigen Input und eine Einzelleistung und betreibt auf gesonderten Informationspaketen. Beispiele solcher Blöcke sind FFT-Module oder binäre Multiplikatoren. Da die Einheiten des Durchsatzes die Gegenseitigkeit der Einheit für propagationsverzögerungen sind, die 'Zins je Botschaft' oder 'Sekunden pro Output' sind, kann der Durchsatz verwendet werden, um ein rechnerisches Gerät mit einer eigens eingerichteten Funktion wie ein ASIC oder eingebetteter Prozessor auf einen Kommunikationskanal zu beziehen, das die Systemanalyse vereinfacht. relative Effizienzsteigerung ist die Fähigkeit eines Systems, eines Netzes oder eines Prozesses, um einen wachsenden Arbeitsumfang in geeigneter Weise zu bewältigen oder seine Fähigkeit zu erweitern, um dem wachsenden Stromverbrauch Rechnung zu tragen. Strommenge, die vom Computer verwendet wird (Stromverbrauch). Dies ist besonders wichtig für Systeme mit begrenzten Energiequellen wie Solar, Batterien, menschliche Kraft. Leistung pro watt System-Designer bauen parallele Computer wie die Hardware von Google auf, die CPUs auf deren Geschwindigkeit pro Kilo der Leistung antreiben, da die Kosten für die Leistung der CPU die Kosten der CPU selbst überwiegen. Für Raumflug Computer ist die Verarbeitungsgeschwindigkeit pro Kilowatt-Quote ein nützliches Leistungskriterium als die Rohverarbeitungsgeschwindigkeit. Kompressionsdichte ist sinnvoll, weil sie zur Verringerung der Ressourcennutzung beiträgt, wie die Speicherfläche oder die Übertragungskapazitäten. Diese zusätzliche Verarbeitung verursacht rechnerische oder sonstige Kosten durch Dekompression; diese Situation ist weit von einem freien Mittagessen entfernt. Datenkomprimierung unterliegt einem raum-time-Komplex. Größe und Gewicht Dies ist ein wichtiges Leistungsmerkmal mobiler Systeme, von den Smartphones, die Sie in Ihrer Tasche auf die tragbaren eingebetteten Systeme in einem Raumfahrzeug halten. Umweltverträglichkeit Auswirkungen eines Computers oder Computers auf die Umwelt, während der Herstellung und des Recyclings sowie während der Nutzung. Messungen werden mit den Zielen der Reduzierung von Abfällen, der Reduzierung gefährlicher Materialien und der Minimierung des ökologischen Fußabdrucks eines Computers getroffen. Grafische Zählung Die Transistor-Bevölkerung ist die Zahl der Transisten auf einem integrierten Schaltkreis (IC). Dynamische Zählung ist die häufigste Maßnahme der IC-Komplexation. Benchmarks Da es so viele Programme gibt, um eine CPU auf allen Leistungsbereichen zu testen, wurden Benchmarks entwickelt. Die bekanntesten Benchmarks sind die SPECint und SPECfp Benchmarks, die von der Standard Performance Evaluation Corporation und dem Zertifizierungs-Mark-Benchmark entwickelt wurden, das vom Embedded Microprozessor Benchmark Consortium EEMBC entwickelt wurde. Leistungsprüfung von Software Leistungstests im Software-Engineering sind in der Regel durchgeführt, um festzustellen, wie ein System in Bezug auf Reaktionsfähigkeit und Stabilität unter einer bestimmten Arbeitsbelastung funktioniert. Sie kann auch dazu dienen, andere Qualitätsmerkmale des Systems zu untersuchen, zu messen, zu validieren oder zu überprüfen, wie etwa die Skalierbarkeit, Zuverlässigkeit und Ressourcennutzung. Leistungstests sind ein Teil des Leistungs-Engineering, einer aufstrebenden computerwissenschaftlichen Praxis, die darauf abzielt, Leistung in die Umsetzung, Gestaltung und Architektur eines Systems zu entwickeln. Profil (Leistungsanalyse)In Software-Engineering, Profilierung ("Programmprofil," "Softwareprofilierung") ist eine Form dynamischer Programmanalyse, die Maßnahmen zum Beispiel den Raum (memory) oder die zeitliche Komplexität eines Programms, die Nutzung besonderer Anweisungen oder Häufigkeit und Dauer von Funktionsgesprächen umfasst. Die häufigste Verwendung von Profilinformationen ist die Unterstützung der Programmoptimierung. Profilierung wird erreicht, indem entweder der Programmiercode oder seine binbare Form mittels eines Werkzeugs, der als Profiler (oder Code-Profiler bezeichnet) bezeichnet wird. Eine Reihe unterschiedlicher Techniken können von Profilern genutzt werden, wie beispielsweise auf Event-Basis, Statistiken, Instrumente und Simulationsmethoden. Leistungsabstimmung Leistungsanzeiger ist die Verbesserung der Systemleistung. In der Regel handelt es sich um einen Computerantrag, aber die gleichen Methoden können auf die wirtschaftlichen Märkte, die Bürokratie oder andere komplexe Systeme angewendet werden. Die Motivation für solche Aktivitäten ist ein Leistungsproblem, das wirklich oder erwartet werden kann. Die meisten Systeme werden auf eine höhere Belastung mit einem gewissen Grad abnehmender Leistung reagieren. Ein System ist in der Lage, eine höhere Belastung zu akzeptieren, wird als Skalierbarkeit bezeichnet und die Änderung eines Systems, um eine höhere Belastung zu bewältigen, ist für Leistungsabstimmung gleich. Systematische Abstimmung folgt diesen Schritten: Bewertung des Problems und Festlegung numerischer Werte, die akzeptables Verhalten kategorisieren. Messung der Leistung des Systems vor Änderung. Identifizierung des Teils des Systems, der für die Verbesserung der Leistung entscheidend ist. Dies ist der Engpass. Änderung dieses Teils des Systems zur Beseitigung der Engpässe. Messung der Leistung des Systems nach Änderung. Wenn die Änderung die Leistung besser macht, nimmt sie an. Wenn die Änderung die Leistung schlechter macht, drückt sie auf die Art und Weise zurück. Perceived Performance Perceived Performance in der Computertechnik verweist darauf, wie schnell eine Software-Funktion ihre Aufgabe erfüllen kann. Das Konzept gilt hauptsächlich für die Akzeptanz von Nutzern. Länge der Zeit, in der ein Antrag aufgenommen wird, oder eine Datei, die heruntergeladen werden soll, wird nicht schneller gemacht, indem ein Startbildschirm (siehe Abspielbildschirm) oder eine Datei-Fortschrittsbox gezeigt wird. Man erfüllt jedoch einige menschliche Bedürfnisse: Es erscheint schneller für den Nutzer und bietet eine visuelle Freiheit, um sie über das System zu informieren. In den meisten Fällen erhöht sich die tatsächliche Leistung, wenn die tatsächliche Leistung aufgrund körperlicher Grenzen nicht erhöht werden kann, können Techniken zur Erhöhung der wahrgenommenen Leistung genutzt werden. Leistungsvergleich Die Gesamtzahl der Zeit (t) für die Durchführung eines bestimmten Benchmark-Programms ist t = N C f WELLdisplaystyle t= fasertfrac NC}{f  oder gleichwertig P = I f N {\displaystyle P= cutfrac If}{N, wo P = 1 t {\text P= SSOfrac 1}{t "die Leistung" im Hinblick auf die Zeit bis zum Excute N VILLEtextstyle N} ist die Zahl der tatsächlich ausgeführten Anweisungen (die Unterrichtslänge). N. Der Wert der N kann entweder genau bestimmt werden, indem ein Anweisungsset Simulator (falls vorhanden) oder eine Schätzung verwendet wird – selbst auf der Grundlage der geschätzten oder tatsächlichen Frequenzverteilung von Inputvariablen und der Prüfung des generierten Maschinencodes von einem HLL-Berater. Es kann nicht aus der Anzahl der HLL-Quellencodes bestimmt werden. N ist nicht von anderen Prozessen betroffen, die im selben Prozessor laufen. Hier kommt es darauf an, dass Hardware normalerweise keine Spur von (oder zumindest leicht zugänglichen) N für ausgeführte Programme hält. Der Wert lässt sich daher nur durch Anweisungsbefugnis bestimmen, die selten angewandt wird. f {\textstyle f} ist die Uhr in Zyklen pro Sekunde. C = 1 I 7.8textstyle C= Finanzfrac 1 1I ist der durchschnittliche Zyklus pro Anleitung (CPI) für diese Benchmark. I = 1 C {\textstyle I= faserfrac 1CC ist die durchschnittliche Anleitung pro Zyklus (IPC) für diese Benchmark. Selbst auf einer Maschine kann ein anderer Pool oder ein anderer Pool mit unterschiedlichen Optimierungsschaltern N und CPI – die Benchmarks, die schneller ausgeführt werden, wenn der neue Pool N oder C verbessern kann, ohne die andere schlechter zu machen, aber oft gibt es einen Zwischenspiel zwischen ihnen, beispielsweise um einige komplizierte Anweisungen zu verwenden, die lange Zeit zur Ausführung oder zur Verwendung von Anweisungen, die sehr schnell ausgeführt werden, auch wenn es mehr dauert, um den Benchmark auszuführen? Ein CPU-Designer ist oft erforderlich, um eine spezielle Anleitung durchzuführen, und kann daher N. Manchmal nicht ändern, indem ein Designer die Leistung verbessern kann, indem er erhebliche Verbesserungen in f (mit Techniken wie tieferen Rohrleitungen und schnelleren Caches) bewirkt, während (sehr) nicht zu viel C – was zu einem schnelleren Design von CPU führt. Manchmal konzentriert sich ein Designer auf die Verbesserung der Leistung, indem er erhebliche Verbesserungen bei CPI (mit Techniken wie der Off-of-order-Durchführung, Superscalar-Prozessoren, größeren Caches, Caches mit verbesserten Treffern, verbesserte Branchenvorhersage, spekulative Ausführung usw.) erzielt, während (sehrlich) nicht zu viel Uhr Frequenzen verloren gehen – was zu einem Gehirn-Prozessor führt. Für eine bestimmte Unterrichtsform (und somit feste N) und Halbleiterverfahren erfordert die maximale Einmalleistung (1/t) ein Gleichgewicht zwischen Gehirntechniken und Geschwindigkeitstechniken. Lesen Sie auch die Leistung von Algorithmic Effizienz Computern durch Bestellungen von Magnitude Network Performance latensorientierter Prozessorarchitekturoptimierung (Computer Science) RAM-Vertriebssatz komplette Anleitung für Hardware Beschleunigungs-Geschwindigkeits-Ersatzpolitik. Links