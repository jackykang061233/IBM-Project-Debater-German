Computational Neuroscience (auch als theoretische Neurowissenschaften oder mathematische Neurowissenschaften bekannt) ist ein Zweig der Neurowissenschaften, der mathematische Modelle, theoretische Analyse und Abstraktionen des Gehirns verwendet, um die Prinzipien zu verstehen, die die Entwicklung, Struktur, Physiologie und kognitive Fähigkeiten des Nervensystems bestimmen. Computational Neuroscience verwendet rechnerische Simulationen, um mathematische Modelle zu validieren und zu lösen, und so kann man als Unterfeld der theoretischen Neurowissenschaften sehen; die beiden Felder sind jedoch oft synonym. Der Begriff mathematische Neurowissenschaften wird manchmal auch verwendet, um die quantitative Natur des Feldes zu betonen. Die rechnerische Neurowissenschaft konzentriert sich auf die Beschreibung biologisch plausibler Neuronen (und neuronale Systeme) und deren Physiologie und Dynamik und beschäftigt sich daher nicht direkt mit biologisch unrealistischen Modellen, die in der Konnektivismus-, Steuertheorie, Kybernetik, quantitativer Psychologie, maschinellem Lernen, künstlichen neuronalen Netzwerken, künstlicher Intelligenz und computergestützter Lerntheorie verwendet werden; obwohl die gegenseitige Inspiration existiert und manchmal gibt es keine strenge Grenze zwischen den Bereichen, mit Modellabstraktion in Modelle in der theoretischen Neurowissenschaften sind darauf ausgerichtet, die wesentlichen Merkmale des biologischen Systems auf mehrere räumlich-temporale Skalen, von Membranströmen und chemische Kopplung über Netzwerkschwingungen, Säulen- und topographische Architektur, Nuklei, bis hin zu psychologischen Fakultäten wie Gedächtnis, Lernen und Verhalten zu erfassen. Diese rechnerischen Modelle rahmen Hypothesen, die direkt durch biologische oder psychologische Experimente getestet werden können. Geschichte Der Begriff "computational neuroscience" wurde von Eric L. Schwartz eingeführt, der eine 1985 in Carmel, Kalifornien veranstaltete Konferenz auf Ersuchen der Systems Development Foundation organisierte, um eine Zusammenfassung des aktuellen Status eines Feldes zu liefern, das bis zu diesem Punkt von einer Vielzahl von Namen wie neuronalen Modellierung, Gehirntheorie und neuronalen Netzwerken bezeichnet wurde. Das Verfahren dieser Definitionssitzung wurde 1990 als Buch Computational Neuroscience veröffentlicht. Die erste der jährlichen offenen internationalen Treffen konzentrierte sich auf Computational Neuroscience wurde 1989 von James M. Bower und John Miller in San Francisco, Kalifornien, organisiert. Das erste Graduiertenausbildungsprogramm in der rechnerischen Neurowissenschaften wurde 1985 als Computational and Neural Systems Ph.D Programm am California Institute of Technology organisiert. Die frühen historischen Wurzeln des Feldes lassen sich auf die Arbeit von Menschen wie Louis Lapicque, Hodgkin & Huxley, Hubel und Wiesel und David Marr zurückführen. Lapicque stellte das Integrations- und Brandmodell des Neurons in einem halbnalen Artikel vor, der 1907 veröffentlicht wurde, ein Modell, das wegen seiner Einfachheit immer noch für künstliche neuronale Netzwerkstudien populär ist (siehe eine kürzliche Überprüfung). Etwa 40 Jahre später entwickelte Hodgkin & Huxley die Spannungsklemme und schuf das erste biophysikalische Modell des Aktionspotentials. Hubel & Wiesel entdeckte, dass Neuronen in der primären visuellen Kortex, der erste kortikale Bereich, um Informationen aus der Netzhaut zu verarbeiten, orientierte Aufnahmefelder haben und in Spalten organisiert werden. David Marrs Arbeit konzentrierte sich auf die Wechselwirkungen zwischen Neuronen und deutete auf rechnerische Ansätze hin, wie funktionelle Gruppen von Neuronen im Hippocampus und Neocortex interagieren, speichern, verarbeiten und Informationen übermitteln. Die rechnerische Modellierung von biophysikalisch realistischen Neuronen und Dendriten begann mit der Arbeit von Wilfrid Rall, mit dem ersten multikompartmentalen Modell mit Kabeltheorie. Wichtige Themen Die Forschung in der rechnerischen Neurowissenschaften kann grob in mehrere Untersuchungslinien eingeteilt werden. Die meisten rechnerischen Neurowissenschaftler arbeiten eng mit Experimentalisten zusammen, um neue Daten zu analysieren und neue Modelle biologischer Phänomene zu synthetisieren. Einzelneuron-Modellierung Selbst einzelne Neuron hat komplexe biophysikalische Eigenschaften und kann Berechnungen durchführen (z.B.). Hodgkin und Huxleys ursprüngliches Modell benutzten nur zwei spannungsempfindliche Ströme (Voltage sensitive Ionenkanäle sind Glykoproteinmoleküle, die sich durch die Lipid-Bischicht erstrecken, wodurch Ionen unter bestimmten Bedingungen durch das Axolemma durchlaufen können), das schnell wirkende Natrium und das nach innen regulierende Kalium. Obwohl es bei der Vorhersage des Timings und der qualitativen Merkmale des Aktionspotenzials erfolgreich war, konnte es dennoch nicht eine Reihe wichtiger Merkmale wie Anpassung und Shunting vorhersagen. Wissenschaftler glauben nun, dass es eine Vielzahl von spannungsempfindlichen Strömen gibt, und die Auswirkungen der unterschiedlichen Dynamiken, Modulationen und Empfindlichkeit dieser Ströme ist ein wichtiges Thema der rechnerischen Neurowissenschaften. Die rechnerischen Funktionen komplexer Dendriten werden ebenfalls intensiv untersucht. Es gibt einen großen Literaturkörper, wie verschiedene Ströme mit geometrischen Eigenschaften von Neuronen interagieren. Einige Modelle verfolgen auch biochemische Pfade in sehr kleinen Skalen wie Spines oder synaptische Clefts. Es gibt viele Softwarepakete, wie GENESIS und NEURON, die eine schnelle und systematische silico Modellierung realistischer Neuronen ermöglichen. Blue Brain, ein Projekt von Henry Markram aus der École Polytechnique Fédérale de Lausanne, zielt darauf ab, eine biophysikalisch detaillierte Simulation einer kortikalen Spalte auf dem Blue Gene Supercomputer zu konstruieren. Die Modellierung des Reichtums an biophysikalischen Eigenschaften im Ein-Neuron-Skala kann Mechanismen liefern, die als Bausteine für die Netzwerkdynamik dienen. Detaillierte Neuron-Beschreibungen sind jedoch rechnerisch teuer und dies kann das Streben nach realistischen Netzwerkuntersuchungen handhaben, wo viele Neuronen simuliert werden müssen. So stellen Forscher, die große neuronale Schaltungen studieren, typischerweise jedes Neuron und Synapse mit einem künstlich einfachen Modell dar, und ignorieren viel des biologischen Details. Es gibt also einen Antrieb, vereinfachte Neuronmodelle zu produzieren, die eine signifikante biologische Treue bei einem niedrigen rechnerischen Überkopf behalten können. Algorithmen wurden entwickelt, um treue, schneller laufende, vereinfachte Surrogat-Neuron-Modelle aus rechnerisch teuren, detaillierten Neuron-Modellen zu produzieren. Entwicklung, axonale Musterung und Anleitung Die Computational Neuroscience zielt darauf ab, eine Vielzahl von Fragen anzugehen. Wie bilden sich Axone und Dendrites während der Entwicklung? Wie wissen Axonen, wo man diese Ziele anstrebt und wie man diese Ziele erreicht? Wie wandern Neuronen in die richtige Position in den zentralen und peripheren Systemen? Wie bilden sich Synapsen? Wir wissen von der Molekularbiologie, dass verschiedene Teile des Nervensystems verschiedene chemische Ellen freisetzen, von Wachstumsfaktoren zu Hormonen, die das Wachstum und die Entwicklung von funktionellen Verbindungen zwischen Neuronen modulieren und beeinflussen. Theoretische Untersuchungen zur Bildung und Musterung synaptischer Verbindung und Morphologie sind immer noch nascent. Eine Hypothese, die vor kurzem etwas Aufmerksamkeit geraubt hat, ist die minimale Verdrahtungshypothese, die postuliert, dass die Bildung von Axonen und Dendriten effektiv minimiert Ressourcenzuweisung unter Beibehaltung der maximalen Informationsspeicherung. Frühe Modelle zur sensorischen Verarbeitung, die in einem theoretischen Rahmen verstanden werden, werden Horace Barlow gutgeschrieben. Etwas ähnlich wie die im vorhergehenden Abschnitt beschriebene minimale Verdrahtungshypothese, verstanden Barlow die Verarbeitung der frühen sensorischen Systeme als eine Form einer effizienten Codierung, wobei die Neuronen kodierten Informationen, die die Anzahl der Spikes minimierten. Experimentelle und rechnerische Arbeiten haben diese Hypothese seither in einer oder anderen Form unterstützt. Zum Beispiel der visuellen Verarbeitung manifestiert sich eine effiziente Codierung in Form einer effizienten Raumcodierung, Farbcodierung, zeitlich/motionscodierung, Stereocodierung und Kombinationen davon. Weiter entlang des visuellen Weges, sogar die effizient kodierten visuellen Informationen ist zu viel für die Kapazität des Informationsengpasses, der visuellen Aufmerksamkeit Engpass. Eine anschließende Theorie, V1 Saliency Hypothesis (V1SH,) wurde auf exogene Aufmerksamkeitsauswahl eines Anteils an visueller Eingabe für die Weiterverarbeitung entwickelt, der durch eine Bottom-up Saliency Map in der primären visuellen Kortex geführt wird. Die aktuelle Forschung in der sensorischen Verarbeitung ist auf eine biophysikalische Modellierung verschiedener Teilsysteme und eine theoretischere Modellierung der Wahrnehmung aufgeteilt. Aktuelle Wahrnehmungsmodelle haben vorgeschlagen, dass das Gehirn eine Form der Bayesischen Inferenz und Integration verschiedener sensorischer Informationen bei der Generierung unserer Wahrnehmung der physischen Welt durchführt. Motorsteuerung Viele Modelle der Art, wie das Gehirn die Bewegung kontrolliert wurden. Dazu gehören Modelle der Verarbeitung im Gehirn wie die Rolle des Cerebellums zur Fehlerkorrektur, das Erlernen von Fähigkeiten im Motorkortex und der Basalganglia oder die Kontrolle des vestibulo okularen Reflexes. Dazu gehören auch viele normative Modelle, wie die des Bayesischen oder optimalen Kontrollgeschmacks, die auf der Idee gebaut werden, dass das Gehirn seine Probleme effizient löst. Erinnerung und synaptische Plastizität Frühere Modelle des Gedächtnisses basieren in erster Linie auf den Postulaten des hebbianischen Lernens. Biologische relevante Modelle wie Hopfield net wurden entwickelt, um die Eigenschaften des assoziativen (auch als inhaltlich adressierbaren) Gedächtnisses, die in biologischen Systemen auftreten, zu adressieren. Diese Versuche konzentrieren sich in erster Linie auf die Bildung von mittel- und langfristigem Gedächtnis, lokalisieren im Hippocampus. Modelle des Arbeitsspeichers, die auf Theorien von Netzwerkschwingungen und persistenter Aktivität angewiesen sind, wurden gebaut, um einige Merkmale der präfrontalen Kortex im kontextbezogenen Speicher zu erfassen. Weitere Modelle sehen die enge Beziehung zwischen der Basalganglia und der präfrontalen Cortex und wie das zum Arbeitsgedächtnis beiträgt. Eines der größten Probleme im neurophysiologischen Gedächtnis ist, wie es durch mehrere Zeitskala beibehalten und geändert wird. Unstabile Synapsen sind einfach zu trainieren, aber auch anfällig für stochastische Störungen. Stabile Synapsen vergessen weniger leicht, aber sie sind auch schwerer zu konsolidieren. Eine jüngste rechnerische Hypothese beinhaltet Kaskaden der Plastizität, die Synapsen zu mehreren Zeitskala funktionieren lassen. Stereochemisch detaillierte Modelle der Acetylcholin-Rezeptor-basierten Synapse mit der Monte Carlo-Methode, die zur Zeitskala von Mikrosekunden arbeitet, wurden gebaut. Es ist wahrscheinlich, dass Rechenwerkzeuge stark zu unserem Verständnis beitragen, wie Synapsen funktionieren und sich in Bezug auf externe Reize in den kommenden Jahrzehnten ändern. Verhaltensweisen von Netzwerken Biologische Neuronen sind komplex, wiederkehrend miteinander verbunden. Diese Verbindungen sind, anders als die meisten künstlichen neuronalen Netze, spärlich und in der Regel spezifisch. Es ist nicht bekannt, wie Informationen über solche sparsam vernetzten Netzwerke übertragen werden, obwohl bestimmte Bereiche des Gehirns, wie z.B. der Visuelle Kortex, im Detail verstanden werden. Es ist auch unbekannt, was die Rechenfunktionen dieser spezifischen Konnektivitätsmuster sind, wenn überhaupt. Die Interaktionen von Neuronen in einem kleinen Netzwerk können oft auf einfache Modelle wie das Ising-Modell reduziert werden. Die statistischen Mechaniken solcher einfacher Systeme sind theoretisch gut charakterisiert. Es gab einige jüngste Beweise, die darauf hindeuten, dass die Dynamik willkürlicher neuronaler Netzwerke auf paarweise Interaktionen reduziert werden kann. Es ist jedoch nicht bekannt, ob eine solche beschreibende Dynamik eine wichtige Rechenfunktion verleiht. Mit der Entstehung von Zweiphotonenmikroskopie und Kalzium-Bildgebung haben wir nun leistungsstarke experimentelle Methoden, mit denen die neuen Theorien über neuronale Netzwerke getestet werden. In manchen Fällen lassen sich die komplexen Wechselwirkungen zwischen Hemm- und Erregerneuronen mit der mittleren Feldtheorie vereinfachen, was das Populationsmodell von neuronalen Netzen hervorruft. Während viele Neurotheoretiker solche Modelle mit reduzierter Komplexität bevorzugen, argumentieren andere, dass das Aufdecken von strukturellen funktionellen Beziehungen hängt davon ab, wie viel neuronale und Netzwerkstruktur wie möglich. Derartige Modelle werden typischerweise in großen Simulationsplattformen wie GENESIS oder NEURON gebaut. Es gab einige Versuche, einheitliche Methoden bereitzustellen, die diese Komplexität überbrücken und integrieren. Visuelle Aufmerksamkeit, Identifikation und Kategorisierung Visuelle Aufmerksamkeit kann als eine Reihe von Mechanismen beschrieben werden, die eine gewisse Verarbeitung auf eine Teilmenge an eingehenden Reize beschränken. Achtungsmechanismen formen, was wir sehen und was wir tun können. Sie ermöglichen die gleichzeitige Auswahl einiger (vorzugsweise relevanter) Informationen und Hemmung anderer Informationen. Um eine konkretere Beschreibung des Mechanismus zu haben, der der visuellen Aufmerksamkeit und der Bindung von Merkmalen zugrunde liegt, wurde eine Reihe von rechnerischen Modellen vorgeschlagen, die psychophysische Erkenntnisse erläutern sollen. In der Regel postulieren alle Modelle die Existenz einer Saliency- oder Prioritätskarte zur Registrierung der potenziell interessanten Bereiche der retinalen Eingabe, und ein Gating-Mechanismus zur Verringerung der Menge an eingehenden visuellen Informationen, so dass die begrenzten rechnerischen Ressourcen des Gehirns es handhaben kann. Eine beispielhafte Theorie, die weitgehend verhaltens- und physiologisch getestet wird, ist die V1 Saliency Hypothese, dass in der primären visuellen Kortex eine Bottom-up-Salzency-Karte erstellt wird, um die Aufmerksamkeit exogen zu führen. Die rechnerische Neurowissenschaft bietet einen mathematischen Rahmen für die Untersuchung der Mechanismen, die an der Hirnfunktion beteiligt sind, und ermöglicht eine vollständige Simulation und Vorhersage neuropsychologischer Syndrome. Kognition, Diskriminierung und Lernen Die rechnerische Modellierung höherer kognitiver Funktionen hat erst kürzlich begonnen. Experimentelle Daten stammen vor allem aus der Einzeleinheit-Aufzeichnung in Primaten. Die frontale Lobe und Parietal Lobe Funktion als Integratoren von Informationen aus mehreren sensorischen Modalitäten. Es gibt einige vorläufige Vorstellungen darüber, wie einfache gegenseitig inhibierende Funktionskreise in diesen Bereichen biologisch relevante Berechnungen durchführen können. Das Gehirn scheint in bestimmten Zusammenhängen besonders gut diskriminieren und anpassen zu können. Zum Beispiel scheinen Menschen eine enorme Fähigkeit zu haben, Gesichter zu merken und zu erkennen. Eines der Hauptziele der rechnerischen Neurowissenschaften ist es, zu deaktivieren, wie biologische Systeme diese komplexen Berechnungen effizient durchführen und diese Prozesse im Aufbau intelligenter Maschinen möglicherweise replizieren. Die großen Organisationsprinzipien des Gehirns werden von vielen Bereichen beleuchtet, darunter Biologie, Psychologie und klinische Praxis. Integrative Neurowissenschaften versuchen, diese Beobachtungen durch einheitliche beschreibende Modelle und Datenbanken von Verhaltensmaßnahmen und Aufnahmen zu konsolidieren. Dies sind die Grundlagen für eine quantitative Modellierung der großen Gehirnaktivität. Das Computational Representational Understanding of Mind (CRUM) ist ein weiterer Versuch, die menschliche Wahrnehmung durch simulierte Prozesse wie erworbene regelbasierte Systeme in der Entscheidungsfindung und die Manipulation visueller Darstellungen in der Entscheidungsfindung zu modellieren. Bewußtsein Eines der letzten Ziele der Psychologie/Nurowissenschaften ist es, die Alltagserfahrung des bewussten Lebens erklären zu können. Francis Crick, Giulio Tononi und Christof Koch hat einige Versuche unternommen, konsequente Rahmenbedingungen für die zukünftige Arbeit in neuralen Korrelaten des Bewusstseins (NCC) zu formulieren, obwohl ein Großteil der Arbeit in diesem Bereich spekulativ bleibt. Konkret hat Crick den Bereich der Neurowissenschaften gewarnt, um nicht auf Themen zuzugehen, die traditionell der Philosophie und Religion überlassen werden. Computational Clinic Neuroscience Computational Clinic Neuroscience ist ein Gebiet, das Experten in Neurowissenschaften, Neurologie, Psychiatrie, Entscheidungswissenschaften und rechnerische Modellierung zusammenbringt, um Probleme bei neurologischen und psychiatrischen Erkrankungen quantitativ zu definieren und zu untersuchen und Wissenschaftler und Kliniker zu trainieren, die diese Modelle auf Diagnose und Behandlung anwenden möchten. Siehe auch Hinweise und Hinweise Bibliographie Chklovskii DB (2004.) "Synaptische Konnektivität und neuronale Morphologie: zwei Seiten derselben Münze".Neuron.43 (5): 609–17.doi:10.1016/j.neuron.2004.08.012.PMID 15339643.S2CID 16217065. Sejnowski, Terrence J.; Churchland, Patricia Smith (1992). Das rechnerische Gehirn. Cambridge, Mass: MIT Press.ISBN 978-0-262-03188-2.Gerstner, W.; Kistler, W.; Naud, R.; Paninski, L. (2014). Neuronale Dynamik. Cambridge, UK: Cambridge University Press.ISBN 9781107447615. Dayan P.; Abbott, L. F. (2001). Theoretische Neurowissenschaften: rechnerische und mathematische Modellierung von neuronalen Systemen. Cambridge, Mass: MIT Drücken. ISBN 978-0-262-04199-7.Eliasmith, Chris; Anderson, Charles H. (2003). Neuraltechnik: Darstellung, Berechnung und Dynamik in neurobiologischen Systemen. Cambridge, Mass: MIT Presse.ISBN 978-0-262-05071-5.Hodgkin AL, Huxley AF (28. August 1952). "Eine quantitative Beschreibung des Membranstroms und seiner Anwendung zur Durchführung und Anregung in Nerven."J Physiol.117 (4): 500–44. doi:10.1113/jphysiol.1952.sp004764.PMC 1392413.PMID 12991237.William Bialek; Rieke, Fred; David Warland; Rob de Ruyter van Steveninck (1999). Spikes: Erforschung des Neuralcodes. Cambridge, Mass: MIT.ISBN 978-0-262-68108-7.Schutter, Erik de (2001). Computational Neuroscience: realistische Modellierung für Experimentalisten. Boca Raton: CRC.ISBN 978-0-8493-2068-2.Sejnowski, Terrence J.; Hemmen, J. L. van (2006). 23 Probleme in Systemen Neurowissenschaften. Oxford [Oxfordshire]: Oxford University Press. ISBN 978-0-19-514822-0.Michael A. Arbib; Shun-ichi Amari; Prudence H. Arbib (2002). Das Handbuch der Gehirntheorie und Neural Networks. Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-01197-6.Zhaoping, Li (2014). Vision verstehen: Theorie, Modelle und Daten. Oxford, UK: Oxford University Press. ISBN 978-0199564668. Siehe auch Software BRIAN, ein Python basierter Simulator Budapest Reference Connectome, webbasiertes 3D-Visualisierungstool, um Verbindungen im menschlichen Gehirn Emergent, neurale Simulationssoftware zu durchsuchen. GENESIS, ein allgemeines neuronales Simulationssystem. NEST ist ein Simulator für neurale Netzwerkmodelle, der sich auf die Dynamik, Größe und Struktur von neuronalen Systemen und nicht auf die genaue Morphologie einzelner Neuronen konzentriert. Journal of Mathematical Neuroscience Journal of Computational Neuroscience Neural Computation Cognitive Neurodynamik Frontiers in Computational Neuroscience PLoS Computational Biology Frontiers in Neuroinformatics Conferences Computational and Systems Neuroscience (COSYNE) – ein computergestütztes Neuroscience Meeting mit einem System Neuroscience Focus. Jährliches Computational Neuroscience Meeting (CNS) – ein jährlich rechnerisches Neuroscience Meeting. Computational Cognitive Neuroscience - ein jährlich rechnerisches neurowissenschaftliches Treffen mit einem Fokus auf kognitive Phänomene. Neural Information Processing Systems (NIPS) – eine führende Jahreskonferenz zum größten Teil maschinelles Lernen. Internationale Konferenz über kognitive Neurodynamik (ICCN) – eine jährliche Konferenz. UK Mathematical Neurosciences Meeting– eine jährliche Konferenz, konzentrierte sich auf mathematische Aspekte. Bernstein Conference on Computational Neuroscience (BCCN)– eine jährlich rechnerische Neurowissenschaften Konferenz .]AREADNE Konferenzen – ein zweijähriges Treffen, das theoretische und experimentelle Ergebnisse beinhaltet. Websites Encyclopedia of Computational Neuroscience, Teil von Scholarpedia, ein Online-Experte kurierte Enzyklopädie auf rechnerische Neurowissenschaften und dynamische Systeme Paul Dundas (geboren 1952) ist Gelehrter und Senior Dozent in Sanskrit Sprache und Leiter asiatischer Studien an der Universität Edinburgh. Zu seinen wichtigsten wissenschaftlichen und Forschungsschwerpunkten zählen Jainismus, Buddhismus, die klassische Sanskrit-Literatur und die Middle Indo-Aryan-Philosophie. Er gilt als einer der führenden westlichen Gelehrten in Jain-Studien. Er ist derzeit Mitglied des Rates der Pali Text Society. Bibliographie und Forschungspapiere BibliographieFollowing ist die Teilliste seiner Bücher: Dundas, P. (1992). Die Jains. Bibliothek der religiösen Überzeugungen und Praktiken. London: Routledge.ISBN 978-0-415-26606-2 Dundas, P. (1998). Das Fleisch bei den Hochzeitsfesten: Krņāna, Vegetarismus und Jain Streit.[Toronto]: Centre for South Asian Studies, University of Toronto.OCLC Nummer: 43745945 Alphen, J. v., Pal, P., & Dundas, P. (2000). Schritte zur Befreiung: 2.500 Jahre Jain Kunst und Religion. Antwerpen: Etnografisch Museum.OCLC Nummer: 44834857 Dundas, P. (2007). Geschichte, Schrift und Kontroverse in einer mittelalterlichen Jain-Sekte. Routledge Fortschritte in Jaina Studien. London: Routledge.ISBN 978-0-415-37611-2 OCLC Nummer: 68373250 Dundas, P (2017). Bearbeitung und Übersetzung von Magha's The Killing of Shishupala, Murty Classical Library of India, Harvard University Press. ISBN 978-0-674-66039-7 Forschungsbeiträge und Konferenzen "Conversion to Jainism : Historical Perspectives" in R. Robinson und S. Clarke (Hg. ), Religiöse Umrechnung in Indien: Moden, Motivationen und Bedeutungen, Neu Delhi: Oxford University Press 2003, S. 125–48."Haribhadras Lalitavistara und die Legende von Siddharsi's Umrechnung zum Jainismus", in O. "Beyond Anekantavada:A Jain Approach to Religious Tolerance", in T. Sethia (Hg. ), Ahimsa, Anekanta und Jainism, Neu Delhi: Motilal Banarsidass 2004, 123–36. Il Jainismo: L'antica Religione indiana della nonviolenza; Prefazione di Raffaele Torella, Roma: Castelvecchi 2005 (Italienische Übersetzung der Jains, zweite erweiterte Ausgabe, London und New York Routledge 2002). "Eine nicht-Imperial Religion? Jainismus in der "Dark Age", in P. Olivelle (ed.,) Zwischen den Empires: Society in India 300BCE-400BCE, New York: Oxford University Press, 2006, pp.383–414 "The Later Fortunes of Jamali", in P. Flugel (Hrsg.) Studien in Jaina Geschichte und Kultur: Streitigkeiten und Dialoge, London und New York: Routledge 2006, pp.33–60 Aktuelle und bevorstehende Projekte Eine Übersetzung mit Kommentar von Yasovijaya Dvatrimsaddvatrimsika. Eine Studie über die historische Darstellung des Jain Pancanamaskara Mantras, mit besonderem Bezug auf Yasovijayas Arhadgita. Eine systematische Untersuchung der Sanskrit- und Prakrit-Texte der "Haribhadra corpus". = Referenzen ==