Digitale Bildverarbeitung ist der Einsatz eines digitalen Computers, um digitale Bilder durch einen Algorithmus zu verarbeiten. digitale Bildverarbeitung als Teilklasse oder Bereich der digitalen Signalverarbeitung hat viele Vorteile gegenüber der analogen Bildverarbeitung. Es ermöglicht eine viel breitere Palette von Algorithmen, die auf die Inputdaten angewendet werden, und kann Probleme wie den Aufbau von Lärm und Verzerrungen während der Verarbeitung vermeiden. Da Bilder über zwei Dimensionen (perhaps mehr) definiert werden, kann die digitale Bildverarbeitung in Form multidimensionaler Systeme konzipiert werden. Produktion und Entwicklung der digitalen Bildverarbeitung sind vor allem von drei Faktoren betroffen: Erstens, Entwicklung von Computern; zweitens die Entwicklung von Mathematik (insbesondere die Schaffung und Verbesserung der diskreten Mathematiktheorie); drittens hat sich die Nachfrage nach einer Vielzahl von Anwendungen in Umwelt, Landwirtschaft, Militär, Industrie und Medizin erhöht. In den 1960er Jahren wurden viele der Techniken der digitalen Bildverarbeitung oder der digitalen Bildverarbeitung, wie sie häufig genannt wurden, in Bell Laboratories, dem Jet Propulsion-Labor, Massachusetts Institute of Technology, University of Maryland und einigen anderen Forschungseinrichtungen entwickelt, die auf Satellitenbilder, Drahtphotonormenumwandlung, medizinische Bildgebung, Videophone, Charaktererkennung und Fotografieverbesserung Anwendung finden. Zweck der frühen Bildverarbeitung war die Verbesserung der Qualität des Images. Man wollte die visuelle Wirkung der Menschen verbessern. In der Bildverarbeitung ist der Input ein qualitativ hochwertiges Bild, und die Produktion ist ein Bild mit verbesserter Qualität. Gemeinsame Bildverarbeitung umfasst Imageverbesserung, Restaurierung, Kodierung und Kompression. Erster erfolgreicher Antrag war das amerikanische Jet Propulsion-Labor (JPL). Sie nutzten Bildverarbeitungstechniken wie z.B. die geometrische Korrektur, die Klassifizierungsumwandlung, die Lärmbeseitigung usw. auf die Tausende von lunar-Fotos, die 1964 vom Weltraumrator Ranger 7 gesendet wurden, unter Berücksichtigung der Lage der Sonne und der Umwelt des Monds. Die Wirkung der erfolgreichen Kartierung der Oberfläche von Mond durch den Computer war ein großer Erfolg. Später wurde eine komplexere Bildverarbeitung auf den fast 100 000 Fotos durchgeführt, die vom Raumfahrzeug zurückgeschickt wurden, so dass die topographische Karte, die Farbkarte und das Panorama des Monds erreicht wurden, was außergewöhnliche Ergebnisse erzielt und eine solide Grundlage für die menschliche Anlandung auf dem Mond geschaffen hat. Die Verarbeitungskosten waren jedoch recht hoch, mit der Rechentechnik dieser Zeit. Das änderte sich in den siebziger Jahren, als die digitale Bildverarbeitung als kostengünstigere Computer und spezielle Hardware verfügbar wurde. Dies führte zu Bilder, die in Echtzeit verarbeitet werden, für einige spezielle Probleme wie die Umrechnung von Fernsehnormen. Als allgemein sinnvolle Computer schneller wurden, begannen sie, die Rolle spezieller Hardware für alle zu übernehmen, aber die spezialisierten und computerintensiven Operationen. Mit den in den 2000er Jahren erhältlichen schnellen Computern und Signalverarbeitern ist die digitale Bildverarbeitung die häufigste Form der Bildverarbeitung geworden und wird in der Regel verwendet, weil sie nicht nur die vielseitigste Methode, sondern auch am billigsten ist. Bildsensoren Grundlage für moderne Bildsensoren ist die Metalloxide-semiconduktor (MOS)-Technologie, die aus der Erfindung der MOSFET (MOS-Feld-Wirkungstransistor) von Mohamed M. Atalla und Dawon Kahng auf Bell Labs 1959 stammt. Dies führte zur Entwicklung digitaler Halbleiterbildsensoren, einschließlich des abgabeunabhängigen Geräts (CCD) und später des CMOS-Sensors. Hosard S. Boyle und George E. Smith wurden 1969 in Bell Labs entwickelt. Obwohl die MOS-Technologie untersucht wurde, stellten sie fest, dass eine elektrische Gebühr die Analogie der Magnetblase war und dass sie auf einem winzigen MOS-Kondens gespeichert werden könnte. Da es recht einfach war, eine Reihe von MOS-Kondens in einer Folge zu strukturieren, haben sie eine geeignete Spannung an sie angeschlossen, damit die Abgabe von einem auf die nächste angehoben werden kann. Der CCD ist ein Halbleiterkreis, der später in den ersten digitalen Videokameras für den Fernsehen eingesetzt wurde. Der NMOS-Aktivitätssensor (APS) wurde Mitte der 80er Jahre von Olympus in Japan entwickelt. Durch Vorschüsse in der MOS-Halbvorrichtung, mit MOSFET-Skalation, die kleinere Mikron und dann sub-Mikron-Level erreicht. Im Jahr 1985 wurde das NMOS APS von dem Tsutomu Nakamuras Team in Olympus hergestellt. später wurde das aktive iPhone-Sensor (CMOS Sensor) von Eric Fossum im NASA Jet Propulsion-Labor entwickelt. Im Jahr 2007 waren die Verkäufe von CMOS-Sensoren CCD-Sensoren überstiegen. Bildkompression Eine wichtige Entwicklung der digitalen Bildkomprimierungstechnik war die diskrete Kosin-Umwandlung (DCT), eine von Nasir Ahmed im Jahr 1972 vorgeschlagene verlusthafte Kompressionstechnik. DCT-Komprimierung wurde die Grundlage für IDE, die 1992 von der Gemeinsamen Expertengruppe für Fotografie eingeführt wurde. JPEG-Komprimes-Bilder bis zu viel kleinerer Dateigrößen, und das meist verwendete Bilddatei-Format im Internet. Sein hochwirksamer DCT-Kompressionsgorithmus war weitgehend für die breite Verbreitung digitaler Bilder und digitaler Fotos verantwortlich, mit mehreren Milliarden Xerox-Bildern, die täglich ab 2015 produziert wurden. Digital Signalverarbeitung (DSP)Elektronische Signalverarbeitung wurde in den siebziger Jahren durch die breite Einführung der MOS-Technologie revolutioniert. MOS integrierte Schaltkreistechnik war die Grundlage für die ersten Mikroprozessoren und Mikrochips in den frühen 70er Jahren und dann die erste digitale Signal-Verarbeiter-Chip-Chips (DSP) in den späten 1970er Jahren. DSP-Chips werden seit langem in der digitalen Bildverarbeitung verwendet. Der diskrete Kosin-Umwandlung (DCT) Bildkompressions-Algorithmus wurde in DSP-Chips weit verbreitet, mit vielen Unternehmen, die auf DCT-Technologie basieren. DCT werden häufig für die Kodierung, Entschlüsselung, Videocode, Audiocode, Multiplexing, Steuerungssignale, Signalgebung, analoge Umstellung, Formatierung von Luminance- und Farbunterschieden und Farbformate wie YUV444 und YUV411. verwendet. DCTs werden auch für die Kodierung von Operationen wie Bewegungsabschätzung, Bewegungsausgleich, Interrahmenvorhersage, Quantisierung, perspektive Gewichtung, entropy-Kodierung, variable Kodierung und Bewegungsvektoren verwendet und Decode-Operationen wie der umgekehrte Betrieb zwischen verschiedenen Farbformaten (YIQ, YUV und LU) zu Anzeigezwecken. DCTs werden auch häufig für hochauflösendes Fernsehen (HDTV) verwendet. Medizinische Bildgebung 1972 hat der Ingenieur des britischen Unternehmens EMI Housfield das Röntgenstrahlgerät für die Kopfdiagnose entwickelt, das normalerweise CT (Computer tomographie) genannt wird. Die CT-Nucleus-Methode basiert auf der Projektion des menschlichen Kopfabschnitts und wird von Computern verarbeitet, um das Querschnittsbild, das als Image-Wiederaufbau bezeichnet wird. Im Jahr 1975 entwickelte EMI erfolgreich ein CT-Gerät für das gesamte Körper, das ein klares, tomographisches Bild verschiedener Teile des menschlichen Körpers erhalten hat. 1979 gewann diese Diagnosetechnik den Nobelpreis. Digital Bildverarbeitungstechnik für medizinische Anwendungen wurde 1994 in die Weltraumstiftung Space Technology Hall of Fame aufgenommen. Tasks Digitale Bildverarbeitung ermöglicht die Verwendung viel komplexer Algorithmen und kann daher sowohl anspruchsvollere Leistungen bei einfachen Aufgaben als auch die Umsetzung von Methoden anbieten, die durch analoge Mittel unmöglich wären. Insbesondere die digitale Bildverarbeitung ist eine konkrete Anwendung und eine praktische Technologie auf der Grundlage: Klassifikations-Aufnahme Multi-scale Signalanalyse Mustererkennungsprojektion Ein-Techniken, die in der digitalen Bildverarbeitung verwendet werden, umfassen: Anisotrope Verbreitung versteckten Markov-Modellen, die unabhängige Bild-Renovierung, die die Analyse von Neural-Netzen Teilialdifferenzierungsprofilen der Komponenten, die die Analyse der wichtigsten Komponenten für die Selbstorganisation von Karten, digitale Bildumwandlungen, Filter Digital-Filter, werden verwendet, um digitale Bilder zu verwischt und zu verschärfen. Filterung kann durchgeführt werden durch: Konvolution mit speziell konzipierten Kernen (Filter) in der räumlichen Bereich, die bestimmte Frequenzregionen in der Frequenz (Fährer) verschleiert, Folgende Beispiele zeigen beide Methoden: Bildpolsterung in Fourier Domain Filterbilder werden in der Regel gestrichen, bevor sie in den Vierer-Raum umgewandelt werden, die nachstehenden hochpassierten Bilder zeigen die Folgen unterschiedlicher Auffüllungstechniken: Hinweis darauf, dass der Hochpassfilter zusätzliche Kanten aufweist, wenn Null gestrichen ist als die wiederholte Abwrackung. Filtercode-Beispiele KIT-Beispiele für flächendeckende Filterung. Affine-Umwandlungen ermöglichen grundlegende Bildumwandlungen, einschließlich Größe, Drehen, Spiegel und Shear, wie in den folgenden Beispielen gezeigt wird: Um die affine Matrix auf ein Bild anzuwenden, wird das Bild in die Matrix umgewandelt, in der jeder Eintrag der Pixelintensität an diesem Ort entspricht. Dann kann jeder Pixel-Standort als Vektor mit Angabe der Koordinierung dieses Pixels im Bild, [x, y,], wo x und y die Sequenz und Spalte eines Pixels in der Bildmatrix sind. Dies ermöglicht eine multiplizierte Koordinierung mit einer affine-Transformationsmatrix, die die Position verleiht, die der Pixel-Wert in das Ausgangsbild aufgenommen wird. Um jedoch Transformationen zu ermöglichen, die Übersetzungstransformationen erfordern, sind dreidimensionale homogene Koordinierungen erforderlich. Die dritte Dimension wird in der Regel auf eine nichtzero konstante, in der Regel 1 festgelegt, so dass die neue Koordinierung [x, y, 1] ist. Dies ermöglicht eine multiplizierte Kombination von 3 bis 3 Matrix, die Übersetzungswechsel ermöglicht. So ermöglicht die dritte Dimension, die konstant 1 ist, die Übersetzung. Da es sich bei der Kombination von Matrix um assoziative, mehrere Kodierungstransformationen handelt, kann man in eine einzige affine Transformation einbinden, indem sie die Matrix der einzelnen Transformationen multiplizieren, um die Transformationen durchzuführen. In einer einzigen Matrix, die bei der Anwendung auf einen Punktvektor das gleiche Ergebnis wie alle einzelnen Veränderungen auf dem Vektor [x, y, 1] in der Sequenz gibt. So kann eine Sequenz der affinen Transformation matrices auf eine einheitliche affine Transformationsmatrix reduziert werden. Beispielsweise erlauben zweidimensionale Koordinierungen nur eine Rotation über den Ursprung (0, 0). 3dimensionale homogene Koordinierungen können jedoch zum ersten Mal verwendet werden, um einen Punkt auf (0, 0) zu übersetzen, dann die Rotation durchzuführen und schließlich den Ursprung (0, 0) auf den ursprünglichen Punkt (das Gegenteil der ersten Übersetzung) zu übersetzen. Diese 3 affine Transformationen können in eine einzige Matrix kombiniert werden, so dass eine Rotation um jeden Punkt im Bild möglich ist. Bildverwechslung mit Morphologie ist für die Verweigerung von Bildern geeignet. Strukturierendes Element ist in der mathematischen Morphologie wichtig. Folgende Beispiele sind die Strukturierungselemente. Kennzeichnend für die Lärmfunktion, Image als I und Strukturierungselement wie B sind folgendes und Tabelle: (I ́ ) = [ 45 50 65 40 60 55 25 15 5 ] B = [ 1 2 1 2 1 0 3 ] faserstilstyle (I)= {b Matrix{b Matrix}52} {I, B)(i,j = m x { I + m , j + n ) + B ( m , n ) } displaystyle max\{I(i+m,j+n)+B(m,n} . Nach der Erosion (I ́ ) = [ 45 50 65 40 2 55 25 15 5 ] KINGstyle (I)= faserbegin{bmatrix}45 &50 &2 &655525 &5 &5\end{b Matrix Eine Öffnungsmethode ist einfach nur die Erosion zuerst und dann die Verwässerung, während die Schließungsmethode umgekehrt ist. In Wirklichkeit können die D(I,B) und E(I,B) durch Convolution umgesetzt werden, um die Verdichterungsmethode auf ein Bild anzuwenden, wird das Bild in graue Weise umgewandelt. Eine Maske mit der Verdichterungsmethode ist eine logische Matrix mit [ 111 ; 111 ] Memedisplaystyle [111;111;111}] .Die Verweigerungsmethoden beginnen vom Zentrum des Bildes mit der Hälfte der Höhe, der Hälfte der Breite und enden mit der Bildgrenze der Nummer, Spalte. Nachbarschaft ist ein Block im Originalbild mit der Grenze [der Punkt unten: der Punkt oben, der Punkt links des Zentrums: der Punkt auf dem rechten Zentrum]. Konvergieren Nachbarschaft und Strukturierung von Element und ersetzen anschließend das Zentrum mit einem Minimum an Nachbarn. Nehmen Sie die Methode zum Beispiel an. Dilation zuerst Lesen Sie das Bild und machen es mit Matlab in grauem Maßstab. Lesen Sie die Größe eines Bildes. Nach dem Rückkehrwert sind die Zahlen und Spaltennummern die Grenzen, die wir später verwenden. Strukturierungselemente hängen von Ihrer Dilation oder Erosionsfunktion ab. Das Mindestmaß an Nachbarn eines Pixels führt zu einer Erosionsmethode und die maximale Nachbarschaft führt zu einer Verdünnungsmethode. Zeit für Dialation, Erosion und Schließung. Erstellung einer Nullmatrix derselben Größe wie das Originalbild. Dilation zuerst mit Strukturierungsfenster. Strukturierungsfenster ist eine 3,3 Matrix und Konvolution Um den Mindestwert mit Fenstern aus der Serie [2 ~ Bildhöhe - 1] mit der Spaltenbreite [2 ~ Bildbreite - 1] zu füllen und ein neues Bild zu erhalten Für die Grenze kann es noch verbessert werden. Seit der Methode wird eine Grenze ignoriert. Kontrollelemente können für den Umgang mit Grenzen verwendet werden. Erosion (Take das Bild als Input) schafft eine Nullmatrix derselben Größe wie das Originalbild. Erosion mit Strukturierungsfenster. Strukturierungsfenster ist eine 3,3 Matrix und Konvolution Lassen Sie den maximalen Abstand mit Fenstern aus der Serie [2 ~ Bildhöhe - 1] mit Spaltenbreite [2 ~ Bildbreite - 1] füllen Sie den maximalen Wert der Nullmatrix und sparen Sie ein neues Bild Für die Grenze kann es noch verbessert werden. Seit der Methode wird die Grenze ignoriert. Kontrollelemente können für den Umgang mit Grenzen verwendet werden. Ergebnisse sind wie oben dargestellte Anwendungen Digitalkamera Bilder Digitalkameras im Allgemeinen spezielle digitale Bildverarbeitungsgeräte – entweder spezielle Chips oder zusätzliche Schaltkreise auf anderen Chips –, um die Rohdaten von ihrem Bildsensor in ein farbloses Bild in einem Standardformat zu verwandeln. Film Westworld (1973) war der erste Spielfilm, der die digitale Bildverarbeitung zu Pixellatfotografien nutzte, um einen Blickpunkt von Android zu simulierten. Gesichtserkennung kann mit mathematischer Morphologie, Discrete cosine Transformation, die in der Regel DCT genannt wird, und horizontaler Projektion (Mthematik) durchgeführt werden. Allgemeine Methode mit funktionsbasierter Methode Die charakteristische Methode der Gesichtserkennung verwendet Hautton, Randerkennung, Gesichtsform und Merkmal eines Gesichts (wie Augen, Maus und usw.) zur Erkennung von Gesicht. Hautton, Gesichtsform und alle einzigartigen Elemente, die nur das menschliche Gesicht haben, können als Merkmale bezeichnet werden. Erklärung In Anbetracht einer Reihe von Gesichtsbildern, zuerst die Hauttonenpalette durch Stichprobenbilder zu extrahieren. Die Hauttonenpalette ist nur ein Hautfilter. Strukturbezogene Indexmaßnahme (SSIM) kann angewendet werden, um Bilder im Hinblick auf die Gewinnung des Hauttons zu vergleichen. normalerweise sind HSV- oder RGB-Farbräume für den Hautfilter geeignet. Eg.HSV-Modus, der Hauttonbereich ist [0,48,50] ~ [20,255,255] Nach Filterbildern mit Hautton werden morphologie und DCT verwendet, um Lärm zu entfernen und fehlende Hautbereiche zu füllen. Öffnungsmethode oder Abschlussmethode können genutzt werden, um die fehlende Haut aufzufüllen.DCT ist die Vermeidung des Gegenstands mit Tonähnlicher Haut. Da menschliche Gesichter immer stärker strukturiert sind. Sobel-Betreiber oder andere Betreiber können zur Erkennung von Gesichtsranden eingesetzt werden. Um menschliche Merkmale wie Augen zu positionieren, die Projektion zu verwenden und den Höhepunkt des histogramms zu finden, helfen Sie, das detaillierte Merkmal wie Maus, Haar und Lippen zu erhalten. Projektion ist gerade das Bild, um die hohe Häufigkeit zu sehen, die in der Regel die Position ist. Verbesserung der Bildqualitätsmethode Bildqualität kann durch Kamera Vibrationen, Überexpositionierung, graue Verteilung zu zentralisiert und Lärm beeinflusst werden. Lärmproblem lässt sich beispielsweise durch eine glatte Methode lösen, während das Problem der Grauebenenverteilung durch die Gleichwertigkeit von Histogramm verbessert werden kann.ing Methode Wenn es einige unzufriedene Farbe gibt, gibt es eine Farbe rund um unzufriedene Farbe und durchschnittlich. Dies ist eine einfache Möglichkeit, um zu überlegen, ob die Methode reibungslos funktioniert. Mit Maske und Convolution kann die Methode zur Anwendung kommen. Nehmen Sie das kleine Bild und verbergen sich beispielsweise wie unten. Bild: [ 2 5 6 5 3 1 4 6 1 28 30 2 2 2 2 2 2 ] displaystyle beginnt {b Matrix}2+2 &5&3 &4 &4+212\\7 &2\\7 &3 &2 &2\end{b Matrix ist [ 1 / 9 1 / 9 1 / 9 1 / 9 1 / 9 1 / 9 1 / 9 1 / 9 1 / 9 1 / 9  9 1 / 9  9  9 ] displaystyle beginnt} Nach der Verwässerung und dem reibungslosen Ablauf ist das Bild [ 2 5 6 5 3 9 10 6 1 9 2 2 7 3 2 2 ] displaystyle beginnen{bmatrix}2 &5 &5\\3 &10 &6\\1 &9 &9 &2&7 &3 &2 &2 Bild[1, 1,] Bild[1, 2,] Bild[2, 1,] und Bild[2, 2] Originalbild Pixel ist 1, 4, 28, 30. Nach einer glatten Maske wird das Pixel 9, 10, 9, 9. neues Bild[1, 1] = 1 9 Glühbirnestyle Memetfrac 1 19 * (image[0,0]+image[0,1]+image[0,2]+image[1,0]+image[1,1]+image[1,2]+image[2,0]+image[2,1] + Bild[2,2] 1}{9 * (2+5+6+3+1+4+1+28+30)= 9 neues Bild[1, 2] = Boden{( 1 9 Memedisplaystyle Memetfrac) 1}{9 * (5+6+5+1+4+6+28+30+2)=10 neues Image[2, 1] = Boden( 1 9 Memedisplaystyle Memetfrac) 1}{9 * (3+1+4+1+28+30+73+3+2) 9 neues Bild[2, 2] = Boden( 1 9 Memestyle fasertfrac 1)9 * (1+4+6+28+30+2+3+2+2)=9 Gray Level Histogramm In Anbetracht eines grauen Niveaus seinestogramms von einem Bild wie unten. Wechseln Sie sein Togramm zu einer gleichmäßigen Verteilung aus einem Bild in der Regel, was wir als Histogramm-Gleichstellung bezeichneten. Kurzfristig ist der Bereich des grauen Niveaus seinestogramms  i i = 0 k H ( p i) Memestyle \sum i=0 ik}H(p_{i) (siehe Abbildung 1), während der Bereich der einheitlichen Verteilung  i i = 0 k G ( q i) Memestyle \sum i=0 ik}G(q_{i) (siehe Abbildung 2). Klar ist, dass das Gebiet nicht verändert wurde, so  i i = 0 k H ( p i) =  i i = 0 k G ( q i ) Glühbirnestyle \sum i=0}^{k}H(p_{i})=\sum i=0}^{k}G(q_{i) .Von der einheitlichen Verteilung ist die Wahrscheinlichkeit von q i KINGstyle q_i} N 2 q k − q 0 {\displaystyle 574tfrac N22qq_{k}-q_{0 während der 0  i i  i k displaystyle 0<iik} Längerfristig ist die Formel  q q N 2 q k − q 0 d s =  p p 0 p H ( s s ) d s livstyle \displaystyle \int q_{0}}^{q}{\tfrac N22qq_{k}-q_{0sds=\displaystyle \int p_{0}}^{p}H(s)ds .Mehrover, basierend auf der Definition einer Funktion, ist die Gray-Methode eine Funktion f Memestyle f}, die f(p)=q entspricht. Fatigue Erkennung und Überwachung von Technologien In den letzten zehn Jahren gab es erhebliche Fortschritte bei der Überwachung der Ermüdung. Diese innovativen Technologielösungen sind jetzt kommerziell verfügbar und bieten echte Sicherheitsvorteile für Fahrer, Betreiber und andere Schichtarbeiter in allen Branchen. Softwareentwickler, Ingenieure und Wissenschaftler entwickeln Ermüdungserkennungssoftware mit verschiedenen physiologischen Gepflogenheiten, um den Zustand der Ermüdung oder Beschwärmung zu bestimmen. Die Messung der Gehirnaktivität (Elektroencephalogramm) wird weithin als Standard bei der Überwachung der Ermüdung anerkannt. Andere Technologien, die zur Bestimmung von Müdigkeit eingesetzt werden, umfassen Verhaltenssymptome wie Augenverhalten, Blickrichtung, Mikro-Korrekturen bei Lenkung und throtter Verwendung sowie Herzkursschwankungen. Lesen Sie auch Digital Imaging Computer Vision CVIP ToolsDigitising Free Border Bedingung GPGPU Homomorphic Filter Bildanalyse an den Multidimensionalen Systemen der intelligenten Verkehrssysteme Society Leichte Software Standard-Testbilder Super Auflösung Total-Diagnose-Verweigerung von Werkzeugmaschinen VisionBounded-Diagnose-Referenzen weiter Lesen Salomon, C.J; Breckon, T.P (2010) Kerne der Digital Image Processing: Ein praktischer Ansatz mit Beispielen in Matlab.Wiley-Blackwell.doi:10.1002/9780470689776.ISBNUR0470844731. Digital Image Processing: Ein Algorithmic-Ansatz Java.Springer.ISBN gegen 1-84628-379-6.R Fisher; K Hz-Wiee; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Wörterbuch der Computer Vision und Bildverarbeitung. John Kuhn.ISBN gegen0-470-01526-1.Rafael C. Gonzalez; Richard E. Woods; Steven L. Eddins (2004). Digital Image Processing mit IDE. Pearson Education.ISBN: 8081-7758-898-9.Tim Morris (2004). Computer Vision und Bildverarbeitung. Palgrave Macmillan.ISBN UV0-333-99451-1.Tyagi Vipin KPMG. Verständnis für digitale Bildverarbeitung. Taylor und Francis CRC Presse.ISBN UV11-3856-6842. Mailand Sonka; Vaclav Hlavac; Roger Boyle (1999). Bildverarbeitung, Analyse und Werkzeugmaschinen Vision.PWS Publishing.ISBN UV0-534-95393-5.Rafael C. Gonzalez (2008). Digital Image Processing. Prentice Hall.ISBN 9780131687288 Externe Links Vorlesungen über Bildverarbeitung, von Alan Peters. Vanderbilt University.Aktualisierte 7 Januar 2016Verarbeitung digitaler Bilder mit Computeralgorithmen Ein Desktop-Zusatzcomputer (DTR) ist ein persönlicher Computer, der die vollen Fähigkeiten eines Desktop-Computers bietet, während das Mobiltelefon verbleiben. Sie sind häufig größere, sperrige Laptops oder in einigen Fällen 2-in-1-PCs mit einem Tablet-ähnlichen Formfaktor und Interface. Infolge ihrer erhöhten Größe umfasst diese Computerklasse in der Regel leistungsfähigere Komponenten und eine größere Anzeige als in kleineren tragbaren Computern üblich und kann eine relativ begrenzte Batteriekapazität aufweisen (oder überhaupt keine). Manche nutzen ein begrenztes Spektrum von Desktop-Komponenten, um eine bessere Leistung auf Kosten des Batterielebens zu gewährleisten. In manchen Fällen werden die Fingerabdrücke genannt, eine Mischung aus Desktop und Notebook, obwohl der Begriff auch für Desktop-Ersatzcomputer im Allgemeinen gilt. Herkunft Die Vorläufer des Desktop-Ersatzes waren die tragbaren Computer der frühen bis Mitte der 80er Jahre, wie das Portal R2E CCMC, die Osborne I, Kaypro II, die Compaq-Hardware und die Weiterentwicklung von Werkzeugmaschinen 64 (500-64) Computern. Diese Computer enthielten die CPU, die Anzeige, die Festplattenantriebe und die Stromversorgung in einem einzigen kurzen Einzelfall. ähnlich wie bei der Leistung der Desktop-Computer der Ära wurden sie leicht transportiert und mit einer beigefügten Tastatur, die sich als Schutzabdeckung verdoppelte, wenn sie nicht verwendet werden. Sie könnten genutzt werden, wo Raum und ein elektrischer Absatz verfügbar waren, da sie keine Batterie hatten. Die Entwicklung des Notebook-Formfaktors hat neue Impulse für eine tragbare Computerentwicklung gegeben. Viele frühen Notebooks waren im Interesse der Portabilität merklich, was solche Mobilitätsgrenzmittel als externe Floppy-Laufwerke oder Clip-on-Track-Bemessungsgeräte benötigte. Einer der ersten Notebooks, die als eigenständiger Computer verwendet werden könnten, war die EUROCOM 2100 auf der Grundlage der CPU-Architektur von Intel 8088, die Funktionalität der Desktop-Modelle verdoppelte, ohne eine externe Dockingstation zu verlangen. Die Entwicklung des modernen Desktop-Ersatzcomputers kam mit der Erkenntnis, dass viele Notebooks in einem halbjährlichen Standort verwendet wurden, der häufig an eine externe Energiequelle angeschlossen ist. Dies schlug vor, dass ein Markt für einen Laptop-style-Computer existiert, der den verringerten Bedarf des Nutzers für die Portabilität nutzen würde, um leistungsfähigere Komponenten, größere Expansionsfähigkeit und höhere Qualitätsmerkmale zu ermöglichen. Desktop-Ersatzcomputer werden oft mit einem Hafen-Replikator verwendet, um den Desktop-komfort voll zu nutzen. Design bietet moderne Desktop-Ersatzungen im Allgemeinen besser als herkömmliche Laptop-style-Computer, da ihre Größe die Aufnahme von leistungsfähigeren Komponenten ermöglicht. Das größere Körper bedeutet eine effizientere Wärmedissipation, die es Herstellern ermöglicht, Komponenten zu verwenden, die ansonsten überhitzen während der normalen Verwendung. Darüber hinaus ermöglicht ihre erhöhte Größe mehr modulare Eigenschaften, die eine größere Expansionsfähigkeit und Merkmale sowie größere und bessere Bildschirme ermöglichen. Jedoch kommen diese Vorteile in der Regel zu einer Preisprämie, mit vielen Computern dieser Klasse, die so viel wie zwei Desktop-Computer mit ähnlichen Spezifikationen kosten. Mit einem Laptop-Formfaktor werden die Desktop-Ersatzungen jedoch immer noch häufig unter Beschränkungen leiden, die denen von mehr mobilen Laptops ähneln. In der Regel fehlt es an der Fähigkeit, Standard- PCIe-Erweiterungskarten zu akzeptieren, wodurch ihre Expansionsfähigkeit etwas eingeschränkt wird. Desktop-Ersatzungen können zwar bessere Kühlung bieten als andere Laptops, aber sie entwässern selten die Wärme effizient genug, um hochwertige Desktop-Klassen-Komponenten zu ermöglichen und können somit nicht die gleichen Leistungsstandards wie Desktop-Computer erreichen. Desktop-Ersatzcomputer sind mit wenigen Ausnahmen im Vergleich zu Desktop-Computern schwierig zu aktualisieren, wobei viele ihrer wichtigsten Komponenten (wie die Anzeige) in die Gestaltung der Maschine integriert sind und andere (wie CPU und CPU) oft schwer Zugang und Ersatz haben. Ein kleiner Teil von Desktop-Ersatzgeräten enthält keine Batterie als Standard-Funktion, während einige nicht die ExpressCard-Unterstützung umfassen. Sie verfügen über dieselben Beschränkungen wie Laptops und können selten identische Komponenten mit einem Desktop-Computer verwenden. Luxemburg Links