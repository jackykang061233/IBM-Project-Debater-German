Mathematische Optimierung (alternativen festgelegte Optimierung) oder mathematische Programmierung ist die Auswahl eines besten Faktors in Bezug auf einige Kriterien, von einigen verfügbaren Alternativen. Optimierungsprobleme entstehen in allen quantitativen Disziplinen von Computerwissenschaften und Ingenieurwesen, um Forschung und Wirtschaft zu betreiben, und die Entwicklung von Lösungsmethoden war seit Jahrhunderten von Interesse für Mathematik. Im einfachen Fall besteht ein Optimierungsproblem darin, eine echte Funktion zu maximieren oder zu minimieren, indem sie die Inputwerte systematisch von einem erlaubten Set auswählen und den Wert der Funktion berechnen. Die allgemeine Einführung von Optimierungstheorien und -techniken für andere Formulierungen ist ein großer Bereich angewandter Mathematik. In der Regel werden „best verfügbare“ Werte einiger objektiver Funktion als definierter Bereich (oder Input) ermittelt, einschließlich einer Vielzahl unterschiedlicher Arten objektiver Funktionen und unterschiedlicher Arten von Bereichen. Optimierung der Probleme Ein Optimierungsproblem kann auf folgende Weise vertreten werden: A → R von einigen Set A bis zur realen Zahl: ein Element x0 AA, das f(x0) ≤ f(x) für alle x  A A (minimation) oder solche, die f(x0) ≥ f(x) für alle x  A A (maximization) Eine solche Formulierung ist ein Optimierungsproblem oder ein mathematisches Programmproblem (ein Begriff, der nicht direkt mit der Computerprogrammierung zusammenhängt, aber noch in linearer Programmierung verwendet wird – siehe die Geschichte unten). In diesem allgemeinen Rahmen können viele echte und theoretische Probleme auftreten. Da folgendes gilt f ( x 0 ) ≥ f ( x ) ) f ~ ( x 0 ) ≤ f ~ ( x ) {\displaystyle f\left(\ Mathematik) {x} {_0}\right)\geq f\left(\ Mathematik {x} Linkrightarrow Memetilde {f)left(\dl {x} {_0)right)\leq 7.8tilde {f}}\left(\ Mathematik) {x} \right)} mit f ~ ( x ) := − f ( x ) , f ~ : A → R {\style 574tilde {f}}\left(\ Mathematik) {x} {x} · rechts):=f\f\-\f\left(\f) A\rightmark \ Mathematikbb {R} } es ist besser, Probleme mit der Minimierung zu lösen. Jedoch wäre die entgegengesetzte Perspektive auch gültig. Probleme, die mit dieser Technik in den Bereichen Physik formuliert werden, können sich auf die Technik als Energieminimierung beziehen, wobei der Wert der Funktion f als repräsentative Energie des zu modellierten Systems zu verstehen ist. In der Maschinenausbildung ist es immer notwendig, die Qualität eines Datenmodells kontinuierlich zu bewerten, indem eine Kostenfunktion verwendet wird, wenn ein Mindestmaß an möglicherweise optimalen Parametern mit einem optimalen (niedrigsten) Fehler besteht. A ist in der Regel ein Teil des Euclidean-Raums Rn, der häufig durch eine Reihe von Zwängen, Gleichberechtigungen oder Ungleichheiten gekennzeichnet ist, die die Mitglieder der A erfüllen müssen. The Domain A of f wird der Suchraum oder die Wahlform genannt, während die Elemente der A Kandidatenlösungen oder machbare Lösungen genannt werden. Die Funktion f wird in verschiedenen Bereichen, einer objektiven Funktion, einer Verlustfunktion oder einer Kostenfunktion (Mindestisierung), einer Gebrauchs- oder Eignungsfunktion (maximisierung) oder in bestimmten Bereichen, einer Energie- oder Energiefunktion genannt. Eine praktikable Lösung, die minimiert (oder maximiert, wenn dies das Ziel ist) ist, ist eine optimale Lösung. Mathematik, konventionelle Optimierungsprobleme werden in der Regel im Hinblick auf die Minimierung angegeben. Ein lokaler Mindest x* ist definiert als Element, für das es einige  >-Liste gibt 0 solcher ∀ x   ; x ; ; ≤ ; , , , {\   {\ {\ {\ ; \forall \ Mathematik {x} inA\; 7.8text{where;\left\ Vert \ Mathematik {x} {x}  ast }right\ Vert \leq \delta ,\,} der Ausdruck f(x)* ≤ f(x) hält; das heißt, in einigen Regionen rund x* sind alle Funktionswerte größer als der Wert auf diesem Element. Lokale Maxima sind ähnlich definiert. Obwohl ein lokales Minimum mindestens so gut wie alle nahe gelegenen Elemente ist, ist ein globales Minimum mindestens so gut wie möglich. Allgemein, es sei denn, die objektive Funktion ist in einem Minimierungsproblem konvex, es gibt mehrere lokale Minima. In einem Konvex-Problem, wenn es ein lokales Minimum gibt (nicht am Rand des Set möglicher Elemente), ist es auch das globale Minimum, aber ein Nichtkonvex-Problem kann mehr als ein lokales Minimum haben, von dem nicht alle globale Minima benötigen. Eine große Anzahl von Algorithmen, die zur Lösung der nicht-konvexen Probleme vorgeschlagen werden, einschließlich der Mehrheit der kommerziell zur Verfügung stehenden Lösungsgruppen, sind nicht in der Lage, eine Unterscheidung zwischen lokal optimalen Lösungen und weltweit optimalen Lösungen zu treffen und die alten als echte Lösungen für das ursprüngliche Problem zu behandeln. Globale Optimierung ist der Bereich angewandter Mathematik und numerischer Analysen, die sich mit der Entwicklung von deterministischen Algorithmen befassen, die in der Lage sind, die Konvergenz der Flossenzeit auf die tatsächliche optimale Lösung eines Problems zu gewährleisten. Mangelnde Optimierungsprobleme werden häufig mit einer besonderen Notation zum Ausdruck gebracht. Hier sind einige Beispiele: Mindest- und Höchstwert einer Funktion betrachten folgendes: min x  R R ( x 2 + 1 ) Memestyle \min {_x\in \bb {R} ;left(x^{2}+1\right) Man unterscheidet den Mindestwert der objektiven Funktion x2 + 1 bei der Auswahl von x aus dem Set der realen Nummern R. In diesem Fall liegt der Mindestwert bei x = 0.Similar, der Notation max x  R R 2 x {\displaystyle \max {_x\in \bb {R};\2x fragt nach dem maximalen Wert der objektiven Funktion 2x, wo x jede tatsächliche Nummer sein kann. In diesem Fall gibt es keinen solchen Höchstbetrag, da die objektive Funktion nicht angebunden ist, so dass die Antwort unklar ist. Optimale Inputs Überlegungen an: ein r g m i n x  i ( − ] , − 1 ] x 2 + 1 , Memedisplaystyle {x\in (-\infty ,-1].ator {arg\,min} ;\x^{2}+1 oder gleichwertig ein r i n x 2 + 1 , vorbehaltlich: x ∈ (  1, − 1 ) KINGstyle HANAunderset x}{\operatorname {arg\,min} x^{2}+1,\; E-Mail::;x\in (-\infty ,-1)] Dies entspricht dem Wert (oder Werten) des Arguments x im Intervall (−∞, -1), der die objektive Funktion x2 + 1 (der tatsächliche Mindestwert dieser Funktion ist nicht das, was das Problem betrifft). In diesem Fall ist die Antwort x = −1, da x = 0 unwiderruflich ist, d. h. sie gehört nicht zu dem machbaren Satz. Gleiches: ein r g m x x x  [ [ , 5 ] , y  R R x cos ⁡ y , Memedisplaystyle {x\in -[5,5],\;y\in \bb {R} ).operator {arg\,max} } } } oder \cos,} oder gleichwertig ein r x · x  y, y cos⁡, :: :,  5,  5 In diesem Fall sind die Lösungen das Paar der Form {5, 2k. und {−5, (2k + 1)},}, in der k sich über alle Zahlen erstreckt. Betreiber einesrgmins und einesrgmax werden manchmal auch als Enrgmin und Argmax geschrieben und halten sich für das Argument des Minimums und des Arguments der Höchstmenge. Geschichte Fermat und Lagrange finden kalorienbasierte Formeln für die Bestimmung von Optima, während Newton und Gaus vorschlagen iterative Methoden für eine optimale Nutzung. Der Begriff "lineare Programmierung" für bestimmte Optimierungsfälle war auf George B. Dantzig zurückzuführen, obwohl viele der Theorie von Leonid Kantorovich im Jahr 1939 eingeführt worden waren.(Programmierung in diesem Zusammenhang bezieht sich nicht auf die Computerprogrammierung, sondern stammt aus dem Einsatz von Programmen der Vereinigten Staaten, um auf die vorgeschlagenen Ausbildungs- und Logistikpläne zu verweisen, die die damals untersuchten Probleme Dantzig waren). Dantzig hat 1947 den einfachenx-Algorithmus veröffentlicht, und John von Steiner entwickelte die Theorie der Dualität im gleichen Jahr. Andere bemerkenswerte Forscher in mathematischer Optimierung umfassen: Großunternehmen Konvergenzprogrammstudien, bei denen die objektive Funktion konvex (Mindestisierung) oder Vermittlung (maximization) ist und die gesetzten Beschränkungen konvex sind. Dies kann als ein besonderer Fall der nichtlinearen Programmierung oder als generellisierung der linearen oder konvex quadratischen Programmierung angesehen werden. Lineare Programmierung (LP), eine Art von Konvex-Programmierung, untersucht den Fall, in dem die objektive Funktion f linear ist und die Zwänge mit nur linearen Gleichwertigkeiten und Ungleichheiten angegeben werden. Eine solche Beschränkung wird als Polyhedron oder Polytope bezeichnet, wenn sie gebunden ist. Zweite konvexische Programmierung (SOCP) ist ein Konvex-Programm und umfasst bestimmte Arten von quadratischen Programmen. Halbdefinite Programmierung (SDP) ist ein Teil der konvexen Optimierung, wo die zugrunde liegenden Variablen halbdefinite matrices sind. Es ist eine generelle lineare und konvex quadratische Programmierung. Konic Programmierung ist eine allgemeine Form der Konvex-Programmierung. LP, SOCP und SDP können alle als konicische Programme mit der entsprechenden Art von Kone sehen. geometrische Programmierung ist eine Technik, bei der objektive und ungleiche Zwänge, die als Posen und Gleichberechtigungen ausgedrückt werden, als Monomials in ein Konvex-Programm umgewandelt werden können. lineare Programme, in denen einige oder alle Variablen eingeschränkt sind, um auf Zahlen zu reagieren. Dies ist nicht konvex, und im Allgemeinen viel schwieriger als reguläre lineare Programmierung. Legic Programmierung ermöglicht die objektive Funktion, kalkulierte Bedingungen zu haben, während die durchführbare Regelung mit linearen Gleichwertigkeiten und Ungleichheiten festgelegt werden muss. Für spezifische Formen des quadratischen Begriffs ist dies eine Art von Konvex-Programmierung. Fractionale Programmierungsstudien Optimierung der Verhältnisse von zwei nichtlinearen Funktionen. Die besondere Klasse der koave-fraktionellen Programme kann zu einem konvexen Optimierungsproblem umgewandelt werden. Nichtlineare Programmierungsstudien, bei denen die objektive Funktion oder die Zwänge oder beide nichtlineare Teile enthalten. Dies kann oder kann kein Konvex-Programm sein. Ganz allgemein, ob das Programm konvex die Schwierigkeit der Lösung beeinträchtigt. Stochastic Programmierungsstudien, in denen einige der Zwänge oder Parameter von Zufallsvariablen abhängen. robuste Optimierung ist wie die kryktische Programmierung ein Versuch, Unsicherheiten in den Daten zu erfassen, die dem Optimierungsproblem zugrunde liegen. robuste Optimierung zielt darauf ab, Lösungen zu finden, die unter allen möglichen Umsetzungen der Unsicherheiten, die durch eine Unsicherheitsregelung definiert sind, gültig sind. Kombinatorialoptimierung ist mit Problemen behaftet, in denen das Set möglicher Lösungen individuell ist oder auf ein bestimmtes reduziert werden kann. Stochastic Optimierung wird mit zufälligen (noisy) Funktionsmessungen oder zufälligen Inputs im Suchverfahren verwendet. Nanodimensionale Optimierungsstudien, bei denen das Set möglicher Lösungen ein Teil eines unendlichen Raums ist, wie z.B. ein Raum der Funktionen. Hetourismus und Metahetourismus machen nur wenige oder keine Annahmen über das Problem, das optimiert wird. In der Regel garantieren er nicht, dass eine optimale Lösung gefunden werden muss. Hingegen werden ertourismus genutzt, um für viele komplizierte Optimierungsprobleme ungefährliche Lösungen zu finden. Gentraintzufriedenheit untersucht den Fall, in dem die objektive Funktion f konstant ist (dies wird in künstlichen Erkenntnissen, insbesondere in automatisierten Gründen, verwendet). Constraint Programmierung ist ein Programmierungsparat, bei dem die Beziehungen zwischen Variablen in Form von Zwängen angegeben werden. Disjunctive Programmierung wird verwendet, wenn mindestens ein Hindernis erfüllt sein muss, aber nicht alle. Es ist von besonderer Bedeutung bei der Planung. Weltraumkartierung ist ein Konzept für die Modellierung und Optimierung eines Engineering-Systems zu einer hochwertigen (fine) Modellgenauigkeit, die eine geeignete materielle Koarse oder ein Surrogate-Modell nutzt. In einer Reihe von Teilbereichen werden die Techniken hauptsächlich für die Optimierung in dynamischen Kontexten entwickelt (d. h. die Entscheidungsfindung über die Zeit): Lässt sich das Risiko von Schwankungen durch eine unterschiedliche Funktion der koordinierten Maßnahmen optimieren, die über einen bestimmten Raum zu einem Extremum gehören. Optimale Kontrolltheorie ist eine generelle Bewertung der Schwankungen, die Kontrollmaßnahmen einführen. Dynamische Programmierung ist der Ansatz zur Lösung des Problems der kryktischen Optimierung mit Stämmen, Randomität und unbekannten Modellparametern. Es untersucht den Fall, in dem die Optimierungsstrategie auf der Aufteilung des Problems in kleinere Probleme beruht. Die Formel, die die Beziehung zwischen diesen Subproblemen beschreibt, wird als Bellman-Gleichheit bezeichnet. Mathematische Programmierung mit Gleichgewichtszwängen, bei denen die Zwänge unterschiedliche Ungleichheiten oder Komplementaritäten umfassen. Multi-objektive Optimierung Mehr als ein Ziel eines Optimierungsproblems erhöht die Komplexität. Zum Beispiel, um ein strukturelles Design zu optimieren, würde man ein Design wünschen, das sowohl Licht als auch starr ist. Wenn zwei Ziele Konflikt sind, muss ein Auslauf geschaffen werden. Man kann ein hellstes Design, ein stetes Design und eine unendliche Anzahl von Geschmacksmustern geben, die ein gewisses Gewicht und Rigidität gefährden. Kennzeichnend für das Paket von Handelsmustern, die sich auf ein Kriterium auf Kosten eines anderen verbessern, ist das Pareto-Set. Die Kurve hat Gewicht gegen die Steifigkeit der besten Modelle geschaffen, ist als Pareto-Grenz bekannt. Ein Design wird als "Pareto optimal" bewertet (entsprechend, "Pareto effizient" oder im Pareto-Set), wenn es nicht von einem anderen Design dominiert wird: Wenn es in einigen Punkten schlechter als ein anderes Design ist und in keiner Hinsicht nicht besser ist, wird es dominiert und ist nicht optimal. Die Wahl zwischen "Pareto optimal"-Lösungen zur Bestimmung der "favorite Lösung" wird dem Entscheidungsträger übertragen. Mit anderen Worten, die Definition des Problems als multi-objektive Optimierungssignale, dass einige Informationen fehlen: wünschenswerte Ziele werden angegeben, aber Kombinationen von ihnen werden nicht im Verhältnis zueinander bewertet. In einigen Fällen können die fehlenden Informationen von interaktiven Sitzungen mit dem Entscheidungsträger abgeleitet werden. Mehr zielgerichtete Optimierungsprobleme wurden in vektoriellen Optimierungsproblemen weiter verbreitet, in denen die (teilweise) Bestellung nicht mehr durch den Pareto-Beauftragten erfolgt. Multimodale oder globale Optimierungsprobleme sind häufig multimodal; das sind mehrere gute Lösungen. Sie könnten alle weltweit gut sein (gleiche Kostenfunktionswerte) oder es könnte eine Mischung aus weltweit guten und lokal guten Lösungen geben. Das Ziel einer multimodalen Optimierung ist es, alle (oder zumindest einige) Lösungen zu finden. Klassische Optimierungstechniken aufgrund ihres iterativen Ansatzes führen nicht zufriedenstellend aus, wenn sie zur Erlangung mehrerer Lösungen verwendet werden, da nicht gewährleistet ist, dass verschiedene Lösungen auch mit unterschiedlichen Ausgangspunkten in mehreren Phasen des Algorithmus gefunden werden. Gemeinsame Ansätze für globale Optimierungsprobleme, bei denen mehrere lokale Extrema vorhanden sein können, umfassen evolutionäre Algorithmen, die Bayesische Optimierung und simulierte Annobel. Klassifizierung kritischer Punkte und Extrema Feasibility-Problem Das Problem der Erlangbarkeit, das auch das Machbarkeitsproblem genannt hat, ist nur das Problem, alle ohne objektiven Wert zu finden. Dies kann als Sonderfall der mathematischen Optimierung angesehen werden, wenn der objektive Wert für jede Lösung identisch ist und daher eine Lösung optimal ist. Viele Optimierungsgorithmen müssen von einem machbaren Punkt beginnen. Eine Möglichkeit, einen solchen Punkt zu erhalten, besteht darin, die Durchführbarkeitsbedingungen mit einer Slackvariable zu lockern; mit genügend Slack ist jeder Startpunkt machbar. In diesem Fall ist die Schwankungsbreite bis zur Slack null oder negativ. Der extreme Wert von Karl Weierstras stellt fest, dass eine kontinuierliche, reale Funktion auf einem Compact-Set seinen maximalen und minimalen Wert erreicht. In der Regel erreicht eine niedrigere halbkontinuierliche Funktion auf einem kompakten Set sein Mindestmaß; eine Oberhalbkontinuierliche Funktion auf einem Compact-Set erreicht maximalen Blick oder Sicht. Bedingungen für die optimale Nutzung Einer von Fermat erklärt, dass es sich bei ortsfesten Stellen um die Lösung von unkonsistenten Problemen handelt, bei denen der erste Derivat oder die Neigung der objektiven Funktion Null beträgt (siehe erster Derivattest). Insgesamt können sie an kritischen Punkten gefunden werden, in denen der erste Derivat oder die Höhe der objektiven Funktion Null oder nicht definiert ist oder die Wahlgrenze überschritten wird.Eine Formel (oder eine Reihe von Gleichungen), in der angegeben wird, dass die ersten Derivate(en) Null in einem Innenausstattungsoptimale als „erstes Zustand“ oder eine Reihe von ersten Geschäftsbedingungen bezeichnet werden. Optima von Gleichbehandlungsproblemen kann durch die Lagrange Multiplikator-Methode gefunden werden. Die Lösung von Problemen mit Gleichberechtigung und/oder Ungleichheit kann unter Verwendung der „Karush-Kuhn-Tucker-Bedingungen“ gefunden werden. Mangelnde Bedingungen für die optimale Nutzung Obwohl der erste Derivattest Punkte ermittelt, die Extrema sein könnten, unterscheidet dieser Test nicht einen Punkt, der ein Minimum von einem ist, der maximal oder eines ist, der nicht. Wenn die objektive Funktion doppelt unterschiedlich ist, können diese Fälle durch Überprüfung der zweiten Derivate oder der Matrix der zweiten Derivate (sogenannte Hessische Matrix) in unausgewogenen Problemen oder durch die Matrix der zweiten Derivate der objektiven Funktion und die Zwänge, die der angrenzte Hessian genannt werden, unterschieden werden. Bedingungen, die Maxima oder Minima von anderen stationären Stellen unterscheiden, werden als „zweite Bedingungen“ bezeichnet (siehe „Second Derivatest“). Wenn eine Kandidatenlösung die ersten Bedingungen erfüllt, reicht die Zufriedenheit der zweiten Geschäftsbedingungen auch aus, um mindestens lokale optimale Bedingungen zu schaffen. Empfindlichkeit und Kontinuität von Optima In dem Finanzrahmen wird beschrieben, wie der Wert einer optimalen Lösungsänderung bei zugrunde liegenden Parameteränderungen liegt. Der Prozess der Rechentechnik wird als Vergleichsstabil bezeichnet. Oberstes Merkmal von Claude Bergen (1963) beschreibt die Kontinuität einer optimalen Lösung als Funktion der zugrunde liegenden Parameter. Klima der Optimierung Keine Probleme mit doppelt differenzierten Funktionen lassen sich durch die Feststellung der Punkte feststellen, in denen die Neigung der objektiven Funktion Null beträgt (d. h. die stationären Punkte). Insgesamt wurde ein Null-Unterwegtest festgestellt, dass ein lokales Mindestmaß für die Minimierung von Problemen mit konvexen Funktionen und anderen lokalen Lipschitzfunktionen gefunden wurde. Weitere kritische Punkte können unter Verwendung der Hoheit der Hessischen Matrix klassifiziert werden: Liegt der Hessian an kritischer Stelle positiv aus, so ist der Punkt ein lokales Minimum; wenn die Hessische Matrix negativ ist, dann ist der Punkt ein lokaler Höchstwert; schließlich, wenn unbestimmte, dann ist der Punkt eine Art von Sattelpunkt. Konsequente Probleme können häufig in unausgewogene Probleme mit Hilfe von Lagrange Multiplikatoren umgewandelt werden. Lagrangische Lockerung kann auch ungefähre Lösungen für schwierige Probleme bieten. Wenn die objektive Funktion eine Konvex-Funktion ist, wird auch ein lokales Minimum ein globales Minimum sein. Es gibt effiziente numerische Techniken zur Minimierung von Konvex-Funktionen, wie Innenausstattungsmethoden. Messtechnik Um Probleme zu lösen, können Forscher Algorithmen verwenden, die in einer Finken Anzahl von Schritten oder iterativen Methoden, die einer Lösung (bei einigen bestimmten Arten von Problemen) konvergieren, oder bei denen es sich um ähnliche Lösungen für einige Probleme handelt (obwohl ihre Erreger nicht konvergieren müssen). Optimierung von Algorithmen Einfachex-Algorithmus von George Dantzig, der für lineare Programmierungserweiterungen des einfachenx-Algorithmus entworfen wurde, die für quadratische Programmierung und für lineare Programmierung bestimmt sind, die für die Netzoptimierung besonders geeignet sind. Kombinatorische Algorithmen Methoden zur Quantenoptimierung Die Methoden zur Lösung von Problemen der nichtlinearen Programmierung unterscheiden sich von der Beurteilung der Hessser, der Gefälle oder nur von Funktionswerten. Während die Bewertung der Hessians (H) und der Unzulänglichkeiten (G) die Konvergenzrate verbessert, für Funktionen, für die diese Mengen existieren und ausreichend reibungslos variieren, erhöhen diese Bewertungen die rechnerische Komplexität (oder die Berechnungskosten) der einzelnen Iteration. In einigen Fällen kann die rechnerische Komplexität zu hoch sein. Ein wichtiges Kriterium für Optimierungsgeräte ist nur die Anzahl der vorgeschriebenen Funktionsbewertungen, da dies oft bereits eine große rechnerische Anstrengung ist, in der Regel viel mehr Aufwand als innerhalb des Optimierungers selbst, der in erster Linie über die N-Parameter arbeiten muss. Die Derivate stellen detaillierte Informationen für solche Optimierungsgeräte bereit, sind aber noch schwieriger zu berechnen, z.B. bei der Angleichung der Neigungen werden mindestens N+1-Funktionsbewertungen vorgenommen. Für die Angleichung der zweiten Derivate (die in der Hessischen Matrix zusammengefasst sind) ist die Anzahl der Funktionsbewertungen in der Reihenfolge der N2.Newton-Methode erforderlich, so dass für jede Erregung die Zahl der Funktionsanrufe in der Reihenfolge von N2, aber für einen einfacheren reinen Aufschwung, der es nur N.Wie immer ist, benötigen die Steiger in der Regel mehr Iterations-Algorithmus. Welches ist am besten in Bezug auf die Anzahl der Funktionsgespräche abhängig vom Problem selbst. Methoden, die Hessians (oder etwa Hessians anhand von finite Differenzen) bewerten: Methode Sequential quadratic Programmierung: A Newton-basierte Methode für kleine und mittlere Probleme. Manche Versionen können großedimensionale Probleme bewältigen. Innenpunkte: Dies ist eine große Klasse von Methoden für eine eingeschränkte Optimierung. Manche Innenausstattungsmethoden verwenden nur (teilweise) Bewegungsinformationen und andere, die die Bewertung von Hessians erfordern. Methoden, die die Verhältnisse bewerten oder die Steigungen in irgendeiner Weise angleichen (oder sogar Unterlagen): Koordinierte Abmessungsmethoden: Algorithms, die eine einzige Koordinierung in jedem Iteration Conjugate-Verfahren aktualisieren: Iterative Methoden für große Probleme. () In der Theorie beenden diese Methoden in einer finite Anzahl von Schritten mit kalkischen objektiven Funktionen, doch wird diese finitee Beendigung in der Praxis nicht in der Praxis auf Finite–präzisionen Computern beobachtet. Krankheitsertrag (alternativen, "steepest" oder "steepest up"): A (slow) Methode des historischen und theoretischen Interesses, die ein erneutes Interesse an der Suche nach ungefähren Lösungen von enormen Problemen hat. Subverfahren: Eine iterative Methode für große lokale Lipschitz-Funktionen unter Verwendung von allgemeinisierten Verhältnissen. Nach Boris T. Polyak sind die Unterwegs- und Projektverfahren ähnlich wie die Methoden der Verfechten. Methode der Abfahrt: Ein iteratives Verfahren für kleine und mittlere Probleme mit lokalen Lipschitzfunktionen, insbesondere bei Konvex-minimierungsproblemen (vergleichbar mit den Methoden der Verfalldichte). Ellipsoid-Methode: Eine iterative Methode für kleine Probleme mit quasiconvex objektiven Funktionen und von großem theoretischem Interesse, insbesondere bei der Festlegung der multinomialen Zeitkomplexe einiger kombinierter Optimierungsprobleme. Es hat Ähnlichkeiten mit Quasi-Newton-Methoden. Konditional-Verfahren (Frank-Wolfe) zur ungefähren Minimierung spezieller strukturierter Probleme mit linearen Zwängen, insbesondere mit Verkehrsnetzen. Für allgemeine unkonsequente Probleme verringert diese Methode die Gefällesmethode, die als obsolet gilt (für fast alle Probleme). Quasi-Newton-Methode: Iterative Methoden für mittelgroße Probleme (z.B. N N1000). Simultane Perturbation Stchastic Annäherungsmethode (SPSA) für krchtische Optimierung; Verwendung der zufälligen (effizienten) Annäherung. Methoden, die nur Funktionswerte bewerten: Wenn ein Problem ständig unterschiedlich ist, können die Verluste anhand von finitären Unterschieden angeglichen werden, in denen eine ziehungsbasierte Methode verwendet werden kann. Interpolationsmethoden Muster-Suchesmethoden, die bessere Konvergenzeigenschaften haben als die Nelder-Mead he Tourist (mit simplices), die unten aufgeführt ist. Globale Konvergenz im Allgemeinen, wenn die objektive Funktion keine pauschale Funktion ist, dann verwenden viele Optimierungsmethoden andere Methoden, um sicherzustellen, dass einige Subsequence von Iterations auf eine optimale Lösung abgestimmt werden. Die erste und noch populäre Methode zur Gewährleistung der Konvergenz beruht auf der Suche nach Anschlüssen, die eine Funktion entlang einer Dimension optimieren. Eine zweite und immer beliebtere Methode zur Gewährleistung der Konvergenz nutzt vertrauensbildende Regionen. Beide Seiten suchen und vertrauensbildende Regionen werden in modernen Methoden der nicht differenzierten Optimierung verwendet. In der Regel ist ein globaler Optimierungsr viel langsamer als fortgeschrittene lokale Optimierungsgeräte (wie BFGS), so dass oft ein effizienter globaler Optimierungsr aufgebaut werden kann, indem der lokale Optimierungsr aus verschiedenen Ausgangspunkten eingesetzt wird. Neben (bestimmten)Algorithmen und (Konvergenz) Iterativen Methoden gibt es Hetourismus. Ein Hetourismus ist jeder Algorithmus, der nicht garantiert (normalerweise) ist, um die Lösung zu finden, aber dies ist auch in bestimmten praktischen Situationen sinnvoll. Liste der bekannten Hetourismus: Anwendungen Anlagenprobleme in der starren Körperdynamik (insbesondere starre Körperdynamik) erfordern häufig mathematische Programmierungstechniken, da Sie eine starre Körperdynamik als Versuch betrachten können, eine normale Differenz zu lösen; die Zwänge sind verschiedene nichtlineare geometrische Zwänge wie "diese zwei Punkte müssen immer übereinstimmen", "die Oberfläche darf nicht alle anderen," oder "Dieser Punkt muss immer auf dieser Kurve liegen". Auch das Problem der Computerkontaktkräfte lässt sich durch die Lösung eines linearen Komplementaritätsproblems lösen, das auch als Problem der QP angesehen werden kann. Viele Designprobleme können auch als Optimierungsprogramme ausgedrückt werden. Dieser Antrag wird als Designoptimierung bezeichnet. Ein Teil ist die technische Optimierung, und ein weiterer neuer und wachsender Teil dieses Bereichs ist eine multidisziplinäre Gestaltungsoptimierung, die in vielen Problemen besonders auf Probleme im Bereich der Luft- und Raumfahrttechnik angewandt wurde. In Kosmologie und Astrophysik kann dieser Ansatz angewendet werden. Wirtschaft und Finanzen sind eng mit der Optimierung von Mitteln verknüpft, die eine einflussreiche Definition im Zusammenhang mit der quantitativen Beschreibung von Wirtschaftswissenschaften als "Anpassung des menschlichen Verhaltens als Verhältnis zwischen Enden und knappen Mitteln" mit alternativen Verwendungen beschreiben. Moderne Optimierungstheorie umfasst traditionelle Optimierungstheorie, aber auch Überschneidungen mit der Spieltheorie und der Studie über wirtschaftliche Gleichgewichte. Journal of Economic Literatur Codes Klassifikation mathematischer Programmierung, Optimierungstechniken und verwandte Themen unter JEL:C61-C63. Mikroökonomie, das Problem der Maximierung des Gebrauchs und sein Doppelproblem, das Problem der Ausgabenminimierung sind wirtschaftliche Optimierungsprobleme. Wenn sie sich konsequent verhalten, werden die Verbraucher davon ausgegangen, dass sie ihren Nutzen maximieren, während die Unternehmen in der Regel davon ausgehen, ihren Gewinn zu maximieren. Außerdem werden die Agenten häufig als riskant eingestuft, wodurch es bevorzugt wird, Risiken zu vermeiden. Vermögenspreise werden auch anhand der Optimierungstheorie modelliert, obwohl die zugrunde liegenden Mathematiken eher auf die Optimierung der blutigen Prozesse als auf die statische Optimierung angewiesen sind. Internationale Handelstheorie nutzt auch Optimierung, um Handelsmuster zwischen Nationen zu erklären. Die Optimierung von Portfolios ist ein Beispiel für eine multi-sachgerechte Optimierung der Wirtschaft. Seit den siebziger Jahren haben Ökonomen dynamische Entscheidungen im Laufe der Zeit unter Verwendung der Kontrolltheorie getroffen. Beispielsweise werden dynamische Suchmodelle verwendet, um das Verhalten auf dem Arbeitsmarkt zu untersuchen. Eine entscheidende Unterscheidung ist zwischen deterministischen und krchtischen Modellen. Makroökonomien bauen dynamische katastrophale allgemeine Gleichgewichtsmodelle (DSGE) auf, die die Dynamik der gesamten Wirtschaft als Ergebnis der interdependenten Optimierung der Entscheidungen von Arbeitnehmern, Verbrauchern, Investoren und Regierungen beschreiben. Elektrotechnik Manche gemeinsame Anwendungen von Optimierungstechniken im Elektrotechnik umfassen aktive Filterdesign, Stray-Felderabbau in superkonduktiven Magnetenergiespeichersystemen, Raumkartierung von Mikrowellenstrukturen, Handetanten Antennen, elektromagnetisches Design. elektromagnetische validierte Gestaltungsoptimierung von Mikrowellenkomponenten und Antennen hat seit der Entdeckung der Weltraumkartierung im Jahre 1993 einen umfassenden Einsatz eines geeigneten physikalisch-basierten oder empirischen Aufrogate-Modells und der Raumkartierungsmethoden gemacht. Civil Engineering Optimierung wurde in der Ziviltechnik weit verbreitet. Baumanagement und Transporttechnik gehören zu den wichtigsten Zweigen der zivilen Technik, die stark auf die Optimierung angewiesen sind. Die häufigsten ziviltechnischen Probleme, die durch Optimierung gelöst werden, werden durch die Verringerung und das Schließen von Straßen, die Lebenszyklusanalyse von Strukturen und Infrastrukturen, die Ressourcenausstattung, die Wasserressourcenallokation, das Verkehrsmanagement und die Planungsoptimierung gelöst. Operations Research Ein weiterer Bereich, der Optimierungstechniken umfassend nutzt, ist die Forschungstätigkeit. Operations-Forschung nutzt auch katastrophale Modellierung und Simulation zur Unterstützung einer verbesserten Entscheidungsfindung. In zunehmendem Maße nutzt die Forschungstätigkeit die kriktische Programmierung, um dynamische Entscheidungen zu entwerfen, die sich an Ereignisse anpassen; solche Probleme können mit großmaßstäblichen Optimierungs- und Sanktionsverfahren gelöst werden. Steuerung der mathematischen Optimierung wird in viel modernen Verantwortlichendesign verwendet. Hochrangige Fluglotsen wie Modellvorhersage (MPC) oder Echtzeitoptimierung (RTO) beschäftigen mathematische Optimierung. Diese Algorithmen laufen online und bestimmen immer wieder Werte für Entscheidungsvariablen, wie z.B. die Öffnungen in einer Prozessanlage, indem sie ein mathematisches Optimierungsproblem einschließlich Zwänge und ein Modell des zu kontrollierenden Systems lösen. Geophysik Optimierungstechniken werden regelmäßig in geophysikalischen Parameterschätzungsproblemen eingesetzt. Angesichts einer Reihe von geophysikalischen Messungen, z.B. seismischen Aufzeichnungen, ist es üblich, die physikalischen Eigenschaften und geometrischen Formen der zugrunde liegenden Gesteinen und Flüssigkeiten zu lösen. Die meisten Probleme in Geophysik sind nichtlinear mit deterministischen und krchtischen Methoden, die weit verbreitet genutzt werden. Molekulare Modellierung nichtlinearer Optimierungsmethoden werden in der Konformitätsanalyse weit verbreitet. Computational Systems Biologie Optimierungstechniken werden in vielen Facetten der Rechensysteme Biologie wie Modellbau, optimale experimentelles Design, Stoffwechseltechnik und synthetische Biologie eingesetzt. Lineare Programmierung wurde zur Berechnung der maximalen Erträge von Gärbenerzeugnissen und zur Einspeisung von Gene-Regulierungsnetzen aus mehreren Mikroarray-Datensets sowie Transkriptal-Regulierungsnetzen aus hochüberlegten Daten angewendet. Nichtlineare Programmierung wurde verwendet, um den Energieverbrauch zu analysieren und wurde auf Stoffwechseltechnik und Parameterschätzung in biochemischen Wegen angewendet. Maschinenlernen Solvers Siehe auch HinweiseFurther Lesung Boyd, Stephen P; Vandenberghe, Hilen (2004). Convex Optimierung. Cambridge: Cambridge University Press.ISBN 0-521-83378-7.Gill, P. E; Murray, W; Wright, M. H. (1982) Praktische Optimierung. London: Fachpresse.ISBN 01/1283952-8.Lee, Jon (2004). Ein erster Kurs in der Kombinatorialoptimierung. Cambridge University Press.ISBN 0-521-01012-8.Nocedal, Jorge; Wright, Stephen J. (2006) Numerical Optimierung (2. Berlin: Springer.ISBN 0-387-30303-0.Snyman, J. A; Wilke, D. N. KPMG. Praktische mathematische Optimierung: Grundoptimierungstheorie und -abgangsbasis Algorithms (2. ed). Berlin: Springer.ISBN. Außenbeziehungen "Entscheidungsbaum für Optimierungssoftware". Links zu Optimierungscodes "Globale Optimierung". " EE364a: Convex Optimierung I". Varoquaux, Gaël. "Mathematische Optimierung: Suche nach Minima of Functions".