In den Bereichen Data Mining und Statistik ist hierarchisches Clustering (auch als hierarchische Clusteranalyse oder HCA bezeichnet) eine Methode der Clusteranalyse, die eine Hierarchie der Cluster erstellen soll. Strategien für hierarchische Clusterbildung fallen im Allgemeinen in zwei Arten: Agglomerative Hierbei handelt es sich um einen Bottom-up-Ansatz: Jede Beobachtung beginnt in ihrem eigenen Cluster, und Paare von Clustern werden als ein Schritt in die Hierarchie zusammengefasst. Kluft: Hierbei handelt es sich um einen Top-down-Ansatz: Alle Beobachtungen beginnen in einem Cluster, und die Spaltungen werden rekonsensiv durchgeführt, als ein Schritt in der Hierarchie. Insgesamt werden die Zusammenschlüsse und Spaltungen auf eine bestimmte Art und Weise bestimmt. Die Ergebnisse der hierarchischen Clusterbildung werden in der Regel in einem derdrogram präsentiert. Der Standard-Algorithmus für hierarchisches Clustering (HAC) hat eine Zeitkomplexisierung von O (n 3 ) Memedisplaystyle Meme Mathematikcal O O(n3)3) und erfordert fluor (n 2 ) Memestyle \nOmega2), was es zu langsam für noch mittelgroße Datensets macht. Manche Sonderfälle sind jedoch bekannt: SLINK für Single-linkage und CLINK für die gesamte Verbindung von Clustern. Mit einem Heap kann die Laufzeit des allgemeinen Falles auf O (n 2 log ) n ) HANAdisplaystyle SSOmathcal O O(n22}\log n)} gesenkt werden, eine Verbesserung der oben genannten Grenze von O (n 3 ) n faserstyle fasercal O O(n3)3) zu den Kosten einer weiteren Erhöhung der Speicheranforderungen. In vielen Fällen sind die Gedächtnisüberschüsse dieses Ansatzes zu groß, um es praktisch nutzbar zu machen. Keines der Algorithmen (ausgenommen erschöpfende Suche in O ( 2 n ) Memedisplaystyle Meme Mathematikcal O((2^{n) kann garantiert werden, um die beste Lösung zu finden. Kläranlage mit einer erschöpfenden Suche ist O ( 2 n ) Memedisplaystyle fasercal O O(2.n) , aber es ist üblich, schnelleren Hetourismus zu nutzen, um verschiedene Arten wie k-means auszuwählen. Cluster-Disimilarität Um zu entscheiden, welche Cluster kombiniert werden sollten (für die Gemeinden), oder in denen ein Cluster aufgeteilt werden sollte (für die Diavisive), ist eine Maßnahme der Ungleichbehandlung zwischen Beobachtungen erforderlich. In den meisten Methoden der hierarchischen Clusterbildung wird dies durch die Verwendung eines geeigneten Parameters (eine Messung der Entfernung zwischen den Beobachtungen) und eines Linkage-Kriteriums erreicht, das die Ungleichbehandlung von Set als Funktion der zweigleisigen Entfernungen der Beobachtungen in den Sätzen definiert. Metric Die Wahl eines geeigneten Parameters wird die Form der Cluster beeinflussen, da einige Elemente relativ nah an einem anderen unter einem Parameter als einem anderen sein können. In zwei Dimensionen, unter der Fernmessung von Manhattan, ist die Entfernung zwischen dem Ursprung (0,0) und (0.5, 0,5) die gleiche wie die Entfernung zwischen dem Ursprung und (0, 1), während die Entfernung zwischen Euclidean, die letztere streng größer ist. Manche häufig verwendete Parameter für hierarchische Cluster sind: Für Text oder andere nicht-numerische Daten werden häufig Parameter wie die Hamming- Entfernung oder Levenshtein Entfernung verwendet. Eine Bewertung der Clusteranalyse in der Medizin-Forschung ergab, dass die häufigste Fernmaßnahme in veröffentlichten Studien in diesem Forschungsbereich die Entfernung von Euclidean oder die Quadratkilometer von Euclidean ist. Linke Kriterien Das Linkage-Kriterium bestimmt die Entfernung zwischen den Beobachtungen als Funktion der zweigleisigen Entfernungen zwischen Beobachtungen. Manche häufig verwendeten Linkage-Kriterien zwischen zwei Arten von Beobachtungen A und B sind: wo d das gewählte Maß ist. Andere Bezugskriterien umfassen: Summe aller Unterschiede innerhalb des Clusters. Je größer die Varianz des Clusters ist (Ward-Kriterium). Wahrscheinlichkeit, dass Kandidatencluster von derselben Vertriebsfunktion (V-linkage) stammen. Produkt in-Grad und Out-Grad auf einem k-norest-neighbour-Schätz (graphial-Linkage). Zusammensetzung einiger Cluster-Deskriptor (d. h. eine Menge, die für die Messung der Qualität eines Clusters bestimmt ist) nach der Zusammenlegung zweier Cluster. Diskussion Hierarchisches Clustering hat den eindeutigen Vorteil, dass jede gültige Fernmessung verwendet werden kann. Konkret sind die Beobachtungen selbst nicht erforderlich: alle, die verwendet werden, sind eine Matrix von Entfernungen. Agglomeratives Cluster-Beispiel Man braucht beispielsweise, dass diese Daten gebündelt werden müssen, und die Entfernung Euclidean ist die Entfernung. Die hierarchische Clustering Dendrogram wäre so: Zerkleinert der Baum auf einer bestimmten Höhe, so wird eine Abspaltung in einer ausgewählten Präzision vorgenommen. In diesem Beispiel wird die Zerkleinerung nach der zweiten Reihe (von oben) des Dendrogram Clusters {a} {b c} {d e} {f} ausmachen. Zerkleinert nach der dritten Folge werden Cluster {a} {b c} {d e f,}, das ist ein grober Cluster mit einer kleineren Zahl, aber größere Cluster. Diese Methode baut die Hierarchie aus den einzelnen Elementen auf, indem Cluster schrittweise zusammengeführt werden. In unserem Beispiel haben wir sechs Elemente {a} {b} {c} {d} {e} und {f}. Der erste Schritt ist die Feststellung, welche Elemente in einem Cluster zusammengefasst werden. In der Regel wollen wir die beiden nächsten Elemente entsprechend der gewählten Entfernung einnehmen. Fakultativ kann man in dieser Phase auch eine Fernmatrix erstellen, in der die Nummer in der i-th-Sequenz die Entfernung zwischen den i-th und j-th-Elementen ist. In diesem Fall werden die Cluster zusammengelegt und die Entfernungen aktualisiert. Dies ist ein gemeinsamer Weg zur Umsetzung dieser Art von Clustern und hat den Vorteil, dass zwischen Clustern Entfernungen entstehen. Ein einfacher agglomerativer Clustering-Algorithmus wird in der Gruppen-Clusterseite beschrieben; es kann leicht an verschiedene Arten von Linkage angepasst werden (siehe unten). Jetzt haben wir die beiden nächsten Elemente b und c zusammengefasst, wir verfügen über die folgenden Cluster {a,} {b, c,} {d,} {e} und {f,} und wollen sie weiter verschmelzen. Zu diesem Zweck müssen wir den Abstand zwischen {a} und {b c) nehmen und daher die Entfernung zwischen zwei Clustern definieren. Regel die Entfernung zwischen zwei Clustern A HANAdisplaystyle {A} und B KINGstyle fasercal {B} gehören zu den folgenden: Höchste Entfernung zwischen den Elementen jedes Clusters (auch als komplettes Bindeglied): max { d ( x , y ) : x  A A , y  B B } . 7.8displaystyle max,d(x,y):x\in haushaltcal {AA,\,y\in ggiocal {B,\} Mindestabstand zwischen den einzelnen Clustern (auch als Single-linkage-Clustering): min { d ( x , y ) : x  A A , y  B B } . Memestyle min\{\,d(x,y):x\in haushaltcal {AA,\,y\in ggiocal {B,\} Die mittlere Entfernung zwischen den Elementen jedes Clusters (auch als durchschnittliche Verbindungscluster bezeichnet, z.B. in UPGMA): 1 | A |  B  A A  A A  B y  B B d ( x , y ) . KINGstyle {1 \over ggio· mathematischcal {A|\cdot WELL mathematischcal {B||}}\sum {_x\in ggio Mathematikcal {AAsum {_y\in ggio Mathematikcal {B(d(x,y)) Summe aller Unterschiede innerhalb des Clusters. Die zunehmende Varianz für das Cluster, das zusammengelegt wird (Ward-Methode)Die Wahrscheinlichkeit, dass Kandidatencluster aus der gleichen Vertriebsfunktion (V-linkage) stammen. Bei gebundenen Mindestentfernungen wird ein Paar zufällig ausgewählt, so dass es in der Lage ist, mehrere strukturelle Unterschiede zu erzeugen. Alternativ können alle gebundenen Paare gleichzeitig mit einer einzigartigen Dendrogram entstehen. Man kann immer entscheiden, Cluster zu stoppen, wenn eine ausreichende Anzahl von Clustern vorhanden ist (Anzahlkriterium). Manche Verknüpfungen können auch garantieren, dass die Ballungsräume in einer größeren Entfernung zwischen Clustern als der vorherigen Ballungsräume auftreten, und dann kann eine Clustering stoppen, wenn die Cluster zu weit auseinander gehen, um zusammenzutreffen (Fernkriterium). Jedoch ist dies nicht der Fall, z.B. der Zenroid-Linkage, wo die sogenannten Umkehren (Einleitungen, Abfahrten aus der Ultrametrie) auftreten können.ing Cluster Der Grundprinzip der visiven Clusterbildung wurde als DIANA (DIvisive ANA Clustering)-Algorithmus veröffentlicht. Anfangs sind alle Daten im gleichen Cluster, und der größte Cluster ist aufgeteilt, bis jeder Gegenstand getrennt ist. Da es O ( 2 n ) Memedisplaystyle O(2^{n)} gibt, wie jeder einzelne Cluster aufgeteilt werden kann, sind ertourismus. DIANA wählt das Objekt mit dem maximalen durchschnittlichen Unterschied aus und bringt dann alle Gegenstände in diesen Cluster, die dem neuen Cluster ähnlich sind als der Rest. Software Open Source Implementations ALGLIB führt mehrere hierarchische Cluster-Algorithmen (Single-link, Komplett-link, Ward) in C+ und C# mit O(n2)-Speicher und O(n3). ELKI umfasst mehrere hierarchische Cluster-Algorithmen, verschiedene Verknüpfungsstrategien und umfasst auch die effizienten SLINK-, CLINK- und Anderberg-Algorithmen, die flexible Clustergewinnung aus dendrograms und verschiedene andere Clusteranalysen. Octave, die GNU analog zu VMware führt hierarchische Clusterbildung in Funktionsverhältnissen durch. Orange, eine Daten-Mining-Software, umfasst hierarchische Clustering mit interaktiver Dendrogram visuelleisierung. R hat viele Pakete, die Funktionen für die hierarchische Clusterbildung bieten. SciPy führt hierarchische Clusterbildung in Skype durch, einschließlich des effizienten SLINK-Algorithmus. Skikit-learn führt auch hierarchische Clusterbildung in Python durch. Wirka umfasst hierarchische Clusteranalyse. Kommerzielle Umsetzungen umfassen hierarchische Clusteranalyse. SAS umfasst hierarchische Clusteranalyse in PROC CLUSTER. KPMG umfasst ein Paket von Hierarchischen Clustern. NCSS umfasst hierarchische Clusteranalyse. SPSS umfasst hierarchische Clusteranalyse. Qlucore Omics Explorer umfasst hierarchische Clusteranalyse. Stata umfasst hierarchische Clusteranalyse. KriminalitätStat umfasst einen nahesten benachbarten hierarchischen Cluster-Algorithmus mit einem grafischen Output für ein geographisches Informationssystem. Siehe auch weitere Lesung Kaufman, L; Rousseeuw, P.J (1990). Gruppen in Daten: Einführung zu Clusteranalyse (1 ed). New York: John Kuhn. 3471-87876-6.Hastie, Trevor; Tibshirani, Robert; Friedman, Volker (2009).“14.3.12 Hierarch Clustering. Elemente des Statistischen Lernens (2. New York: Springer.pp.520–8.ISBN UV0-387-84857-0. Archiviert vom Original (PDF) für 2009-11-10.Retrieved 2009-10-20.