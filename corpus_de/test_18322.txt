Eine Metaanalyse ist eine statistische Analyse, die die Ergebnisse mehrerer wissenschaftlicher Studien kombiniert. Meta-Analysen können durchgeführt werden, wenn es mehrere wissenschaftliche Studien gibt, die dieselbe Frage angehen, wobei jede einzelne Studienberichterstattung Messungen, die voraussichtlich einen gewissen Fehler aufweisen werden, zu erwarten sind. Ziel ist es dann, Ansätze aus Statistiken zu verwenden, um eine gemeinsame Schätzung nahe der unbekannten gemeinsamen Wahrheit zu erstellen, die auf der Wahrnehmung dieses Fehlers beruht. Nicht nur kann die Meta-Analyse eine Schätzung der unbekannten gemeinsamen Wahrheit vorweisen, sie verfügt auch über die Fähigkeit, Ergebnisse aus verschiedenen Studien abzugleichen und Muster zwischen Studienergebnissen, Konfliktquellen zwischen diesen Ergebnissen oder anderen interessanten Beziehungen zu ermitteln, die mit mehreren Studien zu sehen sind. Jedoch muss ein Ermittler bei der Durchführung einer Metaanalyse Entscheidungen treffen, die die Ergebnisse beeinflussen können, einschließlich der Entscheidung, wie Studien zu suchen, Studien auf der Grundlage objektiver Kriterien auszuwählen, unvollständige Daten zu behandeln, die Daten zu analysieren und die Daten zu analysieren oder sich nicht für Veröffentlichungsverzerrungen zu entscheiden. Urteilsersuchen, die bei der Fertigstellung einer Metaanalyse gemacht wurden, können die Ergebnisse beeinflussen. Beispielsweise untersuchten Wanous und Kolleginnen und Kollegen vier Meta-Analysen zu den vier Themen (a) Arbeitsleistung und Zufriedenheitsverhältnis, b) realistische Jobvorausschauen, c) korreliert von Rollekonflikten und Ambiguity und (d) der Arbeitszufriedenheit und des fehlenden Verhältnisses und verwies auf die unterschiedlichen Aufrufe der Forscher. Meta-Analysen sind häufig, aber nicht immer wichtige Bestandteile eines systematischen Überprüfungsverfahrens. Beispielsweise kann eine Metaanalyse auf mehreren klinischen Prüfungen einer medizinischen Behandlung durchgeführt werden, um ein besseres Verständnis der Funktionsweise der Behandlung zu erhalten. Hier ist es zweckmäßig, die von der Cartier-Kooperation verwendete Terminologie zu verfolgen und die Metaanalyse zu verwenden, um statistische Methoden zur Kombination von Beweismitteln zu nennen, andere Aspekte der „Forschungssynthese“ oder der „Evidence-Synthese“, wie die Kombination von Informationen aus qualitativen Studien, für den allgemeineren Kontext systematischer Überprüfungen. Eine Metaanalyse ist eine sekundäre Quelle. Geschichte Die historischen Wurzeln der Metaanalyse lassen sich auf 17th Jahrhundert Studien der Astronomie zurückverfolgen, während ein im Jahr 1904 vom Statistikian Karl Pearson im britischen Medizinischen Journal veröffentlichtes Papier, das Daten aus mehreren Studien über Typhoid-Inoculation gesammelt hat, zum ersten Mal wurde ein Meta-Analysator-Ansatz für die Gesamtergebnis mehrerer klinischer Studien verwendet. Die erste Meta-Analyse aller konzeptuell identischen Experimente, die ein bestimmtes Forschungsthema betreffen und von unabhängigen Forschern durchgeführt werden, wurde als die buchmäßige Veröffentlichung Extrasensory Perzeptanz nach sechzehn Jahren ermittelt, die von Herzog University Psychologen J. G. Pratt, J. B. Rhein und assoziierten Personen genehmigt wurde. In diesem Zusammenhang wurden 145 Berichte über ESP-Tests, die von 1882 bis 1939 veröffentlicht wurden, überprüft und eine Schätzung des Einflusses unveröffentlichter Papiere auf den Gesamteffekt (das Problem der Datei) enthalten. 1976 wurde der Begriff Metaanalyse von der Statistikian Gene V durchgeführt. Glas, der "das große Interesse hat, ist derzeit in dem, was wir haben, die Metaanalyse der Forschung. Der Begriff ist ein wenig groß, aber es ist genau und apt .Meta-Analyse bezieht sich auf die Analyse von Analysen." Obwohl dies dazu führte, dass er als moderner Gründer der Methode weithin anerkannt wurde, legt die Methode hinter dem, was er als Metaanalyse bezeichnete, vor, dass seine Arbeit mehrere Jahrzehnte vornimmt. Die statistische Theorie im Zusammenhang mit der Metaanalyse wurde von der Arbeit von Nambury S. Raju, Larry V. Hedges, Harris Cooper, Ingram Olkin, John E. Hunter, Jacob Cohen, Thomas C. Chalmers, Robert Rosenthal, Frank L. Schmidt, John E. Hunter und Douglas G. Bonett sehr weit fortgeschritten. 1992 wurde die Metaanalyse zunächst auf ökologische Fragen von Jessica Gurevitch angewandt, die Metaanalyse zur Untersuchung des Wettbewerbs in Feldversuchen verwendet haben. Maßnahmen in einer Metaanalyse Eine Metaanalyse wird in der Regel von einer systematischen Überprüfung begleitet, da dies die Ermittlung und kritische Bewertung aller relevanten Beweise (durch Begrenzung des Risikos von Verzerrungen in Zusammenfassungen) ermöglicht. Die allgemeinen Schritte sind wie folgt: Formulierung der Forschungsfrage, z.B. mit dem PICO-Modell (Bevölkerung, Intervention, Vergleich, Outcome). Suche nach Literaturauswahl von Studien „(Einführungskriterien)“ auf der Grundlage von Qualitätskriterien, z.B. der Anforderung der Randomisierung und der Erblindung in einer klinischen Testauswahl bestimmter Studien zu einem gut spezifizierten Thema, z.B. der Behandlung von Brustkrebs. Legt man fest, ob unveröffentlichte Studien enthalten sind, um Veröffentlichungsverzerrungen zu vermeiden (Sache-Problem)Decide, die abhängige Variablen oder zusammenfassende Maßnahmen sind. z.B. bei der Prüfung einer Metaanalyse veröffentlichter (Aggregate) Daten: Unterschiede (unterschiede) Means (kontinuierliche Daten) Hedges g ist eine beliebte Zusammenfassungsmaßnahme für fortlaufende Daten, die standardisiert werden, um Größenunterschiede zu beseitigen, enthält aber einen Index der Differenzen zwischen Gruppen:  = = μ t σ , Memedisplaystyle \delta Meme=frac 7.8mu {_t}-\mu _c}}{\sigma }},}, in dem μ t displaystyle \mu_{t} die Behandlung bedeutet, μ c RARstyle \mu {_c} ist das Kontrollmittel,  2 2 574 \sigma \sigma {^2} die gepoolte Varianz. Auswahl eines Meta-Analyse-Modells, z.B. feste Wirkung oder zufällige Metaanalyse. Prüfen von Quellen der heterogenen Heterogenität zwischen Studiengruppen, z.B. durch Untergruppenanalyse oder Meta-Regression. Formale Leitlinien für die Durchführung und Berichterstattung von Meta-Analysen werden vom Cartier Handbuch bereitgestellt. Berichterstattungsleitlinien, siehe die bevorzugten Berichtspunkte für Systematikprüfungen und Meta-Analysen (PRISMA). Methoden und Annahmen Insgesamt können zwei Arten von Beweismitteln bei der Durchführung einer Metaanalyse unterschieden werden: einzelne Teilnehmerdaten (IPD) und Gesamtdaten (AD). Die Gesamtdaten können direkt oder indirekt sein. AD ist häufiger verfügbar (z.B. aus der Literatur) und stellt in der Regel zusammenfassende Schätzungen wie widersprüchliche Verhältnisse oder relative Risiken dar. Dies kann direkt in konzeptuell ähnlichen Studien mit verschiedenen Ansätzen (siehe unten) zusammengefasst werden. indirekte Aggregatdaten hingegen wirken sich auf die Wirkung von zwei Behandlungen aus, die jeweils gegen eine ähnliche Kontrollgruppe in einer Metaanalyse verglichen wurden. Wenn die Behandlung A und Behandlung B beispielsweise in getrennten Meta-Analysen direkt gegenüber Placebo verglichen wurden, können wir diese beiden zusammengefassten Ergebnisse nutzen, um eine Schätzung der Auswirkungen von A gegen B im indirekten Vergleich zu Placebo minus Effekt B gegen Placebo zu erhalten. IPD-Befunde stellen die von den Studienzentren gesammelten Rohdaten dar. Diese Unterscheidung hat die Notwendigkeit unterschiedlicher Meta-Analysatoren, wenn die Nachweissynthese gewünscht wird, erhöht und zur Entwicklung einer Phase und zweistufiger Methoden geführt. In einemstufigen Verfahren werden die IPD aus allen Studien gleichzeitig modelliert, während sie für die Clusterbildung der Teilnehmer innerhalb von Studien verantwortlich sind. Zweistufige Methoden berechnen zusammenfassende Statistiken für AD aus jeder Studie und berechnen dann die Gesamtstatistiken als gewichteter Durchschnitt der Studienstatistiken. Indem die IPD auf AD reduziert wird, können zweistufige Methoden auch angewendet werden, wenn IPD verfügbar ist; dies macht sie für die Durchführung einer Metaanalyse attraktiv. Obwohl es in der Regel der Ansicht ist, dass einstufige und zweistufige Methoden ähnliche Ergebnisse liefern, haben die jüngsten Studien gezeigt, dass sie gelegentlich zu unterschiedlichen Schlussfolgerungen führen können. Statistische Modelle für die Zusammenfassung der Daten Direct: Modelle, die Studieneffekte nur auf feste Wirkungsmodelle integrieren Das Modell der fixen Wirkung liefert einen gewichteten Durchschnitt einer Reihe von Studienschätzungen. Im Gegensatz zu den Schätzungen wird die Varianz häufig als Studiengewicht verwendet, so dass größere Studien tendenziell mehr als kleinere Studien zum gewichteten Durchschnitt beitragen. Wenn Studien innerhalb einer Metaanalyse von einer sehr großen Studie dominiert, werden die Ergebnisse kleinerer Studien praktisch ignoriert. Insbesondere geht aus dem Modell der festen Effekte hervor, dass alle Studien die gleiche Bevölkerung untersuchen, dieselben variablen und Ergebnisdefinitionen verwenden. Diese Annahme ist in der Regel unrealistisch, da die Forschung häufig auf mehrere Quellen der Heterogenität hinwirken wird; z.B. die Behandlungseffekte können sich je nach Ort, Dosisniveau, Studienbedingungen, ... Zufallswirkungsmodell unterscheiden. Ein gemeinsames Modell, das zur heterogenen Forschung verwendet wird, ist das Zufallsmodell der Metaanalyse. Dies ist einfach der gewichtete Durchschnitt der Wirkungsgrößen einer Studiengruppe. Das Gewicht, das in diesem Prozess gewichteter Mittelwerte mit einer Zufallsanalyse angewendet wird, wird in zwei Schritten erreicht: Schritt 1: Ungleichgewichtung Schritt 2: Ungewichtung dieser Unvarianzgewichtung durch Anwendung einer zufälligen Wirkungskomponente (REVC), die einfach aus dem Umfang der Variabilität der Wirkungsgrößen der zugrunde liegenden Studien abgeleitet ist. Dies bedeutet, dass die größere Variabilität in Effektgrößen (sondere als Heterogenität), die größere Ungewichtung und dies einen Punkt erreichen können, wenn die zufällige Meta-Analyse einfach die ungewichtete durchschnittliche Wirkungsgröße in den Studien darstellt. Extrem, wenn alle Wirkungsgrößen ähnlich sind (oder Variabilität den Probenahmefehler nicht überschreitet), wird keine REVC angewendet und die Zufallswirkungen Meta-Analyse-Standards auf eine fixe Metaanalyse (nur inverse Varianzgewichtung). Der Umfang dieser Umkehr hängt allein von zwei Faktoren ab: Heterogenität der WirkungsgrößeSince weder von diesen Faktoren zeigt automatisch eine fehlerhafte größere Studie oder zuverlässigere kleinere Studien, die Umverteilung der Gewichte im Rahmen dieses Modells wird keine Beziehung zu dem, was diese Studien tatsächlich anbieten könnten. Man hat gezeigt, dass die Umverteilung der Gewichte einfach in einer Richtung von größeren bis kleineren Studien ist, da die Heterogenität bis schließlich alle Studien gleichgewicht sind und keine Umverteilung möglich ist. Ein weiteres Problem mit dem Zufallsmuster ist, dass die am häufigsten verwendeten Vertrauensintervalle in der Regel ihre Deckungswahrscheinlichkeit über dem angegebenen nominalen Niveau nicht behalten und somit den statistischen Fehler deutlich unterschätzt und in ihren Schlussfolgerungen möglicherweise übervertraulich sind. Mehrere Punkte wurden vorgeschlagen, aber die Debatte geht weiter. Mehr Sorge ist, dass der durchschnittliche Behandlungseffekt im Vergleich zum Fixeffektmodell sogar weniger konservativer sein kann und daher in der Praxis irreführend ist. Ein Auslegungsbeschluss, der vorgeschlagen wurde, ist die Erstellung eines Vorhersageintervalls rund um die zufällige Wirkungsschätzung zur Darstellung der möglichen Auswirkungen in der Praxis. Eine Annahme hinter der Berechnung eines solchen Vorhersageintervalls ist jedoch, dass die Prüfungen mehr oder weniger homogene Einheiten gelten und dass Patientenpopulationen und Vergleichsverfahren als austauschbar gelten sollten und dies in der Praxis unhaltbar ist. Die am häufigsten angewandte Methode zur Schätzung der Unterschiede zwischen Studien (REVC) ist das Konzept des Dersimonic-Laird (DL). Mehrere fortgeschrittene Iterative (und rechnerisch teure) Techniken zur Rechenierung der zwischen Studienvariationen bestehenden Unterschiede (wie maximale Wahrscheinlichkeit, Profilwahrscheinlichkeit und eingeschränkte Höchstwahrscheinlichkeits-Methoden) und Zufalls-Wirkungsmodelle, die diese Methoden verwenden, können in Stata mit dem Metaan Befehl betrieben werden. Der Metaan Befehl muss von dem klassischen Metan (Single a) Befehl in Stata unterschieden werden, der den DL-Estimator nutzt. Diese fortgeschrittenen Methoden wurden auch in freier und leicht zu nutzen Microsoft Exzellenz Add-on, MetaEasy. Jedoch zeigte ein Vergleich zwischen diesen fortgeschrittenen Methoden und der DL-Methode der Rechen zwischen Studienvariationen, dass es in den meisten Szenarien wenig zu gewinnen und DL recht angemessen ist. Die meisten Meta-Analysen umfassen jedoch zwischen 2 und 4 Studien und eine solche Probe ist häufiger als nicht ausreichend, um die Heterogenität genau zu schätzen. Man scheint also, dass in kleinen Meta-Analysen eine falsche Null zwischen Studienvarianzschätzung erreicht wird, was zu einer falschen Homogenitätsannahme führt. Insgesamt scheint die Heterogenität in Meta-Analysen und Sensitivitätsanalysen, bei denen hohe heterogene Werte berücksichtigt werden, konsequent zu.. Diese oben genannten Zufalls-Wirkungsmodelle und Software-Pakete beziehen sich auf Studien-Aggregate Meta-Analysen und Forscher, die individuelle Patientendaten (IPD) durchführen möchten, müssen unterschiedliche Wirkungsmodelle in Betracht ziehen. IVhet Modell Doi & Barendregt, das in Zusammenarbeit mit Khan, Thalib und Williams (von der University of Queensland, der University of Southern Queensland und der Kuwait University) arbeitet, haben eine unverse Varianz-ähnliche Alternative (IVhet) zum Modell für zufällige Auswirkungen (RE) geschaffen, für das Informationen online verfügbar sind. Dies wurde in die MetaXL-Version 2.0 aufgenommen, eine kostenlose Microsoft-Exposition für Metaanalyse, die von Epirie International Pty Ltd hergestellt wurde und am 5. April 2014 zur Verfügung gestellt wurde. Die Autoren weisen darauf hin, dass ein eindeutiger Vorteil dieses Modells darin besteht, die beiden wichtigsten Probleme des Zufallsmodells zu lösen. Der erste Vorteil des IVhet-Modells besteht darin, dass die Abdeckung im nominalen (normalerweise 95)%-Niveau für das Vertrauensintervall im Gegensatz zum Zufallseffektmodell, das die Abdeckung mit zunehmender Heterogenität abnimmt, auf dem nominalen Niveau bleibt. Der zweite Vorteil besteht darin, dass das IVhet-Modell im Gegensatz zum RE-Modell, das kleine Studiengewicht (und damit größere Studien weniger) mit zunehmender Heterogenität verleiht, die umgekehrten Unterschiede der einzelnen Studien bewahrt. Wenn die Heterogenität groß wird, werden die einzelnen Studiengewichte des RE-Modells gleich und damit das RE-Modell einen aritmetischen Mittelwert anstelle eines gewichteten Mittelwerts. Diese Nebenwirkung des RE-Modells kommt nicht mit dem IVhet-Modell zustande, das sich somit von der RE-Modellschätzung in zwei Perspektiven unterscheidet: Gepoolte Schätzungen werden größere Versuche (im Gegensatz zur Verhängung größerer Prüfungen im RE-Modell) begünstigen und ein Vertrauensintervall haben, das unter Unsicherheit (Terogenität) bleibt. Doi & Barendregt schlägt vor, dass das RE-Modell zwar eine alternative Methode zur Bündelung der Studiendaten bietet, ihre Simulationsergebnisse zeigen, dass die Verwendung eines genaueren Wahrscheinlichkeitsmodells mit unhaltbaren Annahmen, wie im RE-Modell, nicht unbedingt bessere Ergebnisse bietet. Letztere Studie berichtet auch, dass das IVhet-Modell die Probleme im Zusammenhang mit der Einschüchterung des statistischen Fehlers, der unzureichenden Erfassung des Vertrauensintervalls und der zunehmenden MSE, die mit dem Zufalls-Wirkungsmodell zu sehen sind, und die Autoren kommen zu dem Schluss, dass Forscher die Verwendung des Zufalls-Wirkungsmodells in der Meta-Analyse künftig einstellen sollten. Obwohl ihre Daten zwingend sind, sind die Ramifications (in Bezug auf das Ausmaß der schädlich positiven Ergebnisse in der Cartier-Datenbank) enorm und damit die Annahme dieses Abschlusses erfordert eine sorgfältige unabhängige Bestätigung. Die Verfügbarkeit einer kostenlosen Software (MetaXL), die das IVhet-Modell (und alle anderen Vergleichsmodelle) betreibt, erleichtert dies für die Forschungsgemeinschaft. Direkte Beweise: Modelle, die zusätzliches Qualitäts-Wirkungsmodell Doi und Thalib enthalten, haben ursprünglich das Qualitäts-Wirkungsmodell eingeführt. Sie haben einen neuen Ansatz zur Anpassung der Inter-Study-Sortierbarkeit eingeführt, indem sie den Beitrag der Varianz durch einen relevanten Bestandteil (Qualität) zusätzlich zu dem Beitrag der Varianz aufgrund zufälliger Fehler, die in jedem fixen Metaanalysemodell verwendet werden, um Gewichte für jede Studie zu erzeugen. Stärke der Meta-Analyse von Qualitätseffekten ist, dass die verfügbaren methodischen Nachweise über subjektive zufällige zufällige zufällige Wirkungen genutzt werden können und somit dazu beitragen, die schädlichen Lücke zu schließen, die zwischen Methodik und Statistiken in der klinischen Forschung geöffnet hat. Um dies zu tun, wird eine synthetische einseitige Varianz auf der Grundlage von Qualitätsinformationen berechnet, um die unverse Varianzgewichte anzupassen und das Qualitätsgewicht der Ith-Studie einzuführen. Diese angepassten Gewichte werden dann in der Metaanalyse verwendet. In anderen Worten, wenn die Studie i von guter Qualität ist und andere Studien von schlechter Qualität sind, ist ein Anteil ihrer qualitativ angepassten Gewichte mathematisch umgerechnet, um zu studieren, d. h. sie ist stärker auf die Gesamtwirkungsgröße ausgerichtet. Da die Studien in Bezug auf Qualität immer ähnlich werden, wird die Umverteilung allmählich weniger und endet, wenn alle Studien gleiche Qualität aufweisen (bei gleicher Qualität, den Standardfehlern des IVhet-Modells – siehe vorheriger Abschnitt). Eine kürzlich durchgeführte Bewertung des Qualitäts-Wirkungsmodells (mit einigen Aktualisierungen) zeigt, dass trotz der Qualitätsprüfung die Leistung (MSE und tatsächliche Varianz im Simulationsstadium) dem überlegen ist, das mit dem Zufallsmodell erreichbar ist. Dieses Modell ersetzt somit die unhaltbaren Interpretationen, die in der Literatur und in einer Software verfügbar sind, um diese Methode weiter zu erkunden. Indirekte Beweise: Interdirekte Vergleichsmeta-Analysemethoden (auch als Netzwerk- Metaanalyse bezeichnet, insbesondere wenn mehrere Behandlungen gleichzeitig bewertet werden) verwenden in der Regel zwei Hauptmethoden. Erstens ist die Bucher-Methode, die ein einziger oder wiederholter Vergleich eines geschlossenen Kreislaufs von drei Behandlungen ist, so dass eine von ihnen für die beiden Studien üblich ist und die node bildet, wenn der Teilnehmer beginnt und endet. Daher sind mehrere zwei bis zwei Vergleiche (3 Behandlungsschleifen) erforderlich, um verschiedene Behandlungen zu vergleichen. Diese Methode verlangt, dass Versuche mit mehr als zwei Waffen nur als unabhängige Paarvergleiche ausgewählt werden. Die alternative Methode verwendet komplexe statistische Modellierungen, um die mehrfachen Armversuche und Vergleiche gleichzeitig zwischen allen konkurrierenden Behandlungen einzubeziehen. Diese wurden mithilfe von Bayesian-Methoden, gemischten linearen Modellen und Meta-Regressionsansätzen ausgeführt. Bayesian Framework, in dem ein Modell für die Metaanalyse von Bayesian festgelegt ist, beinhaltet ein zyklisches Modell (DAG) für die allgemeine Markenov Kette Monte Carlo (MCMC) Software wie WinBUGS. Zusätzlich müssen die vorherigen Daten für eine Reihe von Parametern spezifiziert werden, und die Daten müssen in einem bestimmten Format geliefert werden. Gemeinsam bilden die DAG, die Vorstände und die Daten ein Bayesisches Hierarchiemodell. Um weitere Fragen zu erschweren, weil die MCMC-Abschätzungen die übermäßigen Ausgangswerte für eine Reihe unabhängiger Ketten bestimmen müssen, damit die Konvergenz beurteilt werden kann. Derzeit gibt es keine Software, die solche Modelle automatisch produziert, obwohl es einige Instrumente zur Hilfe im Prozess gibt. Die Komplexität des Bayesischen Ansatzes hat eine begrenzte Nutzung dieser Methode. Methode für die Automatisierung dieser Methode wurde vorgeschlagen, verlangt jedoch, dass streitige Ergebnisse verfügbar sind und dies in der Regel nicht möglich ist. Große Ansprüche werden manchmal für die inhärente Fähigkeit des Bayesischen Rahmens zur Bewältigung der Netzmetaanalyse und ihrer größeren Flexibilität gemacht. Jedoch kann diese Wahl der Umsetzung des Rahmens für Gleichgültigkeit, Bayesian oder Vielsprachigkeit weniger wichtig sein als andere Entscheidungen über das Modell der Auswirkungen (siehe Diskussion über die oben genannten Modelle). Multivariativer Rahmen Andererseits beinhalten die häufigen multivarialen Methoden Angleichungen und Annahmen, die bei der Anwendung der Methoden nicht ausdrücklich oder überprüft werden (siehe Diskussion über Metaanalysemodelle oben).Beispielsweise ermöglicht das mvmeta-Paket für Stata eine netzbasierte Metaanalyse in einem häufigen Rahmen. Liegt jedoch kein Gemeinsamer Vergleich im Netz vor, muss dies durch eine Erweiterung des Datensatzes mit fiktiven Waffen mit hoher Varianz behandelt werden, was nicht sehr objektiv ist und eine Entscheidung darüber erfordert, was eine hinreichend hohe Varianz darstellt. Das andere Thema ist die Verwendung des Zufalls-Wirkungsmodells sowohl in diesem häufigen Rahmen als auch im Rahmen der Buchten. Senn berät die Analysten, um vorsichtig zu sein, um die „random Impact“-Analyse zu interpretieren, da nur ein Zufallseffekt zulässig ist, aber eine könnte viele in Betracht ziehen. Senn geht davon aus, dass es eher naıve ist, auch wenn nur zwei Behandlungen verglichen werden, wenn davon ausgegangen wird, dass Zufallsanalysen für alle Ungewisssheit über die Art und Weise, wie die Wirkungen von der Probe bis zum Test variieren können. Neue Modelle der Metaanalyse wie die oben diskutierten würden sicherlich dazu beitragen, diese Situation zu lindern und wurden im nächsten Rahmen umgesetzt. Allgemeines Modellkonzept Ein seit Ende der 90er Jahre erprobter Ansatz ist die Umsetzung der mehrfachen, dreifach behandelten, geschlossenen Kreislaufanalyse. Dies ist nicht populär, weil der Prozess schnell zu überwältigend ist, da die Komplexität der Netze zunimmt. Entwicklung in diesem Bereich wurde dann zugunsten des Bayesischen und multivariativen Methoden, die als Alternative entstanden sind, aufgegeben. Kürzlich wurde die Automatisierung der dreiseitigen Sperrleitungsmethode für komplexe Netze von einigen Forschern entwickelt, um diese Methode der allgemeinen Forschungsgemeinschaft zur Verfügung zu stellen. In diesem Vorschlag wird jeder Versuch auf zwei Interventionen beschränkt, aber auch eine Arbeitsumwandlung für mehrere Armversuche eingeführt: eine andere feste Kontrolle kann in verschiedenen Phasen ausgewählt werden. Sie nutzt auch robuste Meta-Analysemethoden, damit viele der oben genannten Probleme vermieden werden. Weitere Forschungen über diesen Rahmen sind erforderlich, um festzustellen, ob dies tatsächlich den multivarialen oder multivariativen Rahmen der Buchten überlegen ist. Forscher, die bereit sind, dies zu versuchen, haben Zugang zu diesem Rahmen über eine kostenlose Software. Maßgeschneiderte Metaanalyse Eine weitere Form zusätzlicher Informationen stammt aus der geplanten Einrichtung. Wenn die Zielvorgabe für die Anwendung der Meta-Analyseergebnisse bekannt ist, kann es möglich sein, Daten von der Festsetzung zu verwenden, um die Ergebnisse so zu gestalten, dass eine „gemeinsame Metaanalyse“ erstellt wird. Dies wurde bei der Testgenauigkeit von Meta-Analysen verwendet, bei denen empirische Kenntnisse über die Test-positive Rate und die Prävalenz verwendet wurden, um eine Region im Bereich der Empfänger-Funktion (ROC) zu entwickeln, die als „anwendbare Region“ bekannt ist. Studien werden anschließend für das Ziel ausgewählt, das sich auf einen Vergleich mit dieser Region stützt und eine zusammenfassende Schätzung erstellt, die auf die Zielvorgabe zugeschnitten ist. Aggregating IPD und AD Meta-Analyse können auch zur Kombination von IPD und AD angewendet werden. Wenn die Forscher, die die Analyse durchführen, über ihre eigenen Rohdaten verfügen, während sie aggregierte oder zusammenfassende Daten der Literatur sammeln. Das allgemeine Integrationsmodell (GIM) ist eine allgemeine Einführung der Metaanalyse. Das Modell, das auf den einzelnen Teilnehmerdaten (IPD) montiert ist, unterscheidet sich von den Modellen zur Berechnung der Gesamtdaten (AD). GIM kann als Modellkalibrierungsmethode zur Integration von Informationen mit mehr Flexibilität angesehen werden. Validierung von Metaanalyseergebnissen Die Metaanalyse-Schätzung stellt einen gewichteten Durchschnitt für Studien dar, und wenn es heterogen ist, kann dies zu einer zusammenfassenden Schätzung führen, die nicht repräsentativ für einzelne Studien ist. Qualitative Beurteilung der primären Studien mit etablierten Werkzeugen kann mögliche Verzerrungen aufdecken, aber nicht quantifizieren die Gesamtwirkung dieser Verzerrungen auf die Zusammenfassung. Obwohl das Meta-Analyseergebnis mit einer unabhängigen prospektiven Primärstudie verglichen werden könnte, ist diese externe Validierung oft unpraktisch. Dies hat zur Entwicklung von Methoden geführt, die eine Form der einmaligen Cross-validierung nutzen, manchmal als interne externe Cross Validierung (IOCV) bezeichnet. In diesem Zusammenhang wird jeder der k genannten Studien im Vergleich zu der zusammenfassenden Schätzung, die aus der Aggregation der verbleibenden k-1-Studien abgeleitet wurde, gestrichen. Vn auf der Grundlage von IOCV wurde eine allgemeine Validierungsstatistik entwickelt, um die statistische Gültigkeit der Metaanalyseergebnisse zu messen. Für die Testgenauigkeit und Vorhersage, insbesondere wenn es multivariative Effekte gibt, wurden auch andere Ansätze vorgeschlagen, die auf die Schätzung des Vorhersagefehlers abzielen. Herausforderungen Eine Metaanalyse mehrerer kleiner Studien lässt die Ergebnisse einer einzigen großen Studie nicht immer vorhersagen. Manche haben argumentiert, dass eine Schwäche der Methode darin besteht, dass die Quellen von Verzerrungen nicht durch die Methode kontrolliert werden: Eine gute Metaanalyse kann nicht für schlechte Gestaltung oder Verzerrung in den ursprünglichen Studien korrigieren. Dies würde bedeuten, dass nur methodisch solide Studien in eine Metaanalyse aufgenommen werden sollten, eine Praxis, die „beste Nachweissynthese“ genannt wird. Andere Meta-analysts würden schwächere Studien umfassen und eine auf Studienebene vorhergesagte Variablen hinzufügen, die die methodische Qualität der Studien widerspiegelt, um die Wirkung der Studienqualität auf der Wirkungsgröße zu untersuchen. Andere haben jedoch argumentiert, dass ein besserer Ansatz darin besteht, Informationen über die Varianz der Studienprobe zu erhalten, so dass ein Netz so weit wie möglich entsteht und dass die methodischen Auswahlkriterien unerwünschte Diskriminierung einführen und den Zweck des Ansatzes abschwächen. Veröffentlichungsverzerrung: Das Problem eines anderen potenziellen Pitfalles ist die Abhängigkeit vom verfügbaren Gremium veröffentlichter Studien, die aufgrund von Veröffentlichungen zu übertriebenen Ergebnissen führen können, da Studien, die negative Ergebnisse zeigen oder unerhebliche Ergebnisse zeigen, weniger wahrscheinlich veröffentlicht werden. Pharmaunternehmen sind beispielsweise bekannt, negative Studien zu verbergen, und Forscher können unveröffentlichte Studien wie Studien oder Konferenzentnahmen, die keine Veröffentlichung erreicht haben, übersehen haben. Dies ist nicht leicht zu lösen, da man nicht wissen kann, wie viele Studien nicht gemeldet wurden. Dieses Dossier-Problem (die durch negative oder nicht unbedeutende Ergebnisse, die in einem Kabinett wegfallen) kann zu einer unparteiischen Verteilung der Wirkungsgrößen führen und somit zu einer schwerwiegenden Grundzinsverfallung führen, in der die Bedeutung der veröffentlichten Studien überschätzt wird, da andere Studien entweder nicht zur Veröffentlichung oder abgelehnt wurden. Dies sollte bei der Auslegung der Ergebnisse einer Metaanalyse ernst genommen werden. Die Verteilung der Wirkungsgrößen lässt sich mit einem berichterlichen Grundstück vergleichen, das (in seiner am häufigsten verwendeten Version) ein uneinheitliches Bild von Standardfehlern gegenüber der Wirkungsgröße ist. Man nutzt die Tatsache, dass die kleineren Studien (schwerere Standardfehler) die Wirkungshäufigkeit ( weniger präzise) stärker zerstreut haben, während die größeren Studien weniger zersplittert und die Spitze des Sternnels bilden. Wenn viele negative Studien nicht veröffentlicht wurden, ergibt sich aus den verbleibenden positiven Studien ein Bild, in dem die Basis auf eine Seite (Asymmetrie des Fledermausbaus) gedrängt wird. Kontrastlich, wenn es keine Veröffentlichungsvorkehrungen gibt, hat die Wirkung der kleineren Studien keinen Grund, auf eine Seite zu verdrängen und so eine symmetrische Flämigkeit zu erzielen. Dies bedeutet auch, dass, wenn keine Veröffentlichungsverzerrung vorliegt, kein Zusammenhang zwischen Standardfehler und Wirkungsgröße besteht. Ein negativer oder positiver Zusammenhang zwischen Standardfehler und Wirkungsgröße würde bedeuten, dass kleinere Studien, die nur in einer Richtung gefunden haben, wahrscheinlicher veröffentlicht und/oder veröffentlicht werden. Neben der visuellen Tonnel-Anlage wurden auch statistische Methoden zur Erkennung von Veröffentlichungen vorgeschlagen. Diese sind umstritten, weil sie in der Regel eine geringe Macht für die Erkennung von Verzerrungen haben, können aber auch falsche positive Wirkungen unter bestimmten Umständen bewirken. Kleine Studieneffekte (vorhergesehene kleinere Studien), bei denen methodische Unterschiede zwischen kleineren und größeren Studien bestehen, können zu asymmetrischen Effekten in Effektgrößen führen, die der Veröffentlichungsverzerrung entsprechen. Kleine Studieneffekte können jedoch genauso problematisch sein wie die Interpretation von Meta-Analysen, und die Notwendigkeit besteht darin, die möglichen Ursachen von Verzerrungen zu untersuchen. Es wurde vorgeschlagen, falsche positive Fehlerprobleme zu reduzieren. Diese Schweißmethode besteht aus drei Stufen. Erstens berechnet eine Berechnung des Ausfallrisikos von Orwin, um zu prüfen, wie viele Studien hinzugefügt werden sollten, um die Teststatistiken auf eine geringe Größe zu reduzieren. Wenn diese Zahl von Studien größer ist als die Zahl der Studien, die in der Metaanalyse verwendet werden, ist es ein Zeichen, dass es keine Veröffentlichungsvorkehrungen gibt, wie in diesem Fall, eine Menge von Studien benötigt, um die Wirkungsgröße zu verringern. Zweitens kann man einen Regressionstest von Egger durchführen, der prüft, ob das Fledermüll symmetrisch ist. Wie bereits erwähnt: ein symmetrisches Fledermaus ist ein Zeichen dafür, dass es keine Veröffentlichungen gibt, da die Wirkungsgröße und die Stichprobengröße nicht abhängig sind. Drittens kann man die Drei- und Deponierungsmethode tun, die Daten angibt, wenn das Fledermüll asymmetrischer ist. Das Problem der Verzerrung der Veröffentlichungen ist nicht uneinheitlich, da vorgeschlagen wird, dass 25 % der Meta-Analysen in den psychologischen Wissenschaften von Veröffentlichungsverzerrungen betroffen sind. Mangelnde Kraft bestehender Tests und Probleme mit dem visuellen Aussehen des Fledermülls bleibt jedoch ein Problem, und Schätzungen der Veröffentlichungsvorausschätzungen können niedriger sein als das, was wirklich existiert. Die meisten Diskussionen über Veröffentlichungen konzentrieren sich auf Zeitschriftenpraktiken, die die Veröffentlichung statistisch signifikanter Ergebnisse begünstigen. fragwürdige Forschungspraktiken, wie z.B. überarbeitete statistische Modelle, bis die Bedeutung erreicht ist, können jedoch auch statistisch signifikante Ergebnisse zur Unterstützung von Hypothesen der Forscher fördern. Probleme im Zusammenhang mit Studien, die nicht statistisch signifikante Wirkungsstudien melden, melden oft nicht die Auswirkungen, wenn sie nicht statistisch relevant sind. Man kann beispielsweise nur sagen, dass die Gruppen statistisch signifikante Unterschiede aufweisen, ohne andere Informationen (z.B. Statistiken oder p-Wert) zu melden. Ausschluß dieser Studien würde zu einer ähnlichen Situation führen wie die Veröffentlichung von Verzerrungen, aber ihre Einbeziehung (bei Nulleffekten) würde auch die Metaanalyse stören. MetaNSUE, eine von Joaquim Radua geschaffene Methode, hat gezeigt, dass Forscher unvoreingenommen diese Studien einbeziehen können. Ihre Schritte sind wie folgt: Maximale Wahrscheinlichkeitsschätzung des Meta-Analysators und der Heterogenität zwischen Studien. Multiple Anrechnung der NSUEs mit Lärm auf die Schätzung des Effekts. getrennte Meta-Analysen für jede anputierte Datenset. Bündelung der Ergebnisse dieser Meta-Analysen. Probleme im Zusammenhang mit dem statistischen Ansatz Andere Schwächen sind, dass es nicht festgestellt wurde, wenn die statistisch genaueste Methode für die Kombination von Ergebnissen die festen, IVhet-, Zufalls- oder Qualitätseffektmodelle ist, obwohl die Kritik gegen das zufällige Wirkungsmodell aufgrund der Wahrnehmung, dass die neuen zufälligen Wirkungen (die in der Metaanalyse verwendet werden) im Wesentlichen formale Geräte sind, um eine reibungslose oder schrumpfende und vorausschauende Vorhersage zu erleichtern. Das Hauptproblem mit dem Zufallskonzept ist, dass es den klassischen statistischen Gedanken nutzt, einen "Kompromise Ester" zu erzeugen, der die Gewichte in der Nähe des natürlich gewichteten Esters macht, wenn die Heterogenität über Studien groß ist, aber in der Nähe der umgekehrten Varianz gewichteten Estimator, wenn die heterogene Studie klein ist. Was jedoch ignoriert wurde, ist die Unterscheidung zwischen dem Modell, das wir entscheiden, einen bestimmten Datensatz zu analysieren, und dem Mechanismus, mit dem die Daten stammen. Ein Zufallseffekt kann in einer dieser Rollen vorhanden sein, aber die beiden Rollen sind recht unterschiedlich. Kein Grund, das Analysemodell und den Daten-Generations-Mechanismus (Modell) ähnlich zu betrachten, aber viele Teilfelder der Statistiken haben die Gewohnheit der Annahme für Theorie und Simulationen entwickelt, dass der Daten-Generations-Mechanismus (Modell) mit dem Analysemodell identisch ist, das wir wählen (oder andere auswählen möchten). Als hypothetische Mechanismen für die Erstellung der Daten ist das Zufallseffektmodell für die Metaanalyse hoch und es ist besser geeignet, dieses Modell als oberflächliche Beschreibung und etwas zu betrachten, das wir als analytisches Werkzeug wählen – aber diese Entscheidung für die Metaanalyse kann nicht funktionieren, weil die Studieneffekte ein fester Bestandteil der jeweiligen Metaanalyse sind und die Wahrscheinlichkeitsverteilung nur ein beschreibendes Werkzeug ist. Probleme aufgrund von vorsätzlichen Verzerrungen Die schwerwiegendste Fehler in der Metaanalyse kommt häufig vor, wenn die Person oder Personen, die die Metaanalyse durchführen, eine wirtschaftliche, soziale oder politische Agenda haben, wie z.B. der Übergang oder der Abstoß von Rechtsvorschriften. Menschen mit diesen Arten von Tagesordnungen können aufgrund persönlicher Unparteilichkeit eher missbrauchen. Forscher, die für die Agenda des Autors günstig sind, werden wahrscheinlich ihre Studien mit Kirschen vorbereitet, während diejenigen, die nicht günstig sind, ignoriert oder als "nicht glaubwürdig" bezeichnet werden. Darüber hinaus können sich die begünstigten Autoren selbst einseitig oder zahlen, um Ergebnisse zu erzielen, die ihre allgemeinen politischen, sozialen oder wirtschaftlichen Ziele unterstützen, wie z.B. die Auswahl kleiner und günstiger Datensets und nicht die Einbeziehung größerer unvorhersehbarer Datensets. Der Einfluss solcher Verzerrungen auf die Ergebnisse einer Metaanalyse ist möglich, weil die Methode der Metaanalyse höchst verläßlich ist. Eine Studie von 2011 zur Offenlegung möglicher Interessenkonflikte in zugrunde liegenden Forschungsstudien, die zur medizinischen Metaanalyse verwendet werden, hat 29 Meta-Analysen untersucht und festgestellt, dass Interessenkonflikte in den zugrunde liegenden Studien selten offengelegt wurden. Die 29 Meta-Analysen umfassen 11 von allgemeinen Medizinmagazinen, 15 von Spezialmedizinmagazinen und drei von der Cartier-Datenbank für Systematikprüfungen. Die 29 Meta-Analysen überprüften insgesamt 509 zufällig kontrollierte Prüfungen (RCTs). 318 RCTs berichteten über Finanzierungsquellen, wobei 219 (69)% der Mittel der Industrie erhalten (d. h. ein oder mehrere Autoren, die finanzielle Bindungen zur pharmazeutischen Industrie haben). Von den 509 RCTs berichteten 132 Autorenkonflikte bei der Offenlegung von Interessenkonflikten, wobei 91 Studien (69)% eine oder mehrere Autoren mit finanziellen Bindungen zur Industrie diskriminieren. Die Informationen waren jedoch selten in den Meta-Analysen enthalten. Lediglich zwei (7)% berichteten über RCT-Finanzquellen und keine gemeldeten RCT-Genehmigungen. Die Autoren kamen zu dem Schluss, dass "ohne Anerkennung von COI aufgrund von Finanzierungen der Industrie oder von Genehmigungen von RCTs, die in Meta-Analysen enthalten sind, das Verständnis und die Bewertung der Beweise aus der Metaanalyse beeinträchtigt werden können. " 1998 stellte ein US-Föderaler Richter fest, dass die US-Umweltschutzagentur den Prozess der Metaanalyse missbraucht hat, um eine Studie zu erstellen, in der Krebsrisiken für Nichtraucher aus dem Tabakrauch (ETS) mit dem Ziel, die politischen Entscheidungsträger zu beeinflussen, rauchfreie Gesetze zu erlassen. Der Richter stellte fest, dass die Auswahl der EPA-Studie beunruhigend ist. Erstens gibt es Belege für den Vorwurf, dass WPA "cherry" seine Daten sammelt. Ohne Kriterien für die Zusammenlegung von Studien in eine Metaanalyse kann das Gericht nicht feststellen, ob der Ausschluss von Studien, die wahrscheinlich eine vorherige Hypothese des WPA darstellen, zufällig oder vorsätzlich war. Zweitens, das WPA schließt fast die Hälfte der verfügbaren Studien direkt mit dem Zweck des WPA, epidemiologische Studien und Konflikte mit den Leitlinien für die Risikobewertung des WPA zu analysieren. Bewertung des ETS-Risikos bei 4-29 („Diese Daten sollten auch im Interesse der Abwägung aller verfügbaren Beweismittel geprüft werden, wie dies in den krebserregenden Risikobewertungsleitlinien des WPA (U.S WPA, 1986a)(Punkte) empfohlen wird. Drittens, die selektive Nutzung von Datenkonflikten mit dem Radon Research Act. Mit dem Programm „WPA“ werden „Daten und Informationen über alle Aspekte der Innenraum-Luftqualität“ (Radon Research Act § 403(a)(1))(Empfehlung hinzugefügt). Infolge des Missbrauchs verurteilte das Gericht Kapitel 1–6 und die Anhänge zu den "Respiratory Health Impact of Passivrauch: Lungenkrebs und andere Disruptoren". Mangelnde Integrationsstandards führen zu irreführenden Schlussfolgerungen Meta-Analysen im Bildungsbereich führen häufig nicht zu einer Beschränkung der methodischen Qualität der Studien. Studien, die z.B. kleine Proben oder forschungsfertige Maßnahmen umfassen, führen zu unüberwindbaren Wirkungsgrößenschätzungen. Anwendungen in der modernen wissenschaftlichen modernen statistischen Metaanalyse kombinieren mehr als nur die Wirkungsgrößen einer Reihe von Studien mit gewichtetem Durchschnitt. Man kann prüfen, ob die Studienergebnisse aufgrund der Probenahme unterschiedlicher Forschungsteilnehmer eher unterschiedlich ausfallen als die erwarteten Abweichungen. Darüber hinaus können Studieneigenschaften wie Messinstrument, Bevoelkerungsprobe oder Aspekte des Studiums kodiert und verwendet werden, um die Varianz des Esters zu verringern (siehe statistische Modelle oben). Manche methodische Schwächen in Studien können daher statistisch korrigiert werden. Andere Verwendungen von Meta-Analysatoren umfassen die Entwicklung und Validierung von klinischen Vorhersagemodellen, bei denen eine Metaanalyse dazu verwendet werden kann, einzelne Teilnehmerdaten aus verschiedenen Forschungszentren miteinander zu kombinieren und die allgemeine Wirksamkeit des Modells zu beurteilen oder sogar bestehende Vorhersagemodelle zu aggregieren. Meta-Analyse kann mit einem einheitlichen Design sowie mit Forschungskonzepten der Gruppen durchgeführt werden. Dies ist wichtig, weil viele Forschungsarbeiten mit einheitlichen Forschungskonzepten durchgeführt wurden. In Betracht gezogener Streit gibt es für die am besten geeigneten Meta-Analys-Technik für die Einzelforschung. Meta-Analyse führt zu einer Verlagerung des Schwerpunkts von einzelnen Studien auf mehrere Studien. Er unterstreicht die praktische Bedeutung der Wirkungsgröße anstelle der statistischen Bedeutung einzelner Studien. Diese Denkweise wurde als "meta-analysator" bezeichnet. Häufig werden die Ergebnisse einer Metaanalyse in einem Waldgrundstück gezeigt. Ergebnisse aus Studien werden mit verschiedenen Ansätzen kombiniert. Ein Ansatz, der häufig in der Metaanalyse im Bereich der Gesundheitsforschung verwendet wird, wird als „unverse Varianzmethode“ bezeichnet. Die durchschnittliche Wirkungsgröße in allen Studien wird als gewichteter Mittelwert berechnet, wobei die Gewichten der Ungleichbehandlung der Wirkungsabschätzungen der einzelnen Studien entsprechen. Größere Studien und Studien mit weniger zufälligen Abweichungen werden mehr Gewicht erhalten als kleinere Studien. Andere gemeinsame Ansätze umfassen die Methode der Maus-Haenszel und die Peto-Methode. Saatgutbasierte dkartierung (ehemals unterzeichnete Differentialkartierung, SDM) ist eine statistische Technik für Meta-analyzer-Studien über Unterschiede in der Gehirnaktivität oder -struktur, die Neuroimaging-Techniken wie FMRI, VBM oder PET verwendet. Verschiedene Hochleistungstechniken wie Mikroarrays wurden verwendet, um Genession zu verstehen. MikroRNA- Expressionsprofilen wurden verwendet, um unterschiedslose ausgedrückte Mikrometer zu ermitteln RNAs, insbesondere Zell- oder Gewebetyp oder Krankheitsbedingungen, oder die Wirkung einer Behandlung zu überprüfen. Eine Metaanalyse solcher Ausdrucksprofile wurde durchgeführt, um neue Schlussfolgerungen zu ziehen und die bekannten Erkenntnisse zu validieren. Siehe auch Estimation Statistiken Metascience Newcastle-Ottawa-Szenken-Berichterstattungspräferenzprüfung Sekundarforschung Studie heterogengenen Systematic Review Galbraith Plot Data Aggregation Referenzen". Lesen Externe Links Cartier Handbuch für Systematic Reviews von Interventionen Meta-Analysis auf 25 (Gen V Glass)Presigned Reportings für Systematic Reviews und Meta-Analysen (PRISMA) Erklärung „ein nachweisbasiertes Mindestangebot für die Berichterstattung in systematischen Überprüfungen und Meta-Analysen“. “Migansue” Rpaket und grafische Schnittstelle Best Evidence copy