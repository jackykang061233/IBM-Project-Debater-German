CURE (Clustering Using REpresentatives) ist ein effizienter Daten-Clustering-Algorithmus für große Datenbanken. Im Vergleich zu K-Means-Clustern ist es robuster für Ausreißer und in der Lage, Cluster mit nicht-sphärischen Formen und Größenvarianzen zu identifizieren. Zeichnungen traditioneller Algorithmen Der beliebte K-Means-Clustering-Algorithmus minimiert die Summe der quadratischen Fehlerkriterium: E = Σ i = 1 k Σ p ε C i ( p − m i ) 2 , \{displaystyle E=\sum_i=1{k}\sum _{p\in C_{i}(p-m_{i})^^^^2, Bei großen Größenunterschieden oder Geometrien verschiedener Cluster könnte die quadratische Fehlermethode die großen Cluster aufteilen, um den quadratischen Fehler zu minimieren, der nicht immer korrekt ist. Auch bei hierarchischen Clustering-Algorithmen existieren diese Probleme, da keine der Abstandsmaße zwischen Clustern (d m i n , d m e a n \{displaystyle d_{min},d_{mean ) tendenziell mit unterschiedlichen Clusterformen arbeiten. Auch die Laufzeit ist hoch, wenn n groß ist. Das Problem mit dem BIRCH-Algorithmus besteht darin, dass die Cluster nach Schritt 3 mit Schwerpunkten der Cluster generiert werden und jedem Datenpunkt mit dem nächstliegenden Schwerpunkt zugeordnet werden. Mit nur dem Schwerpunkt, um die Daten neu zu verteilen hat Probleme, wenn Cluster nicht einheitliche Größen und Formen. CURE Clustering-Algorithmus Um die Probleme mit nicht-einheitlichen oder geformten Clustern zu vermeiden, verwendet CURE einen hierarchischen Clustering-Algorithmus, der einen mittleren Boden zwischen der zentroidbasierten und allen Punktextremen annimmt. In CURE wird eine konstante Anzahl c gut gestreuter Punkte eines Clusters gewählt und durch einen Bruchteil α auf den Schwerpunkt des Clusters geschrumpft. Die Streupunkte nach dem Schrumpfen werden als Vertreter des Clusters verwendet. Die Cluster mit dem engsten Paar von Vertretern sind die Cluster, die in jedem Schritt des hierarchischen Clustering-Algorithmus von CURE zusammengefasst werden. Dies ermöglicht CURE, die Cluster korrekt zu identifizieren und macht es weniger empfindlich auf Ausreißer. Laufzeit ist O(n2 log n,) so dass es ziemlich teuer, und Platz Komplexität ist O(n). Der Algorithmus kann wegen der hohen Laufzeitkomplexität nicht direkt auf große Datenbanken angewendet werden. Verbesserungen richten sich an diese Anforderung. Zufallsstichprobe : Zufallsstichprobe unterstützt große Datensätze. In der Regel passt die Zufallsprobe in den Hauptspeicher. Bei der Stichprobenstichprobe handelt es sich um einen Handel zwischen Genauigkeit und Effizienz. Partitionierung :Die Grundidee ist, den Probenraum in p-Partitionen zu teilen. Jede Partition enthält n/p-Elemente. Der erste Durchlauf bündelt jede Partition teilweise, bis die letzte Anzahl von Clustern für einige konstante q ≥ 1 auf n/pq reduziert. Eine zweite Clustering-Pass auf n/q teilweise Cluster-Partitionen. Für den zweiten Durchlauf werden nur die repräsentativen Punkte gespeichert, da der Zusammenführungsvorgang nur repräsentative Punkte früherer Cluster benötigt, bevor die repräsentativen Punkte für den zusammengeführten Cluster berechnet werden. Die Partitionierung der Eingabe reduziert die Ausführungszeiten. Kennzeichnungsdaten auf der Festplatte: Bei nur repräsentativen Punkten für k-Cluster werden die übrigen Datenpunkte auch den Clustern zugeordnet. Dazu wird ein Bruchteil von zufällig ausgewählten repräsentativen Punkten für jeden der k-Cluster gewählt und dem Cluster, der den repräsentativen Punkt am nächsten enthält, Datenpunkt zugeordnet. Pseudocode CURE (Nr. der Punkte,k) Eingabe : Ein Satz von Punkten S Ausgabe : k Cluster Für jeden Cluster u (jeder Eingabepunkt) in u.mean und u.rep speichern Sie den Mittelwert der Punkte im Cluster und einen Satz von c repräsentativen Punkten des Clusters (initial c= 1, da jeder Cluster einen Datenpunkt aufweist). Auch u.closest speichert den Cluster am nächsten u. Alle Eingabepunkte werden in einen k-d-Baum T Treat jeder Eingabestelle als separates Cluster eingefügt, für jede u.closest berechnet und dann jeden Cluster in den Haufen Q einfügen (Cluster sind in zunehmender Entfernung zwischen u und u.closest angeordnet). Größe (Q) > k Entfernen Sie das oberste Element von Q (say u) und verschmelzen Sie es mit seinem nächsten Cluster u.closest (say v) und berechnen Sie die neuen repräsentativen Punkte für den zusammengeführten Cluster w. Entfernen Sie u und v von T und Q. Für alle Cluster x in Q, aktualisieren x.closest und verlagern x-Einfügen w in Q reproduzieren Verfügbarkeit pyclustering open source library beinhaltet eine Python und C+ Implementierung von CURE Algorithmus. Siehe auch k-Means Clustering BFR-Algorithmus Referenzen Guha, Sudipto; Rastogi, Rajeev; Shim, Kyuseok (1998). " CURE: Ein effizientes Clustering Algorithm für große Datenbanken" (PDF).Informationssysteme.26 (1:) 35–58. doi:10.1016/S0306-4379(01)00008-4.Kogan, Jacob; Nicholas, Charles K;. Teboulle, M. (2006). Mehrdimensionale Daten zu gruppen: jüngste Fortschritte bei der Clustering. Springer.ISBN 978-3-540-28348-5.Theodoridis, Sergios; Koutroumbas, Konstantinos (2006). Mustererkennung. Wissenschaftliche Presse.pp.572–574.ISBN 978-0-12-369531-4.