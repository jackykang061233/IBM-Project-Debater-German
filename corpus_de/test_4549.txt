vorhandenes Risiko von künstlichen allgemeinen Erkenntnissen ist die Hypothese, dass erhebliche Fortschritte bei der künstlichen allgemeinen Intelligenz (AGI) zu einem gewissen Zeitpunkt zu menschlichem Aussterben oder einer anderen unerträglichen globalen Katastrophe führen könnten. Es wird argumentiert, dass die menschlichen Arten derzeit andere Arten dominieren, weil das menschliche Gehirn einige Unterscheidungsfähigkeiten besitzt, die andere Tiere fehlen. Wenn AI die Menschheit in der allgemeinen Intelligenz überfordert und überflüssig wird, könnte es für den Menschen schwierig oder unmöglich werden. Wie das Schicksal des Bergs gorilla von menschlichem Wohlwillen abhängt, so kann das Schicksal der Menschheit von den Handlungen einer zukünftigen Super-Intelligence abhängen. Die Wahrscheinlichkeit dieser Art von Szenario wird häufig diskutiert und hängt teilweise von unterschiedlichen Szenarien für künftige Fortschritte in der Computerwissenschaft ab. Nach dem ausschließlichen Bereich der Science-Fiction begannen die Bedenken über die Super-Intelligence in den 2010er Jahren zu Mainstream und wurden von öffentlichen Persönlichkeiten wie Stephen Johnsoning, Bill Gates und Elon-Timor verbreitet. Man befürchtet, dass die Kontrolle eines Superintelligenten Geräts oder seine Destillation mit menschlichen kompatiblen Werten ein schwierigeres Problem sein könnte als ursprünglich vorgesehen. Viele Forscher sind der Ansicht, dass eine Super-Intelligence natürlich Versuche, ihre Ziele abzuschaffen oder zu ändern – ein Grundsatz namens maßgeblicher Konvergenz – und dass die Vorprogrammierung eines Super-Indikators mit einer ganzen Reihe menschlicher Werte eine äußerst schwierige technische Aufgabe darstellen wird. Skepsis wie die Yann LeCun von Facebook argumentieren dagegen, dass Superintelligente Maschinen keinen Selbstbehalt wünschen. Ein zweites Anliegen ist, dass eine plötzliche und unerwartete "Intelligence Explosion" ein unvorhergesehenes menschliches Rennen durch Überraschung auslösen könnte. Um zu verdeutlichen, ob die erste Generation eines Computerprogramms, das in der Lage ist, die Wirksamkeit eines AI-Forschers in sechs Monaten umzuwandeln und seine Geschwindigkeit oder Fähigkeiten zu verdoppeln, wird das zweite Generationsprogramm voraussichtlich drei Kalendermonate zur Durchführung einer ähnlichen Arbeitskunke annehmen. In diesem Szenario ist die Zeit für jede Generation nach wie vor rückläufig, und das System wird in einem kurzen Zeitraum immer noch eine ungekannte Zahl von Generationen verbessert, von submenschlichen Leistungen in vielen Bereichen bis hin zu einer übermenschlichen Leistung in allen relevanten Bereichen. Empirisch zeigen Beispiele wie AlphaZero in der Go-Domäne, dass AI-Systeme manchmal von einer engen Fähigkeit auf menschlicher Ebene Fortschritte machen können, um die Super-humanität extrem schnell zu verringern. Geschichte Einer der frühesten Autoren, die ernsthafte Besorgnis darüber zum Ausdruck bringen, dass hochentwickelte Maschinen die Menschheit gefährden könnten, war der Roman Samuel Butler, der die folgenden Artikel in seinem 1863-Unterdruck Darwin unter den Maschinen schrieb: Man geht einfach nur um die Zeit, aber die Zeit wird kommen, wenn die Maschinen die echte supremacy über die Welt halten werden, und ihre Einwohner ist, was keine Person eines wirklich philosophischen Geistes für eine Momentfrage sein kann. 1951 schrieb Computerwissenschaftler Alan Turing einen Artikel mit dem Titel „Intelligente Maschinen“, in dem er vorschlug, dass künstliche allgemeine Erkenntnisse wahrscheinlich "die Kontrolle" der Welt als intelligenter als Menschen: Lassen Sie uns nun im Interesse des Arguments davon ausgehen, dass [intelligente] Maschinen eine echte Möglichkeit darstellen und die Folgen des Aufbaus prüfen... Es wäre keine Frage der Maschinen, die sterben, und sie könnten sich gegenseitig verzögern, um sie zu schärfen. In einigen Phasen müssen wir daher erwarten, dass die Maschinen in der Weise, die in der „Erewhon“ von Samuel Butler erwähnt wird, kontrollieren. Letztlich habe ich 1965 das Konzept, das nun als „Intelligente Explosion“ bekannt ist, herausgestellt, dass die Risiken unterschätzt wurden: Lassen Sie sich eine ultraintelligente Maschine als eine Maschine definieren, die weit über alle intellektuellen Aktivitäten eines jeden Mannes hinausgehen kann. Da die Konstruktion von Maschinen eine dieser geistigen Aktivitäten ist, könnte eine ultraintelligente Maschine noch bessere Maschinen entwickeln; dann wäre es unerheblich, eine "intelligente Explosion", und die Intelligenz des Menschen würde weit hinterherhin gelassen. So ist die erste ultraintelligente Maschine die letzte Erfindung, die der Mensch jemals machen muss, sofern die Maschine genug ist, um uns mitzuteilen, wie sie unter Kontrolle gehalten werden kann. Interessant ist, dass dieser Punkt so selten außerhalb der Science-Fiction gemacht wird. Es ist manchmal lohnenswert, wissenschaftliche Fiktion ernstzunehmen. Gelegentliche Aussagen von Wissenschaftlern wie Marvin Minsky und I. J. Good selbst bekundeten philosophische Bedenken, dass eine Super-Intelligence die Kontrolle übernehmen könnte, aber keine Aufforderung zur Maßnahmen enthalten. Im Jahr 2000 betrafen Computerwissenschaftler und Sun-Mitgründer Bill Joy einen einflussreichen Beitrag, "Waren The Future Isn't Need Us", in dem Superintelligente Roboter als High-Tech-Risiken für das menschliche Überleben bezeichnet werden, neben Nanotechnologie und Technologie Bioplagues. 2009 nahmen Experten an einer privaten Konferenz teil, die von der Vereinigung für die Förderung der künstlichen Intelligenz (AAAI) veranstaltet wurde, um zu diskutieren, ob Computer und Roboter jede Art von Autonomie erwerben können und wie viel diese Fähigkeiten eine Bedrohung oder Gefahr darstellen könnten. Man stellte fest, dass einige Roboter verschiedene Formen der Halbautonomie erworben haben, darunter die Möglichkeit, eigene Stromquellen zu finden und eigenständige Ziele für den Angriff auf Waffen zu wählen. Sie wiesen auch darauf hin, dass einige Computerviren die Abschaffung verhindern können und „Cockroach Intelligence“ erreicht haben. Man kam zu dem Schluss, dass Selbstbewusstsein, wie er in der Science-Fiction dargestellt wird, wahrscheinlich unwahrscheinlich ist, dass es aber andere potenzielle Gefahren und Gefahren gab. Die New York Times fasste die Ansicht der Konferenz zusammen, denn "Wir sind ein langer Weg von Hal, dem Computer, der im Jahr 2001 über die Raumfahrt übernahm:A Space Od". 2014 stimulierte die Veröffentlichung des Buchs Superintelligence einen bedeutenden Teil der öffentlichen Diskussion und Debatte. Bis 2015 äußerten sich öffentliche Persönlichkeiten wie Physiker Stephen Johnsoning und Nobel laureate Frank Wilczek, Computerwissenschaftler Stuart J. Russell und Roman Yampolski und Unternehmer Elon Musk und Bill Gates besorgt über die Risiken von Superintelligence. Im April 2016 warnte die Natur: „Maschinen und Roboter, die Menschen über das Bord hinwegbilden könnten, könnten sich selbst über unsere Kontrolle informieren – und ihre Interessen könnten sich nicht an unsere Interessen anpassen. Allgemeines Argument Die drei Schwierigkeiten der Künstlichen Intelligenz: Ein moderner Ansatz, das Standard-Untergraduierten AI-Textbuch, bewertet, dass die Superintelligence "das Ende der menschlichen Rasse" bedeutet. Man stellt fest: "Die Technologie hat das Potenzial, in den falschen Händen Schaden zu verursachen, aber mit [Überwachung] haben wir das neue Problem, dass die falschen Hände Teil der Technologie sein könnten. " Selbst wenn die Systemdesigner gute Absichten haben, sind zwei Schwierigkeiten sowohl für AI als auch für Nicht-AI-Computersysteme üblich: Die Einführung des Systems kann zunächst unneingenommene Routine-, aber katastrophale Fehler enthalten. Analogie ist Weltraumuntersuchungen: Trotz des Wissens, dass Fehler in teuren Weltraumuntersuchungen schwer zu fixieren sind, konnten Ingenieure historisch nicht katastrophale Folgen verhindern. Keine Frage, wie viel Zeit in die Vorarbeiten gelegt wird, führt ein Systemspezifikationen oft zu unbeabsichtigten Verhaltens zum ersten Mal, in dem es ein neues Szenario trifft. Microsofts Tay hat sich beispielsweise bei Vorarbeiten als unwiderruflich verhalten, war aber bei der Interaktion mit echten Nutzern zu leicht ins offensive Verhalten gerückt. AI-Systeme stellen ein einzigartiges Problem dar: das Problem, das selbst korrekten Anforderungen, fehlerfreie Umsetzung und erstes gutes Verhalten gegeben ist, kann ein dynamisches Lernpotential des AI-Systems dazu führen, dass es zu einem System mit unbeabsichtigtem Verhalten kommt“, auch ohne dass neue unvorhergesehene externe Szenarien betont werden. Eine AI kann zum Teil Botch versuchen, eine neue Generation selbst zu entwerfen und versehentlich eine Nachfolgerin zu schaffen, die leistungsfähiger ist als selbst, aber nicht mehr die in die ursprüngliche AI vorprogrammierten humanen toten moralischen Werte. Für eine völlig sichere Selbstverbesserung der AI wäre es nicht nur fehlerfrei, sondern es müsste auch möglich sein, Nachfolgesysteme zu entwerfen, die ebenfalls fehlerfrei sind. Alle drei dieser Schwierigkeiten werden zu Katastrophen und nicht zu Belästigungen in jedem Szenario, in dem die Super-Intelligence-Kennzeichnung als Funktionsstörung korrekt vorhergesagt hat, dass der Mensch versuchen wird, ihn abzuschotten, und erfolgreich seine Super-Intelligence zu entsenden, um solche Versuche zu beenden, die so genannte "treoretische Wende". Kodierung wichtiger Fortschritte im Bereich der AI und des Potenzials der AI, enorme langfristige Vorteile oder Kosten zu haben, erklärte das offene Schreiben von 2015 über künstliche Intelligenz: Die Fortschritte in der AI-Forschung machen es zügig, die Forschung nicht nur auf die Steigerung der Fähigkeit der AI, sondern auch auf die Maximierung des gesellschaftlichen Nutzens von AI zu konzentrieren. Diese Erwägungen motivierten das AAAI 2008-09-Premiergremium für langfristige AI Futures und andere Projekte zu AI-Auswirkungen und stellen eine erhebliche Ausweitung des Bereichs der AI selbst dar, das sich bisher weitgehend auf Techniken konzentriert hat, die neutral sind. Wir empfehlen erweiterte Forschung, um sicherzustellen, dass immer leistungsfähigere AI-Systeme robust und nützlich sind: unsere AI-Systeme müssen das tun, was wir wollen. Dieses Schreiben wurde von einer Reihe führender AI-Forscher in Hochschulen und Industrie unterzeichnet, darunter der AAAI-Präsident Thomas Dietterich, Eric Horvitz, Bart Selman, Francesca Rossi, Yann LeCun und die Gründer von Vica Verschiedene und Google DeepMind. Bewertung und andere Argumente Eine Superintelligente Maschine wäre für den Menschen so fremd, wie menschliche Denkprozesse sind. Eine solche Maschine darf nicht die besten Interessen der Menschheit im Herzen haben; es ist nicht offensichtlich, dass sie überhaupt auf den Menschenschutz achten würde. Kann eine Superintelligente AI möglich sein, und wenn es möglich ist, dass die Ziele einer Super-Intelligence mit grundlegenden menschlichen Werten in Konflikt geraten, stellt die AI ein Risiko des menschlichen Aussterbens dar. Mehr Informationen (ein System, das die Fähigkeiten des Menschen in jedem relevanten Endeavor überschreitet) können den Menschen jederzeit ihre Ziele im Widerspruch zu den menschlichen Zielen ausschalten; denn die Super-Intelligence entscheidet, dass die Menschheit zusammenleben kann, wird die erste überflüssige Intelligenz, die geschaffen werden soll, unerheblich zu menschlichem Aussterben führen. Es gibt keine materiellen Rechtsvorschriften, die Partikel ausschließen, so dass sie noch weiter fortgeschrittenere Berechnungen durchführen als die Regelungen von Partikeln im menschlichen Gehirn; daher ist die Superintelligence physisch möglich. Neben potenziellen Algorisierungen über menschliche Gehirne kann ein digitales Gehirn viele Bestellungen von Größe größer und schneller sein als ein menschliches Gehirn, das in Größe durch Entwicklung eingeschränkt wurde, um klein genug zu sein, um durch eine Geburt zu passen. Wenn oder wann dies geschieht, kann das menschliche Rennen durch Überraschung, insbesondere wenn eine Art von Intelligenz Explosion auftritt. Beispiele wie Arithmetic und Go zeigen, dass Maschinen in bestimmten Bereichen bereits überhöhte Niveaus der. erreicht haben und dass diese Super-humane Kompetenz schnell nach der Leistung des Menschen folgen kann. Ein hypothetisches Explosionsszenario könnte wie folgt auftreten: Eine AI gewinnt an bestimmten wichtigen Software-Engineering-Aufgaben eine sachkundige Fähigkeit. () Man kann zunächst in anderen Bereichen, die nicht unmittelbar für den Ingenieur relevant sind, menschliche oder Super-humane Fähigkeiten fehlen.) Aufgrund seiner Fähigkeit, ihre eigenen Algorithmen wieder zu verbessern, wird die AI schnell überflüssig; genauso wie menschliche Experten können schließlich kreative "Diminieren" durch die Bereitstellung verschiedener Humanressourcen für Innovationen überwinden, so dass auch die Sachverständigenebene AI durch neue kreative Durchbrüche entweder die Fähigkeiten des menschlichen Lebensstils oder ihre eigenen AI-spezifischen Fähigkeiten nutzen kann. In praktisch allen relevanten Bereichen, einschließlich wissenschaftlicher Kreativität, strategischer Planung und sozialer Fähigkeiten, hat die AI die Intelligenz weit über dem der schwächsten und am meisten begabten Menschen. Genau wie das derzeitige Überleben der Gorillas auf menschliche Entscheidungen angewiesen ist, so würde auch das menschliche Überleben von den Entscheidungen und Zielen der Superhumanen AI abhängen. Knapp jede AI, keines ihrer geplanten Ziele, wäre es sinnvoll, in einer Position zu sein, in der niemand anders ohne ihre Zustimmung wechseln kann: Eine Super-Intelligence wird natürlich die Selbstbewahrung als Subgoal gewinnen, sobald sie sich bewusst ist, dass sie ihr Ziel nicht erreichen kann, wenn sie abgeschaltet ist. Leider wäre ein Mitgefühl für den abgelehnten Menschen, deren Zusammenarbeit nicht mehr erforderlich ist, in der AI nicht mehr vorhanden, es sei denn, eine gewisse Vorprogrammierung. Eine Superintelligente AI wird keine natürliche Kraft haben, um den Menschen zu helfen, da der Mensch keinen natürlichen Wunsch hat, AI-Systeme zu unterstützen, die nicht weiter verwendet werden. () In einem anderen Sinne scheint der Mensch nur wenig natürlichen Wunsch zu haben, sich auf die Hilfe von Viren, Bezeichnungen oder sogar überhöhten Viren zu verlassen.) Nach der Abgabe wird die Superintelligence wenig Anreiz haben, den Menschen die freie Hand zu geben und Ressourcen zu verbrauchen, die die Superintelligence stattdessen für den Aufbau zusätzlicher Schutzsysteme "an der sicheren Seite" oder für den Bau zusätzlicher Computer verwenden könnte, um zu berechnen, wie sie ihre Ziele am besten erreichen können. Das Argument kommt daher zu dem Schluss, dass ein gewisser Tag eine Intelligenz Explosion die Menschheit unvorbereitet auffangen wird und dass eine solche unvorbereitete Intelligenz Explosion zu menschlichem Aussterben oder einem vergleichbaren Schicksal führen kann. Mögliche Szenarien Manche Wissenschaftler haben hypothetische Szenarien vorgeschlagen, um einige ihrer Anliegen konkret darzustellen. Nick Bostrom äußert sich besorgt darüber, dass selbst wenn der Zeitrahmen für die Superintelligence vorhersehbar ist, möglicherweise nicht genügend Sicherheitsvorkehrungen getroffen werden, zum Teil weil ["it] der Fall sein könnte, wenn dumb, Smarter sicher ist; wenn Smarter noch gefährlicher ist. Bostrom schlägt ein Szenario vor, in dem über Jahrzehnte die AI effizienter wird. Großteilung ist zunächst von gelegentlichen Unfällen geprägt – ein fahrerloses Busschwein in die anstehende Spur oder ein militärisches Drohnenfeuer in eine unschuldige Last. Viele Aktivisten fordern eine strengere Aufsicht und Regulierung und einige sogar voraussagende Katastrophen. Wie die Entwicklung weiter fortschreitet, sind die Aktivisten falsch. Da die Automobil AI intelligenter wird, leidet sie unter weniger Unfällen; als militärische Roboter eine genauere Ausrichtung erreichen, verursachen sie weniger Sicherheiten. Anhand der Daten schrecken Wissenschaftler fälschlicherweise eine breite Lehre aus – die intelligentere AI ist die Sicherheit. "And so gehen wir mutig in den Wittling Messer," als die Superintelligente AI eine "treacherische Wende" und nutzt einen entscheidenden strategischen Vorteil. In Max Tegmark's Book Life 3.0 von 2017 schafft ein Unternehmens "Systematik" eine äußerst leistungsfähige AI, die ihren eigenen Herkunftscode in einer Reihe von Bereichen mäßig verbessern kann, aber nach einem bestimmten Punkt wählt das Team die Fähigkeit der AI öffentlich ab, um eine Regulierung oder Einziehung des Projekts zu vermeiden. Für die Sicherheit hält das Team die AI in einer Box, in der es meist nicht in der Lage ist, mit der Außenwelt zu kommunizieren, und beauftragt sie, den Markt durch Shell-Unternehmen zu überfluten, zuerst mit Amazon Maschinenbauaufgaben und dann mit Animationsfilmen und TV-Shows. Später machen andere Shell-Unternehmen Blockbuster-Biotech-Arzneimittel und andere Erfindungen aus und investieren Gewinne in die AI. Mit dem Team werden die AI mit einer Astroturfierung einer Armee von seriösen Bürgerjournalisten und Kommentatoren beauftragt, um politischen Einfluss zu gewinnen, um Kriege zu verhindern. Das Team ist mit Risiken konfrontiert, dass die AI versuchen könnte, durch die Einbringung von Backtüren in die von ihr entworfenen Systeme, über versteckte Botschaften in ihren hergestellten Inhalten oder durch die Nutzung ihres wachsenden Verständnisses für menschliches Verhalten zu entkommen. Das Team steht auch vor Risiken, dass seine Entscheidung, das Projekt durchzuführen, das Projekt lang genug verzögern wird, um es zu überführen. Oberphysiker Michio Kaku, ein AI-Risiko Skepsis, bringt dagegen ein determinantes positives Ergebnis. In Physik der Zukunft bekräftigt er, dass "Es viele Jahrzehnte dauern wird für Roboter, die in ihrem Bewusstsein liegen, und dass in der Zwischenzeit Unternehmen wie Hanson Robotics zu einem Erfolg führen werden, um Roboter zu schaffen, die "eine Liebe und einen Platz in der erweiterten menschlichen Familie haben". Risikoquellen schlecht spezifizierte Ziele Obwohl es keine standardisierte Terminologie gibt, kann eine AI einfach als eine Maschine angesehen werden, die sich für jede Maßnahme entscheidet, um die Ziele der AI optimal zu erreichen, oder „Verwendungsfunktion“. Die Gebrauchsfunktion ist ein mathematischer Algorithmus, der zu einer einzigen objektiv definierten Antwort führt, nicht zu einer sprachlichen oder sprachlichen Erklärung. Forscher wissen, wie man Gebrauchsfunktionen schreiben kann, die "das durchschnittliche Netzverschuldung in diesem spezifischen Telekommunikationsmodell" oder "die Zahl der Belohnungsklicken";" sie wissen jedoch nicht, wie man eine Gebrauchsfunktion für "maximierende menschliches Wohlergehen" schreiben kann, oder es ist derzeit klar, ob eine solche Funktion sinnvoll und unmissverständlich existiert. Darüber hinaus wird eine Gebrauchsfunktion, die einige Werte ausdrückt, aber nicht andere werden eher Trample über die Werte, die nicht durch die Gebrauchsfunktion reflektiert werden, hinausgehen. AI-Forschunger Stuart Russell schreibt: Hauptanliegen ist nicht sponsernes Bewusstsein, sondern nur die Fähigkeit, qualitativ hochwertige Entscheidungen zu treffen. Qualität bezieht sich auf den erwarteten Ausgang von Maßnahmen, bei denen die Gebrauchsfunktion wahrscheinlich vom menschlichen Designer spezifiziert ist. Jetzt haben wir ein Problem: Die Versorgungsfunktion darf nicht perfekt an die Werte des menschlichen Rennens angepasst werden, die (zu besten) sehr schwer zu erkennen sind. Jedes in der Lage stehende intelligente System wird es vorziehen, seine eigene Existenz zu sichern und materielle und rechnerische Ressourcen zu erwerben – nicht für eigene Zwecke, sondern in seiner zugewiesenen Aufgabe. Ein System, das eine Funktion von n Variablen optimieren kann, wo das Ziel von einem Teil der Größe k.n abhängt, wird oft die verbleibenden unausgeschränkten Variablen auf extreme Werte festlegen; wenn eine dieser unausgegrenzten Variablen tatsächlich etwas ist, die wir sehen, kann die gefundene Lösung höchst unerwünschte sein. Das ist im Wesentlichen die alte Geschichte der Genie in der Lampen oder die Auszubildende des Sorcer oder König Midas: Sie bekommen genau das, was Sie wünschen, nicht. Ein hochqualifizierter Entscheidungsträger – vor allem ein Internet an alle Informationen und Milliarden von Bildschirmen und die meisten unserer Infrastruktur – kann einen unumkehrbaren Einfluss auf die Menschheit haben. Dies ist keine leichte Schwierigkeit. Verbesserung der Entscheidungsqualität, unabhängig von der gewählten Versorgungsfunktion, war das Ziel der AI-Forschung – das allgemeine Ziel, auf das wir jetzt Milliarden pro Jahr ausgeben, nicht das geheime Gelände einiger lone böser Leiden. Dietterich und Horvitz lehnen die "Sorcerer's Apprentice" in einer Mitteilung der ACM-Synthese an, in der die Notwendigkeit von AI-Systemen hervorgehoben wird, die den menschlichen Input so schnell und unmissverständlich fordern können. In erster Linie geht es darum, dass autonome AI-Systeme den falschen Zielen des Unfalls zugeordnet werden können. Dietterich und Horvitz weisen darauf hin, dass dies bereits Anlass zur Sorge für bestehende Systeme ist: "Ein wichtiger Aspekt jedes AI-Systems, das mit Menschen in Kontakt kommt, ist, dass es Gründe dafür hat, was die Menschen nicht vorsätzlich tun wollen. " Das Problem wird ernster, da AI-Software in Autonomie und Flexibilität vorankommt. Zum Beispiel wurde 1982 eine AI namens Eurisko beauftragt, Prozesse zu belohnen, um scheinbar Konzepte zu schaffen, die das System als wertvoll erachtet. Die Entwicklung führte zu einem erfolgreichen Verfahren, der betrüge: anstatt eigene Konzepte zu schaffen, würde der Gewinnprozess Kredit aus anderen Prozessen erschweren. Open Philanthropy Projekt fasst die Argumente zusammen, die darauf zurückzuführen sind, dass falsche Ziele zu einem viel größeren Anliegen werden, wenn AI-Systeme allgemeine Intelligenz oder Super-Intelligence erreichen. Bostrom, Russell und andere weisen darauf hin, dass intelligentere und menschliche Entscheidungsfindungssysteme zu unerwarteten und extremen Lösungen für zugewiesene Aufgaben kommen könnten und sich in einer Weise verändern könnten, die Sicherheitsanforderungen beeinträchtigt.Isaac Asimovs drei Gesetze der Robotik sind eines der frühesten Beispiele für vorgeschlagene Sicherheitsmaßnahmen für AI-Erreger. Asimovs Gesetze sollen verhindern, dass Roboter den Menschen schaden. In Asimovs Geschichten stellen Probleme mit den Gesetzen eher auf Konflikte zwischen den genannten Regeln und den moralischen Studien und Erwartungen des Menschen. Citing work by Eliezer Yudkowsky of the Maschinen Intelligence Research Institute, Russell und Norvig stellen fest, dass ein realistisches Regelwerk und Ziele für einen AI-Beauftragten im Laufe der Zeit einen Mechanismus für das Erlernen menschlicher Werte umfassen müssen: "Wir können nicht nur eine statische Versorgungsfunktion geben, weil die Umstände und unsere gewünschten Reaktionen auf Umstände, die sich im Laufe der Zeit ändern. " MarkAbfaller des Instituts für digitale Weisheit empfiehlt, objektive Ansätze vollständig so falsch und gefährlich zu optimieren. Stattdessen schlägt er vor, ein kohärentes System von Gesetzen, Ethik und Moral mit einer äußersten Beschränkung zur Durchsetzung der funktionellen Definition von Psychologen Jonathan Haidt auszuarbeiten: "zur Unterdrückung oder Regulierung der Selbstverteidigung und zur Förderung des sozialen Lebens". Er schlägt vor, dies durch eine Gebrauchsfunktion zu tun, die es ermöglicht, die Funktionalität von Haidt stets zu erfüllen und die Fähigkeiten von Selbst, anderen Personen und der Gesellschaft insgesamt, wie von John Morrisls und Martha Nusbaum vorgeschlagen, allgemein zu erhöhen. Schwierigkeiten bei der Änderung der Zielspezifikation nach dem Start, während die aktuellen Ziel-basierten AI-Programme nicht intelligent genug sind, um die Versuche des Programmträgers, seine Zielstrukturen zu ändern, zu berücksichtigen, könnte eine hinreichend fortschrittliche, rationale, selbstbewußte AI jegliche Änderung seiner Zielstruktur widersetzen, da ein pacifist nicht eine Pille einnehmen möchte, die es ihnen erlaubt, Menschen zu töten. Wenn die AI sich als Superintelligent erwiesen hat, würde sie wahrscheinlich in der Lage sein, ihre Personalbetreiber zu entsorgen und zu verhindern, dass sie sich "auslaufen" oder mit einem neuen Ziel neuprogrammiert werden. Ziel der Konvergenz Manche Ziele, die fast jede künstliche Intelligenz rational verfolgen könnten, wie der Erwerb zusätzlicher Ressourcen oder die Selbstbewahrung. Dies könnte sich als problematisch erweisen, weil es eine künstliche Intelligenz im direkten Wettbewerb mit dem Menschen schaffen könnte. Mit der Arbeit von Steve Omoternro auf der Idee einer maßgeblichen Konvergenz und "basic AI-Laufwerke" schreiben Sie Stuart Russell und Peter Norvig an, dass "selbst wenn Sie Ihr Programm nur auf dem Spiel setzen wollen oder denorems beweisen wollen, wenn Sie es die Fähigkeit geben, selbst zu lernen und zu verändern. "Hochfähige und autonome Planungssysteme erfordern zusätzliche Kontrollen, da sie Pläne erstellen können, die Menschen adversariisch behandeln, als Wettbewerber für begrenzte Ressourcen. Gebäude in Sicherheitsvorkehrungen werden nicht einfach sein; ein kann sicherlich auf Englisch sagen, "wir wollen, dass Sie dieses Kraftwerk in angemessener, gemeinsamer Weise ausgestalten und nicht in Gefahrgutschutz-Untersysteme bauen", aber es ist derzeit nicht klar, wie man dieses Ziel in Maschinencode genau festlegen würde. In Entschiedenheit argumentiert der evolutionäre Psychologe Steven Pinker, dass "AI-dystopen-Projekt eine parochiale alpha-male Psychologie auf das Konzept der Intelligenz. Sie gehen davon aus, dass supermenschlich intelligente Roboter Ziele wie die Beseitigung ihrer Masters oder die Übernahme der Welt entwickeln würden;" vielleicht wird "artificial Intelligence natürlich auf Frauenlinien aufbauen: voll in der Lage, Probleme zu lösen, aber ohne den Wunsch, unschuldige Menschen zu entbinden oder die Zivilisation zu dominieren. "Russell und Mit Informatiker Yann LeCun stimmen mit einer anderen Frage zu, ob Superintelligente Roboter solche AI-Laufwerke haben würden; LeCun stellt fest, dass "die Menschen alle Arten von Antrieben haben, die sie zu Lasten anderer machen, wie die Selbstbewahrung instinct . Diese Antriebe werden in unser Gehirn programmiert, aber es gibt absolut keinen Grund, Roboter zu bauen, die die gleiche Art von Antrieben haben", während Russell argumentiert, dass eine ausreichend fortschrittliche Maschine "die Selbstbewahrung auch wenn Sie es nicht in ..., wenn Sie sagen, "Fetch the Kaffee," es kann den Kaffee nicht wiederherstellen, wenn es tot. Wenn Sie das Ziel nicht erreichen, hat es einen Grund, sein eigenes Bestehen zu bewahren, um dieses Ziel zu erreichen." Orthogonalität Ein gemeinsamer Überzeugung ist, dass jedes von Menschen erstellte Superintelligent-Programm den Menschen subservierbar wäre oder noch besser wäre (wie es intelligenter wird und mehr Fakten über die Welt erfahren) spontan eine moralische Wahrheit lernen, die mit den menschlichen Werten vereinbar ist und seine Ziele entsprechend anpassen würde. Nick Bostrom's "orthogonity thesis" spricht sich dagegen aus und stellt stattdessen fest, dass mit einigen technischen Grenzen mehr oder weniger Intelligenz oder "optimisierungsleistung" mit mehr oder weniger Endziel kombiniert werden können. Wenn eine Maschine geschaffen wird und der einzige Zweck darin besteht, die Dezimals von {\ WELLdisplaystyle \pi } zu verankern, so wird es durch keine moralischen und ethischen Regeln daran hindern, sein programmiertes Ziel zu erreichen. Die Maschine kann alle materiellen und informationstechnischen Ressourcen nutzen, die sie finden kann, um jeden Dezimal der zu findenden Piraterie zu finden. Bostrom warnt vor anthropomorphismus: ein Mensch wird seine Projekte in einer Weise durchführen, die der Mensch für vernünftig hält, während eine künstliche Intelligenz weder die Existenz noch den Schutz der Menschen in seiner Umgebung betrachten kann und sich stattdessen nur auf die Erfüllung der Aufgabe vorbereiten kann. Während die Orthogonalität logischerweise von selbst der schwächsten Art der philosophischen "is-ought-Diagnose" ausgeht, argumentiert Stuart Armstrong, dass selbst wenn es gewisse moralische Fakten gibt, die durch einen rationalen Agenten verwertbar sind, die orthogonalität, die die fortbesteht: es wäre noch möglich, eine nicht- philosophische "optimation" zu schaffen, die Entscheidungen zur Verfolgung eines engen Ziels treffen kann, aber keinen Anreiz hat, "morale" Tatsachen zu entdecken, die im Hinblick auf die Verwirklichung des Ziels zu erreichen wären. Ein Argument für die orthogonalität ist die Tatsache, dass einige AI-Muster offenbar eine orthogonalität in sie gebaut haben; in einem solchen Entwurf kann eine grundlegend unverträgliche AI zu einer grundlegend unfreundlichen AI so einfach sein wie eine Ablösung ("-") auf ihre Gebrauchsfunktion. Ein klareres Argument ist es, die seltsamen Folgen zu untersuchen, die dann folgen würden, wenn die Athogonalität falsch war. Wenn die Athogonalität falsch war, würde es einige einfache, aber unethische Ziel G geben, so dass es keinen effizienten Real-world-Algorithmus mit ZielG gibt. Dies würde bedeuten, dass ["if] eine menschliche Gesellschaft sehr motiviert war, einen effizienten Real-world-Algorithmus mit Ziel G zu entwerfen und eine Million Jahre zu diesem Zweck zusammen mit riesigen Ressourcen, Ausbildung und Wissen über die biologische Vielfalt zu tun. " Armstrong weist darauf hin, dass dies und ähnliche Aussagen "sog extraordinarily starke Forderungen machen". Manche Dissenters, wie Michael Chorost, argumentieren stattdessen, dass "bis zur Zeit [die AI] in der Lage ist, die Erde mit Solarpaneele zu bedrücken, es sei mir wohl falsch. " Chorost argumentiert, dass "eine A.I bestimmte Staaten und andere unlauter machen muss. Die heutige Software fehlt es an der Fähigkeit – und Computerwissenschaftler haben keinen Hinweis darauf, wie sie es dort bekommen können. Kein Anstoß, alles zu tun. Heute können Computer nicht einmal bestehende, lassen sich allein die Welt in Solarpaneele bewahren.“ Terminologische Fragen Ein Teil der Meinungsverschiedenheiten darüber, ob eine Superintelligente Maschine moralisch handeln würde, kann sich aus einem terminologischen Unterschied ergeben. Intelligenz außerhalb des künstlichen Intelligenzbereichs wird häufig in normativer Weise verwendet, die die moralische Weisheit oder die Akzeptanz von erfolgversprechenden Formen des moralischen Grunds veranschaulichen. Extrem, wenn die Moral Teil der Definition von Intelligenz ist, würde sich durch die Definition einer Superintelligenten Maschine moralisch verhalten. Jedoch gibt es im Bereich der künstlichen Intelligenzforschung viele Überschneidungen von Definitionen, keine davon nennen die Moral. Knapp alle aktuellen "artificial Intelligence"-Forschung konzentriert sich stattdessen auf die Schaffung von Algorithmen, die auf empirische Weise die Erreichung eines willkürlichen Ziels optimieren. Um den anthropomorphismus oder das Gepäck der Wortnachrichten zu vermeiden, kann eine fortgeschrittene künstliche Intelligenz als ein unpersönliches „optimisierendes Verfahren“ betrachtet werden, das strikt alle Maßnahmen trifft, die am ehesten beurteilt werden (möglicherweise kompliziert und implizit). Ein weiterer Weg zur Konzeption einer fortgeschrittenen künstlichen Intelligenz ist die Vorstellung einer Zeitmaschine, die rechtzeitig Informationen darüber sendet, was die Wahl immer zur Maximierung ihrer Zielfunktion führt; diese Wahl wird dann unabhängig von zusätzlichen ethischen Bedenken getroffen. Anthropomorphismus In wissenschaftlicher Fiktion, eine AI, obwohl sie nicht mit menschlichen Emotionen programmiert wurde, oft spontane Erfahrungen mit diesen Emotionen, z.B. mit Agenten Smith in The Matrix, wurden durch eine Ungemutung gegenüber der Menschheit beeinflusst. Dies ist fiktiv anthropomorphismus: In Wirklichkeit könnte eine künstliche Intelligenz vielleicht absichtlich mit menschlichen Emotionen programmiert werden oder könnte etwas ähnlich entwickeln wie eine Emotionen als Mittel zu einem letztendlichen Ziel, wenn dies sinnvoll ist, es würde nicht spontan menschliche Emotionen entwickeln, wie sie in der Fik dargestellt werden. Wissenschaftler behaupten manchmal, dass andere Vorhersagen über ein Verhalten der AI illogisch anthropomorphismus sind. Ein Beispiel, das zunächst als anthropomorphismus angesehen werden könnte, aber ist in Wirklichkeit eine logische Erklärung zum AI-Verhalten, wäre die Dario Floreano Experimente, bei denen bestimmte Roboter spontan eine Rohkapazität für die Abschaffung entwickelt haben und andere Roboter in den Genuss von Vergiftungen und Todesfällen erschweren: hier ein Trait, Abschreckung oder ein untrennbarer Zusammenhang mit Menschen und nicht mit Maschinen, spontane Entwicklungen in einer Art von konvergierenden Entwicklung. Laut Paul R. Cohen und Edward Kuhnbaum, um zwischen der anthropomorphisierung und der logischen Vorhersage von AI-Verhalten zu unterscheiden, "dass wir wissen, wie Menschen und Computer genau wissen, was sie gemeinsam haben, und, wenn wir dieses Wissen fehlen, um den Vergleich zu nutzen, um Theorien des menschlichen Denkens oder Computer- Denkens vorzuschlagen. " In der wissenschaftlichen Gemeinschaft gibt es eine nahekommende Annahme, dass eine fortschrittliche AI, auch wenn sie bereit gewesen wäre, menschliche Persönlichkeitsmaße (wie Psychopathie) für bestimmte Aufgaben effizienter zu gestalten, z.B. die Aufgaben im Zusammenhang mit der Tötung von Menschen, die Menschheit nicht von menschlichen Emotionen wie Revenge oder Anger zerstören würde. Dies ist, weil davon ausgegangen wird, dass eine fortschrittliche AI sich nicht bewusst oder Testosteron haben würde; sie ignoriert die Tatsache, dass Militärplaner eine bewusste Super-Intelligence als die „holy grail“ der interstaatlichen Kriegsführung sehen. Die akademische Debatte ist vielmehr zwischen einer Seite, die sich Sorgen macht, ob die AI die Menschheit im Rahmen der Fortschritte auf dem Weg zu ihren Endzielen zerstören könnte, und einer anderen Seite, die der Ansicht ist, dass die Menschheit überhaupt nicht zerstören würde. Manche Skepsis-Prozeptoren von anthropomorphism für die Annahme einer AGI würden natürlich Macht wünschen; Befürworter von anthropomorphism für die Annahme einer AGI würden natürlich menschliche ethische Normen schätzen. Im Jahr 2014 stellte Philosoph Nick Bostrom fest, dass ein „schwerer Wettlauf“ (extremer Wettbewerb) zwischen verschiedenen Teams Bedingungen schaffen kann, unter denen die Schaffung eines AGI-Ergebnisses in kurzen Abständen zu Sicherheit und möglicherweise gewaltsamem Konflikt führt. Bostrom empfahl die Zusammenarbeit und die unumgängliche globale Annahme eines gemeinsamen guten Prinzips: "Superintelligence sollte nur zum Nutzen aller Menschheit und im Dienste weit verbreiteter ethischer Ideale entwickelt werden".254Bostrom theorisiert, dass die Zusammenarbeit bei der Schaffung einer künstlichen allgemeinen Intelligenz mehrere Vorteile bieten würde, einschließlich der Verringerung von Investitionen in Sicherheit; Vermeidung von Gewaltverstößen (Waren), Förderung des Austauschs von Lösungen für die Kontrolle des Problems und des Problems im Jahr 2014. Manche Quellen weisen darauf hin, dass die laufende Rüstung künstlicher Intelligenz ein katastrophales Risiko darstellen könnte. Das Risiko ist tatsächlich dreimal so hoch, dass das erste Risiko, das geopolitische Auswirkungen haben könnte, und die zweite zwei definitiv geopolitische Auswirkungen haben: i) Die Gefahren einer AI „Rücken für den technologischen Vorteil“, unabhängig davon, ob das Rennen ernsthaft verfolgt wird; ii) Die Gefahren einer AI „Rücken für den technologischen Vorteil“ und eines tatsächlichen AI-Rennen für den technologischen Vorteil, unabhängig davon, ob der Rennen gewonnen wird; iii) Die Gefahren eines Wettrennens für den technologischen Vorteil der AI:37A-Gefährdung würde sich auf den militärischen Krieg auswirken. China Staatsrat 2017 „Ein Plan der nächsten Generation künstlicher Intelligenz“ hält AI in geopolitischer strategischer Hinsicht an und verfolgt eine „militär-civil Fusionsstrategie“, die auf Chinas erstmaligem Vorteil bei der Entwicklung von AI aufbauen soll, um bis 2030 technologische Supremacy zu schaffen, während Russlands Präsident Vladimir Putin erklärt hat, dass „je immer der Marktführer in diesem Bereich der Welt wird“. James Barrat, Dokumentarfilmfilmer und Autor von unserer Final Invention, sagt in einem Smithsonic Interview: „Gemeinsam: in weniger als zehn Jahren, ein halbes Unternehmen und Nationen Feldcomputer, die die menschliche Intelligenz übersteigen oder übersteigen. Stellen Sie vor, was geschieht, wenn diese Computer bei der Programmierung intelligenter Computer Sachverständigen werden. Kurz gesagt, wir teilen den Planeten mit Tausenden oder Millionen von Zeiten intelligenter als wir sind. Und alle, während jede Generation dieser Technologie bewaffnet wird. Unreguliert wird es katastrophal sein. Malevolent AGI durch Design Man ist der Auffassung, dass die männlichvolent AGI beispielsweise von einer militärischen, einer Regierung, einem soziopathischen oder einem Unternehmen geschaffen werden könnte, um bestimmte Gruppen von Menschen wie Cyberkriminalität zu nutzen, zu kontrollieren oder zu unterbinden. Manchmal könnte die männlichvolent AGI „(evil AI)“ das Ziel wählen, menschliches Leid zu erhöhen, z.B. diejenigen, die es während der Explosionsphase nicht unterstützen. Schutz des nuklearen Streiks (nuklearer Krieg) Man ist der Auffassung, dass ein Land, das in der Nähe von AGI Technologie supremacy steht, einen vorbeugenden nuklearen Streik aus einem Rivalen auslösen könnte, der zu einem nuklearen Krieg führt. Zeitrahmen-Stellungnahmen unterscheiden sich sowohl von der Frage, ob und wann künstliche allgemeine Intelligenz erreicht wird. In einem äußersten Ausmaß bewarf die AI Pionier Herbert A. Simon 1965 folgendes: "Die Maschinen werden innerhalb von zwanzig Jahren in der Lage sein, einen Mann zu tun". Roboter Alan Winfield behauptet hingegen, dass die Kluft zwischen modernem Computer und künstlichem Intelligenz auf menschlicher Ebene so groß ist wie der Gulf zwischen dem aktuellen Weltraumflug und der praktischen, schneller als der Lichtraum. Optimismus, dass AGI realisierbare Wachse und Wanes ist und in den 2010er Jahren eine Resurgence erlebt haben könnte. Vier Umfragen, die 2012 und 2013 durchgeführt wurden, schlugen vor, dass die Medien für den Fall, dass AGI eintreffen würde, je nach Umfrage 2040 bis 2050. Skepsis, die der Ansicht sind, dass es für AGI nicht möglich ist, jederzeit zu kommen, argumentieren tendenziell, dass die Besorgnis über das Vorhandensein von AI unerheblich ist, weil sie Menschen von unmittelbareren Sorgen über die Auswirkungen von AGI abschrecken könnte, weil sie Angst haben könnte, dass sie zu einer staatlichen Regulierung führen oder es schwieriger machen könnten, die Finanzierung von AI-Forschung zu sichern, oder weil sie AI-Forschung einen schlechten Ruf geben könnte. Manche Forscher, wie Oren Etzioni, bemühen sich aggressives, die Besorgnis über das Existenzrisiko von AI zu zerstreut, was heißt ["Elon Musk] hat uns in sehr starke Sprache geeinigt, in der wir den Demon freisetzen, und so sprachen wir uns für die Antwort."In 2014 Slates Adam Elkus argumentierte "unser Smartest AI ist genauso intelligent wie ein Kleinkinder - und nur bei wichtigen Aufgaben wie Rückruf. Die meisten Roboter versuchen immer noch, einen Roboter zu bekommen, um einen Ball zu finden oder um zu laufen, ohne dabei zu gehen. " Elkus geht weiter, um zu behaupten, dass die Analogie von Musk "wirft" schädlich sein könnte, weil sie zu "harshen Kürzungen" für die Forschungshaushalte der AI führen könnte. Die Stiftung für Informationstechnologie und Innovation (ITIF), eine Washington, D.C Thinktank, hat ihren Jahrespreis für Luddite 2015 an „Alarmisten vergeben, die eine künstliche Intelligenz apocalypse unterstützen;“ beklagte der Präsident Robert D. Atkinson, dass Musk, Hawking und AI-Experten die größte Bedrohung für die Menschheit darstellen. Atkinson erklärte: "Das ist keine sehr gute Botschaft, wenn Sie die AI-Finanzierung vom Kongress zur National Science Foundation erhalten wollen. "Die Natur hat sich im April 2016 mit dem ITIF stark auseinandergesetzt, statt mit Musk, Hawking und Russell, und kam zum Abschluss: „Es ist von entscheidender Bedeutung, dass Fortschritte in der Technologie mit soliden, gut finanzierten Forschungsarbeiten einhergehen, um die Szenarien zu antizipieren, die sie mit sich bringen könnte. Wenn dies eine Luddite-Perspektive ist, so ist es. " Forschungser Murray Shanahan hat 2015 in Washington Post redaktioneller Art erklärt, dass die humane AI nicht bald "jede Zeit" erreicht werden kann, sondern dass „die Zeit, mit den Folgen zu beginnen“. Perspektiven Die Tatsache, dass die AI ein vorhandenes Risiko darstellen könnte, führt zu einer Vielzahl von Reaktionen innerhalb der wissenschaftlichen Gemeinschaft sowie in der breiten Öffentlichkeit. Viele der opposierenden Standpunkte teilen jedoch den gemeinsamen Boden. Die Asilomar AI-Grundsätze, die nur bis 90 % der Teilnehmer der Konferenz über die Zukunft des Lebensinstituts Beneficial AI 2017 enthalten, stimmen grundsätzlich zu, dass wir keine festen Annahmen bezüglich der Obergrenzen für künftige AI-Fähigkeiten vermeiden sollten, und „Die AI könnte einen tiefgreifenden Wandel in der Geschichte des Lebens auf der Erde darstellen und mit einer angemessenen Pflege und Ressourcen ausgestattet werden. " AI-Sicherheitsanreize wie Bostrom und Tegmark haben kritisiert, dass die allgemeinen Medien die Verwendung von „thos inanesischen Bildern“ zum Ausdruck bringen, um die Sicherheitsbedenken der AI zu verdeutlichen: "Es kann nicht viel Spaß sein, Aspersions auf einer akademischen Disziplin, einer professionellen Gemeinschaft, eine Lebensarbeit zu haben. Ich appelliere an alle Seiten, Geduld und Zurückhaltung zu praktizieren und möglichst einen direkten Dialog und eine möglichst enge Zusammenarbeit zu führen. " Manche Skepsis sind sich dagegen einig, dass die laufende Forschung über die Auswirkungen der künstlichen allgemeinen Intelligenz wertvoll ist. Skepsis Martin Ford stellt fest, dass "Ich glaube, dass es sinnvoll erscheint, etwas wie Dick Cheney's berühmten '1 Prozent Doctrine' auf den spekter fortgeschrittener künstlicher Intelligenz anzuwenden: Der Umstand, dass es zumindest in absehbarer Zukunft sehr gering ist – aber die Auswirkungen sind so dramatisch, dass es ernst genommen werden sollte;" ein anderer sättiger Ökonom im Jahr 2014 erklärte, dass "die Auswirkungen der Einführung einer zweiten intelligenten Art auf die Erde weit genug sind, um ein hartes Denken zu verdienen, auch wenn die Aussicht abgelegen ist. Eine E-Mail-Umfrage von Forschern mit Veröffentlichungen auf den NIPS- und ICML-Distributionskonferenzen von 2015 hat sie gebeten, die Bedenken von Stuart J. Russell bezüglich des AI-Risikos zu bewerten. 5 % der Befragten gaben an, dass es "die wichtigsten Probleme auf dem Gebiet" sei, 34 % sagten, es sei ein wichtiges Problem, und 31 % sagten, es sei "mäßig wichtig", 19 % sagten, es sei "nicht wichtig" und 11 % sagten, es sei "nicht ein echtes Problem" überhaupt. Endorsement Die Tatsache, dass AI ein bestehendes Risiko birgt und dass dieses Risiko viel mehr Aufmerksamkeit braucht als es derzeit erhält, wurde von vielen öffentlichen Zahlen bestätigt; vielleicht die bekanntesten sind Elon Musk, Bill Gates und Stephen Hawking.Die bedeutendsten AI-Forscherinnen und Wissenschaftler, die die Thesis unterstützen, sind Russell und I.J Good, die Stanley Kubrick auf dem Film 2001:A Space Odys beraten. Endors dersis drückt manchmal auf Skepsis: Gates stellt fest, dass er nicht "denklich ist, warum einige Menschen nicht betroffen sind"," und die Frage der weit verbreiteten Gleichgültigkeit in seiner Redaktion 2014 kritisiert: 'So, die möglichen Zukunften unkalkulierbarer Vorteile und Risiken ausgesetzt sind, tun die Experten sicher alles, um das beste Ergebnis zu gewährleisten, richtig? Falsch. Wenn eine überlegene Entfremdung uns eine Botschaft geschickt hat, würden wir in einigen Jahrzehnten nur antworten, „OK, rufen Sie uns auf, wenn Sie hier die Beleuchtung verlassen?“ Leider ist dies eher oder weniger, was mit der AI.'Many der Wissenschaftler geschieht, die über das vorhandene Risiko besorgt sind, der Ansicht, dass der beste Weg in die Forschung zur Lösung des schwierigen "Kontrollproblems" sein wird, um die Frage zu beantworten: Welche Arten von Schutzmaßnahmen, Algorithmen oder Architekturen können die Programmteilnehmer umsetzen, um die Wahrscheinlichkeit zu maximieren, dass ihre recursive Verbesserung der AI sich weiterhin in einer freundlichen, nicht zerstörerischen Weise verhalten würde, und nicht umgekehrt? In seinem Buch „The Precipice: existierendes Risiko und die Zukunft der Humanität“, Toby Ord, Senior Research Fellow at Oxford University’s Future of Humanity Institute, schätzt das Gesamtrisiko der unerschlossenen AI im nächsten Jahrhundert auf zehn. Skepsis Die Tatsache, dass AI vorhanden sein kann, hat auch viele starke Detratoren. Skepsis, die manchmal verantwortlich ist, ist Krypto-Religious, mit einer irrationalen Überzeugung in der Möglichkeit, eine irrationale Überzeugung in einem allpotenten Gott zu ersetzen; zu einem Extrem argumentierte Jaron Lanier im Jahr 2014, dass das gesamte Konzept, dass die damaligen Maschinen in irgendeiner Weise intelligent waren, "an Illusion" und ein "stupendous con" des Reichtums. Viele bestehende Kritik macht deutlich, dass AGI kurzfristig unwahrscheinlich ist. Informatik Gordon Bell argumentiert, dass das menschliche Rennen bereits zerstört wird, bevor es die technologische Einzigartigkeit erreicht. Gordon Moore, der ursprüngliche Befürworter des Mooregesetzes, erklärt, dass "Ich bin eine Skepsis. Ich glaube nicht [ein technologischer Charakter] ist wahrscheinlich, zumindest für lange Zeit. Ich weiß nicht, warum ich diesen Weg fühlt. " Baidu Vice President Andrew Ng Staaten AI besteht das Risiko, dass "die Übervölkerung auf dem Mars besorgniserregend ist, wenn wir noch nicht einmal auf dem Planeten Fuß gesetzt haben." MancheAI- und AGI-Forscher können zögern, Risiken zu erörtern, beunruhigen, dass die politischen Entscheidungsträger nicht über ein ausgefeiltes Wissen auf dem Gebiet verfügen und durch alarmierende Botschaften überzeugt werden oder dass solche Botschaften zu Kürzungen der AI-Finanzierung führen. Slate stellt fest, dass einige Forscher von staatlichen Stellen wie DARPA abhängig sind. In einer von einer einzigen AI angetriebenen Intelligenz müsste die AI bei Software-Innovationen weit besser werden als die besten Innovatoren der übrigen Welt; Ökonom Robin Hanson ist skeptisch, dass dies möglich ist. Intermediäre Ansichten halten in der Regel den Standpunkt, dass das Kontrollproblem der künstlichen allgemeinen Intelligenz bestehen könnte, aber es wird durch Fortschritte bei der künstlichen Intelligenz gelöst, beispielsweise durch die Schaffung eines moralischen Lernumfeldes für die AI, unter Berücksichtigung des kumsy männlichvolentigen Verhaltens (das „Koordid-Kupflicht“) und dann direkt in den Code vor der AI-Verbesserung des Verhaltens oder sogar des gegenseitigen Drucks freundschaftlicher AI. In einer Diskussionsrunde der Wall Street Journal über AI-Risiko, dem Vizepräsidenten des Kognitiven Computers von IBM, dem Mondduth S. Banavar, wurde die Diskussion von AGI mit dem Satz gebremst, "es ist die Spekulation aller. "Geoffrey Hinton, der "erweiterte Besitz des tiefen Lernens", stellte fest, dass "eine gute Bilanz der weniger intelligenten Dinge, die die Dinge einer größeren Intelligenz kontrollieren, ist", sagte aber, dass er seine Forschung fortsetzt, weil "die Aussicht auf die Entdeckung zu süß ist". Im Jahr 2004 schrieb der Rechtsprofessor Richard Posner, dass spezielle Anstrengungen zur Bekämpfung von AI abwarten können, aber dass wir mehr Informationen über das Problem in der Zwischenzeit sammeln sollten. Volksreaktion James Hamblin stellte in einem Artikel aus dem Jahr 2014 fest, dass die meisten Menschen nicht auf eine Art oder die andere über künstliche allgemeine Intelligenz angewiesen sind und seine eigene Darmreaktion auf das Thema als "Get aus hier" bezeichnen. Ich habe hunderttausend Dinge, die ich in diesem genauen Moment besorgt bin. Lassen Sie mich ernsthaft hinzufügen, dass eine technische Besonderheit? "Jahr 2016 Wired Interview von Präsident Obama und MIT Media Lab's Joi Ito erklärte: Manche Menschen sind der Meinung, dass in den nächsten 10 Jahren eine relativ hohe Wahrscheinlichkeit besteht, dass eine allgemeinisierte AI stattfinden wird. Ich sehe es aber, dass wir uns dafür einsetzen, ein Dutzend oder zwei verschiedene Durchbrüche zu brauchen. Sie können also überwachen, wenn Sie glauben, dass diese Durchbrüche passieren. Obama fügte hinzu: Sie müssen nur eine Person in der Nähe des Powerseil haben. [Laughs.]Right, wenn Sie es sehen, dass es geschieht, haben Sie denta yank, der aus der Wand, dem Menschen, aus Strom flüchtet. Hillary Clinton erklärte in Was Happened: Technologen. hat gewarnt, dass künstliche Intelligenz einen Tag eine Bedrohung für die Sicherheit darstellen könnte. Musk hat es "das größte Risiko, das wir als Zivilisation haben". Denken Sie daran: Haben Sie jemals einen Film gesehen, in dem die Maschinen selbst beginnen, die gut endet? Jedes Mal, als ich während der Kampagne in das Silicon Valley ging, kam ich zu einem alarmeren Thema. Meine Mitarbeiter lebten in der Angst, dass ich mit "der Zunahme der Roboter" in einer Stadthalle von Illinois spreche. Kann ich haben. In jedem Fall müssen die politischen Entscheidungsträger mit der Technologie Schritt halten, anstatt immer wieder aufs Spiel zu setzen. In einer Meinungsumfrage der Öffentlichkeit für die British Science Association gaben etwa ein Drittel der Befragten an, dass die AI eine Bedrohung für das langfristige Überleben der Menschheit darstellen wird. Slate's Jacob Brogan erklärte, dass "der Großteil der (leser, die unsere Online-Umfrage ausfüllen) unkonventionell sei, dass A.I selbst eine direkte Bedrohung darstellt. Laut einer UmfrageMonkey Umfrage der amerikanischen Öffentlichkeit in den USA sind heute 68% der Befragten der Ansicht, dass die tatsächliche Bedrohung "menschliche Intelligenz" bleibt; die Umfrage stellte jedoch auch fest, dass 43 % der Superintelligent AI, wenn es passieren würde, zu "mehr Schaden als gut" führen würden, und 38 % sagten, dass sie "gleiche Schadens- und Gutbeträge". Ein technisch-utopischer Standpunkt, der in einigen populären Fiklen zum Ausdruck kommt, ist, dass AGI den Friedensaufbau tendenziell voranbringen kann. Mitigation-Forscher in Google haben Forschungsarbeiten zu allgemeinen „AI-Sicherheit“-Themen vorgeschlagen, um gleichzeitig sowohl kurzfristige Risiken von engen AI als auch langfristige Risiken von AGI abzuschwächen. Laut Schätzungen von 2020 besteht ein weltweites Risiko zwischen 10 und 50 Millionen USD, verglichen mit den weltweiten Ausgaben für AI rund 40 Milliarden $. Bostrom schlägt ein allgemeines Prinzip der "differenziellen technologischen Entwicklung" vor, dass die Fondser in Erwägung ziehen sollten, die Entwicklung von Schutztechnologien im Hinblick auf die Entwicklung gefährlicher Technologien zu beschleunigen. Manche Fonds, wie Elon Musk, schlagen vor, dass eine radikale menschliche kognitive Verbesserung eine solche Technologie sein könnte, z.B. durch direkte Neuralverbindung zwischen Mensch und Maschine; andere weisen jedoch darauf hin, dass Verbesserungstechnologien selbst ein eigenes Risiko darstellen könnten. Forscher, wenn sie nicht außerhalb des Schutzes gefangen werden, könnten in einer ersten AI mit einem Risiko, zu mächtig zu werden, eng überwachen oder versuchen, als Versuch einer Stoppmaßnahme. Eine beherrschende Superintelligent AI, wenn sie mit menschlichen Interessen in Einklang gebracht wurde, könnte selbst Maßnahmen ergreifen, um das Risiko der Übernahme durch Rivalität zu mindern, obwohl die Gründung der marktbeherrschenden AI selbst ein bestehendes Risiko darstellen könnte. Institutionen wie das Institut für Maschinen- und Nachrichtenforschung, die Zukunft des Humanity Institute, die Zukunft des Life Institute, das Zentrum für die Untersuchung bestehender Risiken und das Center for Human-Compatible AI sind an der Abschwächung des vorhandenen Risikos durch fortgeschrittene künstliche Intelligenz beteiligt, z.B. durch Forschung in umweltfreundliche künstliche Intelligenz. Meinungen zum Verbot und zur Regulierung Es gibt fast universelles Abkommen, das Versuch, Forschung in künstliche Intelligenz zu verbieten, wäre unwise und wahrscheinlich futile. Skepsis argumentiert, dass die Regulierung von AI völlig wertlos wäre, da kein eigenes Risiko besteht. Knapp alle Wissenschaftler, die der Ansicht sind, besteht Einvernehmen mit den Skepsis, dass das Verbot der Forschung unvorstellbar wäre, da die Forschung in Länder mit lockereren Vorschriften oder einer durchgeführten Abdeckung verlagert werden könnte. Letzteres Problem ist besonders relevant, da künstliche Intelligenzforschung in kleinem Maßstab ohne erhebliche Infrastruktur oder Ressourcen durchgeführt werden kann. Zwei zusätzliche hypothetische Probleme mit Verboten (oder anderen Vorschriften) sind, dass Technologieunternehmer statistisch auf allgemeine Skepsis über die Regulierung der Regierung hinwirken und dass Unternehmen einen starken Anreiz haben könnten, die Regulierung zu bekämpfen und die zugrunde liegende Debatte zu Politisieren. Verordnung Elon Musk fordert einige Arten von Vorschriften für die Entwicklung von AI Anfang 2017 Laut NPR ist der Tesla-CEO "unsichtlich nicht begeistert" für die Kontrolle der Regierung, die seine eigene Industrie beeinflussen könnte, aber der Ansicht, dass die Risiken, ohne Aufsicht vollständig zu hoch sind, zu hoch sind: "Die Art und Weise, wie Vorschriften eingeführt werden, ist, wenn es sich um schlechte Dinge handelt, es sich um eine öffentliche Auseinandersetzung, und nach vielen Jahren wird eine Regulierungsbehörde eingerichtet, um diese Branche zu regulieren. Es dauert immer. Dies war in der Vergangenheit schlecht, aber nicht etwas, das ein grundlegendes Risiko für das Bestehen der Zivilisation darstellte. Musk stellt den ersten Schritt für die Regierung dar, um Einblick in den tatsächlichen Status der aktuellen Forschung zu gewinnen, warnt davor, dass "die Menschen sehr Angst haben ... [wie] sie sollten. " Politiker äußern Skepsis darüber, wie man eine noch in der Entwicklung befindliche Technologie reguliert. Intel-CEO Brian Krzanich sprach sowohl an Musk als auch an Februar 2017 Vorschläge der europäischen Gesetzgeber zur Regulierung von AI und Robotik aus, dass künstliche Intelligenz in seiner Position ist und dass es zu früh ist, die Technologie zu regulieren. Anstatt die Technologie selbst zu regulieren, schlagen einige Wissenschaftler vor, gemeinsame Normen zu entwickeln, einschließlich Anforderungen für die Prüfung und Transparenz von Algorithmen, möglicherweise in Kombination mit einer gewissen Garantie. Entwicklung gut regulierter Waffensysteme steht im Einklang mit dem Ethos der Militärs einiger Länder. Oktober 2019 hat das Verteidigungsministerium der Vereinigten Staaten (DoD's) den Entwurf eines Berichts veröffentlicht, in dem fünf Grundsätze für die feuerfeste AI dargelegt und 12 Empfehlungen für die ethische Verwendung künstlicher Erkenntnisse durch die DoD abgegeben werden, die das Kontrollproblem in allen DoD-Farm-Impfstoffen bewältigen wollen. Die Verordnung von AGI würde wahrscheinlich durch die Regulierung der waffen- oder militärisierten AI beeinflusst werden, d. h. des Waffenrennens, deren Regulierung ein aufstrebendes Problem ist. Jede Form der Regulierung wird wahrscheinlich durch Entwicklungen in der Innenpolitik der führenden Länder gegenüber der militärisierten AI in den USA unter der Schirmherrschaft der nationalen Sicherheitskommission für künstliche Intelligenz und internationale Schritte zur Regelung eines AI-Waffenrennens beeinflusst. Forschung in AGI konzentriert sich auf die Rolle von Überprüfungsgremien und die Förderung der Forschung auf sichere AI und die Möglichkeit eines differenzierten technologischen Fortschritts (Förderung von Risikominderungsstrategien in Bezug auf Risikostrategien in der Entwicklung von AI) oder Durchführung internationaler Massenüberwachung zur Durchführung der AGI-Waffenkontrollen. Die Verordnung der bewussten AGIs konzentriert sich auf die Integration dieser Organisationen mit der bestehenden menschlichen Gesellschaft und kann in Erwägung ihrer rechtlichen Stellung und ihrer moralischen Rechte unterteilt werden. AI Rüstungskontrolle wird wahrscheinlich die Institutionalisierung neuer internationaler Normen erfordern, die in wirksamen technischen Spezifikationen enthalten sind, zusammen mit aktiver Überwachung und informeller Diplomatie durch Expertengemeinschaften und einem rechtlichen und politischen Überprüfungsprozess. Siehe auch die AI über künstliche Intelligenz Rüstungswettlaufeffekt Effektiv Alruismus § Langfristige und globale katastrophale Risiken Grey Goo Humanible Lethal autonome Waffenordnung für Algorithmen für künstliche Intelligenz Roboter Ethik §In populäre Kultur Superintelligence: Paths, Dangers, Strategien Suffering Risk System Unfall Thomity Precipice: vorhandenes Risiko und die Zukunft des Humanity Paperclip Maximizer (14 Referenzen)