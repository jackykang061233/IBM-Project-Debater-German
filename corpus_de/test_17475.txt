Vapnik–Chervonenkis Theorie (auch bekannt als VC-theorie) wurde 1960 bis 90 von Vladimir Vapnik und Alexey Chervonenkis entwickelt. Die Theorie ist eine Form der rechnerischen Lerntheorie, die versucht, den Lernprozess aus statistischer Sicht zu erklären. VC Theorie steht im Zusammenhang mit der statistischen Lerntheorie und den empirischen Prozessen. Richard M. Dudley und Vladimir Vapnik haben unter anderem VC-theore auf empirische Prozesse angewendet. Einführung VC Theorie umfasst mindestens vier Teile (wie in der Natur der statistischen Lerntheorie[1) erläutert:] Konsistenz der Lernprozesse Was sind (erforderlich und ausreichend) Bedingungen für die Kohärenz eines Lernprozesses auf der Grundlage des empirischen Risikominimierungsprinzips? Nonasymptotische Theorie der Konvergenz der Lernprozesse Wie schnell ist die Konvergenz des Lernprozesses? Theorie zur Kontrolle der allgemeinen Lernfähigkeit Wie kann man die Konvergenzquote (die allgemeine Fähigkeit) des Lernprozesses kontrollieren? Theorie des Aufbaus von Lernmaschinen Wie kann man Algorithmen bauen, die die Allgemeinisierungsfähigkeit kontrollieren können? VC Theorie ist ein wichtiger Teil der statistischen Lerntheorie. Eines der wichtigsten Anwendungen der statistischen Lerntheorie ist die Bereitstellung allgemeiner Bedingungen für Lernalgorithmen. Aus diesem Blickwinkel ist die VC-theorie mit Stabilität verknüpft, die ein alternatives Konzept für die Charakterisierung der allgemeinen Ausrichtung ist. Darüber hinaus sind VC Theorie und VC-Dimension maßgeblich für die Theorie empirischer Prozesse, im Falle von Prozessen, die von VC-Klassen indexiert werden. Zweifellich sind dies die wichtigsten Anwendungen der VC- Theorie und werden bei der Allgemeinisierung eingesetzt. Mehrere Techniken werden eingeführt, die im empirischen Prozess und in der VC-theorie weit verbreitet sind. In erster Linie basiert die Diskussion auf dem Buch Weak Convergence und Empirical Prozesse: Mit Anträgen auf Statistiken. [2] Übersicht der VC-theorie in Empirical-Prozess Hintergrund zu Empirischen Prozessen Let X 1 , ... , X n {\displaystyle X_{1},\ldots ,X_{n} sind zufällige Elemente, die auf einem messbaren Raum ( X , A ) Memestyle {(\ Mathematik X X, fical {A)} definiert sind. Für jede Maßnahme Q KINGstyle Q} auf ( X , A ) JPYstyle {(\thecal X}}, {A)} und etwaige messbare Funktionen f : X → R {\displaystyle f: {X}}\to \ Mathematikb {R} }, Definition F =  d d Q KINGstyle Qf=\int fdQ} Measurability Fragen werden hier ignoriert, für mehr technische Details siehe [3]. Lassen Sie F Memedisplaystyle {F} eine Klasse von messbaren Funktionen f : X → R {\displaystyle f: {X}}\to \ Mathematik {R} } und definieren:  F Q  F F = {  | F = { KINGstyle Q\· F==\sup\{\vert Qf\vert \ \:\ f\in ggio Mathematik {F.. Bestimmung der empirischen Maßnahme P n = n − 1  i i = 1 n   X i , JPY {P} _n}=n-1-1.sum _i=1}\n Xdelta X_{i,}, wo  here hier steht für die Dirac Maßnahme. In der empirischen Maßnahme wird eine Karte F → R {\displaystyle {Ftoto \ Mathematik {R} erstellt durch: f  P P n f = 1 n ( f ( X 1 ) + . . . + f ( X n ) ) HANAdisplaystyle f\mapsto \bb {P} _n}f= cufrac 1 1n 1(f(X_{1})+2,00f(X_{n Now P ist die zugrunde liegende Verteilung der Daten, die unbekannt ist. Empirische Prozesse Theorie zielt darauf ab, Klassen zu ermitteln F {\displaystyle {F}, für die Aussagen wie folgt gelten: einheitliches Recht großer Nummern:  P P n  0 F → n 0 , RARstyle \8.5 Mathematikbb {P} n} n} n\to \infty } , nach 1 n ( X 1 ) + f ( X n ) →       n\to \infty } ,  1 1 n ( f) +  f  X  X  X  X  X  G  G  G  G  G  G  G  0  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G  G {G} _n} {P} {_n}-P)\rightsquigarrow \ Mathematikbb {G} ,\quad \text{in )infty {(\} Mathematikcal {F)} In dem früheren Fall F HANAstyle {F} wird Glivenko-Cantelli-Klasse und im letzteren Fall (unter Annahme  x x , sup f  F F | f ( x ) − P:  | .  | \forall x,\sup \nolimits {f\in 574 mathematischcal {F fvert f(x)-Pf\inf\inft })-Pf\inft yf  Donf \inft }. A Donsker-Klasse ist Glivenko-Cantelli wahrscheinlich durch einen Antrag auf die Theorem . Diese Aussagen gelten für einen einzigen f HANAdisplaystyle f} , durch Standard-LLN, CLT-Sätze unter regulären Bedingungen und die Schwierigkeit der Empirischen Prozesse, da gemeinsame Aussagen für alle f . F {\displaystyle f\in ggio Mathematikcal {F} .Intuitiver dann kann die F Memestyle {F} nicht zu groß sein, und da sich herausstellt, dass die Geometrie der F displaystyle {F} eine wichtige Rolle spielt. Eine Methode, um zu messen, wie groß die Funktion F HANAdisplaystyle HANA mathematischcal {F} ist, um die sogenannten Abdeckungsnummern zu verwenden.  N       , ) ) ) ) psilon psilon psilon psilon psilon psilon psilon F{\ {F|,\\\\cdot \ \| \ \ \ \ \ \ \ \| | ). ). ). ). {\ } {g:\ \ |f\ff\| | f | f | psilon psilon ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). ). Die Entropy ist der Logarithm der Abdeckungsnummer. Zwei hinreichende Bedingungen sind nachstehend aufgeführt, unter denen nachgewiesen werden kann, dass die Set F HANAstyle {F} Glivenko-Cantelli oder Donsker ist. A Klasse F Memedisplaystyle {F} ist P-Glivenko-Cantelli, wenn es sich um P-measurable mit F-Mitteln handelt, so dass P  F F  F KINGstyle P.ast }F.infty } und erfüllt:  >  >  >  >  >  > 0 sup Q N ( 0 F ‖ Q, F, L 1 ( Q) ) . {\ {\ {\ {\ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . {_Q}N(\varepsilon) F\|_{Q}, cu Mathematik F}},L_{1}(Q))infty .} Die nächste Bedingung ist eine Version der gefeierten Dudley's theorem. {F} ist eine Klasse von Funktionen, die  0 0  sup  Q N (  F ‖ Q , 2 , F , L 2 ( Q ) )  L ) ) ) ) ) < ) ) ∞ ∞ ∞ {\ int int \int _0}^{\infty  ofsup \nolimits _Q}{\sq rt \left(\vareLDR) F\|_{Q,2}, cu Mathematik F}},L_{2}(Q)\right)}}d\varepsilon \<infty }, dann F {\displaystyle {F} ist P-Donsker für jede mögliche Maßnahme P., dass P  F F 2 < KINGstyle P Fast F22}<\infty } . Letztlich bedeutet die Notation:  f f  Q Q , 2 = (  | | f ) 1 2 7.8displaystyle f\|_{Q,2}=\left(\int f|^{2}dQ\right)^{\frac 1.2 . Symmetrization Mehrheit der Argumente dafür, wie der empirische Prozess gebunden werden kann, hängen von der Systemmmetrierung, der Maximierung und der Konzentration von Ungleichheiten und der Kette ab. Symmetrization ist in der Regel der erste Schritt der Nachweise, und da es in vielen maschinenlesbaren Lernnachweisen zur Bindung von empirischen Verlustfunktionen (einschließlich des im nächsten Abschnitt erörterten Nachweises der VC-Ungleichheit) verwendet wird, wird er hier vorgestellt. Prüfung des empirischen Prozesses: f ) (P n − P ) f = 1 n  i i = 1 n n ( f ( X i) ) Memestyle f\mapsto (\bb {P} _n}-P)f= firac 1}{n in i i=1Xn}(fX_{i})-Pf weist darauf hin, dass eine Verbindung zwischen dem empirischen und dem symmetrischen Verfahren besteht: f i 1  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i  i {P} _n}^{0}f=1,0rac 1}{n)sum _i=1}\n ivarepsilon i}f(X_{i) Der symmetriisierte Prozess ist ein Rademacher-Prozess, der an die Daten X i {\displaystyle X_{i} .Esfore gebunden ist, ist ein sub-Gaussianer Prozess durch die Ungleichbehandlung von Hoeffding. Lemma (Symmetrisation). für jede Nichterhöhung, Konvex ): R → R und Klasse messbarer Funktionen F HANAstyle {F} , E ) ( P P n − P  F F ) ≤ E ) ( 2  P P n 0 ‖ F ) F )  steuerliche \bbstyle {E} \Phi {P} n}-P\|_holcal {F))\leq \ Mathematik {E} \Phi links(2\left\\\mol) {P} n00\right\|_holcal {F)right) Der Nachweis der Symmetrization lemma stützt sich auf die Einführung unabhängiger Exemplare der Originalvariablen X i WELLstyle X_{i} (einmalige Zeiten, die als Scheinprobe bezeichnet werden) und die innere Erwartung der LHS durch diese Kopien. Nach einer Anwendung der Ungleichbehandlung von Jensen könnten verschiedene Zeichen eingeführt werden (hing the Name symmetrization), ohne die Erwartung zu ändern. Nachstehend ist der Nachweis aufgrund seiner Anweisungsbefugnis zu finden. Ein typisches Mittel, um empirische CLTs zu erproben, nutzt zunächst das Systemmmetrisierung, um den empirischen Prozess an P n 0 {\displaystyle \ Mathematikbb {P} _n}^{0 weiterzugeben und dann an die Daten zu binden, wobei Rademacher-Prozesse einfache Prozesse mit guten Eigenschaften sind. VC-Anschluss Es stellt sich heraus, dass es eine faszinierende Verbindung zwischen bestimmten kombinierten Charakteren der F HANAstyle {F} und den Eutropy-Nummern gibt. Einheitliche Erfassungsnummern können durch den Begriff der Vapnik-Chervonenkis-Klassen von Set oder in Kürze VC-Sets kontrolliert werden. Prüfung einer Sammlung C WELLdisplaystyle {C} der Untersets des Proberaums X WELLdisplaystyle {X} . C WELLdisplaystyle {C} ist der Ansicht, dass er eine bestimmte Untergruppe W displaystyle W} des finite Set S = { x 1 , ... x n } } X WELLdisplaystyle S=\{x_{1},\ldots ,x_{n}\}\subset HANA Mathematikcalcal {X} C WELLdisplaystyle W=S\cap C} für einige C  C C KINGstyle C\in ggio Mathematikcal {C}.C WELLdisplaystyle {C} wird gesagt, S zu zerkleinern, wenn sie jedes ihrer 2n Untersets zieht. VC-Index (similar mit VC-Dimension + 1 für eine geeignete Auswahlklasse) V (C ) {\displaystyle V( {C)} von C KINGstyle 0oc {C} ist der kleinste n, für die keine Größe gestrichen wird, die von C {\displaystyle {C} } .Sauer's lemma dann, dass die Nummer  C  C  C ( x 1, x 1,  C,  C,  C,  C,  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  C  n e V (C ) − 1 ) V (C ) − 1 {\displaystyle \max x_{1},\ldots ,x_{nDelDelta _n}(Getränke C C,x_{1},\ldots ,x_{n})\leq \sumq \ _j=0VV(Kapital C C)-1}{n \choose jleq( faserfracf neVoc CCcal Ccal C) Was ist eine polynomische Nummer O ( n V (C ) − 1 ) Memedisplaystyle O(nVV(Kapscal {CC)-1 von Untersets anstelle einer exponentiellen Nummer. Letztlich bedeutet das, dass ein Finite VC-Index bedeutet, dass C {\displaystyle {C} eine offensichtliche simulierte Struktur hat. Eine ähnliche Bindung kann für die sogenannten VC-Unterschriften (mit einem anderen konstanten, gleichen Satz) nachgewiesen werden. Für eine Funktion f: X → R {\displaystyle f: {X}}\to \ Mathematik {R} } ist der Untersatz ein Teil von X × R RARstyle Meme mathematisch {X}}\times \\klon {R} so, dass: { ( x , t) : . ( x ) } } {x, {x) {x, t} {x) {x, t} Eine Sammlung von F {\displaystyle {F} wird als VC-Unterklasse bezeichnet, wenn alle Untertage eine VC-Klasse bilden. Prüfung einer Reihe von Indikatorfunktionen I C = { 1 C : C } 7.8displaystyle HANA mathematischcal I}}_customcal C==11_{C}:C\in ggio Mathematik {C in in L 1 ( Q ) Memedisplaystyle L_{1}(Q) für diskrete empirische Art von Maßnahme Q (oder gleichwertig für jede Wahrscheinlichkeitsmaßnahme Q). Man kann dann nachweisen, dass für r ≥ 1 KINGstyle r\geq 1} : N (, , I C , L r ( Q ) ) ≤ K V (C ) ( 4 e) V (C ) ε − r ( V (C ) − 1 ) {\displaystyle N(\varepsilon , cu mathematische I{\_ñcal C C,L_{r}(Q))\leq KV(Kapital C C)(4e)VV(CC)}\vare(V(V)Vr(V(V)Vr(V(C{\){\){\(V(C{\)V){\r(V(V(C{\)-1)-1 {CC)-1 F {\displaystyle \operatorname {sconv} liv Mathematik {F} ist die Sammlung von Funktionen der Form  i i = 1 m  i i f i RARstyle \sum _i=1}^{m}\alpha i}f_{i with  i i = 1 m [  i i] {_i} 1} .Then ob N (  F {\ F  Q Q , 2 , F , L 2 ( Q ) ≤ C ) {\  V {\ V {\displaystyle N\left(\varepsilon) F\|_{Q,2}, cu Mathematik F}},L_{2}(Q)\leq C\varepsilon V-V} ist der folgende gültig für die Konvex-Schaffung von F displaystyle {F} : Log  N N (ε F  F F ‖ F , 2 , sconv  F F , L 2 ( Q ) ≤ K ) − 2 V + 2 574 \log \left(\vareLDR) F\|_{Q,2},\operatorname {sconv} Geschäftsbereich F}},L_{2}(Q)\leq K\varepsilon psilon-\frac 2VVV+2 Die wichtige Folge dieser Tatsache ist, dass 2 V + 2 , JPYstyle faserfrac 2V+V+2>>2, die gerade genug ist, damit das Eutropy integraler Bestandteil konvergiert wird und daher die Klasse sconv {\ F {\displaystyle \operator {sconv} haushaltcal {F} wird P-Donsker sein. Letztlich wird ein Beispiel für eine VC-Subgraph-Klasse betrachtet. {l · · {F \n · {F} messbarer Funktionen f : X → R displaystyle f: {X}}\to \ Mathematik {R} } ist VC-Subgraph des Index kleiner als oder gleich dim . ( F ) + 2 Memestyle dim(F}}) +2 } .Proof: n =  F (F) : a i > 0 a i ( x i ) − t i ) =  i a i  i a i  i a i ) ( f ( x i ) ,  f f  F F {\displaystyle \sum a_{i}0} (fx_{i})-t_{i}(fx_{i})-t_{i})-t_{i} {F} erwägen das Set S = { ( x i , t i ) : a i > 0 } S=\{(x_{i},t_{i}:a_{i}0 . Dieser Satz kann nicht aufgegriffen werden, da wenn es einige f {\displaystyle f} so ist, dass S = { ( x i , t i ) : f ( x i ) · } KINGstyle S=((x_{i},t_{i}):f(x_{i})] {i, das bedeutet, dass das LHS absolut positiv ist, aber das RHS ist nicht positiv. Es gibt Generalisierungen der Unterklasse VC, z.B. die Begriffsbestimmung der Pseudo-Dimension. Der interessierte Leser kann auf[4.] schauen. Qualität Es wird eine ähnliche Einstellung in Betracht gezogen, die eher dem maschinenlesbaren Lernen entspricht. Letztlich X HANAstyle {X} ist ein Platz und Y = { 0 , 1 } KINGstyle 7.8 Mathematik Y==00,1 .A Funktion f : X → Y WELLdisplaystyle f: fiskal {X}}\to haushaltcal {Y} wird als Klassenberater bezeichnet. Lassen Sie F HANAstyle fasercal {F} eine Reihe von Klassenstellen sein. Gleiches gilt auch für den vorherigen Abschnitt, der den Abbauskoeffizient (auch bekannt als Wachstumsfunktion): S ( F , n ) = max x 1 , ... , x n ( f ( x 1 ) , ... , f ( x n ) , f  F F } {F),n)=\max x_{1},\ldots x_{n|(fx_{1}),\ldots f(x_{n}),f\ldots f(x_{n})),f\in liv Mathematikcal {F\|} Hinweis hier, dass ein 1:1 zwischen den Funktionen in F TONdisplaystyle fasercal {F} und dem Set, auf dem die Funktion 1. Wir können daher C {\displaystyle {C} definieren, um die Sammlung von Untersätzen, die aus der oben genannten Kartierung für jede F {\ F {\displaystyle f\in Memecal {F} .Esfore, in Bezug auf den vorherigen Abschnitt der Zerkleinerungskoeffizient ist genau x 1 , ... , x n Δ n n n (C, x 1 }, x n n } \max x1,\x1,\l Let D n = { ( X 1 , Y 1 ) , ... , ( X n , Y m ) } D_{n}=((X_{1},Y_{1}),\ldots (X_{n},Y_{m)\ ist ein beobachteter Datensatz. Beurteilung, dass die Daten durch eine unbekannte Wahrscheinlichkeitsverteilung P X Y {\displaystyle P_{XY} .Define R ( f) = P ( X )  Y Y ) HANAstyle R(f)=P(f)\neq Y)} generiert werden, um den erwarteten Verlust von 0/1 zu erreichen. Selbstverständlich seit P X Y WELLdisplaystyle P_{XY} ist allgemein unbekannt, ein hat keinen Zugang zu R ( f) faserstyle R(f)} . Wie immer das empirische Risiko, das von: R ^ n ( f) = 1 n  i i = 1 nI ( f ( X i ) ) Y i ) Memedisplaystyle WELLhat R}}_{n}(f)=1,0rac 1}}\n _sum _i=1}\n}\bb {I} f(X_{i})\neq Y_{i)} kann sicherlich bewertet werden. Dann hat man die folgenden Theorem: Theorem (VC Inequality) Für die Klassifizierung von binären und die 0/1 Verlustfunktion haben wir folgende Generalisierung gebunden: P (über f  | F; R ^ n ( f ) − R ( f ) [  | ] ≤ 8 S (F , n ) e − n  2 2 / 32 E [ sup f  F F: ^ n ( f ) − R ( f ) ] ≤ 2 log  S S ( F , n ) + Log  2 2 n {\displaystyle beginnt P\left(\sup {_f\in 7.8 mathematische F|left| cuhat R}}_{n}(f)-R(f)\right\varepsilon rechts) &\leq 8S(Getränken)e^{-n\vare  2}/32}\\\bb {E} 7.8 mathematische F|left|hol R}}_{n}(f)-R(f)\right\right] &\leq 2 cusqrt faserrac liv S( fiskal {F,,n)+\log 2}{n{end Laut VC-Diagnose ist das empirische 0/1-Risiko, da die Probenerhöhungen, sofern F Memestyle {F} eine finite VC-Dimension hat, ein guter Ersatz für das erwartete 0/1-Risiko. Hinweis darauf, dass sowohl RHS der beiden Ungleichheiten auf 0 angeglichen werden, sofern S ( F , n ) Memedisplaystyle S(Kapaler {F,,n) Polynomially in n. Die Verbindung zwischen diesem Rahmen und dem Rahmen des Empirischen Prozesses ist offensichtlich. Hierbei handelt es sich um einen geänderten empirischen Prozess | R ^ n − R (F HANAdisplaystyle links) R__{n}-R\right_livcal {F}, aber die Ideen sind nicht überraschend. Der Nachweis der (erste Teil) VC-Diagnose stützt sich auf die sysmmetrisierung und argumentiert dann an die Daten, die Konzentrationsunterschiede (insbesondere die Ungleichheit von Hoeffding) verwenden. Interessierte Leser können das Buch überprüfen [5] Theorems 12.4 und 12,5 Verweise ^ Vapnik, Vladimir N (2000). Natur der statistischen Lerntheorie.Information Science and Statistics.Springer-Verlag.ISBN UV0-387-98780-4.Vapnik, Vladimir N (1989). Statistische Lerntheorie.Wiley-Interscience.ISBNUNG DES0-471-03003-4^. van der Vaart, Aad W.;Wellner, Jon A. (2000). Wirak Convergence and Empirical Prozesse: Bei Anwendungen zu Statistiken (2. Springer.ISBN gegen0-387-94640-5^. Gyorfi, L; Devroye, L; Lugosi, G. (1996). Eine probabilistische Theorie der Mustererkennung (1st ed). Springer. [0387946184. Verweise auf Artikel: Richard M. Dudley, empirische Prozesse, Shattered Set.^Pollard, David (1990). Empirische Prozesse: Theorie und Anwendung. NSF-CBMS Regionale Konferenzreihe in Probability and Statistics Volumen 2.ISBNUNG DES0-940600-16-4. Bousquet, O; Boucheron, S; Lugosi, G. (2004)."Introduction to Statistische Lerntheorie. In O. Bousquet; U. von Luxburg; G. Ratsch (eds). Fortgeschrittene Lehrveranstaltungen zum Maschinenbau. Lesen Sie in Künstlicher Intelligenz. 3176.Springer.pp.169-207. Vapnik, V; Chervonenkis, A. (2004). " Einheitliche Konvergenz der relativen Frequenzen von Veranstaltungen an ihre Probabilities. Theorie Probab.Appl.16 (2): 264-280.doi:10.1137/1116025