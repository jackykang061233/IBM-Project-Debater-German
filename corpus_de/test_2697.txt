Die richtige verallgemeinerte Zersetzung (PGD) ist eine iterative numerische Methode zur Lösung von Grenzwertproblemen (BVPs), d.h. partielle Differentialgleichungen, die durch eine Reihe von Randbedingungen, wie die Poisson's Gleichung oder die Laplace's Gleichung, bedingt sind. Der PGD-Algorithmus berechnet eine Annäherung der Lösung des BVP durch aufeinanderfolgende Anreicherung. Dies bedeutet, dass bei jeder Iteration eine neue Komponente (oder Modus) berechnet und der Approximation hinzugefügt wird. Je mehr Moden erhalten werden, desto näher liegt die Annäherung an ihre theoretische Lösung. Im Gegensatz zu POD-Hauptkomponenten sind PGD-Modi nicht notwendigerweise orthogonal zueinander. Durch die Wahl nur der relevantesten PGD-Modi ergibt sich ein reduziertes Auftragsmodell der Lösung. Dadurch wird PGD als Dimensionsreduktionsalgorithmus betrachtet. Warenbezeichnung Die ordnungsgemäße verallgemeinerte Zersetzung ist ein Verfahren, das sich durch eine Variationsformulierung des Problems, eine Diskretisierung der Domäne im Stil der Finite-Elemente-Methode, die Annahme, dass die Lösung als separate Darstellung angenähert werden kann und ein numerischer gieriger Algorithmus zur Lösungsfindung. Variationsformulierung Die am meisten implementierte Variationsformulierung in PGD ist die Bubnov-Galerkin-Methode, obwohl es andere Implementierungen gibt. Domain diskretierung Die Diskretisierung der Domäne ist ein definierter Satz von Verfahren, die (a) die Schaffung von endlichen Elementmaschen, (b) die Definition der Basisfunktion auf Referenzelementen (auch Formfunktionen genannt) und (c) die Zuordnung von Referenzelementen auf die Elemente des Netzes abdecken. Die getrennte Darstellung PGD geht davon aus, dass die Lösung u eines (multidimensionalen) Problems als separate Darstellung der Form u ≈ u N ( x 1 , x 2 , ..., x d ) = Σ i = 1 N X 1 i ( x 1 ) Θ X 2 i ( x 2 ) ⋯ X d i ( x d ) , _i=1^{N}\mathbf X_{1} i}(x_{1})\cdot \mathbf X_{2} i}(x_{2})\cdots \mathbf X_{d} i}(x_{d,) wobei die Anzahl der Addends N und der Funktionsprodukte X1(x1,) X2(x2,) ..., Xd( Greedy Algorithmus Die Lösung wird angestrebt, indem ein gieriger Algorithmus, in der Regel der Fixpunkt-Algorithmus, auf die schwache Formulierung des Problems angewendet wird. Für jede Iteration i des Algorithmus wird ein Modus der Lösung berechnet. Jeder Modus besteht aus einem Satz von Zahlenwerten der Funktionsprodukte X1(x1,) ..., Xd(xd), die die Approximation der Lösung bereichern. Aufgrund der gierigen Natur des Algorithmus wird der Begriff Anreicherung anstatt zu verbessern verwendet, da einige Moden den Ansatz tatsächlich verschlechtern können. Die Anzahl der berechneten Modi, die erforderlich sind, um eine Annäherung der Lösung unterhalb einer bestimmten Fehlerschwelle zu erhalten, hängt vom Stop-Criterium des iterativen Algorithmus ab. Eigenschaften PGD eignet sich zur Lösung hochdimensionaler Probleme, da es die Grenzen der klassischen Ansätze überwindet. Insbesondere vermeidet PGD den Fluch der Dimensionalität, da das Lösen von entkoppelten Problemen rechnerisch viel weniger teuer ist als das Lösen von mehrdimensionalen Problemen. Daher ermöglicht PGD die erneute Anpassung parametrischer Probleme in einen mehrdimensionalen Rahmen, indem die Parameter des Problems als zusätzliche Koordinaten eingestellt werden: u ≈U N ( x 1 , ... , x d ; k 1 , ... , k p ) = Σ i = 1 N X 1 i ( x 1 ) X d) i ( x d ) ∙ K 1 i (k 1 ) ⋯ K p i (k p ) , {\displaystyle \mathbf {u} \approx \mathbf {u} ^N}(x_{1},\ldots x_{d};k_{1},\ldots ,k_{p})=\sum * i=1{N}\mathbf X_{1}_{i}(x_{1})\cdots \mathbf X_{d} i}(x_{d})\cdot \mathbf K_{1} i}(k_{1})\cdots \mathbf K_{p} i}(k_{p), wobei eine Reihe von Funktionsprodukten K1(k1,) K2( In diesem Fall wird die erhaltene Approximation der Lösung als rechnerisches Vademecum bezeichnet: ein allgemeines Meta-Modell mit allen jeweiligen Lösungen für jeden möglichen Wert der beteiligten Parameter. Sparse Subspace Learning Das Sparse Subspace Learning (SSL)-Verfahren nutzt hierarchische Collokation, um die numerische Lösung parametrischer Modelle anzunähern. Im Hinblick auf die traditionelle projektionsbasierte reduzierte Auftragsmodellierung ermöglicht die Verwendung einer Kollokation einen nicht-intrusiven Ansatz basierend auf einer sparsamen adaptiven Abtastung des parametrischen Raums. Dies ermöglicht die Wiederherstellung der niederdimensionalen Struktur des parametrischen Lösungsunterraums, während auch das Erlernen der funktionalen Abhängigkeit von den Parametern in expliziter Form. Durch eine inkrementelle Strategie, die nur Zugriff auf die Ausgabe eines deterministischen Solvens haben muss, kann eine sparsame, kurze Tensordarstellung der parametrischen Lösung aufgebaut werden. Unaufdringlichkeit macht diesen Ansatz für herausfordernde Probleme, die sich durch Nichtlinearität oder nicht affine schwache Formen auszeichnen. = Referenzen ==