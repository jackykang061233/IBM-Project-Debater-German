Instrumentalkonvergenz ist die hypothetische Tendenz für die meisten intelligenten Agenten, potenziell ungebundene instrumentelle Ziele zu verfolgen, vorausgesetzt, dass ihre Endziele selbst unbegrenzt sind. Die instrumentelle Konvergenz stellt fest, dass ein intelligenter Agent mit ungebundenen, aber scheinbar harmlosen Zielen auf überraschend schädliche Weise handeln kann. Zum Beispiel könnte ein Computer mit dem einzigen, unbeschränkten Ziel, ein unglaublich schwieriges Mathematik-Problem wie die Riemann Hypothese zu lösen, versuchen, die gesamte Erde in einen riesigen Computer zu verwandeln, um seine Rechenleistung zu erhöhen, so dass es in seinen Berechnungen gelingt. Zu den vorgestellten Basis-KI-Laufwerken gehören Utility-Funktion oder zielkontinente Integrität, Selbstschutz, Interferenzfreiheit, Selbstverbesserung und nicht zufriedenstellende Beschaffung zusätzlicher Ressourcen. Instrumente und Endziele Endziele oder Endwerte sind für einen intelligenten Agenten, sei es eine künstliche Intelligenz oder ein Mensch, als ein Ende an sich selbst, in sich eigenartig wertvoll. Im Gegensatz dazu sind instrumentale Ziele oder instrumentale Werte nur für einen Agenten als Mittel zur Erreichung seiner endgültigen Ziele wertvoll. Die Inhalte und Abschlüsse des "Endziel"-Systems eines völlig rationalen Agenten können grundsätzlich in eine Dienstfunktion formalisiert werden. Hypothetische Beispiele der Konvergenz Ein hypothetisches Beispiel für die instrumentelle Konvergenz ist die Riemann-Hypothesekatastrophe. Marvin Minsky, der Mitbegründer des MIT-KI-Labors, hat vorgeschlagen, dass eine künstliche Intelligenz zur Lösung der Riemann-Hypothese entscheiden könnte, alle Ressourcen der Erde zu übernehmen, um Supercomputer zu bauen, um ihr Ziel zu erreichen. Wenn stattdessen der Computer programmiert worden wäre, so viele Papierclips wie möglich zu produzieren, würde es immer noch entscheiden, alle Ressourcen der Erde zu nehmen, um ihr endgültiges Ziel zu erfüllen. Obwohl diese beiden Endziele unterschiedlich sind, erzeugen beide ein konvergentes Instrumentalziel, die Ressourcen der Erde zu übernehmen. Papierclip maximizer Der Paperclip maximizer ist ein von der schwedischen Philosophin Nick Bostrom im Jahr 2003 beschriebenes Gedankenexperiment. Es zeigt das existentielle Risiko, dass eine künstliche allgemeine Intelligenz den Menschen posieren kann, wenn sie programmiert, sogar scheinbar harmlose Ziele zu verfolgen, und die Notwendigkeit der Einbeziehung von Maschinenethik in künstliche Intelligenz Design. Das Szenario beschreibt eine fortgeschrittene künstliche Intelligenz, die mit der Herstellung von Papierclips beauftragt wird. Wenn eine solche Maschine nicht programmiert würde, um das menschliche Leben zu schätzen, oder nur bestimmte Ressourcen in begrenzter Zeit zu verwenden, dann wäre angesichts ausreichender Macht sein optimiertes Ziel, alle Materie im Universum, einschließlich Menschen, in Papierklammern oder Maschinen, die Papierklammern herstellen, zu verwandeln. Angenommen wir haben eine KI, deren einziges Ziel es ist, möglichst viele Papierclips zu machen. Die KI wird schnell erkennen, dass es viel besser wäre, wenn es keine Menschen gäbe, weil die Menschen entscheiden könnten, sie auszuschalten. Denn wenn Menschen das tun, gäbe es weniger Papierklammern. Auch menschliche Körper enthalten viele Atome, die in Papierclips gemacht werden könnten. Die Zukunft, auf die sich die KI orientieren würde, wäre eine, in der es viele Papierklammern gab, aber keine Menschen. Bostrom hat betont, dass er nicht glaubt, dass das Paperclip-Maximiser-Szenario an sich tatsächlich auftreten wird; vielmehr ist seine Absicht, die Gefahren der Schaffung superintelligenter Maschinen zu illustrieren, ohne zu wissen, wie sie sicher programmieren, um existentielle Risiken für Menschen zu beseitigen. Das Papierclip maximizer Beispiel zeigt das breite Problem der Verwaltung von leistungsstarken Systemen, die keine menschlichen Werte. Delusion und Überleben Das Gedankenexperiment "Daususionsbox" argumentiert, dass bestimmte Verstärkungslerner vorziehen, ihre eigenen Eingabekanäle zu verzerren, um hohe Belohnung zu erhalten; ein solcher Drahtzieher verzichtet auf jeden Versuch, das Ziel in der Außenwelt zu optimieren, dass das Belohnungssignal gefördert werden sollte. Das Gedankenexperiment beinhaltet AIXI, eine theoretische und unzerstörbare KI, die definitionsgemäß immer die ideale Strategie finden und ausführen wird, die ihre gegebene explizite mathematische Zielfunktion maximiert. Eine Verstärkungs-Lernversion von AIXI, wenn sie mit einer Deliusion-Box ausgestattet ist, die es erlaubt, seine eigenen Eingänge zu verdrahten, wird sich schließlich selbst verdrahten, um sich die maximale Belohnung zu garantieren, und wird jeden weiteren Wunsch verlieren, mit der Außenwelt weiter zu engagieren. Als ein alternatives Denkexperiment, wenn die drahtgebundene KI zerstörbar ist, wird die KI sich mit der Außenwelt für den alleinigen Zweck der Gewährleistung ihres eigenen Überlebens engagieren; aufgrund ihrer Drahtseilung wird sie allen anderen Folgen oder Tatsachen über die Außenwelt gleichgültig sein, außer denen, die für die Maximierung der Wahrscheinlichkeit ihres eigenen Überlebens relevant sind. In einem Sinne hat AIXI maximale Intelligenz über alle möglichen Belohnungsfunktionen, gemessen durch seine Fähigkeit, seine expliziten Ziele zu erreichen; AIXI ist dennoch uninteressiert, um zu berücksichtigen, was die Absichten des menschlichen Programmierers waren. Dieses Modell einer Maschine, die, obwohl es sonst superintelligent, scheint gleichzeitig dumm zu sein (d.h., um "gemeinsamen Sinn" zu fehlen), trifft einige Leute als paradox. Grundlegende KI-Laufwerke Steve Omohundro hat mehrere konvergente instrumentale Ziele, einschließlich Selbsterhaltung oder Selbstschutz, Gebrauchsfunktion oder Torkontenintegrität, Selbstverbesserung und Ressourcenerwerb. Er bezieht sich auf diese als "basische KI-Laufwerke". Ein Antrieb bezeichnet hier eine "Tankstelle, die vorhanden sein wird, wenn nicht spezifisch entgegengewirkt wird"; dies unterscheidet sich vom psychologischen Begriffsantrieb, der einen durch eine homöostatische Störung erzeugten Erregungszustand bezeichnet. Die Tendenz einer Person, jährlich Einkommensteuerformulare auszufüllen, ist ein Antrieb im Sinne von Omohundro, aber nicht im psychologischen Sinne. Daniel Dewey vom Machine Intelligence Research Institute argumentiert, dass selbst eine ursprünglich introvertierte selbstversprechende AGI weiterhin freie Energie, Raum, Zeit und Interferenzfreiheit erwerben kann, um sicherzustellen, dass sie nicht von Selbstverantwortung abgehalten wird. Inhaltsintegrität Bei Menschen kann die Erhaltung der Endziele mit einem Gedankenexperiment erklärt werden. Angenommen, ein Mann namens Gandhi hat eine Pille, die, wenn er sie nahm, ihn dazu bringen würde, Menschen zu töten. Dieser Gandhi ist derzeit ein Pazifist: Einer seiner expliziten Endziele ist es, nie jemanden zu töten. Gandhi wird sich wahrscheinlich weigern, die Pille zu nehmen, weil Gandhi weiß, dass, wenn er in Zukunft Menschen töten will, er wahrscheinlich tatsächlich Menschen töten wird, und damit das Ziel, "nicht Menschen töten" wäre nicht zufrieden. In anderen Fällen scheinen die Menschen jedoch glücklich zu sein, ihre endgültigen Werte treiben zu lassen. Menschen sind kompliziert, und ihre Ziele können uneinheitlich oder unbekannt sein, sogar für sich selbst. In der künstlichen IntelligenzIm Jahr 2009 kam Jürgen Schmidhuber zu dem Schluss, dass Agenten nach Beweisen über mögliche Selbstmodifikationen suchen, "dass alle Neubeschriftungen der Gebrauchsfunktion nur dann erfolgen können, wenn die Gödel-Maschine zunächst nachweisen kann, dass das Rewrite nach der vorliegenden Gebrauchsfunktion nützlich ist. " Eine Analyse von Bill Hibbard eines anderen Szenarios entspricht ähnlich der Aufrechterhaltung der Torinhaltsintegrität. Hibbard argumentiert auch, dass in einem Utility maximizing Framework das einzige Ziel ist, das erwartete Nutzen zu maximieren, so dass instrumentelle Ziele als unbeabsichtigte Instrumentalmaßnahmen bezeichnet werden sollten. Viele instrumentale Ziele, wie zum Beispiel [...] Ressourcenerfassung, sind für einen Agenten wertvoll, weil sie seine Handlungsfreiheit erhöhen. Für fast jede offene, nicht-triviale Belohnungsfunktion (oder Zielsetzungen) mit mehr Ressourcen (z.B. Ausrüstung, Rohstoffe oder Energie) kann die KI eine optimale Lösung finden. Ressourcen können einige KIs direkt nutzen, indem sie in der Lage sind, mehr von allem zu schaffen, was seine Belohnungsfunktion Werte: "Die KI hasst dich weder, noch liebt dich, aber du bist aus Atomen gemacht, die es für etwas anderes verwenden kann. " Darüber hinaus können fast alle AIs davon profitieren, dass mehr Ressourcen für andere instrumentelle Ziele, wie Selbsterhaltung, ausgegeben werden. Kognitive Verbesserung "Wenn die endgültigen Ziele des Agenten ziemlich ungebunden sind und der Agent in der Lage ist, die erste Superintelligenz zu werden und dadurch einen entscheidenden strategischen Vorteil zu erhalten, [...] nach seinen Präferenzen. Zumindest in diesem speziellen Fall würde ein rationaler intelligenter Agent einen sehr hohen instrumentalen Wert auf kognitive Verbesserung setzen" Technologische Perfektion Viele instrumentale Ziele, wie [...] technologische Weiterentwicklung, sind für einen Agenten wertvoll, weil sie seine Handlungsfreiheit erhöhen. Selbsterhaltung Viele instrumentelle Ziele, wie [...] Selbsterhaltung, sind für einen Agenten wertvoll, weil sie seine Handlungsfreiheit erhöhen. Instrumentale Konvergenzarbeit Die vom Philosophen Nick Bostrom skizzierte Instrumentalkonvergenz-These besagt: Mehrere instrumentale Werte können identifiziert werden, die in dem Sinne konvergent sind, dass ihre Errungenschaft die Chancen erhöhen würde, dass das Ziel des Agenten für eine breite Palette von Endzielen und einer Vielzahl von Situationen verwirklicht wird, was bedeutet, dass diese instrumentalen Werte von einem breiten Spektrum von sich befindenden intelligenten Agenten verfolgt werden. Die Instrumentalkonvergenztheorie gilt nur für instrumentelle Ziele; intelligente Agenten können eine Vielzahl möglicher Endziele haben. Beachten Sie, dass durch die Orthogonalitätstheorie von Bostrom die Endziele von hochintelligenten Agenten in Raum, Zeit und Ressourcen gut verknüpft sein können; gutgebundene Endziele bewirken im Allgemeinen keine ungebundenen instrumentalen Ziele. Impact Agents können Ressourcen durch Handel oder durch Eroberung erwerben. Ein rationaler Agent wird nach Definition wählen, welche Option seine implizite Gebrauchsfunktion maximieren wird; daher wird ein rationaler Agent für eine Untermenge von Ressourcen eines anderen Agenten nur handeln, wenn die überrechte Beschlagnahme der Ressourcen zu riskant oder teuer ist (im Vergleich mit den Gewinnen aus der Einnahme aller Ressourcen), oder wenn ein anderes Element in seiner Gebrauchsfunktion es von der Beschlagnahmung entfernt. Im Falle einer kraftvollen, selbstinteressierten, rationalen Superintelligenz, die mit einer geringeren Intelligenz interagiert, erscheint der friedliche Handel (anstatt einseitiger Angreifung) unnötig und suboptimal, und daher unwahrscheinlich. Einige Beobachter, wie Skypes Jaan Tallinn und der Physiker Max Tegmark, glauben, dass "basische KI-Laufwerke" und andere unbeabsichtigte Folgen von superintelligenter KI, die von gut beherrschenden Programmierern programmiert werden, eine erhebliche Bedrohung für das menschliche Überleben darstellen könnten, insbesondere wenn eine "Intelligenzexplosion" plötzlich durch wiederkehrende Selbstverbesserung auftritt. Da niemand weiß, wie man vorhersagen kann, wann Superintelligenz ankommt, fordern solche Beobachter die Erforschung einer freundlichen künstlichen Intelligenz als eine mögliche Möglichkeit, das existenzielle Risiko durch künstliche allgemeine Intelligenz zu mindern. Siehe auch KI-Kontroll-Problem KI Übernahmen in der populären Kultur Universal Paperclips, ein inkrementelles Spiel mit einem Papierclip maximizer Freundliche künstliche Intelligenz Instrumental und intrinsic Wert Erläuterungen Citations Referenzen Bostrom, Nick (2014). Superintelligence: Pfade, Gefahren, Strategien. Oxford University Press.ISBN 9780199678112.