Generativer vortrainierter Transformer 2 (GPT-2) ist eine künstliche Intelligenz, die im Februar 2019 von OpenAI erstellt wurde. GPT-2 übersetzt Text, beantwortet Fragen, fasst Passagen zusammen und erzeugt Textausgabe auf einer Ebene, die, wenn auch manchmal von der des Menschen nicht zu unterscheiden, bei langen Passagen repetitiv oder unsinnig werden kann. Es ist ein allgemeiner Lerner; es wurde nicht speziell ausgebildet, um eine dieser Aufgaben zu tun, und seine Fähigkeit, sie auszuführen, ist eine Erweiterung seiner allgemeinen Fähigkeit, den nächsten Artikel in einer beliebigen Reihenfolge genau zu synthetisieren. GPT-2 wurde als "Direkt-Skalierung" des GPT-Modells von OpenAI 2018 mit einer zehnfachen Steigerung sowohl der Parameterzahl als auch der Größe seines Trainingsdatensatzes erstellt. Die GPT-Architektur implementiert ein tiefes neuronales Netzwerk, insbesondere ein Transformatormodell, das anstelle früherer rekurs- und konvolutionsbasierter Architekturen Aufmerksamkeit nutzt. Aufmerksamkeitsmechanismen ermöglichen es dem Modell, selektiv auf Segmente von Eingabetexten zu fokussieren, die es als am relevantesten erachtet. Dieses Modell ermöglicht eine deutlich erhöhte Parallelisierung und erweitert die bisherigen Benchmarks für RNN/CNN/LSTM-basierte Modelle. OpenAI veröffentlichte im November 2019 die komplette Version des GPT-2 Sprachmodells (mit 1,5 Milliarden Parametern). GPT-2 sollte dem 175-Billion-Parameter GPT-3 folgen, der der Öffentlichkeit im Jahr 2020 enthüllt wurde (deren Quellcode noch nie zur Verfügung gestellt wurde). Der Zugriff auf GPT-3 erfolgt ausschließlich über eine von Microsoft angebotene API. Hintergrund Seit den Ursprüngen des Computings ist künstliche Intelligenz ein Untersuchungsobjekt; das 1950 von Alan Turing postulierte "Imitationsspiel" (und oft auch "Turing test" genannt) schlug vor, durch die Fähigkeit eines Evaluators, sein Verhalten von dem eines Menschen zu unterscheiden, ein elektronisches oder mechanisches System für intelligentes Handeln zu schaffen. Der Begriff "Maschinenlernen" wurde erstmals verwendet, um einen möglichen Ansatz für künstliche Intelligenz bereits 1959 von IBM-Forscher Arthur Samuel zu beschreiben; die aktuelle Nutzung des Begriffs umfasst eine breite Palette von statistischen Lern-, Daten- und neuronalen Netzwerkansätzen zu Rechenproblemen (oft unter die Ägide künstlicher Intelligenz fallen). Selbstverständliche Sprachverarbeitung mit Computern, eine ursprünglich als Unterfeld der rechnerischen Linguistik konzipierte Aufgabe, wurde versucht, sobald die Computerhardware die Kapazität hatte; die erste Anwendung einer Wörterbuch-Auflistungstabelle wurde 1948 am Birkbeck College in London entwickelt. Die 1954 Georgetown Experiment war eine Demonstration der vollautomatischen maschinellen Übersetzung, in der sechzig russische Sätze ins Englische übersetzt wurden (meist durch Austausch von Wörtern mit ihren englischen Synonymen). Die Übersetzungen waren oft roh; das System hatte nur 6 Grammatik-Regeln und ein 250-Worte-Vokabular, und es wurde kein Versuch unternommen, syntaktische Struktur zu analysieren oder zu übersetzen. Das Experiment bewies sich jedoch der Öffentlichkeit, dass Computer natürliche Sprache interpretieren und verarbeiten könnten und CIA-Förderung für weitere Forschung gesichert. Die direkte Substitution bleibt ein Standard, gegen den maschinelle Übersetzungsprogramme ausgewertet werden. Auch Systeme zur Verwendung natürlicher Sprache in der Mensch-Computer-Interaktion (HCI) begannen Mitte des 20. Jahrhunderts zu entstehen. SHRDLU, ein Programm, das 1968-1970 am MIT entwickelt wurde, bestand aus einer virtuellen Umgebung von mehreren Objekten, die ein Benutzer mit Befehlen in natürlicher Sprache interagierte (z.B.....). ELIZA, ein 1966 geschriebener Chatterbot, analysierte den Text eines menschlichen Gesprächspartners für Schlüsselwörter und gab konversationsgerechte Antworten. Während viele Subjekte eine Unfähigkeit behaupteten, ELIZAs Gespräch von der eines Menschen zu unterscheiden, die Frage, ob diese konstituierte Intelligenz erwies sich als befriedigend (das berühmteste Skript parodied ein Psychotherapeut, indem, weitgehend, wiederholen, was der Benutzer zu ihnen gesagt hatte). Während anfängliche Versuche an der maschinellen Übersetzung rein rechnerisch waren, hatte die beherrschende Herangehensweise an die rechnerischen Sprachkenntnisse Noam Chomskys Konzept der universalen Grammatik in den 1950er Jahren hervorzuheben; NLP-Forschung in dieser Zeit bestand daher weitgehend aus Versuchen, Aussagen in beliebigen Sprachen zu kürzen, um zugrunde liegende, sprach-agnostische logische Strukturen zu legen. In den 1970er-Jahren würden semantische NLP-Systeme beginnen, syntaktische Kodierungen zugunsten allgemeiner semantischer Kodierungen zu eschewieren. Bis zum Aufkommen neuronaler Netze setzten sich die meisten Systeme jedoch weiterhin auf große (und zunehmend unwiederholte) Sätze manuell programmierter Regeln, die sich nicht wie ursprünglich vorhergesagt aufs Neue ausweiten konnten. Der Bereich der künstlichen Intelligenz entwickelte sich im späten 20. Jahrhundert, aber gelegentliche Stagnationsperioden, die als "AI-Winter" bekannt sind, traten auf. Verschiedene Quellen posit KI-Winters, wie es zu verschiedenen Zeiten aufgetreten ist; 1994, Howe beschrieben, wie eine, die 1973 begonnen und ein Jahrzehnt dauerte, während Russell & Norvig im Jahr 2003 eine andere, wie bald nach 1988. Neurale Netze Ein frühes Konzept in der künstlichen Intelligenz, der Verbindungsismus, suchte, intelligentes Verhalten durch künstliche neuronale Netzwerke zu erzeugen, die das Verhalten von Neuronen in biologischen Gehirnen simulieren. Das erste Beispiel eines künstlichen neuronalen Netzes war das 1951 gebaute SNARC. Der Perceptron (ein Typ binärer Klassifikator) wurde 1957 von dem Psychologen Frank Rosenblatt eingeführt; seine Maschine wurde für die Bilderkennung mit 400 mit Neuronen verbundenen Photozellen konzipiert, wobei durch Potentiometer bestimmte Gewichtungen (und während des Lernprozesses mit Elektromotoren eingestellt) ermittelt wurden. Perceptron-Systeme wurden Gegenstand von großem Interesse; ein New York Times Artikel beschreibt den Perceptron als "der Embryo eines elektronischen Computers, der erwartet wird, dass [die Marine] in der Lage sein, zu gehen, zu sprechen, zu sehen, zu schreiben, sich selbst zu reproduzieren und sein Bewusstsein für seine Existenz". Perceptron-Systeme fielen jedoch seit Jahrzehnten aus Gunsten nach einem 1969er Buch von Marvin Minsky und Seymour Papert (Perceptrons: eine Einführung in die Rechengeometrie), das mehrere Mängel des damaligen Standes der Technik (Single-Layer-Perceptrons) mit einer Unfähigkeit, die ausschließliche oder (XOR) Funktion zu kodieren. Das Buch wurde damals als vielversprechendes Forschungsgebiet betrachtet, den Perceptron-Ansatz (sowie neuronale Netze im Allgemeinen) zu diskreditieren. Neurale Netzwerke werden in der Lage, unterschiedliche Eingaben (d.h. Sortierung in verschiedene Kategorien) durch ein als Lernen bezeichnetes Verfahren zu klassifizieren". Dies beginnt mit den Gewichten des Netzes (die Menge, um die jede Neuron-Aktivierung die Aktivierung jedes bestimmten Neurons in der nachfolgenden Schicht beeinflusst) in zufällige Mengen initialisiert wird; in diesem Zustand ist der Ausgang des Netzes ähnlich zufällig. Eine objektive Funktion ist, wie eine Verlustfunktion, definiert, die quantitativ messen kann, wie nahe der Ausgang des Netzes an seiner gewünschten Leistung liegt (z.B. wie oft eine aus einer handschriftlichen Zahl bestehende Eingabe zur alleinigen Aktivierung des Ausgangsneurons entsprechend dieser Zahl führt). Aus diesem und aus der Leistung des Netzes können die Gewichte angepasst werden, um seine Leistung zu verbessern. Backpropagation, ein beaufsichtigter Algorithmus, der in Paul Werbos 1974 erstmals auf maschinelle Lernsysteme angewandt wird, berechnet effizient Gradienten, die Vektorfelder sind, die die optimale Anpassung aller Gewichte im gesamten Netzwerk für ein gegebenes Ein-/Ausgabebeispiel beschreiben. Die Verwendung dieser Gradienten zur Ausbildung von neuronalen Netzen, eine als Gradientenabstieg bekannte Praxis, ermöglichte die Schaffung viel komplexer Systeme, und eine breite Anwendung von neuronalen Netzen auf natürliche Sprachverarbeitung würde in den 1980er Jahren auftreten. 1985 würde D.B. Parker die Methode von Werbos wiederentdecken; 1986 würden Rumelhart, Hinton und Williams sie anwenden, um interne Darstellungen von eingehenden Daten in neuronalen Netzwerken mit versteckten Schichten zu erzeugen, die als "Deep Learning"-Netzwerke bezeichnet werden; diese Forschung würde später die Grundlage für wiederkehrende neuronale Netzwerke bilden. Traditionelle zukunftsweisende neuronale Netzwerke (FFNs) werden so genannt, weil jede Schicht aus der vorherigen Schicht ausgeht und sie in die nächste führt; eine FFNN-Struktur enthält keine Zyklen, in denen Informationen nach hinten fließen. Demgegenüber weist ein wiederkehrendes neuronales Netz (RN) mindestens einen Aktivierungsstromzyklus auf. RNNs werden häufig für die Verarbeitung von Datensequenzen (und Vorhersage zukünftiger Sequenzen) verwendet, da das Netzwerk jedes Element sowohl mit dem Item selbst als auch mit seinem eigenen Ausgang aus der Verarbeitung des vorherigen Artikels verarbeiten kann. Das von Kunihiko Fukushima im Jahr 1979 vorgeschlagene Neocognitron, das auf neuralen Architekturmodellen im Säuger-Visualkortex basiert, lieferte die Basis für konvolutionale neuronale Netzwerke (CNNs), die häufig in der Bildverarbeitung verwendet werden. Durch Verschieben einer kleinen Schicht über einen größeren Eingang kann ein CNN eine tiefere Verarbeitung mit weniger Rechenaufwand durchführen. Beispielsweise weist ein 100 x 100-Bild 10.000 Pixel auf, die 10.000 Gewichte benötigen, um mit einer vollständig verbundenen Schicht zu verarbeiten; eine Faltungsschicht aus einem 5 x 5-Fenster, das über das Bild gleitet, kann eine Kantenerkennung mit nur 25 erlernbaren Parametern durchführen. Konvolutionsschichten werden durch "Verbundschichten" kombiniert und durch "vollständig verbundene" Schichten (die typischerweise mehrschichtige Perceptronen sind) verarbeitet. Machine Learning for Natural Language Processing Aufgrund ihrer Fähigkeit, sequentielle Informationen zu verarbeiten, haben wiederkehrende neuronale Netzwerke in vielen NLP-Anwendungen Gebrauch gesehen; im Gegensatz zu FFNNs sind sie in der Lage, verschiedene Gewichte (und geben unterschiedliche Leistung) für identische Gegenstände auf der Grundlage ihrer Umgebung in einer Sequenz zu kodieren - das heißt, ein RNNN-System, das ein Wort zu einer Zeit assoziiert konnte noch ein "schwarzer Hund "Pfundiert" mit einem Hund ". Da die Speicherung von Informationen aus früheren Sequenzen rekursiv durchgeführt werden kann, können RNN-Systeme entworfen werden, die beliebig weit zurück in einer Sequenz erinnern: zum Beispiel in der Lage, die Sequenzen "Tom betrachtet den schwarzen Hund", "Tom schaute den Maishund" und "Tom blickte auf den Sonnenhund" mit lieblich, hungrily und indirekt jeweils fortzusetzen. Während viele Schichten FFNNs und RNs in der Lage waren, sich für das verschwindende Gradientenproblem zu verunsichern: Da Gradienten (als endliche Präzisionszahlen bezeichnet) notwendig sind, um sich über alle Schichten eines Modells hinwegzurüsten, können sie über eine ausreichend große Anzahl von Schichten auf Null (oder auf Unendlichkeit explodieren). Das von Sepp Hochreiter und Jürgen Schmidhuber 1995-1997 erstmals vorgeschlagene langfristige Speichernetzwerk (LSTM) versuchte, dieses Problem zu lösen, indem eine neuartige Architektur aus mehreren verschiedenen Zellen mit Eingabe-, Ausgabe- und Vergessensgates eingeführt wurde. 2009 gewann ein LSTM-basiertes Modell, das von Alex Graves' Team eingereicht wurde, den ICDAR-Wettbewerb für die Anerkennung von Handschriften; ein anderes war das genaueste Modell im Wettbewerb und ein Drittel war der schnellste. Ein weiteres Problem der RNNs und LSTMs-Gegenstand ist, dass sie nur den Kontext früherer Sequenzen berücksichtigen können. Dies kann Probleme verursachen, wenn Sätze wie "Tom ritt sein Fahrrad in den Laden, setzen Sie den Kickstand, und schalten Sie den Motor aus", in dem der notwendige Kontext des Fahrrads ein Motorrad ist nur am Ende offenbart. Eine solche Lösungsmethode ist das bidirektionale LSTM, das in beide Richtungen gleichzeitig abläuft und den Zugriff auf vergangene und zukünftige Eingabefunktionen ermöglicht. Bedingte zufällige Felder verwenden Tags, um Eingaben direkt an Ausgänge zu verbinden. Es gibt Kombinationen der obigen Ansätze, wie das LSTM-CRF-Netzwerk und das BI-LSTM-CRF-Netzwerk. Weitere Verbesserungen am RNN-Modell sind neurale Turing-Maschinen, adaptive Rechenzeit, neurale Programmierer und Aufmerksamkeitsmechanismen, deren letztere die Grundlage für GPT-2 und damit verbundene Technologien bilden. Selektive Fokussierung Mit dem Encoder-Decoder-Modell, in dem ein RNN oder LSTM "Encoder-Netzwerk" Quellensätze in Vektoren kodiert, und ein "Decoder-Netzwerk" ähnlicher Architektur verarbeitete diese Vektoren in übersetzte Ausgabe. 2014 wurden deutlich komplexere Aufmerksamkeitsmechanismen eingeführt, die die Performance dieser Modelle erheblich steigern. Die Beobachtungsmechanismen gaben diesen Modellen die Fähigkeit, die Aufmerksamkeit ihrer Decoder-Netzwerke auf bestimmte Aspekte des Quelltextes adaptiv zu fokussieren, anstatt sie dazu zu zwingen, den gesamten Text als einen Vektor zu parsieren.2017 sah dann die Einführung von Transformator-Modellen, die einen Schritt weiter ging, indem sie Aufmerksamkeitsmechanismen nutzten, um die RNN/LSTM-Architektur vollständig zu ersetzen. Achtungsmechanismen Eine Beschränkung von Encoder-Decoder-Modellen war die Schwierigkeit, die Kodierungen von größeren Sätzen in fester Länge Vektoren zu komprimieren; die Leistung verschlechterte sich oft an größeren Eingängen. Im Jahr 2014 hat Bahdanau et al. eine Erweiterung auf das Encoder-Decoder-Modell eingeführt, das "zusammen ausrichten und übersetzen könnte". Für jedes Wort des Quellurteils, das übersetzt wurde, durchsuchte der Kodierer des Bahdanau-Modells (ein bidirektionales RNN mit 1000 versteckten Einheiten in jeder Richtung) den gesamten Rest dieses Satzes für die Positionen relevanter Informationen. Anstatt dem Dekoder eine feste Längenvektorcodierung der gesamten Eingangssequenz (wie frühere Modelle) zu geben, produzierte es "Kontextvektoren", die diesen Positionen sowie zuvor generierten Zielworten zugeordnet waren. Der Decoder (der auch 1000 versteckte Einheiten hatte) nutzte diese Kontextvektoren dann, um zu entscheiden, wo er seine Aufmerksamkeit zu konzentrieren". Die Untersuchung der Aufmerksamkeitsmechanismen wurde von Luong et al.in einer Zeitung 2015 fortgesetzt. Ein globaler Ansatz auf Basis des Bahdanau-Papiers wurde versucht, sowie ein lokaler Ansatz, bei dem nur eine Teilmenge an Quellworten zu einer Zeit betrachtet wurde; der lokale Ansatz, während architektonisch komplizierter, war weniger rechnerisch teuer und einfacher zu trainieren. Es dauerte 7–10 Tage, um ein englisch-deutsches Übersetzungsmodell zu trainieren, das speziell für die Übersetzung von 1.000 Zielworten pro Sekunde konzipiert wurde; seine Genauigkeit wurde gegen die 2014 ACL Workshop on Machine Translation (WMT'14) Aufgabe für englisch-deutsche Satzpaare getestet und erzielte ein Ergebnis von 23.0 BLEU – eine 2.1 BLEU Verbesserung des vorherigen besten Ergebnisses, das durch frühere Versuche erreicht wurde, ein phrasen Sprachmodell von Buck et al. Transformatoren Während die Aufmerksamkeitsmechanismen bei der Verbesserung der Leistung wirksam waren, wenn sie bestehende konvolutionale und wiederkehrende neuronale Netzwerkarchitekturen verwendet, wurde bald entdeckt, dass performante Modelle mit Hilfe von Aufmerksamkeitsmechanismen auf sich selbst gebaut werden konnten, ohne dass ihnen etwas anderes zugrunde lag. Im Juni 2017 wurde die Transformatorarchitektur erstmals in einem von Forschern von Google Brain, Google Research und University of Toronto veröffentlichten Papier vorgestellt. Transformers sind eine Art Modell, das ausschließlich auf Aufmerksamkeitsmechanismen basiert, die Faltung und Wiederauftreten insgesamt verwerfen. Im Gegensatz zu früheren RNN-basierten Modellen können Transformatoren sequentielle Eingaben verarbeiten, ohne dass eine Berechnung auf jedem Element in Folge erfolgen muss; das bedeutet, dass sie massiv parallelisiert werden können. Auf der französischen und englischen WMT'14 konnte ein speziell ausgebildetes französisches Übersetzungsmodell mit der Transformatorarchitektur einen neuen Ein-Modell-Benchmark von 41.8 BLEU aufbauen. Seit ihrer Einführung haben Transformatoren den Einsatz in vielen NLP-Anwendungen gesehen. Generativer vortrainierter TransformerAm 11. Juni 2018 veröffentlichte OpenAI ein Papier mit dem Titel "Improving Language Understanding by Generative Pre-Training", in dem sie den Generativen vortrainierten Transformer (GPT) eingeführt haben. An dieser Stelle beaufsichtigten die am besten durchdringenden neuralen NLP-Modelle das Lernen aus großen Mengen manuell markierter Daten. Diese Abhängigkeit von überwachtem Lernen begrenzte ihre Verwendung auf Datensätzen, die nicht gut bekannt waren, zusätzlich zu machen es vergeblich teuer und zeitraubend, extrem große Modelle zu trainieren; viele Sprachen (wie Swahili oder Haitian Creole) sind schwierig zu übersetzen und zu interpretieren mit solchen Modellen aufgrund eines Mangels an verfügbaren Text für Korpus-Gebäude. Im Gegensatz dazu war der semi-supervised Ansatz von GPT zwei Etappen: eine unübertroffene generative Vorschulstufe, in der ein Ziel der Sprachmodellierung zur Festlegung von Anfangsparametern verwendet wurde, und eine überwachte diskriminative Feinabstimmungsstufe, in der diese Parameter an eine Zielaufgabe angepasst wurden. Die Verwendung einer Transformatorarchitektur, im Gegensatz zu früheren Techniken, die aufmerksamkeitserweiterte RNs, lieferte GPT mit einem strukturierteren Speicher als durch wiederkehrende Mechanismen erreicht werden konnte, was zu "robust transfer performance across diversen Aufgaben" führte. Bei der Übertragung verwenden wir aufgabenspezifische Eingabeanpassungen, die von bahnförmigen Ansätzen abgeleitet werden, die strukturierte Texteingabe als eine einzige zusammenhängende Folge von Token verarbeiten. Korpus Die ununterbrochene Vorschulung wurde mit BooksCorpus durchgeführt, einem Datensatz von über 7.000 unveröffentlichten Fiktionsbüchern aus verschiedenen Genres; während andere Modelle dieser Datensatz zum Teil gewählt wurde, weil seine langen Durchgänge kontinuierlichen Textes das Modell zur Verarbeitung von Langstreckeninformationen konditionierten. Andere verfügbare Datensätze wurden zwar größer, jedoch auf der Grundlage abgelehnt, dass sie dieser langwierigen Struktur fehlten (auf einer Satzebene gesprengt). Die ftfy-Bibliothek wurde verwendet, um den BooksCorpus-Text zu reinigen (Pünctuation und Whitespace zu standardisieren;) es wurde mit Spas Cy. Architecture GPT's Architektur selbst war ein Zwölfschicht-Decoder-only-Transformator, mit zwölf maskierten Selbstaufmerkungsköpfen, mit jeweils 64 dimensionalen Zuständen (für insgesamt 768). Anstelle eines einfachen stochastischen Gradientenabstiegs wurde der Adam-Optimierungsalgorithmus verwendet; die Lernrate wurde linear von Null über die ersten 2.000 Updates auf maximal 2,5 x 10−4 erhöht und mit einem Cosinus-Zeitplan auf 0 getempert. Wir trainieren für 100 Epochen auf Minibatches von 64 zufällig abgetasteten, zusammenhängenden Sequenzen von 512 Token. Da die Schichtnorm im gesamten Modell weitgehend verwendet wird, reichte eine einfache Gewichtsan initialisation von N(0,0.02) aus. Wir nutzten eine Bytepair-Encoding (BPE) Vokabular mit 40.000 Zusammenschlüssen [53] und Rest, Einbettung und Aufmerksamkeit Dropouts mit einer Rate von 0,1 für die Regularisierung. Wir haben auch eine in Loshchilov et al. 2017 vorgeschlagene modifizierte Version der L2-Regulierung eingesetzt, mit w = 0,01 auf allen Nicht-Bias- oder Gewinngewichten[]. Wir haben gelernte Positionseinbettungen anstelle der in der Originalarbeit vorgeschlagenen sinusförmigen Version verwendet.[...] Unbestimmte Zeit verwenden wir die Hyperparameter-Einstellungen von nicht überwachtem Vortraining wieder. Wir fügen dem Klassifikator eine Rate von 0,1 hinzu. Für die meisten Aufgaben verwenden wir eine Lernrate von 6,25e-5 und eine Chargengröße von 32. Unsere Modell-Feintunes schnell und 3 Epochen der Ausbildung war für die meisten Fälle ausreichend. Wir verwenden einen linearen Lernrate Abklingzeitplan mit Erwärmung über 0,2% der Ausbildung. λ wurde auf 0,5 gesetzt. Während die Feinabstimmung von GPT an bestimmte Aufgaben angepasst wurde, war ihr Vortraining nicht; um die verschiedenen Aufgaben zu erfüllen, wurden minimale Änderungen an seiner zugrunde liegenden aufgaben-agnostischen Modellarchitektur vorgenommen. Dennoch verbesserte sich GPT in mehreren Sprachverarbeitungsaufgaben noch auf früheren Benchmarks, um diskriminativ geschulte Modelle mit aufgabenorientierten Architekturen auf einer Reihe unterschiedlicher Aufgaben zu übertreffen. Leistung Bei den Aufgaben der natürlichen Sprache (auch als textuelles Miteinander bezeichnet) werden Modelle auf ihrer Fähigkeit ausgewertet, Sätzepaare aus verschiedenen Datensätzen zu interpretieren und die Beziehung zwischen ihnen als Mitwirkung, Widerspruch oder Neutral einzustufen". Beispiele für solche Datensätze sind QNLI (Wikipedia-Artikel) und MultiNLI (übersetzte Rede, populäre Fiktions- und Regierungsberichte, unter anderem Quellen;) zu diesen GPT erreichten jeweils eine 5,8% und 1.5% Verbesserung gegenüber früheren besten Ergebnissen. Es hat die früheren Modelle auf zwei Aufgaben im Zusammenhang mit der Fragebeantwortung und dem gemeinsamen Verständnis – um 5,7% auf RACE, ein Datensatz der schriftlichen Frage – waren Paare von mittleren und High-School-Prüfungen und um 8,9% auf dem Story Cloze Test.Eine weitere Aufgabe, semantische Ähnlichkeit (oder Paraphrase-Erkennung) beurteilt, ob ein Modell vorhersagen kann, ob zwei Sätze Paraphrasen sind, und am Quora Question Pairs (QQQP)-Datensatz verbesserte sich GPT bei früheren Best-Performing-Modellen um 4,2%. In einer Textklassifikationsaufgabe mit dem Corpus of Linguistic Acceptability (CoLA,) erreichte GPT eine Punktzahl von 45.4, gegenüber einem früheren Besten von 35.0. Schließlich erreichte GPT auf GLUE einen Multi-Task-Test mit einem Gesamtwert von 72,8 (im Vergleich zu einem vorherigen Rekord von 68,9) Scale-up GPT-2 wurde als direktes Scale-up von GPT erstellt, wobei sowohl seine Parameterzahl als auch seine Datensatzgröße um einen Faktor von 10 erhöht wurden. Beide sind nicht überwachte Transformatormodelle, die zur Texterzeugung ausgebildet sind, indem sie das nächste Wort in einer Folge von Token vorhersagen. Das GPT-2 Modell hat 1,5 Milliarden Parameter und wurde auf einem Datensatz von 8 Millionen Webseiten trainiert. Während GPT-2 auf sehr einfachen Kriterien verstärkt wurde (eine Abfolge von Wörtern in einer Textprobe zu interpretieren und das wahrscheinlichste nächste Wort vorherzusagen), produziert es vollständige Sätze und Paragraphen, indem es weiterhin zusätzliche Wörter prognostiziert, wodurch voll verständliche (und semantisch sinnvolle) Aussagen in natürlicher Sprache generiert werden. Insbesondere wurde GPT-2 auf seiner Leistung auf Aufgaben in einer Null-Schall-Einstellung bewertet. Ausbildung Da die Transformator-Architektur eine massive Parallelisierung ermöglichte, könnten die GPT-Serien-Modelle auf größerem Korporat ausgebildet werden als die bisherigen NLP-Modelle. Während das erste GPT-Modell demonstrierte, dass der Ansatz funktionsfähig war, würde GPT-2 die auftauchenden Eigenschaften von Netzwerken, die auf extrem großen Unternehmen ausgebildet sind, weiter erforschen. CommonCrawl, ein großer Korpus, der durch Web-Crawling hergestellt und zuvor in der Ausbildung NLP-Systeme verwendet wurde, wurde aufgrund seiner großen Größe betrachtet, wurde aber nach weiterer Überprüfung abgelehnt, ergab große Mengen an nicht verständlichen Inhalten. Stattdessen hat OpenAI einen neuen Korpus entwickelt, der als WebText bekannt ist; anstatt Inhalte aus dem World Wide Web zu schrotten, wurde WebText durch Schrotten von nur Seiten erzeugt, die von Reddit-Posts, die vor Dezember 2017 mindestens drei Upvotes erhalten hatten, verbunden waren. Anschließend wurde der Korpus gereinigt; HTML-Dokumente wurden in Klartext parsiert, doppelte Seiten eliminiert und Wikipedia-Seiten entfernt (da ihre Anwesenheit in vielen anderen Datensätzen zu Überbelegungen führen konnte). Während die Kosten für die Ausbildung GPT-2 bekanntermaßen 256 $ pro Stunde gewesen sind, ist die Höhe der Stunden, die sie für die vollständige Ausbildung benötigt, nicht bekannt; daher können die Gesamtbildungskosten nicht genau geschätzt werden. Vergleichbare Großsprachenmodelle mit Transformatorarchitekturen haben jedoch ihre Kosten genauer dokumentiert; die Trainingsprozesse für BERT und XLNet verbrauchten jeweils $6,912 und $245.000 Ressourcen. Performance Durch die Weite seines Datensatzes und die Weite seines Ansatzes konnte GPT-2 eine Vielzahl von Aufgaben jenseits der einfachen Texterzeugung ausführen: Fragen beantworten, zusammenfassen und sogar Übersetzung zwischen Sprachen in einer Vielzahl spezifischer Domänen, ohne in etwas über die Vorhersage des nächsten Wortes in einer Sequenz hinaus unterrichtet zu werden. Ein Beispiel für das allgemeine Lernen ist die Fähigkeit von GPT-2, maschinelle Übersetzung zwischen Französisch und Englisch durchzuführen, für die die Leistung von GPT-2 mit Hilfe von WMT-14 Übersetzungsaufgaben bewertet wurde. Die Trainingskorpus von GPT-2 enthielten praktisch keinen französischen Text; nicht-englischer Text wurde bewusst entfernt, während der Datensatz vor dem Training gereinigt wurde, und folglich waren nur 10 MB Franzosen der restlichen 40.000MB für das Modell zur Verfügung, um von (meist aus Fremdsprachen-Zitaten in englischen Posts und Artikeln) zu lernen. Trotzdem erreichte GPT-2 5 BLEU auf dem WMT-14 English-to-Französischen Testsatz (leicht unter der Punktzahl einer Übersetzung über Word-for-word-Substitution). Es konnte auch mehrere zeitgemäße (2017) unübertroffene maschinelle Übersetzungsbasen auf dem Französisch-Englisch-Test-Set übertreffen, wo GPT-2 11.5 BLEU erreichte. Dies blieb unter dem höchsten, zeitgemäßen, unsupervised Ansatz (2019), der 33.5 BLEU erreicht hatte. Andere Modelle nutzten jedoch große Mengen französischer Texte, um diese Ergebnisse zu erzielen; GPT-2 wurde geschätzt, dass ein einsprachiger französischer Korpus etwa 1/500 die Größe vergleichbarer Ansätze verwendet haben. Die Veröffentlichung GPT-2 wurde am 14. Februar 2019 erstmals bekannt gegeben. Ein Februar 2019 Artikel in The Verge von James Vincent sagte, dass, während ["die] schreiben es produziert ist in der Regel leicht erkennbar als nicht-menschlich", es blieb "eine der aufregendsten Beispiele noch" der Sprachgenerierungsprogramme: Geben Sie ihm eine gefälschte Überschrift, und es wird den Rest des Artikels schreiben, komplett mit gefälschten Zitaten und Statistiken. Füttern Sie es die erste Zeile einer kurzen Geschichte, und es wird Ihnen sagen, was mit Ihrem Charakter als nächstes passiert. Es kann sogar Fanfiction schreiben, gegeben die richtige Aufforderung. Der Guardian beschrieb diese Ausgabe als "plausible Zeitungsprose", sagte Kelsey Piper von Vox: "Eine der coolsten KI-Systeme, die ich je gesehen habe, könnte auch derjenige sein, der mich aus meinem Job rausschmeißen wird". Die Flexibilität von GPT-2 wurde von The Verge als beeindruckend bezeichnet; speziell seine Fähigkeit, Text zwischen Sprachen zu übersetzen, lange Artikel zusammenzufassen und trivia Fragen zu beantworten. Eine Studie der Universität Amsterdam mit einem modifizierten Turing-Test ergab, dass die Teilnehmer zumindest in einigen Szenarien nicht in der Lage waren, Gedichte von GPT-2 von den Menschen zu unterscheiden. Einschränkungen und teilweise Freigabe Während frühere OpenAI-Modelle der Öffentlichkeit sofort zur Verfügung gestellt worden waren, lehnte OpenAI zunächst ab, eine öffentliche Veröffentlichung des Quellcodes von GPT-2 zu machen, wenn es im Februar angekündigt wurde, indem es das Risiko einer schädlichen Nutzung betonte; begrenzter Zugriff auf das Modell (d.h. eine Schnittstelle, die Eingabe und Ausgabe erlaubte, nicht der Quellcode selbst) wurde für ausgewählte Presseausgänge bei der Ankündigung erlaubt. Eine allgemein zitierte Rechtfertigung war, dass, da generierter Text in der Regel völlig neu war, es von Spammern verwendet werden konnte, um automatisierte Filter zu umgehen; OpenAI zeigte eine Version von GPT-2 fein abgestimmt auf " unendlich positive – oder negative – Bewertungen von Produkten generieren". Ein anderer war, dass GPT-2 verwendet werden konnte, um Text zu erzeugen, der obszön oder rassistisch war. Forscher wie Jeremy Howard warnten davor, "die Technologie, um ganz zu füllen Twitter, E-Mail und das Web mit vernünftig-sündenden, kontextgerechten Prosa, die alle andere Sprache ertrinken und unmöglich zu filtern sein würde". Das Allen Institut für Künstliche Intelligenz, in Reaktion auf GPT-2, kündigte ein Tool, um "neue gefälschte Nachrichten zu erkennen. Die Stellungnahme wurde jedoch geteilt. Ein Februar 2019 Artikel in The Verge argumentierte, dass die Bedrohung von GPT-2 übertrieben worden sei; Anima Anandkumar, Professor an Caltech und Leiter der maschinellen Lernforschung in Nvidia, sagte, dass es keinen Beweis dafür gab, dass GPT-2 die Fähigkeit hatte, die Bedrohungen von OpenAI zu posieren, und dass das, was sie taten, das "gegenüber offen", charakterisieren ihre Ablehnung, das komplette Modell als "malicious BS". Der Gradient veröffentlichte einen offenen Brief an OpenAI und forderte, dass sie das Modell öffentlich freigeben, die Bedrohung der Textgeneration AI mit der Bedrohung der Druckmaschine vergleichen und Photoshop als Beispiel für "eine Technologie, die die moderne Gesellschaft trotz ihres Chaospotenzials nicht zerstört hat (danklich) nicht zerstört hat" Dreißig Jahre später ist die Gesellschaft relativ unbesäumt, obwohl Photoshop einfach genug ist, um Schüler zu benutzen und allgegenwärtig genug, um ihr eigenes Verb zu beauftragen. Warum? Genau, weil jeder von Photoshop weiß. 774M Veröffentlichung Während Open KI veröffentlichte nicht das voll ausgebildete Modell oder die von ihm geschulte Gesellschaft, Beschreibung ihrer Methoden in früheren Publikationen (und die freie Verfügbarkeit der zugrunde liegenden Technologie) ermöglichte es GPT-2 von anderen als freie Software zu replizieren; eine solche Replikation, OpenGPT-2, wurde im August 2019 veröffentlicht, in Verbindung mit einer frei lizenzierten Version von WebText genannt OpenWebText. Die Cloud-Compute-Kosten für OpenGPT-2 wurden als ungefähr $50.000 angegeben. Am 20. August 2019, Open AI veröffentlichte eine Teilversion von GPT-2, mit 774 Millionen Parametern (etwa die Hälfte der Größe des vollen 1,5 Milliarden Parametermodells.) Volle 1.5B-Freigabe Erste Bedenken, dass GPT-2 sich an weit verbreitete Mißbrauch verleihen würde nicht vorbeikommen; Die Verge sagte, dass "Es gibt Gründe, um skeptisch zu sein über Behauptungen, dass KI-Technologie an einer Art von "infopocalypse" zu sein. Für einen Anfang haben wir bereits Programme, die plausible Texte zu einem hohen Volumen für kleine Kosten generieren können: Menschen."Am November 2019, Open AI sagte, dass sie "keine starken Beweise für Missbrauch bisher" und die Vollversion mit 1,5 Milliarden Parametern wurde am 5. November 2019 veröffentlicht. Einschränkungen Während die Fähigkeit von GPT-2, plausible Passagen von natürlichen Sprachtexten zu erzeugen, allgemein positiv bemerkt wurde, wurden auch seine Mängel bemerkt, vor allem bei der Generierung von Texten länger als ein paar Absätze; Vox sagte "die Prosa ist ziemlich grob, es gibt die gelegentliche Nicht-Sequitur, und die Artikel erhalten weniger kohärent, je länger sie erhalten". Die Verge bemerkte ähnlich, dass längere Proben von GPT-2 Schreiben tendenziell "aus dem Thema" und mangelnde Gesamtkohärenz; Das Register behauptete, dass "eine menschliche Lektüre es sollte, nach einer kurzen Weile, realisieren etwas ist nach oben", und bemerkte, dass "GPT-2 beantwortet nicht Fragen sowie andere Systeme, die auf Algorithmen verlassen, um Informationen zu extrahieren und abzurufen. "GPT-2 ist ressourcenintensiv; die Vollversion des Modells ist größer als fünf Gigabyte, wodurch es schwierig ist, lokal in Anwendungen einzubetten und verbraucht große Mengen an RAM. Darüber hinaus kann eine einzelne Vorhersage "eine CPU bei 100% Auslastung für mehrere Minuten besetzen", und auch bei der GPU-Verarbeitung, "eine einzige Vorhersage kann Sekunden dauern". Um diese Probleme zu lindern, hat das Unternehmen HuggingFace DistilGPT2 mit Wissensdestillation geschaffen, um ein kleineres Modell zu produzieren, das "ein paar Punkte niedriger auf einige Qualitäts-Benchmarks" bringt, aber "33% kleiner und doppelt so schnell". Implementierungen und anschließende Forschung Mögliche Anwendungen von GPT-2, die von Journalisten beschrieben wurden, waren die Unterstützung von Menschen in Textform wie Nachrichtenartikel. Auch vor der Veröffentlichung der Vollversion wurde GPT-2 für eine Vielzahl von Anwendungen und Dienstleistungen sowie für Unterhaltung verwendet. Im Juni 2019 wurde ein Subreddit mit dem Namen r/SubSimulatorGPT2 erstellt, in dem eine Vielzahl von GPT-2 Instanzen, die auf verschiedenen Subreddits trainiert wurden, Posts erstellt und auf die Kommentare der anderen geantwortet wurden, eine Situation, in der man beobachten konnte "eine KI-Personifizierung von r/Bitcoin argumentieren mit dem maschinellen Lernen-erzeugten Geist von r/ShiplettyFoodPorn;" bis Juli dieses Jahres, ein GPT Im Jahr 2019 wurde AI Dungeon gestartet, der GPT-2 verwendet, um dynamische Textabenteuer basierend auf Benutzereingabe zu erzeugen. AI Dungeon bietet nun Zugang zur größten Veröffentlichung der GPT-3 API als optionales kostenpflichtiges Upgrade, die kostenlose Version der Website nutzt die 2. größte Veröffentlichung von GPT-3.Latitude, das Unternehmen um AI Dungeon gebildet, erhöhte $3,3 Millionen in der Saatgutfinanzierung im Jahr 2021. Mehrere Websites führen interaktive Demonstrationen verschiedener Instanzen von GPT-2 und anderen Transformatorenmodellen. Im Februar 2021 kündigte ein Krisenzentrum für beunruhigte Teenager an, dass sie mit einem GPT-2-derived Chatbot beginnen würden, um den Beratern zu helfen, indem sie Gespräche mit simulierten Teenagern führen (diese Verwendung war rein für interne Zwecke, und es war nicht mit GPT-2 kommunizieren mit den Teenagern selbst.) = Referenzen ==