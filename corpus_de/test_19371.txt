Gated recurrent Units (GRUs) sind ein Mechanismus für die Wiederauffüllung der im Jahr 2014 von Kyunghyun Cho et al eingeführten Neuralnetze. Die GRU ist wie ein langfristiges Gedächtnis (LSTM) mit einem vergessenen Tor, hat aber weniger Parameter als die LSTM, da es keinen Output-Gates gibt. GRUs Leistung bei bestimmten Aufgaben von polyphonischen Musikmodellen, Redesignalmodellierung und natürlichen Sprachverarbeitung wurde als ähnlich angesehen wie der von LSTM. GRUs hat gezeigt, dass bei bestimmten kleineren und weniger häufigen Datensätzen bessere Ergebnisse erzielt werden. Architektur In verschiedenen Kombinationen gibt es mehrere Abweichungen in der gesamten gestreckten Einheit, wobei der frühere versteckte Staat und die Unparteilichkeit in verschiedenen Kombinationen genutzt werden, und eine vereinfachte Form namens minimale gesteuerte Einheit. Der Betreiber   Memedisplaystyle \odot } verdichtet das Hadamard-Produkt im folgenden. Voll gesteuerte Einheit zunächst, für t = 0 Memestyle t=0} , ist der Outputvektor h 0 = 0 displaystyle h_{0}=0 . z. t = σ g ( W z x t + U z h t − 1 + b) t = ) g ( W x t − 1 h · h · h · h ) h } h =  W h  U h ( W x t  U t  U t  z t  1 t  1 t  1 t  1 t  1 t  1  1 t  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 h  1 h  1  1  1  1  1  1  1 h  1  1  1 h  1 h  1  1 h  1  1  1  1  1 h  1  1  1  1  1  1  1  1  1  1  1  1  1 h  1  1  1  1  1  1  1  1 g}(W_{z}x_{t}+U_{z}h_{t-1}+b_{z})\\r_{t} &=\sigma g}(W_{r}x_{t}+U_{r}h_{t-1}+b_{r})\\{\hat h__{t} h}(W_{h} h_{t-1})+b_{h} h_{t-1}+z_{t}\odot 7.8hat h__{t}\end{aligned Variablen x t {\displaystyle x_{t}: Input Vektor h t Memestyle h_{t} : Output Vektor h ^ t Memedisplaystyle Memehat h__{t : Aktivierungsvektor Z t WELLdisplaystyle z_{t}: Aktualisierung der Torf t Memedisplaystyle r_{t}: Neuankömmlinge W Memedisplaystyle W} , U {\displaystyle U} und b {\displaystyle b}: Parameter matrices und Vektoraktivierungsfunktionen  g g {\displaystyle \sigma {_g}: Das Original ist eine sigmoide Funktion. . h Memedisplaystyle \phi {_h}: Das Original ist ein Hyperbolic tangent. Alternative Aktivierungsfunktionen sind möglich, sofern . g ( x )  [ [ 0 , 1 ] Memestyle \sigma {_g}(x)\in [0,1] . Alternat Formen können durch eine Änderung der z t {\displaystyle z_{t} und r t {\displaystyle r_{t} geschaffen werden. Typ 1, jedes Tor hängt nur vom vorherigen versteckten Staat und der Unparteilichkeit ab. z t = ) g (U z h t − 1 + b z ) r t =  g g (U r h t − 1 + b r ) KINGstyle beginnt {aligned}z_{t} &=\sigma g}(U_{z}h_{t-1}+b_{z} g}(U_{r}h_{t-1}+b_{r})\\\end Typ 2, jedes Tor hängt nur vom vorherigen versteckten Staat ab. z t =  g g (U z h - 1 ) r t = ) g (U r h  - 1 ) Memestyle beginnt {aligned}z_{t} &=\sigma g}(U_{z}h_{t-1})\\r_{t} &=\sigma g}(U_{r}h_{t-1})\\\end{aligneded Typ 3, jedes Tor wird nur mit der Vorauswahl berechnet. z = ) g (b z ) r t =  g g (b r ) {\displaystyle beginnt {aligned}z_{t} &=\sigma g}(b_z})\\r_{t} &=\sigma g}(b_{r})\\\end Minimalisierte Einheit Die minimale gestreckte Einheit ähnelt der vollständig besetzten Einheit, es sei denn, der aktualisierte und neu angesiedelte Torf wird in ein vergessenes Tor zusammengefasst. Dies bedeutet auch, dass die Formel für den Outputs geändert werden muss: f t = σ g ( W x t + U f h t − 1 + b f ) h ^ t = φ h ( W h x t + U h ( f t  t h  - 1 ) + h ) h = ( 1 − f )  1 h  1 h ^ h ^ h ^ h {\ t ^ t displaystyle beginnt g}(W_{f}+U_{f}h_{t-1}+b_{f})\\{\hat h__{t} h}(W_{h} h_{t-1})+b_{h} h_{t-1}+f_{t}\odot 7.8hat h__{t}\end{aligned Variablen x t {\displaystyle x_{t}: Input Vektor h t Memestyle h_{t} : Output Vektor h ^ t Memedisplaystyle Memehat h__{t : Aktivierungsvektor f t WELLdisplaystyle f_{t}: vergessene Vektor W Memedisplaystyle W} , U {\displaystyle U} und b {\displaystyle b}: Parameter matrices und Vektor = #