WellenNet ist ein tiefgreifendes Neuralnetz für die Produktion von Rohstoffen. Es wurde von Forschern in London gegründet, die sich auf künstliche Intelligenz in DeepMind befinden. Die im September 2016 in einem Papier dargelegte Technik ist in der Lage, relativ realistische humanartige Stimmen durch direkte Modellierung von Wellenforms mit einer neuralen Netzmethode zu erzeugen, die mit Aufzeichnungen einer echten Rede ausgestattet ist. Tests mit US-amerikanischen Englisch und Mandarin berichteten, dass das System die besten bestehenden Text-to-speech (TTS)-Systeme von Google ausschließt, obwohl seine Text-to-speech-Synthese bis 2016 noch weniger überzeugend war als die tatsächliche menschliche Rede. die Fähigkeit von Wellenforms, Rohstoffe zu erzeugen, bedeutet, dass es jede Art von Audio, einschließlich Musik,modellieren kann. Konsequente Rede von Text ist eine zunehmend gemeinsame Aufgabe, da Software wie Apples Siri, Microsoft Cortana, Amazon Alexa und Google-Assistent bekannt sind. Die meisten dieser Systeme verwenden eine Variante einer Technik, die verzinste Tonfragmente zusammenführt, um erkennbare Geräusche und Worte zu bilden. Am häufigsten handelt es sich um Vermittlungsverfahren. Es besteht aus einer großen Bibliothek von Sprachfragmenten, die von einem einzigen Redner erfasst wird, der dann in der Lage ist, vollständige Wörter und Geräusche zu produzieren. Das Ergebnis ist unnatürlicher, ungeheuer und Ton. Die Abhängigkeit von einer registrierten Bibliothek macht es auch schwierig, die Stimme zu ändern oder zu ändern. Eine andere Technik, die als parametrisches TTS bekannt ist, verwendet mathematische Modelle, um Geräusche wieder herzustellen, die dann in Wörter und Sätze montiert werden. Informationen, die erforderlich sind, um die Geräusche zu erzeugen, werden in den Parametern des Modells gespeichert. Die Merkmale der Outputrede werden über die Beiträge zum Modell kontrolliert, während die Rede in der Regel mit einem Sprachsyntheseer erstellt wird, der als vocoder bekannt ist. Dies kann auch zu unnatürlichem Tonnieren führen. Design und laufendes Forschungs-Hintergrund-WellNet ist eine Art von Feedforward-Neuralnetz, bekannt als tiefes konvolutionales Neuralnetz (CNN). In WelleNet nimmt die CNN ein Rohsignal als Input an und stellt eine erste Probe zu einem Zeitpunkt zusammen. Mit einer Softmax (d. h. kategorisch) Verteilung eines Signalwerts, der mit μ-Recht verschlüsselt wird, um Transformation zu kompensieren und auf 256 mögliche Werte zu beziffern. Konzept und Ergebnisse Laut dem ursprünglichen September 2016 DeepMind Research Paper WelleNet: Ein Generatives Modell für Roh Audio wurde das Netz in englischer und chinesischer Sprache mit echten Reden gefüttert. Wie diese über das Netz übertragen werden, lernen sie eine Reihe von Regeln, um zu beschreiben, wie sich die Audio-Wellform im Laufe der Zeit entwickelt. Das ausgebildete Netzwerk kann dann verwendet werden, um neue Sprach-ähnliche Wellenforms mit 16.000 Proben pro Sekunde zu schaffen. Diese Wellenformen umfassen realistische Atem- und Lip-Seques, entsprechen aber keiner Sprache. WellenNet ist in der Lage, unterschiedliche Stimmen mit Schwerpunkt und Ton des Inputs, der mit dem Output korreliert, genau zu formulieren. Wenn es beispielsweise mit Deutsch ausgebildet wird, produziert es deutsche Reden. Die Fähigkeit bedeutet auch, dass, wenn das WellenNet andere Inputs – wie Musik – seine Produktion Musik sein wird. DeepMind zeigte zum Zeitpunkt der Freisetzung, dass das Wellennetz Wellenforms hervorbringt, die wie klassische Musik gesund sind. Inhalt (voice) Swapping Laut dem Papier Disentangated Sequential Autoencoder vom Juni 2018 hat DeepMind erfolgreich das Wellennetz für Audio- und Sprachtelefon "Inhalte Swapping:" das Netzwerk kann die Stimme auf einer Audioaufnahme für eine andere, vorbeugende Stimme unter Beibehaltung des Textes und anderer Merkmale aus der Originalaufzeichnung tauschen. "Wir testen auch auf Audio-Seque-Daten. Unsere unentangierte Vertretung ermöglicht es uns, die Sprecheridentität ineinander zu verwandeln und gleichzeitig auf den Inhalt der Rede zu achten." 5) "Für Audio ermöglicht dies, einen männlichen Sprecher in einen weiblichen Sprecher zu verwandeln und umgekehrt (["]p 1) In Übereinstimmung mit dem Papier ist ein zweistelliger Mindestbetrag von Stunden (c. 50 Stunden) bestehender Sprachaufzeichnungen sowohl der Quelle als auch der Zielsprache erforderlich, um in das Programm einfließen zu lassen, um ihre individuellen Merkmale zu lernen, bevor es in der Lage ist, die Umrechnung von einer Stimme in eine andere zu erfüllen. Die Autoren weisen darauf hin, dass ["a]n Vorteil des Modells darin besteht, dass es dynamisch von statischen Merkmalen (["]p 8), d. h. von WellenNet ist in der Lage, zwischen dem gesprochenen Text und den Formen der Lieferung (Modulation, Geschwindigkeit, Spiel, Stimmung usw.) während der Umrechnung von einer Stimme auf eine andere Seite und den grundlegenden Merkmalen der beiden Quelle und der Zielgruppen zu unterscheiden, die es benötigt, um auf der anderen Seite zu werden. Folgepapier vom Januar 2019 Unüberwindete Sprachvertretung unter Verwendung von FlashNet Autoencoders legt eine Methode dar, um die ordnungsgemäße automatische Anerkennung und Diskriminierung zwischen dynamischen und statischen Merkmalen für "Inhaltewaping" – insbesondere Swapping-Sprachen auf bestehenden Audioaufzeichnungen – erfolgreich zu verbessern, um sie zuverlässiger zu machen. Ein weiteres Follow-up-Papier, Probeneffektive Anpassung Text-to-Speech, vom September 2018 (letzte Überarbeitung im Januar 2019), stellt fest, dass DeepMind den Mindestbetrag der Echtzeitaufzeichnungen erfolgreich verringert hat, die erforderlich sind, um eine bestehende Stimme über das Funknetz zu testen, um ein paar Minuten Audiodaten zu erfassen und gleichzeitig hochwertige Ergebnisse zu erhalten. Seine Fähigkeit, Klonen zu blockieren, hat ethische Bedenken in Bezug auf die Fähigkeit von WellenNet, die Stimme von Lebens- und toten Personen zu verwechseln. Laut einem Artikel der BBC 2016 wollen Unternehmen, die auf ähnlichen Sprachabschlußtechnologien (wie beispielsweise Adobe Voco) arbeiten, den Menschen unkontrollierbar machen, um Fälschungen zu verhindern, während sie zu gewährleisten, dass beispielsweise die Bedürfnisse von Unterhaltungs-Industriezwecken von einer weitaus geringeren Komplexität und Verwendung unterschiedlicher Methoden als erforderlich sind, um die für die Verbreitung von Methoden und elektronischen ID-Geräten erforderlichen Methoden zu kollidieren, damit natürliche Stimmen und Stimmen, die für Unterhaltungszwecke geklont werden, immer noch von technologischer Analyse abgesehen werden können. Anwendungen DeepMind sagte zum Zeitpunkt seiner Freisetzung, dass das WellenNet zu viel Rechenleistung benötigt, um in echten Weltanwendungen zu verwenden. Google kündigte im Oktober 2017 eine 1000-fache Leistungsverbesserung sowie eine bessere Sprachqualität an. WelleNet wurde dann verwendet, um Google-Assistenten für US-amerikanische und japanische Internet-Plattformen zu generieren. Im November 2017 veröffentlichten DeepMind-Forscher ein Forschungspapier, in dem eine vorgeschlagene Methode der „Erzeugung hochwertiger Sprachproben in mehr als 20-mal schneller als in Echtzeit“ erläutert wurde. Im Rahmen der jährlichen I/O-Projektkonferenz im Mai 2018 wurde angekündigt, dass neue Google-Assistent-Sprachen verfügbar waren und von WellenNet möglich gemacht wurden; WellenNet hat die Anzahl der Tonaufzeichnungen erheblich gesenkt, die erforderlich waren, um ein Sprachmodell zu erstellen, indem es das Roh-Audio-Audio der Sprach-Akteur-Probenmodellen modelliert. Referenzen Externe Links WellenNet: Ein Generatives Modell für Roh Audio