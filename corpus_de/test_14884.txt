Bei der Signalverarbeitung ist die unabhängige Komponentenanalyse (ICA) ein rechnerisches Verfahren zum Trennen eines Multivariatsignals in Additiv-Teilkomponenten. Dies geschieht durch die Annahme, dass die Teilkomponenten nicht-Gaussische Signale sind und dass sie statistisch unabhängig voneinander sind. ICA ist ein besonderer Fall der blinden Quellentrennung. Eine häufige Beispielanwendung ist das "Cocktail-Party-Problem" des Einhörens auf einer Person in einem lauten Raum. Einführung Unabhängige Komponentenanalyse versucht, ein Multivariat-Signal in unabhängige nicht-Gaussische Signale zu zersetzen. Als Beispiel ist Schall in der Regel ein Signal, das aus der numerischen Addition, zu jedem Zeitpunkt t, von Signalen aus mehreren Quellen zusammengesetzt ist. Die Frage ist dann, ob es möglich ist, diese beitragenden Quellen von dem beobachteten Gesamtsignal zu trennen. Wenn die statistische Unabhängigkeitsannahme korrekt ist, ergibt die blinde ICA-Trennung eines gemischten Signals sehr gute Ergebnisse. Es wird auch für Signale verwendet, die nicht durch Mischen zu Analysezwecken erzeugt werden sollen. Eine einfache Anwendung von ICA ist das "Cocktail-Party-Problem", bei dem die zugrunde liegenden Sprachsignale von einer Stichprobendaten, bestehend aus Personen, die gleichzeitig in einem Raum reden, getrennt werden. Üblicherweise wird das Problem dadurch vereinfacht, dass keine Zeitverzögerungen oder Echos angenommen werden. Beachten Sie, dass ein gefiltertes und verzögertes Signal eine Kopie einer abhängigen Komponente ist und somit die statistische Unabhängigkeitsannahme nicht verletzt wird. Mischgewichte zum Konstruieren der M \{textstyle M} beobachteten Signale der N \{textstyle N} Komponenten können in einer M × N \{textstyle M\times N} Matrix platziert werden. Wichtig ist, dass bei Vorhandensein von N \{textstyle N}-Quellen mindestens N \{textstyle N}-Beobachtungen (z.B. Mikrofone, wenn das beobachtete Signal Audio ist) zur Rückgewinnung der ursprünglichen Signale benötigt werden. Bei gleicher Anzahl von Beobachtungen und Quellsignalen ist die Mischmatrix quadratisch (M = N \{textstyle M=N}.)Andere Fälle von unterbestimmten (M < N \{textstyle M<N}) und überbestimmten (M > N \{textstyle M>N}) wurden untersucht. Dass die ICA-Trennung von Mischsignalen sehr gute Ergebnisse liefert, basiert auf zwei Annahmen und drei Wirkungen von Mischquellensignalen. Zwei Annahmen: Die Quellsignale sind unabhängig voneinander. Die Werte in jedem Quellsignal weisen nicht-Gaussische Verteilungen auf. Drei Effekte des Mischens von Quellsignalen: Unabhängigkeit: Die Quellsignale sind gemäß Annahme 1 unabhängig; ihre Signalmischungen sind jedoch nicht. Dies liegt daran, dass die Signalmischungen dieselben Quellsignale teilen. Normalität: Nach der Zentralen Grenze Theorem, die Verteilung einer Summe von unabhängigen Zufallsvarianzen mit endlicher Varianz tendiert zu einer gaussischen Verteilung. Selten hat eine Summe von zwei unabhängigen Zufallsvariablen in der Regel eine der Gaussian nähere Verteilung als jede der beiden ursprünglichen Variablen. Hier betrachten wir den Wert jedes Signals als Zufallsgröße. Komplexität: Die zeitliche Komplexität jedes Signalgemisches ist größer als die seines einfachsten Bestandteilsquellensignals. Diese Grundsätze tragen zur grundlegenden Einrichtung von ICA bei. Sind die aus einem Gemischsatz gewonnenen Signale unabhängig und haben nicht-Gaussische Histogramme oder haben eine geringe Komplexität, so müssen sie Quellsignale sein. Die Definition der KomponentenunabhängigkeitICA findet die unabhängigen Komponenten (auch Faktoren, latente Variablen oder Quellen genannt) durch Maximierung der statistischen Unabhängigkeit der geschätzten Komponenten. Wir können eine von vielen Möglichkeiten wählen, um einen Proxy für die Unabhängigkeit zu definieren, und diese Wahl regelt die Form des ICA-Algorithmus. Die zwei größten Definitionen der Unabhängigkeit für ICA sind Minimierung der gegenseitigen Informationen Maximierung der Nicht-GaussianitätDie Minimierung-of-Mutual Information (MMI) Familie der ICA-Algorithmen verwendet Maßnahmen wie Kullback-Leibler Divergence und maximale Entropie. Die nicht-Gaussianität Familie der ICA Algorithmen, motiviert durch die zentrale Grenze Theorem, verwendet Kurtosis und Negentropie. Typische Algorithmen für die ICA verwenden Zentrierung (subtract the mean to create a zero mean signal,) Aufhellung (in der Regel mit der Eigenwertzersetzung) und Dimensionalitätsreduzierung als Vorverarbeitungsschritte, um die Komplexität des Problems für den eigentlichen iterativen Algorithmus zu vereinfachen und zu reduzieren. Durch die Hauptkomponentenanalyse oder die Einzigartigkeit des Wertes kann eine Aufhellung und Dimensionsreduktion erreicht werden. Das Aufhellen sorgt dafür, dass alle Abmessungen vor Ablauf des Algorithmus a priori behandelt werden. Bekannte Algorithmen für ICA umfassen u.a. Infomax, FastICA, JADE und kernelunabhängige Komponentenanalyse. Im allgemeinen kann ICA die tatsächliche Anzahl von Quellsignalen, eine eindeutig korrekte Reihenfolge der Quellsignale sowie die richtige Skalierung (einschließlich Vorzeichen) der Quellsignale nicht identifizieren. ICA ist wichtig für die blinde Signaltrennung und hat viele praktische Anwendungen. Es ist eng mit (oder sogar einem speziellen Fall) der Suche nach einem Faktorcode der Daten, d.h. einer neuen vektorbewerteten Darstellung jedes Datenvektors verbunden, so dass er durch den resultierenden Codevektor (lossfreie Codierung) eindeutig kodiert wird, aber die Codekomponenten statistisch unabhängig sind. Mathematische Definitionen Lineare unabhängige Komponentenanalyse kann in geräuschlose und laute Fälle unterteilt werden, wo geräuschlose ICA ein besonderer Fall von lauten ICA ist. Nichtlineare ICA sollte als separater Fall betrachtet werden. Allgemeine Definition Die Daten werden durch den beobachteten Zufallsvektor x = ( x 1 , ..., x m ) dargestellt. T \{displaystyle \{boldsymbol x}=(x_{1},\ldots ,x_{m})^{T und die versteckten Komponenten als Zufallsvektor s = (s 1 , ..., s n ) T . \{displaystyle \{boldsymbol s}=(s_{1},\ldots ,s_{n})^{T. Die Aufgabe besteht darin, die beobachteten Daten x , \{displaystyle \{bold {x} unter Verwendung einer linearen statischen Transformation W \{displaystyle \{boldsymbol {W} als s = W x , \{displaystyle \{bold s}={\boldsymbol W}{\boldsymbol {x} in einen Vektor von maximal unabhängigen Komponenten s \{display} zu transformieren. Generatives Modell Lineare geräuschlose ICADie Komponenten x i \{displaystyle x_{i} des beobachteten Zufallsvektors x = ( x 1 , ..., x m ) T \{displaystyle \{boldsymbol x}=(x_{1},\ldots ,x_{m})^{T werden als Summe der unabhängigen Komponenten s k \{displaystyle s_{k}, k = 1 , ..., n \{displaystyle k=1,\ldots ,n} : x i= a i ⋯, 1 s 1 + x_{i}=a_{i,1}s_{1}+\cdots a_{i,k}s_{k}+\cdots +a_{i,n}s_{n gewichtet durch die Mischgewichte a i, k \{displaystyle a_{i,k} . Das gleiche generative Modell kann in Vektorform wie x = Σ k = geschrieben werden 1 n s k a k \ n \ 'displaystyle \{boldsymbol}=\sum ^{n}{n}{k}{\boldsymbol a}_{k , wobei der beobachtete Zufallsvektor x \{displaystyle \{boldsymbol {x} durch die Basisvektoren a k = (a 1 , k , ..., a m , k ) dargestellt ist {displaystyle \{boldsymbol a}_{k}=({\boldsymbol a}_{1,k},\ldots ,\{boldsymbol a}_{m,k})^{T .Die Basisvektoren a k{displaystyle \{bold a}_{k die Spalten der Mischmatrix A = (a 1 , ... T \{displaystyle \{boldsymbol s}=(s_{1},\ldots ,s_{n})^{T .Angesichts des Modells und der Realisierungen (Proben) x 1 , ..., x N \{displaystyle \{boldsymbol x}_{1},\ldots ,\{boldsymbol x}_{ N des Zufallsvektors x \{displaystyle \{boldsymbol {x}, die Aufgabe ist, sowohl die Mischmatrix abzuschätzen A \{displaystyle \{boldsymbol {A} und die Quellen s \{displaystyle \{boldsymbol {s} . Dies geschieht durch adaptive Berechnung des w \{displaystyle \{boldsymbol {w}-Vektoren und Einrichtung einer Kostenfunktion, die entweder die Nicht-gaussianität des berechneten s k = w T x \{displaystyle s_{k}={\boldsymbol w}{T}{\boldsymbol {x} max. In einigen Fällen kann in der Kostenfunktion a priori Kenntnis der Wahrscheinlichkeitsverteilungen der Quellen verwendet werden. Die ursprünglichen Quellen s \{displaystyle \{boldsymbol {s} können durch Multiplikation der beobachteten Signale x \{displaystyle \{boldsymbol {x} mit der Inverse der Mischmatrix W = A - 1 \{displaystyle \{boldsymbol W}={boldsymbol A}}^{-1 , auch als ungemischte Matrix bekannt, wiederhergestellt werden. Dabei wird angenommen, dass die Mischmatrix quadratisch ist (n = m \{displaystyle n=m}). Ist die Anzahl der Basisvektoren größer als die Dimensionalität der beobachteten Vektoren, n > m \{displaystyle n>m}, so ist die Aufgabe übervollständig, ist aber mit dem Pseudo-Inverse noch lösbar. Lineare laute ICA Mit der zusätzlichen Annahme von Null-Mean und unkorreliertem Gaussian-Geräusch n ∼ N ( 0 , diag ċ ( Σ ) ) \{displaystyle n\sim N(0,\operatorname {diag} \(Sigma ))} nimmt das ICA-Modell das Formular x = A s + n \{displaystyle \{{boldsymbol *}}={\boldsymbol A{\boldsymbol {s}+n . Nichtlineare ICA Die Vermischung der Quellen muss nicht linear sein. Mit einer nichtlinearen Mischfunktion f ( ⋅ | θ ) \{displaystyle f(\cdot |\theta )} mit Parametern θ \{displaystyle \theta } ist das nichtlineare ICA-Modell x = f ( s | θ ) + n \{displaystyle x=f(s|\theta +)n} . Kennung Die unabhängigen Komponenten sind bis zu einer Permutation und Skalierung der Quellen identifizierbar. Diese Kennung erfordert: Bei den meisten Quellen s k \{displaystyle s_{k} ist Gaussian, Die Anzahl der beobachteten Mischungen, m \{displaystyle m}, muss mindestens so groß sein wie die Anzahl der geschätzten Komponenten n \{displaystyle n}: m ≥ n \{displaystyle m\geq n} . Es ist gleichwertig zu sagen, dass die Mischmatrix A \{displaystyle \{boldsymbol {A} für ihre Inversität von vollem Rang sein muss. Die binäre ICAA-Sondervariante von ICA ist binäre ICA, bei der sowohl Signalquellen als auch Monitore binär vorliegen und Beobachtungen von Monitoren disjunktive Gemische binärer unabhängiger Quellen sind. Das Problem wurde gezeigt, dass Anwendungen in vielen Bereichen wie medizinische Diagnose, Multi-Cluster-Beauftragung, Netzwerktomographie und Internet-Ressourcen-Management. Lassen Sie x 1 , x 2 , ... , x m \{displaystyle x_{1},x_{2},\ldots ,x_{m} die Menge der binären Variablen von m \{displaystyle m} Monitoren und y 1 , y 2 , ..., y n \{displaystyle y_{1},y_{2},\ldots } Source-Monitor-Verbindungen werden durch die (unbekannte) Mischmatrix dargestellt G \{textstyle \{boldsymbol {G}, wobei g i j = 1 \{displaystyle g_{ij}=1 anzeigt, dass das Signal der i-ten Quelle durch den j-ten Monitor beobachtet werden kann. Das System funktioniert wie folgt: Wenn eine Quelle i \{displaystyle i} aktiv ist ( y i = 1 \{displaystyle y_{i}=1 ) und sie mit dem Monitor j \{displaystyle j} verbunden ist ( g i j = 1 \{displaystyle g_{ij}=1 )dann wird der Monitor j \{displaystyle jstyle} einige Aktivität beobachten ( x j = 1 \{displaystyle). Förmlich haben wir: x i = ⋁ j = 1 n (g i j ≠ y j ) , i = 1 , 2 , ..., m , \{displaystyle x_{i}=\bigvee j=1}^{n}(g_{ij}\wedge y_{j}),i=1,2,\ldots m} wobei ξ \{displaystyle \wedge } Boolean UND und peptid \{displaystyle \vee } Boolean OR ist. Beachten Sie, dass Lärm nicht explizit modelliert wird, sondern als unabhängige Quellen behandelt werden kann. Das obige Problem kann heuristisch gelöst werden, indem angenommen wird, dass die Variablen kontinuierlich sind und die FastICA auf binären Beobachtungsdaten läuft, um die Mischmatrix G \{textstyle \{boldsymbol {G} (reale Werte) zu erhalten, dann wenden Sie auf G \{textstyle \{boldsymbol {G} Rundnummerntechniken an, um die Binärwerte zu erhalten.Dieser Ansatz hat sich gezeigt, dass ein sehr ungenaues Ergebnis entsteht. Eine weitere Methode ist die dynamische Programmierung: die Beobachtungsmatrix X \{textstyle \{boldsymbol {X} erneut in ihre Untermatrizen brechen und den Inferenzalgorithmus auf diesen Untermatrizen ausführen. Die Schlüsselbeobachtung, die zu diesem Algorithmus führt, ist die Submatrix X 0 \{textstyle \{boldsymbol X}^{0 von X \{textstyle \{boldsymbol {X}, wobei x i j = 0, ã j \{textstyle x_{ij}=0,\forall j} der unvoreingenommenen Beobachtungsmatrix versteckter Komponenten entspricht, die keine Verbindung zum i \{display i} haben. Experimentelle Ergebnisse zeigen, dass dieser Ansatz unter moderaten Geräuschpegeln genau ist. Das Allgemeine Binary ICA Framework führt eine breitere Problemformulierung ein, die keine Kenntnisse über das generative Modell erfordert. Mit anderen Worten versucht diese Methode, eine Quelle in ihre unabhängigen Komponenten (so viel wie möglich und ohne jegliche Information zu verlieren) zu zersetzen, ohne dass vorher davon ausgegangen wird, wie sie erzeugt wurde. Obwohl dieses Problem ziemlich komplex erscheint, kann es mit einem Ast und gebundenem Suchbaum-Algorithmus oder eng oben mit einer einzigen Multiplikation einer Matrix mit einem Vektor begrenzt gelöst werden. Methoden zur blinden Quellentrennung Projektionsverfolgung Signalgemische neigen dazu, gaußische Wahrscheinlichkeitsdichtefunktionen zu haben, und Quellsignale neigen dazu, nicht-gossische Wahrscheinlichkeitsdichtefunktionen zu haben. Jedes Quellsignal kann aus einem Satz von Signalgemischen durch Aufnahme des inneren Produkts eines Gewichtsvektors und der Signalgemische extrahiert werden, wobei dieses innere Produkt eine orthogonale Projektion der Signalgemische liefert. Die verbleibende Herausforderung ist, einen solchen Gewichtsvektor zu finden. Eine Art der Methode dazu ist die Projektionsverfolgung. Die Projektionssuche sucht eine Projektion zu einer Zeit, so dass das extrahierte Signal möglichst nicht-Gaussian ist. Dies kontrastiert mit ICA, die typischerweise Extrakte M signalisiert gleichzeitig aus M-Signalgemischen, was eine M × M-Entmischungsmatrix abschätzt. Ein praktischer Vorteil des Projektionsverlaufs über ICA besteht darin, dass bei Bedarf weniger als M Signale extrahiert werden können, wobei jedes Quellsignal mit einem M-Element-Gewichtsvektor aus M-Signalgemischen extrahiert wird. Wir können kurtosis verwenden, um das Mehrfach-Quellensignal wiederherzustellen, indem wir die richtigen Gewichtsvektoren mit der Verwendung von Projektionsverfolgung finden. Die Kurtose der Wahrscheinlichkeitsdichtefunktion eines Signals für eine endliche Probe wird als K = E ≠ berechnet ( y - y ̄) 4 ) (E ζ ( y − y ̄ ̄ ̄ ̄) 2 − 3 \{displaystyle K={\frac \{operatorname {E} [(\mathbf {y} -\mathbf \{overline {y} ^)4}}{(\operatorname {E} [(\mathbf {y} -\mathbf \{overline {y}2}}-3, wo y ̄{displaystyle \mathbf \{overline {y}} Die Konstante 3 sorgt dafür, dass Gaussian-Signale Nullkurtose haben, Super-Gaussian-Signale positive Kurtose haben und Sub-Gaussian-Signale negative Kurtose haben. Der Nenner ist die Varianz von y \{displaystyle \mathbf {y} } und sorgt dafür, dass die gemessene Kurtose die Signalvarianz berücksichtigt. Ziel der Projektionsverfolgung ist es, die Kurtose zu maximieren und das extrahierte Signal so unnormal wie möglich zu machen. Mit Kurtosis als Maß für Nicht-Normalität können wir nun untersuchen, wie die Kurtose eines Signals y = w T x \{displaystyle \mathbf {y} \=mathbf {w} ^{T}\mathbf {x} aus einem Satz von M-Gemischen x = ( x 1, x 2 , ..., x M ) extrahiert wird. T \{displaystyle \mathbf {x} (=x_{1},x_{2},\ldots ,x_{M})^T variiert, da der Gewichtsvektor w \{displaystyle \mathbf {w} um den Ursprung gedreht wird. Bei der Annahme, dass jedes Quellsignal s \displaystyle \mathbf {s} super-gaussisch ist, würden wir erwarten, dass die Kurtosis des extrahierten Signals y \{displaystyle \mathbf{y} maximal ist, wenn y = s \kurtstyle \mathbf} \=mathbfstyle {y} Bei M-Signalgemischen in einem M-dimensionalen Raum projiziert GSO diese Daten auf einen (M-1)-dimensionalen Raum unter Verwendung des Gewichtsvektors. Wir können die Unabhängigkeit der extrahierten Signale unter Verwendung von GSO gewährleisten. Um den richtigen Wert von w \{displaystyle \mathbf {w} } zu finden, können wir Gradientenabstiegsmethode verwenden. Wir verbergen zunächst die Daten und transformieren x \{displaystyle \mathbf {x} in eine neue Mischung z \{displaystyle \mathbf {z} }, die Einheitsvarianz hat, und z = (z 1 , z 2 , ..., z M ) T \{displaystyle \mathbf {z} (=z_{1},z_{2},\ldots ,z_{M})^^T .Dieser Vorgang kann durch Anwendung von Singularwertzersetzung auf x \{displaystyle \mathbf {x}, x = U D V erreicht werden T \{displaystyle \mathbf {x} \=mathbf {U} \mathbf {D} \mathbf {V} ^{T} Jeder Vektor U i = U i / E ‡ (U i 2 ) \{displaystyle U_{i}=U_{i}/\operatorname {E} (U_{i}^{2) und let z = U \{displaystyle \mathbf {z} \=mathbf {U} } .Das Signal, das von einem gewichteten Vektor w \{displaystyle \mathbf {w} extrahiert wird, ist y = w = w T z \{displaystyle \mathbf {y} \=mathbf {w} ^{T}\mathbf {z} .Wenn der Gewichtsvektor w eine Einheitslänge hat, dann ist die Varianz y auch 1, das heißt E ≠ (w T z) 2 ] = 1 \{displaystyle \operatorname {E} [(\mathbf} ^ .Die Kurtose kann somit wie: K = E ≠ [ y 4] ( E ≠ ( y 2 ) 2 - 3 = E ‡ [ w T z ] 4 ] − 3. \{displaystyle K={\frac \{operatorname {E} [\mathbf {y} ^4}}{(\operatorname) (E} [\mathbf {y} 2})^{2}}}-3=\operatorname (E) [(\mathbf {w} ^{T}\mathbf {z} {^)4}]-3. Der Aktualisierungsprozess für w \{displaystyle \mathbf {w} ist: w n e w = w o l d - η E ‡ [ z (w o l d T z ) 3 ] . \{displaystyle \mathbf} _{new}=\mathbf {w} _{old}-\eta \operatorname {E} [\mathbf {z} \(mathbf {w}_old}^^{T}\mathbf {z} {^)3}.] wobei η \{displaystyle \eta } eine kleine Konstante ist, um sicherzustellen, dass w \{displaystyle \mathbfw} zu der optimalen Lösung konvergiert. Nach jedem Update normalisieren wir w n e w = w n e w | w n e w | \{displaystyle \mathbf {w} _new}={\frac \{mathbf {w} _new}{\\\mathbf {w} _{new}| und setzen w o l d = w n e w \{displaystyle \mathbf} Wir können auch einen anderen Algorithmus verwenden, um den Gewichtsvektor w \{displaystyle \mathbf {w} } zu aktualisieren.Ein anderer Ansatz verwendet negentropy statt kurtosis. Die Verwendung von Negentropie ist eine robustere Methode als Kurtose, da Kurtose sehr empfindlich gegenüber Ausreißern ist. Die negentropischen Methoden basieren auf einer wichtigen Eigenschaft der Gaussschen Verteilung: eine Gausssche Variable hat die größte Entropie unter allen ununterbrochenen Zufallsvarianzvarianzvarianzgrößen. Dies ist auch der Grund, warum wir die nicht-gaussischen Variablen finden wollen. Ein einfacher Beweis ist in der Differential Entropie zu finden. J ( x ) = S ( y ) - S ( x ) \{displaystyle J(x)=S(y)-S(x},\ y is a Gausssian random variable of the same covariance matrix as x S (x ) = - δ p x (u ) log ‡ p x (u ) d u \{displaystyle S(x)=-\int p_{x(x) Eine Näherung zur Negentropie ist J ( x ) = 1 12 (E ( x 3 ) ) 2 + 1 48 (k u r t ( x ) ) 2 \{displaystyle J(x)={\frac 1}{12}}(E(x^{3}))^{2}+{\frac 1}{48}}(kurt(x)^{2}) Ein Beweis ist in den Original-Papier von Comon zu finden; es wurde im Buch Unabhängige Komponentenanalyse von Aapo Hyvärinen, Juha Karhunen, undErkki Oja wiedergegeben.Diese Annäherung leidet auch unter dem gleichen Problem wie Kurtosis (Sensitivität zu Ausreißern). Weitere Ansätze wurden entwickelt. J ( y ) = k 1 (G 1 ( y ) ) 2 + k 2 (E (G 2 ( y ) ) - E (G 2 (v ) ) 2 \{Displaystyle J(y)=k_{1}(E(G_{1}(y))))^2}(E(G_{2}(y)) Eine Auswahl von G 1 \{displaystyle G_{1} und G 2 \{displaystyle G_{2} sind G 1 = 1 a 1 log ‡ (cosh ‡ ( a 1 u ) ) ^displaystyle G_{1}={\fra 1}{a_{1}}\log(\cosh(a_{1}u) und G 2 = − exp fälschlich ( − u 2 2) \{displaystyle (-) Auf der Basis von infomax Infomax ICA ist im Wesentlichen eine multivariate, parallele Version des Projektions-Fortschritts. Während Projektionsverfolgung eine Reihe von Signalen zu einem Zeitpunkt aus einem Satz von M Signalgemischen extrahiert, extrahiert ICA parallel M Signale. Dies tendiert dazu, ICA robuster zu machen als Projektionsübung. Die Projektionsverfolgungsmethode verwendet die Gram-Schmidt-Orgonalisierung, um die Unabhängigkeit des extrahierten Signals zu gewährleisten, während ICA mit Infomax und maximaler Wahrscheinlichkeitsschätzung die Unabhängigkeit des extrahierten Signals gewährleistet. Die Nicht-Normalität des extrahierten Signals wird dadurch erreicht, dass für das Signal ein entsprechendes Modell oder vorab zugeordnet wird. Der auf Infomax basierende Prozess der ICA ist: Bei einer Reihe von Signalgemischen x \{displaystyle \mathbf {x} und einer Reihe von identischen unabhängigen Modell-Kumulativ-Verteilungsfunktionen (cdfs) g \{displaystyle g} suchen wir die unmixende Matrix W \{displaystyle \mathbf {W}, die die gemeinsame Entropie der Signale Y = g ( y ) \{displaystyle \mathbf {Y} =g(\mathbf {y} )} maximiert, wobei y = W x \{displaystyle \mathbf {y} \=mathbf {Wx} sind die Signale, die von W \{displaystyle \mathbf {W} } extrahiert werden.g \{displaystyle g} ist eine invertierbare Funktion und ist das Signalmodell. Beachten Sie, dass, wenn die Quellsignalmodell-Wahrscheinlichkeitsfunktion p s \{displaystyle p_{s} der Wahrscheinlichkeitsdichtefunktion des extrahierten Signals p y \{displaystyle p_{\mathbf {y}} , dann maximiert die gemeinsame Entropie von Y \{displaystyle Y} auch die Menge der gegenseitigen Informationen zwischen x \{displaystyle \mathbf {x} und Betrachten Sie die Entropie der Vektorvariablen Y = g ( y ) \{displaystyle \mathbf {Y} =g(\mathbf {y} )}, wobei y = W x \{displaystyle \mathbf {y} \=mathbf {Wx} } ist der Satz von Signalen, die von der ungemischten Matrix W \{displaystyle \mathbf {W} extrahiert werden.Für eine endliche Menge von Werten, die aus einer Verteilung mit pdf p y \{displaystyle p_{\mathbfstyle {}} abgetastet werden (Y){\=fra 1}{N}\sum _t=1}{N}\ln p_{\mathbf}\(mathbf {Y} ^{t} Die gemeinsame pdf p Y \{displaystyle p_{\mathbf {Y}}} kann gezeigt werden, dass sie mit dem gemeinsamen pdf p y \{displaystyle p_{\mathbf {y}} der extrahierten Signale durch die multivariate Form verwandt werden: p Y ( Y) = p y ( y ∂ ∂ Y ∂ y ∂ \{displaystyle p_{\\mathbf {Y}(Y} }(Y)={\frac p_{\\mathbf {y}\(mathbf {y} |{}\frac \{partial \mathbf {y}\partial \mathbf {y}}}}}}}}}}}}}}}}} Y ∂ y \{displaystyle \mathbf {J} \={frac \{partial \mathbf {Y}\{partial \mathbf {y}}}}}} ist die Jacobian Matrix. Wir haben | J | = g' ( y ) \{displaystyle |\mathbf {J} |=g'(\mathbf {y}})} und g' \{displaystyle g}' ist das pdf, das für Quellsignale g' = angenommen wird {\c} {\c} {\c} {\c} {\c} {\c} {\c}} {\c} {\c}} {\c\c\c} {\c} {\c}} {\c}} {\c}}} {\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\c\ Daher H ( Y ) = - 1 N Σ t = 1 N ln ρ y ( y ) p s ( y ) \{displaystyle H(\mathbf (Y){\=fra 1}{N}\sum _t=1}{N}\ln \{frac p_{\mathbf}\(mathbf {y}}})p_{\mathbf}\\\\\\(mathbf {y}}}}}}}}}}}}}} Wir wissen, wenn p y = p s \{displaystyle p_{\mathbf {y} =p_{s , p Y \{displaystyle p_{\mathbf {Y}} ist gleichmäßig verteilt, und H ( Y) \{displaystyle H({\mathbf {Y}}}} wird maximiert. Da p y ( y | = p x ( x ∂ ∂ y ∂ x | = p x ( x ) | W | \{displaystyle p_{\mathbf {y}\(mathbf {y} {y} {\=)frac p_{\mathbf {x}part {x}}\x}}{}\}\\\\\\\\\\f}\f}}} |{}\=frac p_{\\mathbf {x}\(mathbf {x}}}{}\mathbf {W}}}}}}, wobei | W | \{displaystyle |\mathbf {W} |} der absolute Wert des Determinanten der unmixing matix W \{displaystyle \mathbf (Y){\=fra ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 1}{N}\sum _t=1}{N}\ln p_{\mathbf}\(mathbf {x} ^{t} und Maximierung ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ Wenn M marginale pdfs der Modellfuge pdf p s \{displaystyle p_{\mathbf {s}} sind unabhängig und verwenden Sie das allgemein super-gaussische Modell pdf für die Quellsignale p s = ( 1 - tanh ¶ (') In der Summe, bei einem beobachteten Signalgemisch x \{displaystyle \mathbf {x}, dem entsprechenden Satz von extrahierten Signalen y \{displaystyle \mathbf {y}} und dem Quellsignalmodell p s = g' \{displaystyle p_{\\mathbf {s} =g', können wir die optimale ungemischte Matrix finden W \{displaystyle \mathbf {W} , und machen die extrahierten Signale unabhängig und nicht-gaussisch. Wie die Projektions-Spur-Situation, können wir Gradienten-Abstieg-Methode verwenden, um die optimale Lösung der unmischenden Matrix zu finden. Die maximale Wahrscheinlichkeitsschätzung (MLE) ist ein Standard-Statistik-Tool zum Auffinden von Parameterwerten (z.B. die ungemischte Matrix W \{displaystyle \mathbf {W} } ), die die beste Passform einiger Daten (z.B. die extrahierten Signale y \{displaystyle y}) zu einem gegebenen Modell (z.B. die angenommene gemeinsame Wahrscheinlichkeitsdichtefunktion) liefern. Das ML-Modell enthält eine Spezifikation eines pdf, die in diesem Fall das pdf p s \{displaystyle p_{s} der unbekannten Quellsignale s \{displaystyle s} ist. Das Ziel ist es, eine ungemischte Matrix zu finden, die extrahierte Signale y = W x \{displaystyle y=\mathbfstyle {W} x} mit einem gemeinsamen pdf so ähnlich wie möglich liefert MLE basiert somit auf der Annahme, dass wenn das Modell pdf p s \{displaystyle p_{s} und die Modellparameter A \{displaystyle \mathbf {A} sind korrekt, dann sollte eine hohe Wahrscheinlichkeit für die tatsächlich beobachteten Daten x \{displaystyle x} erhalten werden. Ist dagegen A \{displaystyle \mathbf {A} weit von den richtigen Parameterwerten entfernt, so würde eine geringe Wahrscheinlichkeit der beobachteten Daten erwartet. Mit MLE nennen wir die Wahrscheinlichkeit der beobachteten Daten für einen bestimmten Satz von Modellparameterwerten (z.B. ein pdf p s \{displaystyle p_{s} und eine Matrix A \{displaystyle \mathbf {A} } ) die Wahrscheinlichkeit der Modellparameterwerte bei den beobachteten Daten. Wir definieren eine Wahrscheinlichkeitsfunktion L (W ) \{displaystyle \mathbf {L(W}) } von W \{displaystyle \mathbf {W} : L ( W) = p s ( W x ) | det W | . \{displaystyle \mathbf {L(W}) =p_{s}(\mathbf {W} x)|\det \mathbf {W} |} Dies entspricht der Wahrscheinlichkeitsdichte bei x \{displaystyle x}, da s = W x \{displaystyle s=\mathbf {W} x} .Wenn wir also eine W \{displaystyle \mathbf {W} finden wollen, die die beobachteten Mischungen x \{displaystyle x} aus den unbekannten Quellsignalen s \{displaystyle s} mit pdf p s \{displaystyle p_{s} erzeugt haben, dann müssen wir nur feststellen, dass W \{displaystyle \mathbf {W} die die Wahrscheinlichkeit maximiert Die unmischende Matrix, die die Gleichung maximiert, ist als MLE der optimalen unmischenden Matrix bekannt. Es ist üblich, die log Wahrscheinlichkeit zu verwenden, weil dies einfacher zu bewerten ist. Da das Logarithm eine monotone Funktion ist, maximiert die W \{displaystyle \mathbf {W} } die Funktion L (W ) \{displaystyle \mathbf {L(W}) } auch ihr Logarithm ln ‡ L (W ) \{displaystyle \ln \mathbf {L(W}}} . Dies ermöglicht es uns, das Logarithm der obigen Gleichung zu nehmen, was die log-Lieferenzfunktion ln ≠ L (W) = Σ ergibt i Σ t ln | | det W | \{displaystyle \ln \mathbf {L(W)}=\sum _{i}\sum t}\lnp_{s}(w_{i}^{T}x_{t}+ N\ln |det\mathbf Wenn wir für die Quellsignale p s = ( 1 - tanh ‡ (s ) 2 ) \{displaystyle p_{s}=(1-\tanh(s)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ (W) Diese Matrix W \{displaystyle \mathbf {W}, die diese Funktion maximiert, ist die maximale Wahrscheinlichkeitsschätzung. Geschichte und Hintergrund Der frühe allgemeine Rahmen für die unabhängige Komponentenanalyse wurde von Jeanny Hérault und Bernard Ans von 1984 eingeführt, der 1985 und 1986 von Christian Jutten weiterentwickelt und 1991 von Pierre Comon verfeinert wurde und in seinem Papier von 1994 populär wurde. 1995 haben Tony Bell und Terry Sejnowski einen schnellen und effizienten ICA-Algorithmus auf der Basis von infomax eingeführt, ein Prinzip, das 1987 von Ralph Linsker eingeführt wurde. Es gibt viele Algorithmen in der Literatur, die ICA. Ein weitgehend genutzter, auch in industriellen Anwendungen, ist der von Hyvärinen und Oja entwickelte FastICA-Algorithmus, der die Kurtose als Kostenfunktion nutzt. Andere Beispiele beziehen sich eher auf die blinde Quellentrennung, bei der ein allgemeiner Ansatz verwendet wird. Beispielsweise kann man die Unabhängigkeitsannahme abfallen und voneinander korrelierte Signale trennen, also statistisch abhängige Signale. Sepp Hochreiter und Jürgen Schmidhuber zeigten, wie man nichtlineare ICA- oder Quelltrennung als Nebenprodukt der Regularisierung (1999) erhält. Ihre Methode erfordert keine Vorkenntnisse über die Anzahl der unabhängigen Quellen. Anwendungen ICA kann erweitert werden, um nicht-physikalische Signale zu analysieren. Zum Beispiel wurde ICA angewendet, um Diskussionsthemen über eine Tasche von Nachrichtenlisten-Archiven zu entdecken. Einige ICA-Anwendungen sind unten aufgeführt: optische Bildverarbeitung von Neuronen neuronalen Spike-Sortung Gesichtserkennung modellieren empfängliche Felder von primären visuellen Neuronen vorhersagen Aktienmarktpreise Mobilfunk-Farb-basierte Erkennung der Reife von Tomaten Entfernung Artefakte, wie Augenblinks, aus EEG-Daten. Analyse von Veränderungen der Genexpression über die Zeit in Einzelzellen-RNA-Sequencing-Experimenten. Untersuchungen des restlichen Zustandsnetzwerks des Gehirns. Astronomie und Kosmologie Siehe auch Hinweise Referenzen Comon, Pierre (1994:) "Unabhängige Komponentenanalyse: ein neues Konzept?" Signalverarbeitung, 36(3):287–314 (Das Originalpapier, das das Konzept der ICA beschreibt) Hyvärinen, A;. Karhunen, J;. Oja, E. (2001:) Unabhängige Komponentenanalyse, New York: Wiley, ISBN 978-0-471-40540-5 (Einführendes Kapitel)Hyvärinen, A;. Oja, E. (2000:) "Independent Component Analysis: Algorithmen and Application", Neural Networks, 13(4-5):411-430.(Technische aber pädagogische Einführung). Comon, P;. Jutten C,. (2010:) Handbuch der Blind Source Separation, Unabhängige Komponentenanalyse und Anwendungen. Akademische Presse, Oxford UK.ISBN 978-0-12-374726-6 Lee, T.-W (1998): Unabhängige Komponentenanalyse: Theorie und Anwendungen, Boston, Messe: Kluwer Academic Publishers, ISBN 0-7923-8261-7 Acharyya, Ranjan (2008:) Ein neuer Ansatz für Blind Source Separation von konvolutiven Quellen - Wavelet Based Separation using Shrinkage Function ISBN 3-639-07797-0 ISBN 978-3639077971 (dieses Buch konzentriert sich auf ununterbrochenes Lernen mit Quellentrennung) Externe Links Was ist eine unabhängige Komponentenanalyse? von Aapo Hyvärinen Independent Component Analysis: A Tutorial von Aapo Hyvärinen Ein Tutorial auf Unabhängige Komponentenanalyse FastICA als Paket für Matlab, in R-Sprache, C+ ICALAB Toolboxes für Matlab, entwickelt bei RIKEN High Performance Signal Analysis Toolkit bietet C+ Implementierungen von FastICA und Infomax ICA Toolbox Matlab Werkzeuge für ICA mit Bell-Sejnowski, Molter und ICA. Entwickelt bei der DTU. Vorführung des Cocktail-Party-Problems EEGLAB Toolbox ICA von EEG für Matlab, entwickelt bei UCSD. FMRLAB Toolbox ICA von fMRI für Matlab, entwickelt bei UCSD MELODIC, Teil der FMRIB Software Library. Diskussion von ICA verwendet in einem biomedizinischen Form-Repräsentation Kontext FastICA, CuBICA, JADE und TDSEP Algorithmus für Python und mehr... Group ICA Toolbox und Fusion ICA Toolbox Tutorial: Mit ICA zur Reinigung von EEG-Signalen Eine Polität ist eine identifizierbare politische Einheit – jede Gruppe von Menschen, die eine kollektive Identität haben, die von einer Form institutionalisierter sozialer Beziehungen organisiert werden und die eine Fähigkeit haben, Ressourcen zu mobilisieren. Eine Politität kann jede andere Gruppe von Personen sein, die für Governance (z.B. ein Corporate Board,) die Regierung eines Landes, Land Unterteilung oder einen souveränen Staat organisiert. Überblick In der Geopolitik kann sich eine Polität in verschiedenen Formen manifestieren, wie z.B. einem Staat, einem Imperium, einer internationalen Organisation, einer politischen Organisation und anderen identifizierbaren, ressourcenschonenden Organisationsstrukturen. Eine Polität wie ein Staat muss nicht eine souveräne Einheit sein. Die wichtigsten Polititäten sind heute Westfalenstaaten und Nationalstaaten, die allgemein als Länder bezeichnet werden und auch falsch von den Begriff Nationen bezeichnet werden. Eine Politität verkapselt eine Vielzahl von Organisationen, von denen viele das grundlegende Instrument zeitgenössischer Staaten wie ihre untergeordneten zivilen und lokalen Behörden bilden. Die Politik muss nicht in der Kontrolle über alle geographischen Gebiete sein, da nicht alle politischen Wesen und Regierungen die Ressourcen eines festen geographischen Raums kontrolliert haben. Die historischen Steppe Empires aus der Eurasischen Steppe sind das prominenteste Beispiel für nicht-sedentäre Politäten. Diese Politäten unterscheiden sich von Staaten wegen ihres Fehlens eines festen, definierten Gebiets. Imperium unterscheidet sich auch von den Zuständen, dass ihre Territorien nicht statisch definiert oder dauerhaft fixiert sind und folglich auch deren Körperpolitik dynamisch und flüssig war. Es ist nützlich, dann an eine Polität als politische Gemeinschaft zu denken. Eine Polität kann auch entweder als Fraktion innerhalb einer größeren (normalerweise) Einheit oder zu unterschiedlichen Zeiten als Einheit selbst definiert werden. Zum Beispiel sind Kurden im irakischen Kurdistan Teile ihrer eigenen separaten und unterschiedlichen Polität. Sie sind aber auch Mitglieder des souveränen Staates Irak, der selbst eine Polität ist, wenn auch eine, die viel weniger spezifisch ist und dadurch viel weniger kohäsiv ist. Es ist daher möglich, dass ein Individuum zu einem Zeitpunkt mehr als eine Polität aufweist. Thomas Hobbes war eine sehr bedeutende Figur bei der Konzeption von Politäten, insbesondere von Staaten. Hobbes betrachtete Begriffe des Staates und des Körpers in Leviathan, seine bemerkenswerteste Arbeit. Die Politik muss nicht unbedingt Regierungen sein. Ein Unternehmen ist beispielsweise in der Lage, Ressourcen zu sammeln, verfügt über eine Governance-Struktur, Rechtsrechte und ausschließliche Zuständigkeit gegenüber internen Entscheidungsfindungen. Eine ethnische Gemeinschaft innerhalb eines Landes oder einer subnationalen Einheit kann eine Politität sein, wenn sie über ausreichende Organisations- und Kohäsivinteressen verfügen, die von dieser Organisation weitergeführt werden können. Siehe auch Kokutai Nation Politeia Politisches System Referenzen Externe Links Wörterbuch der Geschichte der Ideen – Analogie der körperpolitischen (Erarbeitung von Korrespondenzen zwischen Gesellschaft oder Staat und dem individuellen menschlichen Körper)