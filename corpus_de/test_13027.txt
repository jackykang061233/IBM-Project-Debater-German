Ray Solomonoff (25. Juli 1926 – 7. Dezember 2009) war der Erfinder der algorithmischen Wahrscheinlichkeit, seiner allgemeinen Theorie der induktiven Inferenz (auch bekannt als universelle induktive Inferenz), und war Gründer der algorithmischen Informationstheorie. Er war ein Urheber des Zweiges der künstlichen Intelligenz basierend auf maschinellem Lernen, Vorhersage und Wahrscheinlichkeit. Er verbreitete den ersten Bericht über das nicht-semantische maschinelle Lernen im Jahr 1956. Solomonoff beschrieb erstmals die algorithmische Wahrscheinlichkeit 1960 und veröffentlichte das Theorem, das Kolmogorov Komplexität und algorithmische Informationstheorie lancierte. Er beschrieb diese Ergebnisse zunächst auf einer Konferenz in Caltech 1960, und in einem Bericht, Februar 1960, "Ein Vorbericht über eine allgemeine Theorie der induktiven Inferenz. " Er erklärte diese Ideen in seinen Veröffentlichungen von 1964, "A Formal Theory of Inductive Inference", Teil I und Teil II. Algorithmische Wahrscheinlichkeit ist eine mathematisch formalisierte Kombination von Occam Rasierer, und das Prinzip der mehreren Erklärungen. Es ist eine maschinenunabhängige Methode, jedem Hypothesen (Algorithm/Programm) einen Wahrscheinlichkeitswert zuzuordnen, der eine bestimmte Beobachtung erklärt, wobei die einfachste Hypothese (das kürzeste Programm) die höchste Wahrscheinlichkeit aufweist und die zunehmend komplexen Hypothesen zunehmend kleine Wahrscheinlichkeiten erhalten. Solomonoff gründete die Theorie der universellen induktiven Inferenz, die auf soliden philosophischen Grundlagen basiert und ihre Wurzeln in der Kolmogorov-Komplexität und algorithmischen Informationstheorie hat. Die Theorie nutzt algorithmische Wahrscheinlichkeit in einem Bayesischen Rahmen. Der universale Vorname wird über die Klasse aller rechnerischen Maßnahmen übernommen; keine Hypothese wird eine Nullwahrscheinlichkeit haben. Dies ermöglicht es Bayes' Regel (der Kausierung) verwendet werden, um das wahrscheinlich nächste Ereignis in einer Reihe von Ereignissen vorherzusagen, und wie wahrscheinlich es sein wird. Obwohl er am besten für algorithmische Wahrscheinlichkeit und seine allgemeine Theorie der induktiven Inferenz bekannt ist, machte er viele andere wichtige Entdeckungen während seines Lebens, die meisten von ihnen gerichtet auf sein Ziel in der künstlichen Intelligenz: eine Maschine zu entwickeln, die harte Probleme mit probabilistischen Methoden lösen könnte. Lebensgeschichte bis 1964 Ray Solomonoff wurde am 25. Juli 1926 in Cleveland, Ohio, Sohn jüdischer russischer Einwanderer Phillip Julius und Sarah Mashman Solomonoff geboren. Er besuchte die Glenville High School, die 1944 studierte. 1944 trat er der United States Navy als Instructor in Electronics bei. Von 1947–1951 besuchte er die Universität Chicago, studierte unter Professoren wie Rudolf Carnap und Enrico Fermi und absolvierte 1951 einen M.S in Physik. Von seinen frühesten Jahren war er von der reinen Freude der mathematischen Entdeckung und vom Wunsch, zu erkunden, wo niemand zuvor gegangen war. Im Alter von 16, im Jahre 1942 begann er nach einer allgemeinen Methode zur Lösung mathematischer Probleme zu suchen. 1952 traf er Marvin Minsky, John McCarthy und andere, die sich für die maschinelle Intelligenz interessieren. Im Jahr 1956 organisierten Minsky und McCarthy und andere die Dartmouth Summer Research Conference on Artificial Intelligence, wo Solomonoff einer der ursprünglichen 10 Einladungen war – er, McCarthy und Minsky waren die einzigen, die den ganzen Sommer bleiben. Für diese Gruppe wurde zunächst künstliche Intelligenz als Wissenschaft bezeichnet. Computer zur Zeit könnten sehr spezifische mathematische Probleme lösen, aber nicht viel anderes. Solomonoff wollte eine größere Frage verfolgen, wie man Maschinen allgemein intelligenter macht und wie Computer für diesen Zweck die Wahrscheinlichkeit nutzen könnten. Geschichte der Arbeit bis 1964 Er schrieb drei Papiere, zwei mit Anatol Rapoport, 1950–52, die als die früheste statistische Analyse von Netzwerken angesehen werden. Er war einer der 10 Teilnehmer des 1956 Dartmouth Summer Research Project on Artificial Intelligence. Er schrieb und verbreitete einen Bericht unter den Teilnehmern: "Eine induktive Inferenzmaschine". Es betrachtete maschinelles Lernen als probabilistisch, mit einem Schwerpunkt auf der Bedeutung von Trainingssequenzen und auf der Verwendung von Teilen früherer Lösungen für Probleme bei der Konstruktion von Testlösungen für neue Probleme. 1957 veröffentlichte er eine Version seiner Ergebnisse. Dies waren die ersten Beiträge, die auf probabilistischen maschinellen Lernen geschrieben werden. In den späten 1950er Jahren erfand er probabilistische Sprachen und ihre zugehörigen Grammatiken. Eine probabilistische Sprache gibt jedem möglichen String einen Wahrscheinlichkeitswert zu. Die Verallgemeinerung des Konzepts der probabilistischen Grammatik führte ihn 1960 zu seiner Entdeckung der Algorithmischen Wahrscheinlichkeit und der allgemeinen Theorie der induktiven Inferenz. Vor den 1960er Jahren basierte die übliche Methode der Berechnungswahrscheinlichkeit auf der Frequenz: das Verhältnis der günstigen Ergebnisse zur Gesamtzahl der Versuche.In seiner 1960er-Veröffentlichung, und noch vollständiger, in seinen 1964 Publikationen, Solomonoff ernsthaft überarbeitet diese Definition der Wahrscheinlichkeit. Er nannte diese neue Form der Wahrscheinlichkeit "Algorithmische Wahrscheinlichkeit" und zeigte, wie sie für Vorhersage in seiner Theorie der induktiven Inferenz verwenden. Als Teil dieser Arbeit produzierte er die philosophische Grundlage für die Verwendung von Bayes-Regel der Kausierung zur Vorhersage. Das grundlegende Theorem dessen, was später Kolmogorov Complexity genannt wurde, war Teil seiner allgemeinen Theorie. 1960 schreibt, beginnt er: "Betrachten Sie eine sehr lange Zeichenfolge. Wir werden eine solche Abfolge von Symbolen als einfach betrachten und eine hohe a priori Wahrscheinlichkeit haben, wenn es eine sehr kurze Beschreibung dieser Abfolge gibt – natürlich eine Art von festgelegter Beschreibungsmethode. Genauer gesagt, wenn wir nur die Symbole 0 und 1 verwenden, um unsere Beschreibung auszudrücken, werden wir die Wahrscheinlichkeit 2-N einer Zeichenfolge zuordnen, wenn ihre kürzeste mögliche binäre Beschreibung N Ziffern enthält. " Die Wahrscheinlichkeit bezieht sich auf eine bestimmte Universal-Turniermaschine. Solomonoff zeigte und im Jahr 1964 bewiesen, dass die Wahl der Maschine, während es einen konstanten Faktor hinzufügen könnte, die Wahrscheinlichkeitsverhältnisse nicht sehr ändern würde. Diese Wahrscheinlichkeiten sind maschinenunabhängig. 1965 veröffentlichte der russische Mathematiker Kolmogorov selbstständig ähnliche Ideen. Als er sich der Arbeit von Solomonoff bewusst wurde, erkannte er Solomonoff an, und Solomonoffs Arbeit war in der Sowjetunion besser bekannt als in der westlichen Welt. Der allgemeine Konsens in der wissenschaftlichen Gemeinschaft war jedoch, diese Art von Komplexität mit Kolmogorov zu verbinden, die mehr mit Zufall einer Sequenz beschäftigt war. Algorithmische Probability und Universal (Solomonoff) Induktion wurden mit Solomonoff verbunden, der auf Vorhersage konzentriert war – die Extrapolation einer Sequenz. Später in der gleichen 1960er-Publikation beschreibt Solomonoff seine Erweiterung der ein-hortest-Code-Theorie. Das ist Algorithmische Wahrscheinlichkeit. Er sagt: "Es scheint, dass, wenn es mehrere verschiedene Methoden der Beschreibung einer Sequenz gibt, jeder dieser Methoden sollte ein gewisses Gewicht bei der Bestimmung der Wahrscheinlichkeit dieser Sequenz gegeben werden. " Er zeigt dann, wie diese Idee verwendet werden kann, um die universelle a priori Wahrscheinlichkeitsverteilung zu erzeugen und wie es die Verwendung von Bayes Regel in induktiver Inferenz ermöglicht. Induktive Inferenz, indem die Vorhersagen aller Modelle, die eine bestimmte Sequenz beschreiben, unter Verwendung geeigneter Gewichte basierend auf den Längen dieser Modelle addiert werden, erhält die Wahrscheinlichkeitsverteilung für die Erweiterung dieser Sequenz. Diese Methode der Vorhersage ist seither als Solomonoff Induktion bekannt geworden. Er erweiterte seine Theorie und veröffentlichte eine Reihe von Berichten, die bis zu den Publikationen im Jahr 1964. Die Arbeiten von 1964 geben eine detailliertere Beschreibung der Algorithmischen Wahrscheinlichkeit, und Solomonoff Induktion, die fünf verschiedene Modelle, einschließlich des Modells populär genannt die Universal Distribution. Arbeitsgeschichte von 1964 bis 1984 Andere Wissenschaftler, die auf der 1956 Dartmouth Summer Conference (wie Newell und Simon) waren, entwickelten den Zweig der Künstlichen Intelligenz, der Maschinen verwendet, die von if-then Regeln, Fakt basierten. Solomonoff entwickelte den Zweig der Künstlichen Intelligenz, der sich auf Wahrscheinlichkeit und Vorhersage konzentrierte; seine spezifische Ansicht von A.I beschreibt Maschinen, die durch die Algorithmische Wahrscheinlichkeitsverteilung geregelt wurden. Die Maschine erzeugt Theorien zusammen mit ihren zugehörigen Wahrscheinlichkeiten, Probleme zu lösen, und wie neue Probleme und Theorien entwickeln, aktualisiert die Wahrscheinlichkeitsverteilung auf den Theorien. 1968 fand er einen Beweis für die Wirksamkeit der Algorithmischen Wahrscheinlichkeit, aber vor allem wegen des Mangels an allgemeinem Interesse damals, nicht veröffentlichte es bis 10 Jahre später. In seinem Bericht veröffentlichte er den Beweis für die Konvergenztheorie. In den Jahren nach seiner Entdeckung Algorithmischer Wahrscheinlichkeit konzentrierte er sich auf, wie man diese Wahrscheinlichkeit und Solomonoff Induktion in der tatsächlichen Vorhersage und Problemlösung für A.I Er wollte auch die tieferen Auswirkungen dieses Wahrscheinlichkeitssystems verstehen. Ein wichtiger Aspekt der Algorithmischen Wahrscheinlichkeit ist, dass sie vollständig und unbestritten ist. Im Bericht von 1968 zeigt er, dass die agorithmische Wahrscheinlichkeit vollständig ist; d.h. wenn es eine beschreibbare Regelmäßigkeit in einem Körper von Daten gibt, wird Algorithmische Wahrscheinlichkeit schließlich feststellen, dass Regelmäßigkeit, die eine relativ kleine Probe dieser Daten erfordert. Algorithmische Wahrscheinlichkeit ist das einzige Wahrscheinlichkeitssystem, das auf diese Weise vollständig ist.Als notwendige Folge seiner Vollständigkeit ist sie unbestritten. Die Incomputability ist, weil einige Algorithmen - eine Teilmenge derjenigen, die teilweise rekursiv sind - nie vollständig ausgewertet werden können, weil sie zu lange dauern würde. Aber diese Programme werden zumindest als mögliche Lösungen erkannt werden. Andererseits ist jedes Rechensystem unvollständig. Es werden immer Beschreibungen außerhalb des Suchraums des Systems vorhanden sein, die niemals anerkannt oder berücksichtigt werden, auch in unendlicher Zeit. Computable Vorhersagemodelle verbergen diese Tatsache, indem sie solche Algorithmen ignorieren. In vielen seiner Papiere beschrieb er, wie man nach Lösungen für Probleme sucht, und in den 1970er und frühen 1980er Jahren entwickelte er, was er als der beste Weg war, die Maschine zu aktualisieren. Die Verwendung von Wahrscheinlichkeit in A.I hatte jedoch keinen völlig glatten Weg. In den frühen Jahren von A.I war die Relevanz der Wahrscheinlichkeit problematisch. Viele in der A.I-Gemeinschaft fühlten die Wahrscheinlichkeit in ihrer Arbeit nicht nutzbar. Der Bereich der Mustererkennung nutzte eine Form der Wahrscheinlichkeit, aber weil es keine breit angelegte Theorie, wie man Wahrscheinlichkeit in jedem A.I-Feld, die meisten Felder nicht verwendet es überhaupt. Es gab jedoch Forscher wie Pearl und Peter Cheeseman, die argumentierten, dass Wahrscheinlichkeit in der künstlichen Intelligenz verwendet werden könnte. Rund 1984 wurde auf einer jährlichen Sitzung der American Association for Artificial Intelligence (AAAI) beschlossen, dass die Wahrscheinlichkeit in keiner Weise für A.I relevant war Eine Protestgruppe bildete sich, und im nächsten Jahr gab es einen Workshop auf dem AAAI-Treffen, der sich mit "Wahrhaftigkeit und Unsicherheit in KI" befasste. Dieser jährlich stattfindende Workshop hat bis heute fortgesetzt. Im Rahmen des Protests in der ersten Werkstatt gab Solomonoff einen Bericht über die Anwendung der universellen Verteilung auf Probleme in A.I.This war eine frühe Version des Systems, das er seit dieser Zeit entwickelt. In diesem Bericht beschreibt er die von ihm entwickelte Suchtechnik. Bei Suchproblemen ist die beste Reihenfolge der Suche die Zeit T i / P i {\displaystyle T_{i}/P_{i , wobei T i {\displaystyle T_{i} die Zeit ist, die benötigt wird, um den Test zu testen und P i {\displaystyle P_{i} ist die Wahrscheinlichkeit des Erfolgs dieser Studie. Er nannte dies die "Conceptual Jump Size" des Problems. Levins Suchtechnik nähert sich dieser Ordnung, und so Solomonoff, der Levins Arbeit studiert hatte, nannte diese Suchtechnik Lsearch. Arbeitsgeschichte — die späteren Jahre In anderen Papieren erforschte er, wie man die Zeit begrenzt, die für die Suche nach Lösungen benötigt wird, indem er auf ressourcengebundene Suche schreibt. Der Suchraum ist begrenzt durch verfügbare Zeit- oder Rechenkosten anstatt durch das Ausschneiden von Suchraum, wie es in einigen anderen Vorhersagemethoden, wie Minimum Beschreibung Länge getan wird. Während seiner Karriere beschäftigte sich Solomonoff mit den potenziellen Vorteilen und Gefahren von A.I, die es in vielen seiner veröffentlichten Berichte diskutierten. 1985 analysierte er eine wahrscheinliche Entwicklung von A.I und gab eine Formel, die vorhersagte, wann sie den "Infinity Point" erreichen würde. Diese Arbeit ist Teil der Geschichte des Gedankens über eine mögliche technologische Einzigartigkeit. Ursprünglich algorithmische Induktionsverfahren extrapoliert geordnete Sequenzen von Strings. Für den Umgang mit anderen Daten wurden Methoden benötigt. Ein Bericht von 1999 verallgemeinert die Universal Distribution und die damit verbundenen Konvergenztheorien zu ungeordneten Strings und einem Bericht von 2008, zu ungeordneten Paaren von Strings. In den Jahren 1997, 2003 und 2006 zeigte er, dass Incomputability und Subjektivität sowohl notwendig als auch wünschenswerte Eigenschaften eines Hochleistungsinduktionssystems sind. 1970 gründete er seine eigene ein-Mann-Firma Oxbridge Research und setzte dort seine Forschung fort, mit Ausnahme von Zeiten an anderen Institutionen wie MIT, Universität Saarland in Deutschland und dem Dalle Molle Institut für Künstliche Intelligenz in Lugano, Schweiz. 2003 war er der erste Preisträger des Kolmogorov Awards des Computer Learning Research Centers an der Royal Holloway, University of London, wo er die inaugural Kolmogorov Lecture gab. Solomonoff war zuletzt Gastprofessor am CLRC. Im Jahr 2006 sprach er unter AI@50, "Dartmouth Artificial Intelligence Conference: the Next 50 Years" zum fünfzigsten Jahrestag der ursprünglichen Dartmouth Sommer-Studiengruppe.Solomonoff war einer von fünf ursprünglichen Teilnehmern. Im Februar 2008 gab er die Keynote Adresse auf der Konferenz "Current Trends in the Theory and Application of Computer Science" (CTTACS), die an der Notre Dame University in Libanon stattfand. Er folgte diesem mit einer kurzen Vortragsreihe und begann mit der Erforschung neuer Anwendungen der Algorithmischen Wahrscheinlichkeit. Algorithmische Wahrscheinlichkeit und Solomonoff Induktion haben viele Vorteile für künstliche Intelligenz. Algorithmische Wahrscheinlichkeit gibt extrem genaue Wahrscheinlichkeitsschätzungen. Diese Schätzungen können durch eine zuverlässige Methode überprüft werden, damit sie weiterhin akzeptabel sind. Es nutzt die Suchzeit auf eine sehr effiziente Weise. Neben den Wahrscheinlichkeitsschätzungen "hat die Algorithmische Wahrscheinlichkeit für KI einen weiteren wichtigen Wert: ihre Vielzahl von Modellen gibt uns viele verschiedene Möglichkeiten, unsere Daten zu verstehen; Eine Beschreibung von Solomonoffs Leben und Arbeit vor 1997 ist in "The Discovery of Algorithmic Probability", Journal of Computer and System Sciences, Vol 55, No. 1, pp 73–88, August 1997. Das Papier, sowie die meisten anderen hier genannten, sind auf seiner Website auf der Veröffentlichungen-Seite verfügbar. In einem Artikel, der das Jahr seines Todes veröffentlichte, sagte ein Zeitschriftenartikel von Solomonoff: "Ein sehr konventioneller Wissenschaftler versteht seine Wissenschaft mit einem einzigen "aktuellen Paradigma" – die Art des Verständnisses, die derzeit am meisten in vogue ist. Ein kreativer Wissenschaftler versteht seine Wissenschaft auf sehr viele Weise und kann leichter neue Theorien, neue Wege des Verständnisses schaffen, wenn das "current paradigm" nicht mehr den aktuellen Daten entspricht." Siehe auch Ming Li und Paul Vitanyi, Eine Einführung in Kolmogorov Komplexität und seine Anwendungen. Springer-Verlag, N.Y, 2008, enthält historische Notizen zu Solomonoff sowie eine Beschreibung und Analyse seiner Arbeit. Marcus Hutter's Universal Künstliche Intelligenz Referenzen Externe Links Ray Solomonoff's Homepage Für eine detaillierte Beschreibung der Algorithmischen Wahrscheinlichkeit siehe "Algorithmische Wahrscheinlichkeit" von Hutter, Legg und Vitanyi in der Gelehrten. Ray Solomonoff (1926–2009) 85. Gedenkkonferenz, Melbourne, Australien, Nov/Dez 2011 und Proceedings, "Algorithmische Wahrscheinlichkeit und Freunde. Bayesische Vorhersage und künstliche Intelligenz", Springer, LNAI/LNCS 7070. Pionier des maschinellen Lernens gefeiert 14. Dezember 2011