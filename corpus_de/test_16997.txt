Batch Normalisierung (auch bekannt als Charge-Norm) ist eine Methode, die verwendet wird, um künstliche Neuralnetze schneller und stabiler zu machen durch die Normalisierung der Vorleistungen der Schichten durch Umschichtung und Rekalierung. Es wurde 2015 von Sergey Ioffe und Christian Szegedy vorgeschlagen. Obwohl die Wirkung der Chargen-Normung offensichtlich ist, werden die Gründe für ihre Wirksamkeit weiterhin erörtert. Es wurde davon ausgegangen, dass es das Problem der internen Kovariatumschichtung mindern kann, wo die Paraphierung von Parameter und Änderungen der Verteilung der Inputs der einzelnen Ebenen die Lernrate des Netzes beeinflussen. Manche Wissenschaftler haben vor kurzem argumentiert, dass die Normalisierung der Chargenmenge nicht die interne Kovariate verlagert, sondern die objektive Funktion, die wiederum die Leistung verbessert. Kontinuierlich führt die Normalisierung der Charge zu einer drastischen Zunahme der tiefen Netze, die nur durch die Ausführerverbindungen in Restnetzen gemildert wird. Andere halten die Normalisierung der Chargen weiterhin aufrecht und beschleunigen damit die Neuralnetze. Motivation: Das Phänomen der internen Kovariateumschichtung Jede Schicht eines Neuralnetzes hat Inputs mit einer entsprechenden Verteilung, die während des Ausbildungsprozesses durch die Randomisierung des Parameters und die Randomität in den Eingangsdaten betroffen ist. Die Wirkung dieser Zufallsquellen auf die Verteilung der Inputs auf interne Ebenen während der Ausbildung wird als interne Kovariate bezeichnet. Obwohl eine klare genaue Definition fehlt, ist das in Experimenten beobachtete Phänomen die Veränderung der Mittel und Schwankungen der Inputs an internen Ebenen während der Ausbildung. Batch Normalisierung wurde ursprünglich vorgeschlagen, die interne Kovariatumstellung abzuschwächen. Während der Ausbildungsphase der Netze, wie die Parameter der vorausgegangenen Schichtenwechsel, wird die Verteilung der Inputs auf die aktuellen Schicht entsprechend geändert, so dass die derzeitige Schicht ständig an neue Vertriebsmöglichkeiten angepasst werden muss. Dieses Problem ist besonders schwer für tiefe Netze, da kleine Veränderungen in flacheren versteckten Schichten, wie sie innerhalb des Netzes propagate, zu einer erheblichen Verlagerung in tieferen versteckten Schichten führen werden. Daher wird vorgeschlagen, diese unerwünschten Verschiebungen zu verringern, um die Ausbildung zu beschleunigen und zuverlässigere Modelle zu erstellen. Neben der Verringerung der internen Kovariate wird davon ausgegangen, dass die Normalisierung der Chargen viele andere Vorteile bringt. Mit diesem zusätzlichen Betrieb kann das Netzwerk eine höhere Lernrate ohne schwindenden oder explodierenden Erkrankungen verwenden. Des Weiteren scheint die Standardisierung der Chargen eine regelmäßige Wirkung zu haben, so dass das Netz seine allgemeinen Charakterisierungseigenschaften verbessert, und es ist daher unnötig, die Ablagerung zu verringern. Es wurde auch festgestellt, dass das Netz mit Charge-Norm robuster wird, um unterschiedliche Anfangsregelungen und Lernraten zu unterscheiden. Verfahren Batch Normalisierung In einem Neuralnetz wird die Vereinheitlichung durch einen Normalisierungsschritt erreicht, der die Mittel und Schwankungen der Vorleistungen der einzelnen Ebenen festlegt. Idealerweise würde die Normalisierung über die gesamte Ausbildungseinrichtung durchgeführt, aber diesen Schritt gemeinsam mit kryktischen Optimierungsmethoden nutzen, ist es unpraktisch, die globalen Informationen zu nutzen. So wird die Normalisierung auf jeden Mini-batch im Ausbildungsprozess eingeschränkt. Verwendung von B, um eine Mini-Diskussion der Größe m der gesamten Ausbildungseinrichtung zu verdichten. empirische Mittel und Varianz von B könnten somit als μ B = 1 m  i i = 1 m x i {\displaystyle \mu _B}=1,002 i=1}m}x_{i , und  B B 2 = 1 m  i i = 1 m ( x i − μ B ) 2 KINGstyle \sigma _B22} i=1.m}(x_{i}-\mu _B})^{2 . Für eine Schicht des Netzes mit d-dimensionalem Input, x = ( x 1 ) , . , x ( d ) ) displaystyle x=(x(1)(1)},...,x((d , jede Dimension seines Inputs wird dann normalisiert (d. h. neu ausgerichtet und re-scaled) , x  i } (k) · }  B B B ( k)  B B B B B B B ( k) )  2  2  2  2  2  2  2  2  2  2  2  2  2   WELLdisplaystyle \epsilon } wird im Nenner für numerische Stabilität hinzugefügt und ist eine willkürlich kleine Kontinuität. Die daraus resultierende Normalisierte Aktivierung x ^ ( k ) {\displaystyle Memehat x}}^{(k) hat Null und Einheitsvarianz, wenn ε \epsilon } nicht berücksichtigt wird. Um die Repräsentationsleistung des Netzes wie y i ( k ) =  i ( k ) x ^ i ( k ) + β ( k ) {\displaystyle y_{i}^{(k)}=\gamma (^k) x x x_{i}^{(k)}+\beta {^k)}, wo die Parameter . ( k) \gamma \gamma(k) {k) {k) {k) und {k) {k) \ kbeta ] Formell ist der Betrieb, der die Chargen-Normung durchführt, eine Umwandlung B N γ ( k ) , β ( k ) : x 1. m ( k ) → y 1. m ( k ) {\displaystyle BN_ggiogamma {(^k)},\beta k)}}:x_{1...m((k)}\rightmark y_{1...m...(k) bezeichnete den Batch Normalisierungswandel. Output des BN Transformation y ( k ) = B N γ ( k ) , β ( k ) ( x ( k ) {\displaystyle y^{(k)}=BN_ggiogamma {(^k)},\beta k)((x((k) wird dann an andere Netzschichten weitergegeben, während die normalisierte Output x ^ i) KINGstyle x{\ (k) . x.x}}_}}i(k) die interne Schicht bleibt. Zurückhaltung Die beschriebene BN-Umwandlung ist ein unterschiedlicher Betrieb, und die Kluft zwischen dem Verlust l und den verschiedenen Parametern kann direkt mit der Kettenregel berechnet werden. Konkret hängt  l l  i y i ( k ) {\displaydisplaystyle SSOfrac casapartial l lpartial y_{i((k) von der Wahl der Aktivierungsfunktion ab und die Differenz gegenüber anderen Parametern könnte als Funktion von  l l  i y i ( k ) {\displaystyle ggiofracial l)partial y_{i(k) ausgedrückt werden:  l l  i x ^ i ( k ) =  l l  i y i ( k ) {\ ( k ) {\frac casapartial l)partial 7.8hat x}}_{i((k)}}}= {\ {\ {\ {\ {\ {\               ) ) ) ) ) i = 1 m  l l  i y i ( k ) x ^ i ( k ) {\displayfrac ggiopartial l lpartial \gamma {(^k)}}}=\sum _i=1}{\mffrac casapartial l lpartialy_{i)(k) x_{i}^{(k) ,  l l β β ( k ) = ) i = 1 m  l l  i y i ( k ) {\displaydisplaystyle Memefrac ggiopartial l lpartial \beta {(^k)}}}=\sum _i=1}{\mffrac ggiopartial l lpartial y_{i}^{(k) ,  l l  B B ( k ) 2 =  i i = ) 1 m  l l  i y i ( k ) ( x i ( k ) − μ B ( k ) ) −  k ( k ) 2 (  B B ( k ) 2 +  3 ) − 3 / 2 ) 7.8displaystyle 7.8frac livial l}{\partial \sigma _B((k)^{2==\sum _i=1}^{m lfrac }{\partial y_{i)(k)}(k)}(k) \mu}(k)}(k)}(k)}(k)}2 B psilon 1 m  l l  i y i ( k ) ) ( k )  B B ( k ) 2 +  + +  B l  B B ( k ) 2 1 m  i i = 1 m ( - 2 ) ⋅ ( x i ( k ) − μ B ( k ) ) {\frac livial l}{\partial \mu _B((k)}}}=\sum _i=1}^{mffrac }{\partial y_{iifrac ifrac {k)}}}{\frac {k\gamma (^k){\ {\ {\s{\s{\s{\(k){\s^{s^{s _(k)}} {+\}frac ggiopartial l lpartial \sigma _B((k) 1frac 1 1m _sum _i=1}^{m}(-2)\cdot x_{i((k)}-\mu _B)(k) und  l l ) x )i ( k) =  l l ∂ x ^ i ( k ) 1  B B ( k ) 2 +  + +  l l  B B ( k ) 2 (2 x i ( k ) − μ B ( k ) ) +  l l  B μ B ( k ) 1 m 7.8displaystyle  cuf Racpartial l}{\partial x_{i((k)}}}= racial l}{\ 7.8hat x}}_{i((k)}}}{\frac 1 1sqrt ggiosigma _B)(k)^{2++\epsilon {+\}frac Colapartial l}{\partial \sigma _B((k) 2frac 2(x_{i}^{(k)}-\mu _B((k)})}{m++ 7.8frac ggiopartial l}{\partial l}{\partial \mu _Bmu(k)}}}{\frac 1.m . Gleichgültig mit Batch-Normalisierten Netzen während der Ausbildungsphase hängen die Normalisierungsschritte von den Mini-batten ab, um eine effiziente und zuverlässige Ausbildung zu gewährleisten. In der Gleichgültigkeitsphase ist diese Abhängigkeit jedoch nicht mehr sinnvoll. stattdessen wird der Normalisierungsschritt in dieser Phase anhand der Bevölkerungsstatistiken berechnet, so dass die Produktion von dem Input auf deterministische Weise abhängt. E [ x ( k ) ] Memedisplaystyle E[x^{(k)} und Varianz, Var  [ [ x ( k ) ] Memestyle \operator {Var} [x)(k)}] , werden als: E [ x ( k )] = B [ μ B ( k ] ] ] ] KING E[x^{(k)] } {Var} [x((k)}] m-1m-1EE_{B}[\sigma _B((k)^{2] .Die Bevölkerungsstatistiken sind somit eine vollständige Darstellung der Mini-batten. Das BN-Umwandlungsschritt wird y (k ) = B N γ ( k ) , β ( k ) inf ( x ( k ) ) = ) ( k ) Var  [ [ x ( k ) ] + ε x ( k ) + ( β ( k ) ) ( k ) E [ x ( k ) ] Var  [ [ x ( k ) ] + ) ) TONdisplaystyle yk(k)}=BN_ggiogamma {(^k)},\beta k)}}^{\textinf((x((k)})= {\ {\ {\ {\ {\ {\ {\ {\ {\ k) ) ) q q psilon psilon psilon q k psilon q psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon k q V V V V V V V V V V V V V V V V V V V V V V V V V V V V psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon psilon . Die Parameter werden in dieser Umgestaltung festgelegt, das Standardverfahren der Chargen wird im Wesentlichen auf eine lineare Umwandlung in die Aktivierung angewendet. Vereinbarung der Normalisierung von Batch Zwar ist die Normalisierung der Charge aufgrund ihrer Stärken zu einer beliebten Methode geworden, doch ist der Arbeitsmechanismus der Methode noch nicht gut entwickelt. Wissenschaftler zeigen, dass die interne Kovariatumstellung trotz gemeinsamer Überzeugung nicht durch eine einheitliche Vereinheitlichung erheblich verringert wird. Manche Wissenschaftler weisen die gute Leistung auf, um die objektive Funktion zu lockern, während andere vorschlagen, dass die Entkoppelung der Länge den Grund für ihre Wirksamkeit ist. Batch Normalisierung und interne Kovariation Die Korrelation zwischen der Normalisierung und der internen Kovariation wird weitgehend akzeptiert, wurde jedoch nicht durch experimentelle Ergebnisse unterstützt. Wissenschaftler zeigen vor kurzem mit Experimenten, dass die hypothetische Beziehung nicht genau ist. Vielmehr scheint die erhöhte Genauigkeit mit der Standardisierungsschicht der Charge von der internen Kovariatverschiebung unabhängig zu sein. Hinzufügen von Kovariate-Umkehr zu Batch Normalisierungsschichten Um zu verstehen, ob es eine Korrelation zwischen der Verringerung der Kovariatumwandlung und der Verbesserung der Leistung gibt, wird ein Experiment durchgeführt, um die Beziehung zu verringern. Konkret werden drei Modelle ausgebildet und verglichen: ein Standard-VGG-Netz ohne mengenmäßige Normalisierung, ein VGG-Netz mit Standard-Normungsschichten und ein VGG-Netz mit Standardnormierungsschichten und zufälligem Lärm. Im dritten Modell hat der Lärm nichtzero bedeutend und uneinheitliche Varianz und wird auf Zufall für jede Schicht erzeugt. Anschließend wird es nach der Standardisierungsschicht der Charge hinzugefügt, um eine Kovariatumstellung in die Aktivierung absichtlich einzuführen. Mit diesen drei Modellen werden zwei Beobachtungen abgegeben. Erstens hat das dritte, laut Modell weniger stabile Vertriebene auf allen Ebenen im Vergleich zu den beiden anderen Modellen aufgrund der zusätzlichen Lärmschicht. Trotz des Lärms sind die Ausbildungsgenauigkeit des zweiten und des dritten Modells ähnlich und sind beide höher als das des ersten Modells. Während die internen Kovariationen auf allen Ebenen größer sind, ist das Modell mit der Chargennormalisierung noch besser als das Standard-VGG-Modell. Man könnte daher zu dem Schluss kommen, dass die interne Kovariatumstellung nicht der Beitragsfaktor für die Leistung der Chargen-Normung sein könnte. Messung der internen Kovariate mit und ohne Batch Normalisierungsschichten, da es Hypothese ist, dass die Standardisierungsschichten die interne Kovariatumwandlung verringern könnten, wird ein Experiment eingeführt, um quantitativ zu messen, wie viel Kovariateumsetzung reduziert wird. Erstens muss der Begriff der internen Kovariation mathematisch definiert werden. Konkret wird gemessen, ob die Parameter einer Schicht auf die Aktualisierungen in früheren Schichten reagieren, die Korrelation zwischen den Verlusten vor und nach allen vorangehenden Ebenen wird gemessen, da die Verläufe von der ersten Fortbildungsmethode abweichen könnten. Liegt der durch die Veränderungen in früheren Schichten eingeführte Wandel klein, dann wäre der Zusammenhang zwischen den Verhältnissen in der Nähe von 1. Die Korrelation zwischen den Gefälle wird für vier Modelle berechnet: ein Standard VGG-Netz, ein VGG-Netz mit Chargen-Normungsschichten, ein 25-schichtiges lineares Netz (DLN), das mit einer vollen Streitbeilegung und einem DLN-Netz mit Standard-Normungsschichten ausgebildet wird. Interessanterweise wird gezeigt, dass die Standard-VGG- und DLN-Modelle im Vergleich zu ihren Kollegen höhere Korrelationen aufweisen, was darauf hinweist, dass die zusätzlichen Normalisierungsschichten der Chargen nicht die interne Kovariation reduzieren. Komplementarität der Optimierungslandschaft Manche Wissenschaftler haben vorgeschlagen und bewiesen, dass die Standardisierung der Chargen eine größere Lipschitzness in den Verlust und die Verschlechterung während der Ausbildung bringen könnte und dass diese verbesserte Einfachheit ihre große Leistung erklären könnte. Diese Effekte können durch den Vergleich von VGG-Netzen, die mit der und ohne mengenmäßige Normalisierung ausgebildet sind, beobachtet werden und sind auch in anderen Netzen wie linearen Tiefnetzen konsistent. Konkret wird festgestellt, dass sich die Verluste weniger ändern, und dass die Verluste geringer sind und mehr Lipschitz sind. Außerdem werden die normalisierten Modelle mit Modellen mit unterschiedlichen Normalisierungstechniken verglichen. Konkret arbeiten diese Normalisierungsmethoden, indem sie zunächst den ersten Stand der Aktivierung festsetzen und dann um den Durchschnitt der l p Memestyle l_{p} normalisieren. Diese Methoden haben somit einen größeren Verteilungswechsel, aber eine einfachere Landschaft. Kurz gesagt, diese Modelle liefern ähnliche Leistung wie die Serien normalisierter Modelle. Diese zweigleisige Beziehung könnte daher darauf hinweisen, dass das reibungslose Funktionieren der Optimierungslandschaft einen Beitrag zur überlegenen Leistung der Chargen Normalisierung leisten könnte. Neben der experimentellen Analyse dieser Korrelation ist theoretische Analysen auch vorgesehen, um zu überprüfen, dass die Normalisierung der Chargen zu einer besseren Landschaft führen könnte. Betrachtet man zwei identische Netze, eine enthält eine Vielzahl von Normalisierungsschichten und die andere nicht, so wird das Verhalten dieser beiden Netze dann verglichen. Lesen Sie die Verlustfunktionen als L WELLdisplaystyle L} und L ^ Memestyle Memehat {L}. Lassen Sie sich die Beiträge zu beiden Netzen x WELLdisplaystyle x} , und die Produktion ist Yendisplaystyle y}, für die y = W x KINGstyle y=Wx} , wo W KINGstyle W} ist die Schichtgewichte. Yendisplaystyle y} Im zweiten Netz geht es auch um eine Standard-Diagnostik. Denoteieren Sie die normale Aktivierung als y ^ WELLdisplaystyle Memehat {y} , was Null bedeutet und variabel ist. Lassen Sie sich die transformierte Aktivierung z = . y ^ + β {\displaystyle z=\gamma Memehat {y}}+\beta } und . Memestyle \gamma } und β dampfdisplaystyle \beta } anhalten. Letztendlich verdeutlicht die Standardabweichung über ein Mini-batch y j ^ ∈ R m HANAdisplaystyle Memehat y_{j}}}\in \ Mathematikbb {R} {^m} als  j j {\displaystyle \sigma {_j} .First, es kann gezeigt werden, dass die Neigung zu einem normalisierten,  be y i L ^ | Memestyle \sigdowndown y_{i hat {L|} ist gebunden, mit dem gebundenen Ausdruck, der unter der Bezeichnung  | y i L ^ | 2 ≤  j 2  | j 2 ( | ▽ y i L | 2 − 1 m  L 1 ,  L y i L ed 2 − 1 m  L  i  L  L  L  L  L  L  j  j  2  2 2  j 2 ⟩  j 2 ⟩  j 2 )  2 2  2  2  2  2 2  2  2  2 2  2  2  2  2 2  2  2  2  2  2  2  2 2  2  2  2  2 2  2 2  2  2  2  2  2  2  2 2 ) )  2  2 ) )  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \|triangledowndown y_{i Lhat L}}||2}\leq faser ^2 _sigma _j}^{2}}}{\ Großg (\|} y_{iLL||2}-Getr 1 1m 1langle 1,\triangledowndown y_{iLL\rwinkel ^2}- 1 1m langle \triwinkeldowndown y_{iLL. y)_{j}\rwinkel ^2}{\bigg ) .Since the varinitude stellt die Lipschitzität des Verlustes dar, zeigt diese Beziehung, dass eine Menge normalisierter Netze zu mehr Lipschitzness vergleichsweise führen könnte. Hinweis darauf, dass der gebundene sich straffer wird, wenn die Kluft  i y i L ^ KINGstyle \triangledown y_{i}}{\hat {L} korreliert mit der Aktivierung y i ^ KINGstyle Memehat y_{i}, die ein gemeinsames Phänomen ist.  j 2  j j 2 HANAdisplaystyle 002frakgamma ^2^sigma _j}^{2 ist ebenfalls bedeutsam, da die Varianz häufig groß ist. Zweitens kann die quadratische Form des Verlustes Hessian in Bezug auf Aktivierung in der Bewegungsrichtung gebunden werden (▽ y j L ^ ) T  L L ^  y y j ∂ y j (▽ y j L ^ ) ≤  2 2 () L ^ ∂ y j )T ( L L  L y j  y y j ) (  L L ^  j y j ) ▽ 2  L  L  L  L  L  j  L  j  j  |  |  |  |  |  |  L  L  L  L  L  L  L  L  L  L  |  |  L  |  2  2  2  2  2 y_{j}\partial y_{j)(\triwinkeldown_{y_{j}}{\hat {L))\leq ggiofrac ggiogamma ^2}}{\sigma ^2}}}{\bigg {(\}frac ggiopartial) 7.8hat L}}}{\partial y_{jbigbigg )^T}{\bigg {(\}frac ggiopartial L Lpartial y_{j}\partial y_{jbigbigg )bigg {({\frac ggiopartial ggiohat L}}}{\partial) y_{jbigbigg )-}{\frac ggiogamma  m\sigma {^2}}}\langle \triangledown y_{jLL, cuhat y_{j}}}\rwinkel SSObigg WELL}bigg WELL} 7.8hat L}}}{\partial y_{jbigbigg WELL195}bigg ||2 .Die Verschärfung von  2 2  j {\displaystyle SSOfrac SSOgamma ^2}}{\sigma _j}^{2 zeigt, dass der Verlust Hesian gegen die Mini-Diskriminierung widerstandsfähig ist, während der zweite Begriff auf der rechten Seite darauf hindeutet, dass er reibungsloser wird, wenn der Hesian und das Innenprodukt nicht negativ sind. Liegt der Verlust vor Ort vor, so ist der Hessian positiv halbjährlich, während das Innenprodukt positiv ist, wenn g j ^ displaystyle Memehat g_{j} in Richtung auf das Minimum des Verlusts liegt. Es könnte daher aus dieser Ungleichbehandlung geschlossen werden, dass die Tendenz im Allgemeinen eher prädiktiv wird mit der Standardisierungsschicht der Charge. Anschließend folgt es, die Bindungen im Zusammenhang mit dem Verlust in Bezug auf die normalisierte Aktivierung an einen bestimmten Verlust in Bezug auf die Netzgewichte zu übersetzen: g j ^ ≤ 2  j 2  j 2 ( g j 2 − m μ g j 2 − ) 2 ▽ y j L , y ^ j ⟩ 2  {\ 2 ) {\displaystyle HANAhat g_{j}}}\leq  steuerlicher  steuerlicher  steuerlicher schwunggamma ^2}}{\sigma j}^{2((g_{j}^{2}-m\mu_{g_{j}}^{2}-\lambda  2\langle\triwinkeldowndown y_{jLL, cuhat y}}_{j}\rwinkel {^2)}, wo g j = m x | X g_{j}=max_ {| \leq \lambda \|triangle _W}|2 und g ^ j = m a x | X \|}triwinkel _W Lhat L}}| L2 . Neben der glatteren Landschaft wird weiter deutlich, dass die Chargennormalisierung zu einer besseren Paraphierung mit folgenden Ungleichheiten führen könnte: {0} W**} W_{0}-W**} 1||W**} W**},W_{0}\rwinkel ){^2} , wo W ∗ KINGstyle W**} und W ^ {\ {\hat W^*} die lokalen optimalen Gewichte für die beiden Netze sind. Manche Wissenschaftler argumentieren, dass die vorstehende Analyse die Leistung der Chargen-Normung nicht voll erfassen kann, da der Nachweis nur den größten Eigenwert oder gleichwertig eine Richtung in der Landschaft an allen Punkten betrifft. Es wird vorgeschlagen, dass die vollständige Eigenart berücksichtigt werden muss, um eine schlüssige Analyse vorzunehmen. Kontaminierte Fülle der Optimierungslandschaft bei der Erstisierung, obwohl die Chargennorm ursprünglich eingeführt wurde, um die Kluft- oder Explosionsprobleme zu lindern, leidet ein tiefes Chargennormnetz in der Tat an einer Explosion in der Anfangsphase, egal was es für die Nichtlinearität verwendet. So ist die Optimierungslandschaft sehr weit von einem zufällig paraphierten, tiefen Chargennormnetz entfernt. Genauer gesagt, wenn das Netz L {\displaydisplaystyle L}schichten hat, so hat die Kluft der ersten Schichtgewichte norm > c  L L RARstyle >c\lambda {^L} für einige . > 1 , c > 0 Memestyle \lambda 1,c>0} je nach Nichtlinearität.  non WELLdisplaystyle \lambda } verringert sich, wenn die Chargengröße steigt. . \lambda } verringert sich zum Beispiel auf  1 / ( 1 − 1 ) 67 1.467 RARstyle \pi (\/pi -1)\ca 1.467}, da die Größe der Chargengröße tendenziell unnachlässig ist. Konkret bedeutet dies, dass tiefgreifende Chargennorm-Netze unbeschränkt sind. Dies wird nur durch den Ausstieg aus den restlichen Netzen erleichtert. Diese bedingte Explosion auf der Oberfläche widersprüchlich ist das im vorherigen Abschnitt erläuterte glatte Eigentum, aber in Wirklichkeit sind sie konsequent. In dem vorherigen Abschnitt wird untersucht, wie sich eine einheitliche Chargennorm in einem Netz einfügen lässt, während die Explosion von der stapelnden Anzahlnormen abhängt, die typisch für moderne tiefe Neuralnetze sind. Länge der Richtlinie Entkopplung Es wird argumentiert, dass der Erfolg der Chargen-Normung zumindest teilweise auf den langdirektionen Entkoppelungseffekt, den die Methode vorsieht, gutgeschrieben werden könnte. Durch die Auslegung des Standardverfahrens für die Chargenvereinheitlichung als Reparametrisierung der Gewichtfläche konnte gezeigt werden, dass die Länge und die Richtung der Gewichte nach dem Verfahren getrennt sind und daher gesondert geschult werden könnten. Für eine bestimmte Neuralnetzeinheit mit Input x Memestyle x} und Gewicht Vektor w {\displaystyle w} , d. h. seine Leistung als f ( w) = E x [ φ ( x T w ) ] Memestyle f(w)=E_{x}[\phi (xTT})] , wo φ \phi \phi } die Aktivierungsfunktion ist und die S S = S = E [ x x T ] KINGstyle S=E[xx^{T} .Assume, dass E [ x ] = 0 {\displaystyle E[x]=0} und dass das Spektrum der Matrix S Memestyle S} als 0 · μ =  i m i n ( S ) Memestyle 0 0mu \=lambda {_min}(S) , L = . m x (S) .  L  L  L  L  L  L  L L=\bda {Smaxmax_max) {S)  S, S. Addition der Chargen-Normung in diese Einheit führt zu f B N ( w , ) , β ) = E x [  N ( B N ( x T w ) ] = E x [ ing ( x T w − E x [ x T w ] v a r x [ x T w ] 1 / 2 ) + β ] ] {BN} (w,\gamma ,\gamma \ )=E_{x}[\phi BN(x^{T}w)]=E_{x}{\bigg \[}phi Memebigg (\}gamma {\frac x^{T}w-E_{x[T}[xTTw]}{var_{x^{Tw}[x^{Tw]1/21/2}}})+\beta 574bigg ){\}g .g .g .g ., Definition Der Varianzbegriff kann so vereinfacht werden, dass v a r x [ x T w ] = w T S w ggiostyle var_{x}[xTT}w]=wTT}Sw .Assume, dass x {\displaystyle x} Null bedeutet und β dampfstyle \beta } freigesetzt werden kann, dann folgt es, dass f B N ( w , γ ) = E x [  T ( T x T w ) 1 / 2 ) ] KINGstyle f_{B}(w,\gamma } {g } \g \g \g \g \g }  |g  | ( S ammagamma  S  S  S  S  S  S  S  S  S  S  S  S  S  2  2  2  2  2  S  2  S  S  2  2  2 {w==\gamma WELLfrac w}{|w|w|_{s , und {\ {\displaystyle \gamma } und w {\displaystyle w} Rechnung für seine Länge und Richtung. Dieses Eigentum könnte dann genutzt werden, um die schnellere Konvergenz der Probleme mit der einheitlichen Chargennutzung zu beweisen. Lineare Konvergenz des Least-Square-Problems mit Batch Normalisierung Mit der Reparametrisierungsverdolmetschung konnte dann nachgewiesen werden, dass die Anwendung der Chargen-Normung auf das normale Problem der kleinsten Quadrate zu einer linearen Konvergenz der Absterben führt, die schneller ist als die regelmäßige Abwanderung mit nur sub-linearer Konvergenz.Hinweis auf das Ziel, ein normales, mindestens Quadratmeter Problem als m i n w ~ ∈ R d f O L S ( w ~ ) = m i n w ~ ∈ R d (E x , y [ ( y − x T w ~ ) 2 ] ) = m i n w ~ ∈ R d ( 2 u T w ~ + w ~ T S w ~ ~ KINGstyle min_{{\tilde {w}}\in R)dff_{OLS}(Gettilde w}})=min_{{\tilde {winin R^{d((E_{x,y}[(y-xTtilde w}}) w2}])=min_.tilde {w Rin R^{d((2uTT}{\T}{\tilde w}}+ WELLtilde w{\T}Sggiotilde {w)}, wo u = E [ − y ] KINGstyle u=Ex} {w==\gamma 7.8frac w||w|_{s , das Ziel wird somit m i n w  R R d ) { 0 } ,  R R f O L S ( w , )     ) = m i n w  R R d   { 0 } ,    R R ( 2  u u T · [ ]  | 2 ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) R)d)backslash {0\},\gamma \in R}f_{OLS}(w,\gamma )=min_{w\in R.d.backslash {0\},\gamma \in R}{\bigg (}2\gamma ggiofrac u^{T}w||w|_{S}+\gamma ^2}}}{\bigg ) , wo 0 vom Nenner ausgeschlossen ist, 0 zu vermeiden. Da das Ziel in Bezug auf   Memedisplaystyle \gamma } ist, könnte der optimale Wert berechnet werden, indem der teilweise Derivat des Ziels gegen   HANAstyle \gamma } auf 0 festgesetzt wird. Das Ziel könnte weiter vereinfacht werden, um m i n w  R R d   { 0 }  be ( w ) = m i n w   R d   { 0 } ( - w T u T w T w T W T S w ) Memedisplaystyle min_{w\in R)d)backslash {0\}}\rho (w)=min_{w\in Rsdsbacks 0\}}{\bigg (-ffrac w^{T}uu^{T}w^{T}Sw T})g )  . stellt fest, dass dieses Ziel eine Form der allgemeinen Raykia quotient ρ ~ ( w ) = w T B w T B w T W T A w T A w A w {\displaystyle Memerho (w)=grof wT}BTwTT}Aw , wo B d × d \d \d \d Matrix \d \d \d Kennzeichnend dafür ist, dass die sprungweisende Konvergenz der allgemeinen Raykia quotient  1 1  of ( w t + 1 )  of (w t + 1 −  2 2 ) ≤ ( 1 −  1 1  i 2  i 1  -  i m i n ) 2 t  1 1  w 1 )  w ( w t )  w ( w t ) {\ 2  2 )  2 2 ) ) {\ {\ {\ {\ {\ {\ {\ {\ {\ 2  2 ) ) ) ) ) ) ) ) ) ) 2 ) )  2  2 ) ) ) ) ) ) ) ) ) ) ) ) 2 )  2  2 ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) {_1}-\rho (w_{t+1})}{\rho (w_{t+1}-\lambda {_2})}}\leq ggiobigg (}1-Barfrac ggiolambda {_1}-\lambda _2}}{\lambda {_1}-\lambda _min}}}{\bigg )^2t}{\frac Memelambda {_1}-\rho (w_{t})}{\rho (w_{t})-\lambda {_2} , wo  1 1 Memestyle \lambda u {1} der größte Eigenwert von B },  2 2 sys \bda \bda {bda {bda {bda {bda ] \ m2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \cuu } Konkret prüfen Sie die Höhenstufen der Form w t + 1 = w t  of t ρ )  w ) ( w t ) Memestyle w_{t+1}=w_{t}-\eta {_t)triwinkeldown \rho (w_{t)} mit Stufengröße η t = w T S w t 2 L | ρ ( w t ) _t} w_{tTT}Sw_{t}}{2L\rho (w_{t)} und beginnend von ρ ( w 0 )  0 0 Memestyle \rho (w_{0})\neq 0} , dann  ( (w ) .  w (w ) mu  1 (  ) ) } ( 1  1 } } 2 t ( 0 0  0  0  0 ∗ ∗ ∗ ∗                             * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *  Lineare Konvergenz des Lern-Halbraumproblems mit Batch Normalisierung Das Problem des Erlernens von Halbräumen bezieht sich auf die Ausbildung des Perceptron, das die einfachste Form des Neuralnetzes ist. Das Optimierungsproblem in diesem Fall ist m i n w ~ ∈ R d F L H ( w ~ ) = E y , x [ ) ( z T w ~ ) ] KINGstyle min_{{\tilde {w}}\in R.dff_{LH}(Gettilde w}})=E_{y,x}[\phi (zTtilde {w)}], wo z = y x {\displaystyle z=-yx} und φ KINGstyle \phi } eine willkürliche Verlustfunktion ist. Unter Hinweis darauf, dass   KINGstyle \phi } unbegrenzt anders ist und einen gebundenen Derivat hat. Assume, dass die objektive Funktion f L H WELLdisplaystyle f_{LH} ist ζ \zeta } -smooth und dass eine Lösung  i = ein r m i    |[  f  f  |  |  |  f  w  |         ] {*}{^\infty } . Auch übernehmen Z Memedisplaystyle z} eine multivariate normale Zufallsvariable. Mit der Gaussian-Annahme kann gezeigt werden, dass alle kritischen Punkte auf der gleichen Linie liegen, für jede Wahl der Verlustfunktion   {\displaystyle \phi } . Speifisch, die Kluft zwischen L H ONAdisplaystyle f_{LH} könnte als  w w ~ L H ( w ~ ) = c 1 ( ~ ) u + c 2 ( w ~ ) S w ~ Derivate \triwinkeldown Meme_tilde wLf_{LH}(Kapitel)=c_{1}(Kapitel)++c_{2}(Kapitel) Smogtilde {w} {d) {d) {d) · · · } } (T) }  2  2  2  2  2  2 2  2  z  2  2  2  2 Indem man den Aufwärtstrend auf 0 gesetzt hat, kommt es daher darauf an, dass die gebundenen kritischen Punkte w ~ {\ ·  be  1  1  1  1  1 {\  1  u  u − 1 u RARstyle ggiotilde W__{*} S-1-1}u , wo g ∗ {\displaystyle g**_ von w ~ ∗ KINGstyle Memetilde w_*} und φ KINGstyle \phi } abhängt.Kombination dieses globalen Eigentums mit langer Entkoppelung könnte daher bewiesen werden, dass sich dieses Optimierungsproblem linear annähert. Erstens ist eine Differenzierung des Abgangs mit der Chargen-Normung, dem Grad der Normalisierung (GDNP) für die objektive Funktion m i n w  d R d } { 0 }  of  R  R  R R L H ( w , {\ {\ ) faserstilstyle min_w\in R.d.backslash {0\},\gamma \in R}f_{LH}(w,\gamma )}, so dass die Richtung und Länge der Gewichte gesondert aktualisiert werden. Hinweis auf das Einstellungskriterium der GDNP als h ( w t ,  of t ) = E z [   ′ ( z T w ~ t ] (u T w t ) - E z [   ′′ ( z T w ~ t ] (u T w t ) 2 {\displaystyle h(w_{t},\gamma t})=E_{z}[\phi '(zTT wtilde w__{t})](uwT}w_{t})-E_{z}[\phi '(zTTtilde w__{t})](uTT} {t})2 Let. KINGstyle s_{t}=s(w_{t},\gamma _t} w_{t}|_{S33LLg_{t}h(w_{t},\gamma {_t)}. für jeden Schritt, wenn h ( w t ,  t t )  0 0 KINGstyle h(w_{t},\gamma {_t})\neq 0} , dann aktualisieren Sie die Richtung wie w t + 1 = w t ▽ f ( w t , ) t {\ t  t t ) t ) ) t ) {\ {\ ) t ) ) {\ t ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) w_{t+1}=w_{t}-s_{t}\triwinkeldown w}f(w_{t},\gamma {_t)} .Die Länge nach  t t = B i s e c t i n ( T s , f , w t ) {\displaystyle \gamma t}=Bisection(T_{s},f,w_{t) , wo B i s e c t i n i n ( ) Memestyle Bisection Bisection) ist der klassische Binomus, und Ts s sys T_ fis {t},f,w_{t) ist der Gesamtschritt. Kennen Sie die Gesamtzahl der Iterationsarten als T d Memestyle T_{d} , dann ist die Endergebnisse der GDNP w ~ T d =  T T d · T d.V. w__{T_{d==\gamma T_{d}}{\frac {T_{T_{d||w_{T_{d}}|_{S . Der GDNP-Algorithmus verändert somit den Schritt zur Normalisierung der Charge, um die mathematische Analyse zu erleichtern. In GDNP kann nachgewiesen werden, dass der teilweise Derivat von F L H {\displaystyle f_{LH} gegen die Länge der Komponente auf Null an linearem Satz angeglichen wird, so dass (   L f L H ( w t, a t ( T s )) ) 2 ≤ 2 − T s  | ( 0 ) − a t ( 0 ) · μ 2 {\displaystyle (\partial Meme_gamma f_{LH}(w_{t},a_{t((T_{s}))}2{\leq ggiofrac 2^{-T_{s}}\zeta b_{t(0)(0)}-a_{t}^{(0)}||mu {^2} , wo ein t ( 0 ) Memestyle a_{t_(0) und b t 0 Memestyle b_{t}^{0 sind die beiden Ausgangspunkte des Bisection-Algorithmus auf dem linken und rechts. Mehr, für jede Iteration, die Norm des Gefälles von F L H {\displaystyle f_{LH} in Bezug auf w {\displaystyle w} linear, so dass dies, wenn man mithilfe von  2 2  S ( 0 0)  | ( S  L  L  0  L  L  0  0  0  0  0) KINGstyle w_{t}|_{S22} f_{LH}(w_{t},g_{t})|_{S-1-1-12qleqggiobigg (}1-Barfrac WELLmu L}}{\bigg ){^2t}\ Phi  2ammagamma _t}^{2}(\rho (w_{0})-\rho ^{*} .Kombinierung dieser beiden Ungleichbehandlungen könnte daher eine gebundene Voraussetzung für die ziehungen in Bezug auf w ~ T d {\displaystyle ggiotilde erhalten. w__ T_{d :  |  w ~ f ( w ~ T d ) w__{T_{d)|2}\leq) 7.8bigg (}1-Barfrac 7.8mu L}}{\bigg )^2T_{d) Phi  2} (w_{0})-\rho )* 2+ 2frac 2^{-T_{s.zeta b_{t}(0)}-a_{t}^{(0)} |2}, so dass der Algorithmus linear garantiert wird. Obwohl der Beweis auf der Annahme von Gausssian-Input steht, wird er auch in Experimenten gezeigt, dass die GDNP die Optimierung ohne diesen Druck beschleunigen könnte. Lineare Konvergenz der Neuralnetze mit Batch Normalisierung sieht eine mehrschichtige Perceptron (MLP) mit einer versteckten Schicht und m varistyle m} versteckte Einheiten mit Kartierung von Input x  d R d {\displaystyle x\in R Rd} zu einem scalar Output, der als F x ( W ~ Θ ) = ) bezeichnet wird i = 1 m  i i ) ( x T w ~ ( i ) ) 7.8displaydisplaystyle F_{x}(Ehetilde {W}},\Theta )\=sum _i=1themthetheta {_iTphi (xTT wtilde w((i) , wo w ~ ( i ) Memedisplaystyle Memetilde w((i) und  i i KINGstyle \theta \theta {i} die Eingangs- und Outputgewichte der i KING i} und ., die Funktion und die Funktion der Faser ist \h. Die Eingangs- und Outputgewichte könnten dann mit m i n W ~ ,  be ( f N (W ~ , ) ) = E y , x [ l ( − y F x ( W ~ ~ ,   ) ) ) ) ) 7.8displaystyle min_{{\tilde {W}},\Theta f_{NN}(E_{W}},\Theta )=E_{y,x}[l(-yF_{x}(Kapsilien {W}},\Theta )])} , wo l KINGstyle l} eine Verlustfunktion ist, W ~ { · { · 1 ) . . . . . . . . . . . . . . . . . W== wtilde w(1)(1)},..., cutilde w}}^{(m)\ , und  = = { ) ( 1 ) , . , θ ( m) } faserstil \Theta Meme=theta {(3),...,\theta { m)^ .Consider fix   WELLdisplaystyle \Theta } und Optimierung nur W ~ {\displaystyle Memetilde {W} , es kann gezeigt werden, dass die kritischen Punkte f N N ( W ~ ) {\displaystyle f_NN}(Zahl) {W)} einer bestimmten versteckten Einheit i KINGstyle i}, w ^ ( i) KINGstyle 7.8hat w((i) , alle auf einer Linie je nach Informationen in die neue Informationsschicht, c) . S − 1 u {\displaystyle WELLhat w}}^{(i)}= Finanzhat c((i)}S-1-1}u , wo c ^ ( i)) R KINGstyle WELLhat c((i)}\in R} ist ein scalar, i = 1 , . . , m displaystyle i=1,...,m} . Dieses Ergebnis könnte durch die Festlegung der Kluft der f N VILLEdisplaystyle f_{NN} auf Null und die Lösung des Systems der Gleichungen bewiesen werden. Wenden Sie den GDNP-Algorithmus an dieses Optimierungsproblem an, indem er die Optimierung über die verschiedenen versteckten Einheiten verändert. Konkret laufen GDNP für jede versteckte Einheit, um die optimale W Memestyle W} und   {\displaystyle \gamma } . Mit derselben Wahl des Einstellungskriteriums und der Schritte folgt sie: 2 t C (  ( ( w 0 ) ∗ ) )  + ) + 2 − T s ( i )  | [ ] ) 0 ) − a t ( 0 ) · μ 2 {\displaystyle \|tridown 7.8_tildew((i)ff( {\f({\(i)} 7.8bigg (}1-Barfrac 7.8mu L}}{\bigg ^2t}C(\rho(w_{0})-\rho )*}{+ffrac 2T-T_{s((i)}}\zeta b_{t}^{(0)}-a_{t}^{(0)} |2} .Since theparameter jeder versteckten Einheit linear, das gesamte Optimierungsproblem hat eine lineare Konvergenzquote. Link Ioffe, Sergey; Szegedy, Christian (2015) " Batch Normalisierung: Beschleunigung der Deep Network Training durch Reducing Interner Kovariatum," ICML'15: Konferenz zum Thema "Internationale Konferenz über Maschinenbau" - Band 37, Juli 2015 Seiten 448–456