Computerarchitektur ist eine Reihe von Regeln und Methoden, die die Funktionalität, Organisation und Einführung von Computersystemen beschreiben. Manche Definitionen der Architektur definieren sie als Beschreibung der Fähigkeiten und des Programmierungsmodells eines Computers, aber nicht als besondere Umsetzung. In anderen Definitionen ist die Computerarchitektur mit Anweisungsform, Mikroarchitekturdesign, Logikdesign und Umsetzung verbunden. Geschichte Die erste dokumentierte Computerarchitektur war in der Korrespondenz zwischen Charles Babbage und Ada Lovelace, die den Analysemotor beschreibt. Konrad Zuse bezeichnete beim Bau des Computers Z1 im Jahr 1936 in zwei Patentanträgen für seine künftigen Projekte, die in der gleichen Lagerung für Daten, d.h. das gespeicherte Programm, aufbewahrt werden könnten. Zwei weitere frühe und wichtige Beispiele sind: John von Steiners Papier von 1945, Erster Entwurf eines Berichts über die EDVAC, der eine Organisation logischer Elemente beschrieben hat, und Alan Turings detailliertere Vorschläge für elektronisches Computer für den automatischen Rechenmotor, 1945 und das John vonheimer Papier. Der Begriff „Architektur“ in der Computerliteratur kann auf die Arbeit von Lyle R. Johnson und Frederick P. Brooks, Jr., Mitglied der Abteilung Maschinenorganisation, im Jahr 1959 in IBMs Hauptforschungszentrum zurückverfolgt werden. Johnson hatte die Gelegenheit, eine eigene Forschungsmitteilung über die Entfernung, einen IBM- entwickelten Supercomputer für Los Champs National Laboratory (zu der Zeit, die als Los Cots Scientific Laboratory bekannt ist) zu schreiben. Zur Beschreibung des Detailniveaus für die Erörterung des plausiblen embellierten Computers stellte er fest, dass seine Beschreibung von Formaten, Gebrauchsmustern, Hardware-Parametern und Geschwindigkeitsverbesserungen auf dem Niveau der „Systemarchitektur“ lag, die besser als „Maschinenorganisation“ schien. Künftig öffneten Brooks, ein Stretch-Designer, Kapitel 2 eines Buchs namens Planung eines Computersystems: Projektbereich: „Computerarchitektur, wie andere Architektur, ist die Art der Bestimmung der Bedürfnisse des Benutzers einer Struktur und dann die Gestaltung, um diese Anforderungen so effizient wie möglich in wirtschaftlichen und technologischen Zwängen zu erfüllen.“Brooks ging auf die Entwicklung der IBM System/360 (jetzt als IBM z Series bezeichnet) Linie von Computern ein, in der „Architektur“ ein Begriff „Was der Nutzer wissen muss“. Später kamen Computernutzer in viele weniger explizite Weise zum Einsatz. Die frühesten Computerarchitekturen wurden auf Papier entworfen und anschließend direkt in das endgültige Hardware-Format eingebaut. Später wurden Computerarchitektur-Prototypen physisch in Form eines transistor-transistor-logs (TTL)-Computers gebaut – wie die Prototypen der 6800 und der PA-RISC – getestet und tweakiert, bevor sie sich auf die endgültige Hardwareform verpflichten. Nach den 90er Jahren werden in der Regel neue Computerarchitekturen gebaut, getestet und tweakediert – in einigen anderen Computerarchitekturen in einem Computerarchitektur Simulator, oder innerhalb eines FPGA als Soft-Mikroprozessor oder beide – vor dem Abschluss der Hardwareform. Unterkategorien Die Disziplin der Computerarchitektur umfasst drei Hauptkategorien: Lehrplanarchitektur (ISA): definiert den Maschinencode, den ein Verarbeiter gelesen und handelt, sowie die Wortgröße, die Speichermodus, Prozessorregister und Datentyp. Microarchitecture: auch als „Computerorganisation“ bekannt, beschreibt dies, wie ein bestimmter Prozessor die ISA umsetzen wird. Die Größe der CPU-Cache eines Computers ist ein Problem, das in der Regel nichts mit der ISA zu tun hat. Systemdesign: umfasst alle anderen Hardwarekomponenten innerhalb eines Rechensystems, wie Datenverarbeitung mit Ausnahme der CPU (z.B. direkter Speicherzugang), Virtualisierung und Mehrverarbeitung In der Computerarchitektur gibt es andere Technologien. Folgende Technologien werden in größeren Unternehmen wie Intel eingesetzt und wurden 2002 auf 1 % aller Computerarchitektur geschätzt: Makroarchtecture: Architekturschichten mehr abstrakter als Mikrobau-Versammlungsanleitung: Ein intelligenter Pool kann eine abstrakte Montagesprache in eine Gruppe von Maschinen in leicht unterschiedliche Maschinensprache für unterschiedliche Umsetzungen umwandeln. Programmer-visible Makroarchitecture: Mehrsprachigkeitsinstrumente, wie z.B. die Zusammenstellungsstellen, können eine einheitliche Schnittstelle oder einen Vertrag an Programmierer mit ihnen festlegen, Unterschiede zwischen den zugrunde ISA, UISA und Mikroarchitectures. Beispielsweise definieren die C, C,+ oder Java-Normen unterschiedliche programr-visible Makroarchitekturen. Mikrocode: Mikrocode ist Software, die Anweisungen zur Verwendung auf einem Chip enthält. Es handelt sich um eine Verpackung rund um die Hardware, die eine bevorzugte Version der Hardware-App-Schnittstelle darstellt. Diese App-Übersetzungsanlage bietet Chipdesignern flexible Optionen: E.g.A neue verbesserte Version des Chips kann den Mikrocode verwenden, um die genauen gleichen Anweisungen wie die alte Chip-Version zu präsentieren, so dass alle Software, die auf diese Anleitung ausgerichtet ist, auf dem neuen Chip laufen, ohne Änderungen zu erfordern. E.g. 2.Microcode kann eine Vielzahl von Unterrichtssets für denselben zugrundeen Chip vorstellen, damit er eine breitere Palette an Software betreiben kann. UISA: Gebrauchsanleitung Set Architektur, bezieht sich auf eine der drei Untersätze der RISC-Prozessoren von PowerPC RISC Prozessoren. UISA-Unterset sind die RISC-Anweisungen von Interesse an Anwendungsentwicklern. Die anderen beiden Untergruppen sind VEA (Virtual Environment Architecture) Anweisungen, die von Entwicklern von Virtualisierungssystemen und OEA (Operating Environment Architektur) verwendet werden, die von den Betriebssystemen verwendet werden. Pin-Architektur: Die Hardware funktioniert, dass ein Mikroprozessor eine Hardwareplattform bereitstellen sollte, z.B. die x86 legt A20M, FERR/IGNNE oder FLUSH fest. Pressemitteilungen, die der Verarbeiter ausstellen sollte, damit externe Klärchen ungültig werden können (bestimmt). Pin-Architekturfunktionen sind flexibler als ISA-Funktionen, weil externe Hardware sich an neue Kodizes anpassen kann oder sich von einer Anzeige ändern kann. Die Begriffsarchitektur passt an, weil die Funktionen für kompatible Systeme bereitgestellt werden müssen, auch wenn die detaillierten Methoden geändert werden. Rollendefinition Computerarchitektur ist besorgt über Leistungsausgleich, Effizienz, Kosten und Zuverlässigkeit eines Computersystems. Um das Gleichgewicht dieser konkurrierenden Faktoren zu veranschaulichen, kann die Unterrichtsstruktur genutzt werden. Mehr komplexere Lehrpläne ermöglichen es den Programmierern, mehr Raumfahrteffizienzprogramme zu schreiben, da eine einzige Anleitung eine höhere Auswahl (wie z.B. die x86-Anbindung) codieren kann. Länger- und komplexere Anweisungen gehen jedoch länger für den Verarbeiter zur Decode und können für eine wirksame Umsetzung teurer werden. Die zunehmende Komplexität einer großen Unterrichtsform schafft auch mehr Spielraum für Unzuverlänglichkeit, wenn Anweisungen in unerwarteter Weise interagieren. Die Umsetzung umfasst integrierte Schaltdesign, Verpackung, Energie und Kühlung. Optimierung des Entwurfs erfordert die Vertrautheit mit Sammlern, Betriebssystemen, um Logikdesign und Verpackung. Lehrplanarchitektur Eine Anweisungsform (ISA) ist die Schnittstelle zwischen der Software und der Hardware des Computers und kann auch als Ansicht des Programmierers angesehen werden. Computer verstehen nicht hochwertige Programmiersprachen wie Java, C,+ oder die meisten verwendeten Programmiersprachen. Ein Verarbeiter versteht nur Anweisungen, die in numerischer Form, in der Regel als binäre Nummern, codiert werden. Software-Werkzeuge, z.B. Zusammenstellungen, übersetzen diese hohen Sprachen in Anweisungen, die der Verarbeiter verstehen kann. Neben Anweisungen definiert die ISA Gegenstände im Computer, die einem Programm zur Verfügung stehen – z.B. Datentypen, Register, Adressen und Gedächtnis. Anweisungen finden Sie diese verfügbaren Gegenstände mit Registerindexen (oder Namen) und Speicherlösungen. Die ISA eines Computers wird in der Regel in einem kleinen Anleitungshandbuch beschrieben, in dem beschrieben wird, wie die Anweisungen codiert sind. Kurz (vagely) mnemonic-Namen können auch für die Anweisungen definiert werden. Die Namen können durch ein Software-Entwicklungsinstrument namens Ein Pool anerkannt werden. Ein Sammler ist ein Computerprogramm, das eine humanible Form der ISA in eine computerlesbare Form übersetzt. Disassemblers sind auch weit verbreitet, in der Regel in Debuggers und Softwareprogrammen, um Funktionsstörungen in binären Computerprogrammen zu isolieren und zu korrigieren. ISA unterscheiden sich in Qualität und Vollständigkeit. Eine gute ISA-Regelung zwischen Programmanbietern (wie leicht der Code ist zu verstehen), Größe des Codes (wie viel Code erforderlich ist, um ein spezifisches Handeln durchzuführen), Kosten des Computers zur Auslegung der Anweisungen (mehr Komplexität bedeutet mehr Hardware, die benötigt werden, um die Anweisungen zu entschlüsseln und auszuführen) und Geschwindigkeit des Computers (mit komplexerer Decode-Hardware ist mehr Zeit). Gedächtnisorganisation legt fest, wie Anweisungen mit dem Gedächtnis interagieren und wie das Gedächtnis mit sich interagieren kann. Emulatoren können während der Konversion Programme durchführen, die in einem vorgeschlagenen Lehrplan geschrieben sind. Moderne Emulatoren können Größe, Kosten und Geschwindigkeit messen, um festzustellen, ob eine bestimmte ISA ihre Ziele erreicht. Computerorganisation Computerorganisation hilft, leistungsbasierte Produkte zu optimieren. Software-Ingenieure müssen beispielsweise die Verarbeitungsleistung von Prozessoren kennen. Sie müssen die Software optimieren, um die beste Leistung für den niedrigsten Preis zu erzielen. Dies kann eine ausführliche Analyse der Computerorganisation erfordern. Beispielsweise müssen die Designer die Karte so gestalten, dass die meisten Daten so schnell wie möglich verarbeitet werden können. Computerorganisation hilft auch, die Auswahl eines Prozessors für ein bestimmtes Projekt zu planen. Multimedia-Projekte benötigen einen sehr schnellen Datenzugriff, während virtuelle Maschinen schnelle Unterbrechungen benötigen. Manchmal brauchen bestimmte Aufgaben auch zusätzliche Komponenten. Zum Beispiel benötigt ein Computer, der in der Lage ist, eine virtuelle Speicher-Hardware zu betreiben, damit das Speicher verschiedener virtueller Computer getrennt werden kann. Computerorganisation und Merkmale beeinflussen auch den Stromverbrauch und die Prozessorkosten. ImplementierungOnce eine Anleitung und Mikro-Architektur wurden entworfen, eine praktische Maschine muss entwickelt werden. Dieses Designverfahren ist die Umsetzung. Umsetzung ist in der Regel nicht als architektonisches Design, sondern eher Hardware-Design Engineering anzusehen. Umsetzung kann weiter in mehrere Schritte unterteilt werden: Logistische Umsetzung entwirft die Schaltkreise, die auf einer Logikebene erforderlich sind. Rollendurchführung umfasst transistor-Level-Designs von Grundelementen (z.B. Tore, Multiplexers, Latchen) sowie einige größere Blöcke (ALUs, Caches usw.), die auf der Logikebene oder sogar auf der physischen Ebene umgesetzt werden können, wenn das Design anfordert. körperliche Umsetzung zieht physische Schaltkreise. Die verschiedenen Leitungskomponenten werden in einem Chipbodenplan oder an Bord platziert, und die Verbindungsleitungen werden geschaffen. Designvalidierung prüft den Computer als Ganzes, um zu sehen, ob er in allen Situationen und allen Zeitpunkten funktioniert. Sobald der Entwurf abgeschlossen ist, wird das Design auf der Logiksebene anhand von Logikmulatoren getestet. Jedoch ist dies in der Regel zu langsam, um einen realistischen Test durchzuführen. Nach Korrekturen, die auf dem ersten Test basieren, werden Prototypen mit Feld-Programmable Gate-Arrays (FPGAs) gebaut. Die meisten Hobbyprojekte stehen an dieser Stelle. Letzter Schritt ist es, Prototypen integrierte Schaltkreise zu testen, die möglicherweise mehrere Redesigns erfordern. Für CPUs wird der gesamte Umsetzungsprozess unterschiedlich organisiert und wird häufig als CPU-Design bezeichnet. Designziele Die genaue Form eines Computersystems hängt von den Zwängen und Zielen ab. Computerarchitekturen in der Regel Handel mit Standards, Leistung, Kosten, Speicherkapazität, Verspätung (Lage ist die Menge der Zeit, die sie für Informationen von einem Node auf Reisen in die Quelle benötigt) und Durchsatz. Manchmal sind auch andere Faktoren wie Merkmale, Größe, Gewicht, Zuverlässigkeit und Erweiterungsfähigkeit. Die häufigste Regelung ist eine eingehende Energieanalyse und zeigt, wie der Stromverbrauch niedrig gehalten und gleichzeitig eine angemessene Leistung erhalten bleibt. Leistung moderne Computerleistung wird oft in Anweisungen pro Zyklus (IPC) beschrieben, die die Effizienz der Architektur bei jeder Taktfrequenz messen; eine schnellere IPC-Quote bedeutet, dass der Computer schneller ist. ältere Computer hatten IPC so gering wie 0,1 während moderne Prozessoren in der Nähe von 1. Superscalar-Verarbeitern drei bis fünf IPC erreichen können, indem sie mehrere Anweisungen pro Uhr-Zyklus erfüllen. maschinenlesbare Anweisungen wären irreführend, weil sie unterschiedliche Arbeitsmengen in verschiedenen ISAs ausüben können. Die Lehre in den Standardmessungen ist nicht die ISA-Anschrift, sondern eine Messeinheit, die in der Regel auf der Geschwindigkeit der VAX Computerarchitektur basiert. Viele Personen, die verwendet werden, um die Geschwindigkeit eines Computers bis zur Uhr (normalerweise im MHz oder im GHz) zu messen. Dies bezieht sich auf die Zyklen pro Sekunde der Hauptuhren der CPU. Diese Parameter sind jedoch etwas irreführend, da eine Maschine mit einer höheren Taktrate möglicherweise nicht unbedingt höhere Leistung haben kann. Infolgedessen sind die Hersteller von der Taktgeschwindigkeit als Leistungsmaßnahme weggefallen. Andere Faktoren beeinflussen Geschwindigkeit, wie die Mischung funktionaler Einheiten, Busgeschwindigkeiten, verfügbares Gedächtnis und Art und Reihenfolge der Anweisungen in den Programmen. Es gibt zwei große Geschwindigkeitsarten: Verspätung und Durchsatz. Missstand ist die Zeit zwischen dem Beginn eines Prozesses und seinem Abschluss. Durchput ist der Arbeitsaufwand pro Stück. Interrupt verspätet ist die garantierte maximale Antwortzeit des Systems auf ein elektronisches Ereignis (etwa wenn die Festplattenantriebe einige Daten bewegen). Leistung ist von einer sehr breiten Palette von Gestaltungsentscheidungen betroffen – z.B. die Leitung eines Verarbeiters, der in der Regel zu spät ist, macht aber einen besseren Durchsatz. Computer, die Kontrollmaschinen in der Regel nur geringe Unterbrechungen erfordern. Diese Computer arbeiten in einem Echtzeit-Umfeld und scheitern, wenn ein Betrieb nicht in einer bestimmten Zeit abgeschlossen ist. Computerkontrollierte Antiblockierbremsen müssen beispielsweise innerhalb einer vorhersehbaren und begrenzten Zeit nach dem Bremshebel oder einem anderen Ausfall der Bremse in Betrieb nehmen. Benchmarking berücksichtigt alle diese Faktoren, indem die Zeit gemessen wird, in der ein Computer über eine Reihe von Testprogrammen läuft. Obwohl Benchmarking Stärken zeigt, sollte es nicht sein, wie Sie einen Computer wählen. Häufig werden die gemessenen Maschinen auf verschiedene Maßnahmen aufgeteilt. Zum Beispiel könnte ein System wissenschaftliche Anwendungen schnell handhaben, während ein anderes Videospiel reibungsloser gestalten könnte. Des Weiteren können die Designer ihre Produkte durch Hardware oder Software gezielt auswerten und spezielle Merkmale hinzufügen, die eine bestimmte Benchmark ermöglichen, schnell auszuführen, aber keine ähnlichen Vorteile für allgemeine Aufgaben bieten. Leistungseffizienz ist eine weitere wichtige Messung in modernen Computern. Mehr Leistungsfähigkeit kann häufig für niedrigere Geschwindigkeit oder höhere Kosten gehandelt werden. MIPS/W (Millionen von Anweisungen pro Sekunde pro Kilowatt) ist die typische Messung des Stromverbrauchs in der Computerarchitektur. moderne Schaltkreise haben weniger Macht je Transistor, da die Zahl der Transisten pro Chip wächst. Dies ist, weil jeder transistor, der in einem neuen Chip eingebaut wird, seine eigene Stromversorgung erfordert und neue Wege benötigt, um zu bauen. Jedoch beginnt die Zahl der Transisten pro Chip mit einer langsameren Rate. Leistungseffizienz beginnt daher so wichtig, wenn nicht wichtiger als die Installation von mehr und mehr Transistoren in einen einzigen Chip. Jüngste Prozessor-Designer haben diesen Schwerpunkt deutlich gezeigt, da sie mehr Gewicht auf die Leistungsfähigkeit legen als so viele Transisten möglichst in einen einzigen Chip. In der Welt von eingebetteten Computern ist die Leistungsfähigkeit seit langem ein wichtiges Ziel für den nächsten Durch- und Ablauf. Veränderungen der Marktnachfrage sind in den letzten Jahren langsamer gestiegen, verglichen mit Verbesserungen bei der Leistungsreduzierung. Dies wurde am Ende des Moore-Gesetzes und der Nachfrage nach längerer Batterieleben und einer Verringerung der Größe für mobile Technologien angetrieben. Diese Veränderung im Fokus von höheren Uhrraten auf den Stromverbrauch und die Miniaturisierung lässt sich durch die signifikanten Senkungen des Stromverbrauchs deutlich machen, wie etwa 50 %, die Intel in ihrer Freisetzung der Haswell-Mikroarchitecture gemeldet hatte; wo sie ihren Leistungsindikator von 30 bis 40 Watt auf 10-20 Watt herabgesetzt haben. Vergleicht dies der Verarbeitungsgeschwindigkeit von 3 GHz auf 4 GHz (2002 bis 2006), so ist zu sehen, dass der Schwerpunkt in Forschung und Entwicklung von der Uhr Frequenz verlagert und sich auf weniger Energie verlagert und weniger Raum nimmt. Siehe auch den Vergleich von CPU-Architekturen Computer Hardware Design Thermopunkt Harvard Architektur (Modifizierte) Dataflow Architektur Verkehr löste Architekturreconfigurable Rechen Einfluss des IBM PC auf den persönlichen Computermarkt Orthogonal Unterrichts Set Softwarearchitektur von der IDE-Architektur Flynn’s Taxonomy Referenzen Quelles John L. Hennessy und David Patterson (2006). Computerarchitektur: Ein quantitativer Ansatz (Fourth ed). Morgan Kaufmann.ISBN:30-12-370490-0.Barton, Robert S, "Funktionsdesign von Computern", Mitteilungen der ACM 4(9): 405 (1961). Barton, Robert S, „Ein neues Konzept für das funktionelle Design eines digitalen Computers“, „Ausschreibungen der westlichen gemeinsamen Computerkonferenz, Mai 1961, S.393-96. Über die Gestaltung des Burroughs B5000 Computer. Bell, C. Gordon und Newell, Allen (1971). " Computerstrukturen: Lesen und Beispiele," McGraw-Water. Blaauw, G.A und Brooks, F.P, Jr. "The Structure of System/360, Teil I-Outline of the Logicalstruktur", IBM Systems Journal, vol. 3, no. 2, pp.119-135, 1964.Tanenbaum, Andrew S. 1979. Strukturierte Computerorganisation. Englewood Cliffs, New Jersey: Prentice-Hall.ISBN 0-13-148521-0. Außenbeziehungen ISCA: Proceedings of the International Symposium on Computerarchitektur Micro: EL/ACM International Symposium on Microarchitecture HPCA: International Symposium über High Performance Computerarchitektur ASPLOS: Internationale Konferenz über architektonische Unterstützung für Programmiersprachen und Betriebssysteme ACM-Transaktionen über Architektur und Codeoptimierung an Computer-Transaktionen Bibliothek der Informatiksysteme