In der computergestützten Lerntheorie ist das Lernen wahrscheinlich annähernd korrekt (PAC) ein Rahmen für die mathematische Analyse des maschinellen Lernens. Es wurde 1984 von Leslie Valiant vorgeschlagen. In diesem Rahmen erhält der Lernende Proben und muss aus einer bestimmten Klasse von möglichen Funktionen eine Verallgemeinerungsfunktion (die Hypothese genannt) auswählen. Ziel ist, dass mit hoher Wahrscheinlichkeit (der wahrscheinliche Teil) die ausgewählte Funktion einen niedrigen Verallgemeinerungsfehler (der "ungefähr korrekte" Teil) aufweist. Der Lernende muss in der Lage sein, das Konzept bei beliebigem Annäherungsverhältnis, Erfolgswahrscheinlichkeit oder Verteilung der Proben zu erlernen. Das Modell wurde später auf Lärm (mit klassifizierten Proben) erweitert. Eine wichtige Neuerung des PAC-Frameworks ist die Einführung von rechnergestützten Komplexitätstheoriekonzepten zum maschinellen Lernen. Insbesondere wird erwartet, dass der Lernende effiziente Funktionen (Zeit- und Raumbedarf an ein Polynom der Beispielgröße gebunden) und der Lernende selbst ein effizientes Verfahren implementieren muss (Anforderung eines an ein Polynom der durch die Näherungs- und Wahrscheinlichkeitsgrenze modifizierten Beispielszählung). Definitionen und Terminologie Um die Definition für etwas zu geben, das PAC-learnable ist, müssen wir zunächst einige Terminologie einführen. Für die folgenden Definitionen werden zwei Beispiele verwendet. Das erste ist das Problem der Zeichenerkennung bei einem Array von n \{displaystyle n} Bits, die ein binärwertiges Bild kodieren. Das andere Beispiel ist das Problem, ein Intervall zu finden, das Punkte innerhalb des Intervalls als positiv und die Punkte außerhalb des Bereichs als negativ korrekt klassifiziert. X \{displaystyle X} ein Satz namens Instanz-Raum oder die Kodierung aller Proben. Im Zeichenerkennungsproblem ist der Instanzraum X = {0, 1} n \{displaystyle X=\{0,1\}^{n . Im Intervall-Problem ist der Instanz-Raum, X \{displaystyle X}, der Satz aller gebundenen Intervalle in R \{displaystyle \mathbb {R} }, wobei R \{displaystyle \mathbb{R} } den Satz aller realen Zahlen bedeutet. Ein Konzept ist eine Untermenge c ∈ X \{displaystyle c\subset X} . Ein Konzept ist der Satz aller Bitmuster in X = {0, 1 } n \{displaystyle X=\{0,1\}^{n, die ein Bild des Buchstabens P kodieren. Ein Beispielkonzept aus dem zweiten Beispiel ist der Satz offener Intervalle, { (a, b) ∣ 0 ≤ a ≤ π / 2 , π ≤ b ≤ 13 } \{displaystyle ({a,b)\mid 0\leq a\leq \pi /2,\pi leqb\leq \{sqrt {13}\ , die jeweils nur die positiven Punkte enthalten. Eine Konzeptklasse C \{displaystyle C} ist eine Sammlung von Konzepten über X \{displaystyle X} . Dies könnte der Satz aller Teilmengen der Bits sein, die geskelettt 4-verknüpft sind (Breite der Schrift ist 1). Lassen Sie E X (c, D ) \{displaystyle EX(c,D}) ein Verfahren sein, das ein Beispiel zieht, x \{displaystyle x}, mit einer Wahrscheinlichkeitsverteilung D \{displaystyle D} und gibt das richtige Label c ( x ) \{displaystyle c(x}), das ist 1 wenn x ε c \{displaystyle x\in c} und 0 andernfalls. Jetzt, gegeben 0 < ε , δ < 1 \{displaystyle 0<\epsilon ,\delta <1}, vorausgesetzt, es gibt einen Algorithmus A \displaystyle A} und ein Polynom p \{displaystyle p} in 1 / ε, 1 / δ \{displaystyle 1/\epsilon ,1/\delta } (und andere relevante Parameter der Klasse C \{displaystyle C}) so, dass, gegeben eine Stichprobe der Größe p \{displaystyle p} nach E X gezogen wird ( Außerdem, wenn die obige Aussage für den Algorithmus A \{displaystyle A} für jedes Konzept c ε C \{displaystyle c\in C} und für jede Verteilung D \{displaystyle D} über X \{displaystyle X} und für alle 0 < ε, δ < 1 \{displaystyle 0\epsilon ,\delta <1} dann C \{displaystyle C} ist ( Wir können auch sagen, dass A \{displaystyle A} ein PAC Lernalgorithmus für C \{displaystyle C} ist. Equivalence Unter einigen Regularitätsbedingungen sind diese Bedingungen gleichwertig: Die Konzeptklasse C ist PAC erlernbar. Die VC-Dimension von C ist endlich. C ist eine einheitliche Glivenko–Cantelli Klasse. C ist komprimierbar im Sinne von Littlestone und Warmuth Siehe auch Machine Learning Data Mining Fehlertoleranz (PAC Lernen) Komplexität der Probe Referenzen https://users.soe.ucsc.edu/~manfred/pubs/lrnk-olivier.pdf Moran, Shay; Yehudayoff, Amir (2015). " Beispielkompressionsschemata für VC-Klassen".arXiv:1503.06960 [cs.LG] Weiter lesen M. Kearns, U. Vazirani. Eine Einführung in die Computational Learning Theory.MIT Presse, 1994. Ein Lehrbuch.M Mohri, A. Rostamizadeh und A. Talwalkar. Grundlagen des maschinellen Lernens.MIT Presse, 2018. Kapitel 2 enthält eine detaillierte Behandlung der PAC-Lernbarkeit. Durch den offenen Zugang des Verlags lesbar. D Haussler. Überblick über den wahrscheinlich ungefähr korrekten (PAC) Lernrahmen. Eine Einführung zum Thema. L Valiant. Wahrscheinlich ungefähr richtig. Basic Books, 2013. In dem Valiant argumentiert, dass PAC-Learning beschreibt, wie Organismen sich entwickeln und lernen.