Die Geschichte der Künstlichen Intelligenz (AI) begann in der Antike mit Mythen, Geschichten und Wiederkäuern von künstlichen Menschen, die von den Handwerkern mit Intelligenz oder Bewusstsein ausgestattet sind. Die Samen moderner AI wurden von klassischen Philosophen bepflanzt, die versuchten, den menschlichen Denken als mechanische Manipulation von Symbolen zu beschreiben. Diese Arbeit wurde in den 1940er Jahren in der Erfindung des programmierbaren digitalen Computers, einer Maschine, die auf dem abstrakten Charakter von mathematischen Grundstoffen basiert. Dieses Gerät und die Ideen hinter ihm inspirierten eine Handvoll von Wissenschaftlern, ernsthaft über die Möglichkeit eines elektronischen Gehirns zu diskutieren. Im Sommer 1956 wurde der Bereich der AI-Forschung auf einem Workshop gegründet, der im Campus des Trinity College stattfindet. Diejenigen, die an der Veranstaltung teilnehmen, werden seit Jahrzehnten die Führer der AI-Forschung werden. Viele von ihnen haben vorhergesagt, dass eine Maschine als intelligentes Personal in keinem Generation existiert, und Millionen von Dollar erhalten, um diese Vision wahrzunehmen. Letztlich wurde deutlich, dass kommerzielle Entwickler und Forscher die Schwierigkeit des Projekts deutlich unterschätzt hatten. In Reaktion auf die Kritik von James Lighthill und den anhaltenden Druck des Kongresses haben die US-Regierungen und die britischen Regierungen die Finanzierung undirektierter Forschungen in der künstlichen Intelligenz gestoppt und die später als „AI Winter“ bezeichneten schwierigen Jahre. Sieben Jahre später inspirierte eine visionäre Initiative der japanischen Regierung Regierungen und Industrie, die AI mit Milliarden Dollar zu versorgen, aber von den späten 80er Jahren wurden die Investoren unillusioniert und rückgezogen. Investitionen und Interesse an der AI sind in den ersten Jahrzehnten des 21. Jahrhunderts entstanden, als das maschinelle Lernen erfolgreich auf viele Probleme in Hochschulen und Industrie angewendet wurde, weil neue Methoden, die Anwendung leistungsfähiger Computer-Hardware und die Sammlung von riesigen Datensets. Precursors Mythical, fiktiver und spekulativer Vorläufer Mythen und Geschichten in der Griechischen Mythologie war Talos ein riesiger Bau von Bronze, der als Hüterin für die Insel Kreta fungierte. Er würde auf den Schiffen von Einschiffen schleichen und würde 3 Schaltkreise rund um den Perimeter der Insel täglich abschließen. Laut Pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos mit Hilfe eines Zyklons und stellte das automaton als Geschenk für Minos vor. Jason und die Argonauts lehnten ihn mit einem einzigen Schalter in der Nähe seines Fußs ab, der es dem lebenswichtigen Ior ermöglichte, aus seinem Körper zu kommen und ihn zu verlassen. Pygmalion war ein legendärer König und Bildhauer der griechischen Mythologie, der in den Metamorphoseen der Ovid bekannt ist. Pygmalion wird im 10. Buch von Ovid's Erzählung poem mit Frauen verfallen, wenn er Zeugen die Art und Weise, in der sich die Propoetides selbst aussetzen. Trotzs macht er Angebote auf dem Tempel der Venus, um die Gottden zu bitten, ihm eine Frau gleich wie eine Skulptur zu bringen, die erchnet In den Schriftstücken von Eleazar beneti von Worms z.B. im Nahen Alter von 12 bis 13 Jahren wurde davon ausgegangen, dass die Animation eines Golem durch die Einfügung eines Papiers mit allen Namen von God in den Mund des Tons erreicht werden könnte. Anders als legendäre automata wie Brazen Heads konnte ein Golem nicht sprechen. Alchemische Mittel der künstlichen Intelligenz In der Natur der Dinge, die von dem Schweizer Alchemist Paracelsus geschrieben wurde, beschreibt er ein Verfahren, das er behauptet, ein "artificialer Mann" zu strukturieren. Durch das Inverkehrbringen des "sperm eines Mannes" in Pferden und die Fütterung des "Arcanum of Mans" nach 40 Tagen wird die Koncoction zu einem Lebendkind werden. Vorschreibung Paracelsus war Jābir Abdullah Hayyān auf der Homunité: Takwin In Faust, der zweite Teil der Tragödie von Johann Wolfgang von Goethe, ein alchemisches Homunité, das für immer im Flachsk zu leben ist, in dem er hergestellt wurde, endeavors, in ein menschliches Organ geboren zu werden. Nach Beginn dieser Transformation sterben jedoch die Flask-Stauchen und die Homunité. Moderne Fiktion Nach dem 19. Jahrhundert wurden Ideen über künstliche Männer und Denkmaschinen entwickelt, wie Mary Shelley's Magnus oder Karel Čapek's R.U.R (Rossum's Universal Roboter) und Spekulationen, wie Samuel Butler's "Darwin" in den Maschinen," und in realen Weltfällen, darunter Edgar Allan Poe's "Maelzels Schach Player". AI ist ein regelmäßiges Thema der Science-Fiction durch die Gegenwart geworden. Automata Realistic Humanoid automata wurde von Handwerksmann jeder Zivilisation gebaut, darunter Yan Shi,Hero von Alexandria,Al-Jazari,Pierre Jaquet-Droz und Wolfgang von Kempelen. Die ältesten bekannten automata waren die heiligen Statuen der alten Ägyptens und Griechenlands. Man glaubte, dass Handwerksmann diese Zahlen mit sehr echten Köpfen, die in der Lage sind, Weisheit und Emotionen –Hermes Trismegistus schrieb, dass "durch die wahre Art der Mythen, die der Mensch in der Lage ist, ihn zu reproduzieren. " In der frühen modernen Zeit wurden diese legendären automata erklärt, die magische Fähigkeit zu besitzen, Fragen zu beantworten, die ihnen gestellt wurden. Der späte mittelalterliche Alchemist und der Wissenschaftler Roger Bacon wurden getreut, einen Brazenkopf zu haben, der eine Aufschrift als Zauber entwickelt hat. Diese Geschichten ähnelten dem Norse Mythen des Leiters von Mímir. Laut der Aufschrift war Mímir bekannt für seine intellect und Weisheit und wurde im Æsir-Vanir-Krieg bejaht. Odin soll den Kopf mit Kräutern verwechseln und darüber sprechen, dass der Kopf von Mímir in der Lage war, die Weisheit an Odin zu sprechen. Odin hielt den Kopf in der Nähe seines Beraters. Formale Gründe für künstliche Intelligenz basieren auf der Annahme, dass der Prozess des menschlichen Denkens mechanisiert werden kann. Die Untersuchung von mechanischem – oder formalem“ – hat eine lange Geschichte. chinesische, indische und griechische Philosophen alle entwickelten strukturierten Methoden der formalen Abführung im ersten Jahrtausend BCE. Ihre Ideen wurden über die Jahrhunderte von Philosophen wie Aristotle (die eine formale Analyse des Syllogismus, Euclid (die Elemente waren ein Modell der formalen Begründung), Al-Khwāmī (die Al-Khwāmī (die Al-Algorith entwickelt und seinen Namen an den Algorithmus) und europäischen scholistischen Philosophen wie William Ockham und Duns Scotus entwickelt. Spanischer Philosoph Ramon Llull (1232–1315) entwickelte mehrere logische Maschinen, die der Gewinnung von Wissen logischerweise gewidmet sind; Llull bezeichnete seine Maschinen als mechanische Einheiten, die grundlegende und unerträgliche Wahrheiten durch einfache, durch mechanische Zwecke erzeugte, so dass alle möglichen Kenntnisse hergestellt werden können. Llulls Arbeit hatte einen großen Einfluss auf das Büro von Korfu, das seine Ideen weiter entwickelt hat. Im 17. Jahrhundert untersuchten Thomas Hobbes und René Descartes die Möglichkeit, dass alle rationalen Gedanken systematisch als Algen oder Geometrie gemacht werden könnten. Hobbes wurde in Leviathan geschrieben: „Für uns gibt es nichts, aber wieder. Leibniz hat eine universelle Grundsprache (als Merkmala Universalis) eingeführt, die das Argumentation auf die Berechnung verringern würde, so dass "dass zwischen zwei Philosophen nicht mehr als zwischen zwei Buchhaltern verfahren werden muss. Es wäre suffice, ihre Bleistifts in Hand zu nehmen, bis zu ihren Mänden, und einander zu sagen (mit einem Freund, wenn sie gewünscht haben): Lassen Sie uns berechnen. " Diese Philosophen hatten begonnen, das physische Symbolsystem hypothesis zu formulieren, das zum Leitmotiv der AI-Forschung werden würde. Im zwanzigsten Jahrhundert gab die mathematische Logik den entscheidenden Durchbruch, der künstliche Intelligenz plausibel erscheint. Die Stiftungen wurden durch solche Arbeiten wie die Absichtserklärung von Rayle und die Kunst von Frege geschaffen. Aufbauend auf dem System von Frege haben Russell und Whitehead eine formale Behandlung der Grundlagen der Mathematik in ihren Meisterwerken, der Principia KPMG im Jahr 1994. David Hilbert hat mit seinem Erfolg die Mathematiker der 1920er und 30er Jahre in Frage gestellt, um diese grundlegende Frage zu beantworten: "Für alle mathematischen Gründe ist formalisiert? "Hisfrage wurde von Gödels unvollständiger Beweis, Turing's Maschinen und Church's Lambda-Risiko beantwortet. Ihre Antwort war in zweierlei Hinsicht überraschend. Erstens haben sie bewiesen, dass es tatsächlich Grenzen für die mathematische Logik gibt. Letztere (und mehr Bedeutung für die AI) schlug jedoch vor, dass innerhalb dieser Grenzen jede Form von mathematischen Grundstoffen mechanisiert werden könnte. Die Kirche-Turing thesis führte aus, dass ein mechanisches Gerät, geschärfte Symbole wie 0 und 1, einen denkbaren mathematischen Abzug ermöglichen könnte. Hauptanschauung war die Turing-Maschine – ein einfacher theoretischer Bau, der den Wesen der abstrakten Symbolmanipulation erfasste. Mit dieser Erfindung würden Wissenschaftler die Möglichkeit von Denkmaschinen erörtern. Computerwissenschaftliche Rechenmaschinen wurden in der Antike gebaut und in der gesamten Geschichte von vielen mathematischen Menschen verbessert, darunter auch (aufmals) Philosophen Wolfgang Leibniz. Charles Babbage hat Anfang des 19. Jahrhunderts einen programmierbaren Computer (Analytical Engine) entwickelt, obwohl es nie gebaut wurde. Ada Lovelace spekuliert, dass die Maschine "might compose und wissenschaftliche Musikstücke jeglicher Art von Komplexität oder Ausmaß" entwickelt. () Sie wird oft als erster Programmr gutgeschrieben, weil sie eine Reihe von Anmerkungen schrieb, die eine Methode zur Berechnung der Bernoulli-Nummern mit dem Motor vollständig darlegten.) Die ersten modernen Computer waren die massiven Code-Durchbrüche des Zweiten Weltkriegs (wie Z3, ENIAC und Coverlustus). Letztere zwei dieser Maschinen basieren auf der theoretischen Grundlage von Alan Turing und entwickelt von John von Neumann. Geburt künstlicher Intelligenz 1952-56 In den 1940er und 50er Jahren begannen die Wissenschaftler aus verschiedenen Bereichen (Mthematik, Psychologie, Technik, Wirtschaft und politische Wissenschaft) mit der Möglichkeit, ein künstliches Gehirn zu schaffen. 1956 wurde der Bereich der künstlichen Intelligenzforschung als akademische Disziplin gegründet. Cybernetics und frühe Neuralnetze Die frühe Forschung an Denkmaschinen wurde durch eine Grippe von Ideen inspiriert, die in den späten 30er Jahren, 1940er und frühen 1950er Jahren vorherrschen. Jüngste Forschung in der Neurologie hat gezeigt, dass das Gehirn ein elektrisches Netzwerk von Neuronen war, das in allen oder gar nicht realisierten Pulsen zerstört wurde. Norbert Wiener Cybernetics bezeichnete Kontrolle und Stabilität in elektrischen Netzen. Claude Shannons Informationstheorie beschreibt digitale Signale (d. h. alle-oder-nothing-Signale). Alan Turings Rechentheorie zeigte, dass jede Form der Berechnung digital beschrieben werden könnte. Die enge Beziehung zwischen diesen Ideen schlug vor, ein elektronisches Gehirn zu schaffen. Beispiele für die Arbeit in diesem Zusammenhang sind Roboter wie W. Grey Walters Schildkröten und das Johns Hopkins-Halter. Diese Maschinen verwenden keine Computer, digitale Elektronik oder symbolische Gründe; sie wurden vollständig durch analoge Schaltkreise kontrolliert. Walter Pitts und Oliver McCulloch untersuchten Netzwerke idealisierter künstlicher Neuronen und zeigten, wie sie im Jahr 1964 einfache logische Funktionen ausüben könnten. Sie waren die erste, um zu beschreiben, was später Forscher ein Neuralnetz fordern würden. Einer der Studierenden, die von Pitts und McCulloch inspiriert wurden, war ein junger Marvin Minsky, dann ein 24-Jähriger. Im Jahr 1951 (mit Dean Edmonds) errichtete er die erste Neuralnetzmaschine, die SNARC. Minsky war in den nächsten 50 Jahren einer der wichtigsten Führer und Innovatoren in der AI. The Turing's Test 1950 hat Alan Turing ein wegweisendes Papier veröffentlicht, in dem er über die Möglichkeit, Maschinen zu schaffen, die denken. Er stellte fest, dass es schwierig ist, seinen berühmten Turing Test zu definieren und zu entwerfen. Wenn eine Maschine ein Gespräch (über einen Teleprinter) führen könnte, das von einem Gespräch mit einem menschlichen Wohlbefinden unzerstörbar war, dann war es vernünftig, zu sagen, dass die Maschine Denkweise hat." Durch diese vereinfachte Version des Problems konnte Turing überzeugend argumentieren, dass eine "Thinking-Maschine" zumindest plausibel war und das Papier alle gemeinsamen Einwände gegen die Vorgabe beantwortete. Turing Test war der erste ernsthafte Vorschlag in der Philosophie der künstlichen Intelligenz. Spiel AIIn 1951 unter Verwendung des Ferranti Mark 1 Maschinen der Universität Manchester schrieb Christopher Strachey ein Testprogramm und Dietrich Prinz ein für den Schach. Arthur Samuel's Checkers-Programm, das in der Mitte 50er und frühen 60er Jahre entwickelt wurde, hat schließlich ausreichende Fähigkeiten zur Herausforderung eines respektvollen Amateurs erreicht. Game AI würde in der gesamten Geschichte weiterhin als Fortschrittsmaßnahme in der AI genutzt werden. Symbolische Gründe und die Logik Wenn der Zugang zu digitalen Computern in den mittleren Fängen möglich wurde, erkannten einige Wissenschaftler an, dass eine Maschine, die Nummern manipulieren könnte, auch Symbole angreifen könnte und dass die Manipulation von Symbolen das Wesen menschlicher Gedanken sein könnte. Dies war ein neuer Ansatz zur Schaffung von Denkmaschinen. Im Jahr 1955 gründeten Allen Newell und (future Nobel Laureate)Herbert A. Simon die "Logic Theorist" (mit Hilfe von J. C. Shaw). Letztendlich würde das Programm 38 der ersten 52 Theorems in Russell und Whitehead's Principia KPMG beweisen und neue und anspruchsvollere Nachweise für einige finden. Simon sagte, sie hätten "die betrogenen Köpfe/Körper-Probleme gelöst und erläutert, wie ein System, das aus Materie besteht, die Eigenschaften des Geistes haben kann. "Dies war eine frühe Erklärung der philosophischen Position John Searle, die später "Strong AI:" fordern würde, dass Maschinen Denkanstöße wie menschliche Einrichtungen enthalten können. Investitionsworkshop 1956: Die Geburt des AI Thebourne-Workshops von 1956 wurde von Marvin Minsky, John McCarthy und zwei führenden Wissenschaftlern organisiert: Claude Shannon und Nathan Rochester von IBM. Der Vorschlag für die Konferenz umfasst diese Behauptung: "jeder Aspekt des Lernens oder jeder sonstigen Kennzeichen kann genau beschrieben werden, dass eine Maschine zur Simulation dieses Problems eingesetzt werden kann". Die Teilnehmer waren Ray Salomonoff, Oliver Selbstridge, Trenchard More, Arthur Samuel, Allen Newell und Herbert A. Simon, die in den ersten Jahrzehnten der AI-Forschung wichtige Programme schaffen würden. Bei der Konferenz Newell und Simon haben die "Logic Theorist" und McCarthy die Teilnehmer aufgefordert, "Artificial Intelligence" als Namen des Feldes zu akzeptieren. Diese Konferenz von 1956 war der Zeitpunkt, an dem die AI ihren Namen, ihre Mission, ihren ersten Erfolg und ihre wichtigsten Akteure erworben hat, und gilt weithin als die Geburt der AI. Der Begriff „Artificial Intelligence“ wurde von McCarthy gewählt, um Verbände mit Cybernetics und Verbindungen mit dem einflussreichen Cyberneticist Norbert Wiener zu vermeiden. Jahre 1956-1974 Die Programme, die in den Jahren nach dem Workshop von Hampton entwickelt wurden, waren für die meisten Menschen einfach erstaunlich: Computer lösen Algen Wortprobleme aus, indem sie die Hemmlinge in der Geometrie und das Lernen auf Englisch sprechen. Nur wenige von der Zeit hätten glauben, dass ein solches intelligentes Verhalten von Maschinen überhaupt möglich sei. Forscher äußerten einen intensiven Optimismus in Privat und Druck, der vorausstellte, dass in weniger als 20 Jahren eine voll intelligente Maschine gebaut werden soll. Regierungsstellen wie DARPA Geld in den neuen Bereich. Arbeit In den späten 50er und 1960er Jahren gab es viele erfolgreiche Programme und neue Richtungen. Eines der einflussreichsten waren dies: Mit der Suche nach vielen frühen AI-Programmen wurde der gleiche grundlegende Algorithmus verwendet. Um ein gewisses Ziel zu erreichen (wie ein Spiel zu gewinnen oder ein Hemmnis zu schaffen), haben sie einen Schritt auf dem Weg (durch einen Umzug oder einen Abzug) getan, als wenn sie mit einem Moratorium suchen, wenn sie ein tödliches Ende erreicht haben. Dieses Paradigma wurde als "besonder" bezeichnet. Hauptschwierigkeit war, dass für viele Probleme die Anzahl möglicher Pfade durch das Maze schlichte war (eine Situation, die als "kombinatorielle Explosion" bekannt ist). Forscher würden den Suchraum durch die Nutzung von Hetourismus oder "Galmonen" verringern, die diese Pfade beseitigen würden, die nicht zu einer Lösung führen könnten. Newell und Simon versuchten, eine allgemeine Version dieses Algorithmus in einem Programm namens "General Problem Solver" zu erfassen. Andere Suchprogramme waren in der Lage, beeindruckende Aufgaben wie die Lösung von Problemen in der Geometrie und Algen, wie Herbert Gelerns Gemetrie Theorem Prover (1958) und SAINT, geschrieben von dem Studenten von Minsky James Slagle (1961). Andere Programme suchten nach Zielen und Subgos, um Maßnahmen zu planen, z.B. das STRIPS-System, das auf dem Flughafen Stanford entwickelt wurde, um das Verhalten ihrer Roboter Shakey zu kontrollieren. Natursprache Ein wichtiges Ziel der AI-Forschung ist es, Computer in natürlichen Sprachen wie Englisch zu kommunizieren. Anfangserfolg war das Programm STUDENT von Daniel Bobrow, das Probleme mit hohen Schul-Algorithmen lösen könnte. Ein semantisches Netz stellt Begriffe (z.B. Haus), als Knoten und Beziehungen zwischen Konzepten (z.B. Das erste AI-Programm zur Nutzung eines semantischen Netzes wurde von Ross Quillian geschrieben und die erfolgreichste (und umstrittenste) Version war Roger Rutks Konzeptualabhängigkeitstheorie. Joseph-Salzbaums ELIZA könnte so realistische Gespräche führen, dass die Nutzer gelegentlich ins Denken geraten waren, dass sie mit einem menschlichen Leben kommunizieren und kein Programm (Siehe ELIZA-Effekt). ELIZA hatte jedoch keine Vorstellung davon, was sie mit sich bringt. Sie gab einfach eine Antwort oder wiederholte zurück, was ihnen gesagt wurde, indem sie ihre Antwort mit einigen molekularen Vorschriften wiederaufnehmen. ELIZA war der erste Chatterbot. Micro-worlds Marvin Minsky und Felix Papert of the MIT AI Laboratory schlugen in den späten 60er Jahren vor, dass die AI-Forschung sich auf künstlich einfache Situationen konzentrieren sollte, die als Mikroworld bekannt sind. Sie wiesen darauf hin, dass in erfolgreichen Wissenschaften wie der Physik die Grundprinzipien oft am besten durch vereinfachte Modelle wie resistente Flugzeuge oder vollkommen starre Stellen verstanden wurden. Viele der Forschungsarbeiten konzentrierten sich auf eine "Blocks Welt", die aus farbigen Blöcken verschiedener Formen und Größen bestehen, die auf einer flachen Oberfläche liegen. Dieses Paradigma führte zu innovativen Arbeiten in der maschinellen Vision von Gerald Sussman (die das Team geleitet hat), Adolfo Guzman, David Waltz (die "Kontraint propagation" und insbesondere Patrick Winston. Gleichzeitig gründeten Minsky und Papert einen Roboterarm, der Blöcke stapeln und die Blöcke der Welt zum Leben bringen könnte. Der Erfolg des Mikro-Weltprogramms war Terry Winograds SHRDLU. Es könnte in gewöhnlichen englischen Strafen, Planungsvorgängen und deren Ausführung kommunizieren. Optimismus Die erste Generation von AI-Forschern hat diese Vorhersagen über ihre Arbeit gemacht: 1958, H. A. Simon und Allen Newell: „In zehn Jahren wird ein digitaler Computer der Welt-Weltmeisterschaft sein“ und „In zehn Jahren wird ein digitaler Computer einen wichtigen neuen mathematischen Theorem entdecken und beweisen.“1965, H. Simon: „Die Maschinen werden innerhalb von zwanzig Jahren in der Lage sein, einen Mann zu tun.“1967, Marvin Minsky: Mit einem Problem der Generation „Dies ist das Leben“ (in Minsk). "In drei bis acht Jahren werden wir eine Maschine mit der allgemeinen Intelligenz eines durchschnittlichen menschlichen Lebens haben." Im Juni 1963 erhielt der MIT einen Zuschuss in Höhe von 2,2 Mio. $ von der neu geschaffenen Advanced Research Projects Agency (als DARPA bekannt). Das Geld wurde verwendet, um das Projekt MAC zu finanzieren, das die von Minsky und McCarthy gegründete „AI-Gruppe“ fünf Jahre zuvor hinterließ. DARPA stellt drei Millionen Dollar pro Jahr bis zu den 70er Jahren bereit. DARPA gewährte ähnliche Zuschüsse an Newell und Simon's Programm an der Kapitalmarktunion und an das AZ AI-Projekt (begründet von John McCarthy 1963). 1965 wurde an der Universität Edinburgh ein weiteres wichtiges AI-Labor eingerichtet. Diese vier Organe würden die wichtigsten Zentren der AI-Forschung (und der Finanzierung) in der akademischen Welt seit vielen Jahren sein. J. C. R. Licklider, der Direktor von ARPA, glaubte, dass seine Organisation "Fördergelder, nicht Projekte"! und es ermöglichten Forschern, alle Richtungen zu verfolgen.Mit der Geburt der Hackerkultur hat dies eine neutrale Atmosphäre geschaffen, doch dieses Konzept wäre nicht erfolglos. Robotik in Japan startete die Waseda University 1967 das WABOT-Projekt und hat 1972 das WABOT-1, den weltweit ersten intelligenten humanoiden Roboter, oder Android abgeschlossen. Ihr System zur Kontrolle der Gliedmaße erlaubte es, mit den unteren Gliedmaßen zu Fuß zu fassen und mit den Händen und mithilfe von Tactile-Sensoren Gegenstände zu bepacken. Ihr visionäres System erlaubt es, Entfernungen und Richtungen zu messen, um Gegenstände mit externen Rezeptoren, künstlichen Augen und Ohren zu verwenden. Und das Gesprächssystem erlaubte es, mit einer Person in Japan mit einem künstlichen Mund zu kommunizieren. Erster AI Winter 1974-80 In den siebziger Jahren war die AI kritisch und finanzielle Rückschläge unterworfen. AI-Forscher hatten es versäumt, die Schwierigkeiten zu schätzen, vor denen sie stehen. Ihr enormer Optimismus hatte die Erwartungen möglicherweise hoch angehoben, und wenn die versprochenen Ergebnisse nicht zu einer Materialisierung führten, verschwanden die Finanzierungen für AI. Gleichzeitig wurde der Bereich des Anschlussismus (oder der Neuralnetze) fast alle zehn Jahre von der verheerenden Kritik von Marvin Minsky an Perceptronen abgeschottet. Trotz der Schwierigkeiten mit der öffentlichen Wahrnehmung von AI in den späten 70er Jahren wurden neue Ideen in der Planung der Logik, der gemeinsamen Begründung und vielen anderen Bereichen untersucht. Probleme Anfang der siebziger Jahre waren die Fähigkeiten von AI-Programmen begrenzt. Selbst die beeindruckendsten könnten nur unharmonische Versionen der Probleme behandeln, die sie lösen wollten; alle Programme waren in gewissem Maße Spielzeug. AI-Forscher hatten begonnen, in mehrere grundlegende Grenzen zu laufen, die in den siebziger Jahren nicht überwunden werden könnten. Obwohl einige dieser Grenzwerte in den späteren Jahrzehnten gestrichen werden, werden andere bis heute den Feld behalten. begrenzte Computerleistung: Es gab keine ausreichende Speicher- oder Verarbeitungsgeschwindigkeit, um alles wirklich sinnvoll zu erreichen. Ross Quillians erfolgreiche Arbeit auf der natürlichen Sprache wurde beispielsweise mit einem bulgarischen Votum von nur zwanzig Worten bewiesen, da dies alles in der Erinnerung passte. Hans Moravec argumentierte 1976, dass Computer immer noch Millionen von Zeiten zu schwach waren, um Intelligenz zu zeigen. Er schlug eine Analogie vor: künstliche Intelligenz erfordert die Computerleistung in derselben Weise, dass Luftfahrzeuge Pferdekraft benötigen. Nach einem bestimmten Schwellenwert ist es unmöglich, aber als Erhöhung der Leistung könnte es einfach werden. In Bezug auf Computervisionen schätzt Moravec, dass eine einfache Anpassung der Spitzen- und Bewegungserkennungsfähigkeit der menschlichen Retina in Echtzeit einen allgemein geeigneten Computer erfordern würde, der in der Lage ist, 109 Operationen/z (1000 MIPS) durchzuführen. Im Jahr 2011 benötigen praktische Computervisionsanwendungen 10.000 bis 1.000.000 MIPS. Vergleichsweise war der schnellste Supercomputer im Jahr 1976, Cray-1 (um 5 Millionen $ bis 8 Millionen $), nur in der Lage, etwa 80 bis 130 MIPS zu erreichen, und ein typischer Desktop zu der Zeit, die weniger als 1 MIPS erreicht hat. Intractability und die kombinierte Explosion. Im Jahr 1972 zeigte Richard Karp (Aufbau von Stephen Cook 1971, Theorem) viele Probleme, die wahrscheinlich nur in exponentieller Zeit gelöst werden können (in der Größe der Inputs). optimale Lösungen für diese Probleme erfordern unvorstellbare Mengen von Computerzeit, außer wenn die Probleme uneinheitlich sind. Knapp bedeutet dies, dass viele der von der AI verwendeten Spielzeuglösungen wahrscheinlich nie in nützliche Systeme einbauen würden. Know-how und Grundkenntnisse. Viele wichtige künstliche Intelligenzanwendungen wie Vision oder natürliche Sprache erfordern einfach enorme Mengen an Informationen über die Welt: Das Programm muss einige Gedanken darüber haben, was es vielleicht suchen könnte oder was es geht. Dies erfordert, dass das Programm die meisten der gleichen Dinge über die Welt kennt, die ein Kind tut. Forscher haben bald erkannt, dass dies eine wirklich große Menge an Informationen war. Keines von 1970 könnte eine Datenbank so groß bauen, und niemand wusste, wie ein Programm so viele Informationen lernen könnte. Moravec's Paradox: Die Lösung von Geometrieproblemen ist für Computer vergleichsweise einfach, aber eine angeblich einfache Aufgabe wie die Anerkennung eines Gesichts oder die Überquerung eines Raums ohne Beanspruchung in alles ist äußerst schwierig. Dies trägt dazu bei, warum die Forschung in Vision und Robotik bis Mitte der siebziger Jahre so wenig vorangebracht hat. Rahmen- und Qualifikationsprobleme. AI-Forscher (wie John McCarthy), die die Logik nutzten, haben festgestellt, dass sie keine normalen Abzüge darstellen könnten, die mit der Planung oder dem Ausfall von Gründen verbunden sind, ohne Änderungen der Logik selbst vorzunehmen. Sie entwickelten neue Logiken (wie nicht-Montonische Logiken und Modalitäten), um die Probleme zu lösen. Ende der Finanzierung Die mit der AI-Forschung (wie die britische Regierung, DARPA und die NRC) finanzierten Agenturen wurden mit dem Mangel an Fortschritten und schließlich nahezu allen Mitteln für undirektierte Forschungen in die AI ausgehebelt. Anfang des Jahres 1966 begann das Muster, als der ALPAC-Bericht Kritik an maschinellen Übersetzungsbemühungen zeigte. Nach 20 Millionen Dollar endete die NRC alle Unterstützung. 1973 kritisierte der Lichthill-Bericht über den Zustand der AI-Forschung in England das Hauptversagen der AI zur Erreichung ihrer „Grandiose-Ziele“ und führte zur Abwrackung der AI-Forschung in diesem Land. () Konkret erwähnte der Bericht das kombinierte Explosionsproblem als Grund für Mängel der AI.) DARPA war zutiefst enttäuscht mit Forschern, die an dem Forschungsprogramm zur Rede halten und einen jährlichen Zuschuss von drei Millionen Dollar annullierten. Bis 1974 war die Finanzierung von AI-Projekten schwer zu finden. Hans Moravec verschuldete die Krise auf die unrealistischen Vorhersagen seiner Kollegen. " Viele Forscher wurden in einem Web der zunehmenden Übertreibung gefangen. " Jedoch gab es noch ein anderes Thema: Seit der Einführung der Änderung von Bradford im Jahr 1969 war DARPA zunehmend Druck auf die Finanzierung von "exmissionsorientierten Direktforschungen und nicht der Grundlagenforschung". Mittel für die kreative, freie Radikalisierung, die in den 60er Jahren angelaufen war, würden nicht aus DARPA kommen. Stattdessen wurde das Geld gezielt für spezifische Projekte mit klaren Zielen wie autonome Tanks und Gefechtsschutzsysteme eingesetzt. Kritiker aus dem gesamten Campus hatten mehrere Philosophen starke Einwände gegen die Forderungen von AI-Forschern. John Lucas, der argumentierte, dass die Unvollständigkeit von Gödel zeigte, dass ein formales System (wie ein Computerprogramm) nie die Wahrheit bestimmter Aussagen sehen könnte, während ein Mensch die Möglichkeit hätte. Hubert Dreyfus beseitigte die zerbrochenen Versprechen der 1960er Jahre und kritisierte die Annahmen der AI, dass die menschliche Begründung tatsächlich sehr wenig "Symbol-Verarbeitung" und ein großes emittiertes, instinktives, unbewußtes "Wissensbewusstsein". John Searles chinesisches Zimmer, das 1980 vorgestellt wurde, versucht zu zeigen, dass ein Programm nicht gesagt werden kann, um die Symbole zu verstehen, die es verwendet (eine Qualität namens Absichtserklärung). Wenn die Symbole keine Bedeutung für die Maschine haben, kann Searle argumentiert werden, dann kann die Maschine nicht als Denkweise bezeichnet werden. Diese Kritiken wurden von AI-Forschern nicht ernst genommen, oft weil sie den Punkt weit entfernt hatten. Probleme wie die Intractability und das gemeinsame Wissen scheinen viel unmittelbarer und ernster. Es war unklar, was "anerkannt, wie" oder beabsichtigt, zu einem echten Computerprogramm gemacht zu werden. Minsky sagte von Dreyfus und Searle: sie verfallen und sollten ignoriert werden. " Dreyfus, der im MIT unterrichtet wurde, wurde eine kalte Schulter erhalten: er sagte später, dass AI-Forscher nicht mit mir Mittagessen sehen. "Pierre Heilbaum, der Autor von ELIZA, vertrat die Ansicht, dass seine Kollegen die Behandlung von Dreyfus unprofessionelle und kinderreiche. Er war zwar ein unausgewogener Kritik an den Positionen Dreyfus, aber er hat es "daßlich klar gemacht, dass ihre Menschen nicht den Weg zur Behandlung eines Menschen seien. " Kochbaum begann ernsthafte ethische Zweifel an der AI zu haben, wenn Kenneth Colby ein "Computerprogramm schrieb, das den psychotherapeutischen Dialog auf der Grundlage von ELIZA führen kann. Kohlbaum wurde gestört, dass Colby ein sinnloses Programm als ein ernstes therapeutisches Instrument sah. Ein Foud begann, und die Situation wurde nicht dazu beigetragen, wenn Colby keinen Kredit an das Programm leistete. Hydrobaum hat 1976 Computer Power und menschliche Gründe veröffentlicht, die argumentierten, dass der Missbrauch künstlicher Intelligenz das Potenzial hat, das menschliche Leben abzuwerten. Perceptrons und der Angriff auf den AnschlussismusA Perceptron war eine Form des Neuralnetzes, das 1958 von Frank Rosenblatt eingeführt wurde, das als Schulbesuch von Marvin Minsky in der Bronx High School of Science tätig war. Wie die meisten AI-Forscher war er optimistisch über ihre Macht und bewarf, dass "perceptron schließlich in der Lage sein könnte, zu lernen, Entscheidungen zu treffen und Sprachen zu übersetzen". In den 60er Jahren wurde ein aktives Forschungsprogramm zum Paradigmen durchgeführt, doch kam es zu einem plötzlichen Stillstand mit der Veröffentlichung des Buches Minsky und Papert von 1969 Perceptrons. Man schlug vor, dass die Proceptrons schwer eingeschränkt werden könnten und dass die Vorhersagen von Frank Rosenblatt grob überzogen wurden. Die Wirkung des Buches war verheerender: praktisch keine Forschung in allen zehn Jahren wurde im Zusammenhang mit dem Titel „Konflikt“ durchgeführt. Künftig würde eine neue Generation von Forschern das Feld wiederbeleben und dann ein wichtiger und sinnvoller Bestandteil der künstlichen Intelligenz werden. Rosenblatt würde nicht leben, um dies zu sehen, da er kurz nach der Veröffentlichung des Buchs in einem Schiffbruch starb. Logik und symbolische Grundstofflogik wurden Anfang 1959 in die AI-Forschung eingeführt, von John McCarthy in seinem Vorschlag für Beratung. J. Alan Robinson hatte 1963 eine einfache Methode zur Einführung des Abzugs auf Computern, der Auflösungs- und Einstimmigkeitsgorithmus entdeckt. Klare Umsetzungen wie die von McCarthy und seinen Studierenden in den späten 60er Jahren versuchten, waren jedoch besonders unüberwindbar: Die Programme verlangten eine gewaltige Anzahl von Schritten, um einfache Theorems zu beweisen. In den siebziger Jahren von Robert Kowalski an der Universität Edinburgh wurde ein fruchtbarer Ansatz für die Logik entwickelt, und bald führte dies zur Zusammenarbeit mit französischen Forschern Alain Colmerauer und Philippe Roussel, die die erfolgreiche Programmiersprache Prolog geschaffen haben. Prolog verwendet einen Teil der Logik (Horn-Klauseln, die eng mit Regeln und "Produktionsregeln" zusammenhängen, die eine praktikable Berechnung ermöglichen. Regeln würden weiterhin einflussreich sein, die eine Grundlage für die Sachverständigensysteme von Edward Kuhnbaum und die fortgesetzte Arbeit von Allen Newell und Herbert A. Simon bilden, die zu Soar und ihren einheitlichen Theorien der Kognition führen würde. Kritiker des logischen Ansatzes wie Dreyfus hatten, dass Menschen selten bei der Lösung von Problemen Logik benutzten. Experimenten von Psychologen wie Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman und anderen haben Nachweis erbracht. McCarthy reagierte darauf, dass die Menschen irrelevant sind. Er argumentierte, was wirklich notwendig ist, seien Maschinen, die Probleme lösen können – keine Maschinen, die als Menschen denken. Fenster und Schriftstücke Unter den Kritikern des Ansatzes von McCarthy waren seine Kollegen im ganzen Land im MIT. Marvin Minsky, Magnus Papert und Roger Stak versuchen, Probleme wie "Story Understanding" und "Zielanerkennung" zu lösen, die eine Maschine benötigen, um zu denken. Um normale Konzepte wie den Vorsitz oder das Restaurant zu nutzen, mussten sie alle gleichen illogischen Annahmen machen, die die Menschen normalerweise gemacht haben. Leider sind unpräzise Begriffe wie diese schwer zu vertreten. Gerald Sussman stellte fest, dass "die Verwendung einer genauen Sprache, um im Wesentlichen unpräzise Begriffe zu beschreiben, sie nicht genauer machen. " Käk bezeichnete ihre anti-logic-Konzepte als mühsam, im Gegensatz zu den von McCarthy, Kowalski, Kuhnbaum, Newell und Simon verwendeten neuen Paradigmen. Minsky stellte 1975 in einem halben Papier fest, dass viele seiner Mitredner die gleiche Art von Werkzeug verwenden: ein Rahmen, der alle gemeinsamen Sinnesannahmen über etwas erfasst. Wenn wir beispielsweise das Konzept eines Vogels verwenden, gibt es eine Konstellation von Fakten, die sofort zu berücksichtigen sind: Wir können davon ausgehen, dass es schleppt, essen die Würmer und damit. Wir wissen, dass diese Fakten nicht immer richtig sind und dass Abzüge, die diese Fakten verwenden, nicht logisch sein werden, aber diese strukturierten Annahmen sind Teil des Kontextes aller, wir sagen und denken. Er nannte diese Strukturen. Käk benutzte eine Version von Rahmen, die er für die erfolgreiche Beantwortung von Fragen zu Kurzgeschichten in englischer Sprache anmahnte. Viele Jahre später würde eine objektorientierte Programmierung die wesentliche Idee der Erbschaft aus der AI-Forschung zu Rahmen annehmen. Boom 1980-87 In den 80er Jahren wurde eine Art von AI-Programm namens "Expertensysteme" von Unternehmen weltweit angenommen und das Wissen wurde zum Schwerpunkt der allgemeinen AI-Forschung. In diesen gleichen Jahren hat die japanische Regierung aggressive AI mit ihrem fünften Computerprojekt gefördert. Ein weiteres ermutigendes Ereignis in den frühen 80er Jahren war die Wiederbelebung des Zusammenhangs bei der Arbeit von John Hopfield und David Rumelhart. einmal war die AI erfolgreich. Anstieg der Expertensysteme Ein Expertensystem ist ein Programm, das Fragen beantwortet oder Probleme mit einem bestimmten Bereich des Wissens gelöst, wobei logische Regeln, die aus dem Wissen von Experten abgeleitet werden, verwendet werden. Als erste Beispiele wurden Edward Kuhnbaum und seine Studenten entwickelt. Dendral, begann 1965, identifizierte Verbindungen aus Spektrometer-Lesungen. MYCIN, entwickelt 1972, diagnostizierte Infektionskrankheiten. Sie haben die Durchführbarkeit des Ansatzes bewiesen. Expertensysteme beschränkten sich auf einen kleinen Bereich von spezifischen Kenntnissen (hum das gemeinsame Wissen zu vermeiden) und ihre einfache Gestaltung hat es für Programme relativ einfach gemacht, zu bauen und anschließend zu ändern. Insgesamt haben sich die Programme als nützlich erwiesen: etwas, das AI nicht in der Lage war, bis zu diesem Punkt zu erreichen. 1980 wurde ein Sachverständigensystem namens XCON bei der CMU für die Digital Equipment Corporation abgeschlossen. Es war ein enormer Erfolg: Das Unternehmen konnte bis 1986 jährlich 40 Millionen Dollar sparen. Unternehmen in der ganzen Welt haben begonnen, Sachverständigensysteme zu entwickeln und zu entsorgen, und bis 1985 wurden sie über eine Milliarde US-Dollar auf der AI, die meisten davon in interne Aufsichtsbehörden. In einer Branche stiegen sie auf, darunter Hardware-Unternehmen wie Symbolik und Lisp-Maschinen und Softwareunternehmen, wie z. Wissensrevolution Die Macht der Expertensysteme stammt aus dem von ihnen enthaltenen Fachwissen. Sie waren Teil einer neuen Ausrichtung in der AI-Forschung, die in den 70er Jahren Boden gewinnen konnte. „AI-Forscher wurden zu Verdächtigen – in erheblichem Maße, weil sie gegen die wissenschaftlichen Fähigkeiten von Parsimony verstoßen –, dass die Intelligenz sehr gut auf der Fähigkeit beruhen könnte, große Mengen unterschiedlicher Kenntnisse auf unterschiedliche Weise zu nutzen“, schreibt die Firma DX McCorduck. ["T]he große Lektion aus den siebziger Jahren war, dass intelligentes Verhalten sehr stark vom Umgang mit Wissen abhängig ist, manchmal recht detailliertes Wissen, eines Bereichs, in dem eine bestimmte Aufgabe besteht. Wissensbasierte Systeme und Know-how-Engineering haben in den 80er Jahren einen großen Schwerpunkt der AI-Forschung. In den 80er Jahren wurde auch die Geburt von Cyc, der erste Versuch, das gemeinsame Wissen direkt anzugreifen, durch die Schaffung einer umfangreichen Datenbank, die alle mundane Fakten enthalten würde, die der durchschnittliche Mensch kennt. Douglas Lenat, der das Projekt eingeleitet und geleitet hat, argumentierte, dass es keine kurze  short die einzige Möglichkeit für Maschinen gibt, um die Bedeutung menschlicher Konzepte kennen zu lernen, ein Konzept zu einer Zeit. Das Projekt wurde für viele Jahrzehnte nicht abgeschlossen. Schachspielprogramme HiTech und vertiefte Gedanken haben 1989 die Schach Masters abgelehnt. beide wurden von der University von Carnegie Mellon entwickelt; vertiefte Gedankenentwicklung ebnete den Weg für Deep Blue. Die Geldrendite: das Fünfte Generation-Projekt 1981, das japanische Ministerium für internationale Handel und Industrie hat 850 Millionen $ für das Fünfte Generation-Computerprojekt bereitgestellt. Ihr Ziel bestand darin, Programme und Maschinen zu schreiben, die auf Gesprächen führen könnten, Sprachen übersetzen, Bilder auswerten und warum Menschen. Man wählte Prolog als primäre Computersprache für das Projekt aus. Andere Länder reagierten mit neuen Programmen ihrer eigenen. Das Vereinigte Königreich begann mit 350 Mio. £ Alvey-Projekt. Ein Konsortium aus amerikanischen Unternehmen gründete die Microelectronics und Computer Technology Corporation (oder MCC), um Großprojekte in der AI und Informationstechnologie zu finanzieren. DARPA reagierte ebenso wie die Gründung der strategischen IKT-Initiative und ihre Investition in die AI zwischen 1984 und 1988. Physiker John Hopfield konnte 1982 nachweisen, dass eine Form des Neuralnetzes (jetzt als "Hopfield Net" bezeichnet) in völlig neuer Weise lernen und verarbeiten könnte. Geoffrey Hinton und David Rumelhart sprachen sich für eine Methode zur Schulung von Neuralnetzen aus, die als Rückpropagation bezeichnet werden, auch bekannt als umgekehrter Modus der automatischen Differenzierung, veröffentlicht von Seppo Linnainmaa0%) und für Neuralnetze von Paul Werbos. Diese beiden Entdeckungen trugen dazu bei, den Bereich der Verbindung zu beleben. Im Jahr 1986 wurde das neue Feld geeinigt und inspiriert durch das Auftreten einer Parallelverteilten Verarbeitung – eine zwei umfangreiche Sammlung von Papieren von Rumelhart und Psychologen James McClelland. Neural-Netze würden in den 90er Jahren kommerziell erfolgreich werden, wenn sie als Motorantriebsprogramme wie optische Anerkennung und Spracherkennung eingesetzt werden. Die Entwicklung von Metall-Oxide–Semiconduktor (MOS) sehr breit angelegte Integration (VLSI) in Form einerCn MOS (CMOS)-Technologie ermöglichte die Entwicklung praktischer künstlicher Neuralnetze (ANN) in den 1980er Jahren. Eine wegweisende Veröffentlichung im Bereich war das Buch Analog VLSI Implementierung von Neural Systems durch Carver A. Mead und Mohammed Ismail. Bust: der zweite AI Winter 1987-93 In den 1980er-Jahren stiegen die Begeisterung der Wirtschaft mit der AI und fiel in der klassischen Form einer wirtschaftlichen Blase. Der Zusammenbruch war auf das Versäumnis kommerzieller Anbieter zurückzuführen, eine Vielzahl von praktikablen Lösungen zu entwickeln. Dutzende von Unternehmen haben es versäumt, dass die Technologie nicht tragfähig war. Jedoch hat das Feld trotz der Kritik weiterhin Fortschritte gemacht. Viele Forscher, einschließlich Roboterentwickler Rodney Brooks und Hans Moravec sprachen sich für einen völlig neuen Ansatz für künstliche Intelligenz aus. AI Winter Der Begriff „AI Winter“ wurde von Forschern gelobt, die die Mittelkürzungen von 1974 überlebt hatten, wenn sie besorgt waren, dass die Begeisterung für Expertensysteme aus Kontrolle gerät und dass Enttäuschung sicherlich folgen würde. Ihre Ängste wurden begründet: Ende der 80er Jahre und Anfang der 90er Jahre litt AI eine Reihe finanzieller Rückschläge. erste Anzeichen für eine Änderung des Wetters waren der plötzliche Zusammenbruch des Marktes für spezielle AI-Hardware im Jahr 1987. Desktop-Computer von Apple und IBM haben immer schneller und Strom gewonnen, und 1987 wurden sie leistungsfähiger als die teureren Lisp-Maschinen, die durch Symbolik und andere hergestellt wurden. Es gab keinen guten Grund, sie zu kaufen. Eine ganze Industrie mit einem Wert von 50 Milliarden Dollar wurde über Nacht abgerissen. Letztendlich haben sich die erfolgreichsten Expertensysteme wie XCON als zu teuer erwiesen. Sie waren schwer zu aktualisieren, sie konnten nicht lernen, sie waren Bestechung (d. h. sie könnten bei außergewöhnlichen Inputs Fehler machen), und sie gingen auf Probleme (wie das Qualifikationsproblem) zurück, die früher erkannt wurden. Expertensysteme haben sich als nützlich erwiesen, aber nur in einigen besonderen Kontexten. Ende der achtziger Jahre reduziert die strategische Computer-Initiative die Finanzierung von AI "deeply und brutal". Neue Führung bei DARPA hatte beschlossen, dass AI nicht "die nächste Welle" war und Mittel für Projekte bereitstellte, die wahrscheinlichere Ergebnisse erzielen. 1991 wurde die beeindruckende Liste der im Jahr 1981 für das Fünfte Generationsprojekt Japans bestraften Ziele nicht erreicht. Manche von ihnen, wie zum Beispiel "Karry on afrau", waren bis 2010 nicht erfüllt. Wie bei anderen AI-Projekten waren die Erwartungen viel höher als das, was eigentlich möglich war.Mehr als 300 AI-Unternehmen hatten Abschaltung, Konkurs oder wurden Ende 1993 erworben, um die erste Handelswelle von AI wirksam zu beenden. Nouvelle AI und eminenter Grund Ende der 80er Jahre sprachen sich mehrere Forscher für einen völlig neuen Ansatz für künstliche Intelligenz aus, der auf Robotik basiert. Sie waren der Meinung, dass eine Maschine, um echte Intelligenz zu zeigen, einen Körper haben muss – sie muss sehen, bewegen, überleben und mit der Welt umgehen. Sie argumentierten, dass diese Sensorimotor-Fähigkeiten für höhere Qualifikationen wie gemeinsames Denken und abstrakte Gründe unerlässlich sind und dass die abstrakte Begründung tatsächlich das geringste interessante oder wichtige menschliche Fähigkeiten war (siehe Moravec's Paradox). Sie plädierten für den Aufbau von Intelligenz. " Der Ansatz hat Ideen von Cybernetics und Kontrolltheorie wiederbelebt, die seit den sechziger Jahren unpopulär war. David Marr, der in den späten siebziger Jahren aus einem erfolgreichen Hintergrund in der theoretischen Neurowissenschaften gekommen war, war ein weiterer Vorläufer. Er lehnte alle symbolischen Ansätze ab (sowie die Logik von McCarthy und Minsky's Framework), weshalb er behauptete, dass die AI notwendig sei, um die physikalischen Mechanismen der Vision vom Boden bis hin zu einer symbolischen Verarbeitung zu verstehen. (Marr's work würde 1980 durch Leukämie gekürzt werden).) In einem Papier von 1990 haben "Elephants Don't Play Schach" die Robotik-Forschunger Rodney Brooks direkt auf das physische Symbolsystem hypothesis gerichtet und behauptet, dass Symbole nicht immer notwendig sind, da "die Welt ihr eigenes Modell ist. Es ist immer genau bis heute. Jedes Detail ist immer bekannt. Es ist schwierig, es angemessen und oft genug zu verstehen. " In den achtziger und 90er Jahren lehnten viele kognitive Wissenschaftler auch das Symbolverarbeitungsmodell des Geistes ab und argumentierten, dass der Körper für die Begründung von wesentlicher Bedeutung sei, eine Theorie, die das Festlicht bezeichnete. AI 1993-2011 Mehr als ein halbes Jahrhundert alt, erreichten endlich einige seiner ältesten Ziele. In der gesamten Technologiebranche wurde es bereits erfolgreich genutzt, doch etwas hinter den Kulissen. Manche Erfolge waren auf die Erhöhung der Computerleistung zurückzuführen, und einige wurden erreicht, indem sie sich auf spezifische isolierte Probleme konzentrieren und sie mit den höchsten Standards der wissenschaftlichen Verantwortlichkeit verfolgen. Jedoch war der Ruf der AI zumindest in der Geschäftswelt weniger als unbeschränkt. Innerhalb des Bereichs gab es wenig Einvernehmen über die Gründe, warum AI den Traum der menschlichen Intelligenz nicht erfüllt hat, die in den 1960er Jahren die Vorstellung der Welt erfasst hatten. Gemeinsam haben alle diese Faktoren dazu beigetragen, dass die AI in konkurrierende Unterfelder zersplittert wurde, die sich auf bestimmte Probleme oder Ansätze konzentrierten, manchmal sogar unter neuen Namen, die den tarnished-Risiko von "artificial Intelligence" verschleiern. AI war sowohl vorsichtig als auch erfolgreicher als je zuvor. Meilensteine und Moore Recht am 11. Mai 1997, Tief Blue wurde das erste Computer-Wettlaufsystem, um einen Welt-Wettlaufführer, Garry Kasparov, zu schlagen. Der Supercomputer war eine spezielle Version eines von IBM hergestellten Rahmens und konnte zweimal so viele Schritte pro Sekunde verarbeiten, wie es während des ersten Spiel (das Deep Blue verloren hatte), berichteten über 200 000 Schritte pro Sekunde. Die Veranstaltung wurde live im Internet übertragen und erhielt über 74 Millionen Treffer. Im Jahr 2005 gewann ein Roboter von Stanford die DARPA Grand Challenge, indem er autonom für 131 Meilen entlang eines ungehörten Wüstenpfads fährt. Zwei Jahre später erhielt ein Team aus der Kapitalmarktunion den DARPA Urban Challenge, indem es 55 Meilen in einer städtischen Umgebung autonom beschifft und gleichzeitig Verkehrsgefahren und alle Verkehrsgesetze unterstützt. Mai 2011 in Jeopardy! Quiz-Ausstellungsspiel, IBM-Fragenantwortsystem, Watson, lehnte die beiden größten Jeopardy! Gewinner, Brad Rutter und Ken Jennings, mit einer erheblichen Marge. Diese Erfolge waren nicht auf ein revolutionäres neues Paradigma zurückzuführen, sondern vor allem auf die verschuldete Anwendung von Ingenieurkenntnissen und auf die enorme Erhöhung der Geschwindigkeit und Leistungsfähigkeit von Computern durch die 90er Jahre. Tatsächlich war der Deep Blue-Computer 10 Millionen Mal schneller als der Ferranti Mark 1, dass Christopher Strachey im Jahr 1951 als Schach unterrichtete. Dieser dramatische Anstieg wird durch das Gesetz von Moore gemessen, das vorausstellt, dass die Geschwindigkeits- und Speicherkapazität von Computern alle zwei Jahre verdoppelt, da Metall-Oxide-Semiconduktor (MOS) transistor alle zwei Jahre verdoppelt. Das grundlegende Problem der "belassenen Computerleistung" wurde langsam überwunden. Intelligente Agenten Ein neues Paradigma namens "intelligente Agenten" wurde in den 90er Jahren weitgehend akzeptiert. Obwohl frühere Forscher modulare "Diktid and Islands"-Ansätze für die AI vorgeschlagen hatten, konnte der intelligente Agenten seine moderne Form nicht erreichen, bis Ju Judea Pearl, Allen Newell, Marion P. Kaelbling und andere Konzepte aus der Entscheidungstheorie und Wirtschaft in die Studie der AI eingebracht haben. Wenn die Definition eines rationalen Erregers mit der Definition eines Gegenstands oder Moduls der Computerwissenschaft verheiratet war, war das Smart Agent Paradigma abgeschlossen. Ein intelligenter Agenten ist ein System, das seine Umwelt wahrnimmt und Maßnahmen trifft, die ihre Erfolgsaussichten maximieren. Klare Programme, die spezifische Probleme lösen, sind "intelligente Agenten", wie Menschen und Organisationen des Menschen, wie Unternehmen. Laut dem Modell „Intelligente Agenten“ ist die AI-Forschung „die Studie intelligenter Agenten“. Es handelt sich um eine allgemeine Definition einiger früherer Definitionen von AI: sie geht über das Studium menschlicher Erkenntnisse hinaus; sie untersucht alle Arten von Erkenntnissen. Das Paradigma gab den Forschern die Erlaubnis, isolierte Probleme zu untersuchen und Lösungen zu finden, die sowohl überprüfbar als auch nützlich waren. Es gab eine gemeinsame Sprache, um Probleme zu beschreiben und ihre Lösungen untereinander und mit anderen Bereichen zu teilen, die auch Konzepte von abstrakten Agenten wie Wirtschafts- und Kontrolltheorie verwenden. Man war zuversichtlich, dass ein komplettes System (wie das SOAR von Newell) den Forschern die Möglichkeit geben würde, aus der Interaktion intelligenter Wirkstoffe vielfältigere und intelligente Systeme aufzubauen. Die „Beurteilung der akkumulierten“ AI-Forscher begann, anspruchsvolle mathematische Werkzeuge zu entwickeln und zu nutzen, als sie jemals zuvor hatten. Es gab eine weit verbreitete Erkenntnis, dass viele der Probleme, die AI zur Lösung benötigt, bereits von Forschern in Bereichen wie Mathematik, Elektrotechnik, Wirtschaft oder Betriebsforschung untersucht wurden. Die gemeinsame mathematische Sprache erlaubte sowohl ein höheres Maß an Zusammenarbeit mit mehr etablierten und erfolgreichen Bereichen als auch die Erzielung von Ergebnissen, die messbar und erfolgversprechend waren; AI wurde zu einer strengeren wissenschaftlichen Disziplin. Russell & Norvig (2003) beschreiben dies als nichts weniger als eine Revolution und „der Sieg der Klaren“. Jua Pearls einflussreiches Buch von 1988 brachte Wahrscheinlichkeit und Entscheidungstheorie in die AI. Zu den vielen neuen Werkzeugen gehören die Bayesischen Netze, versteckte Markov-Modelle, die Informationstheorie, das Modell des Modells und die klassische Optimierung. Konkrete mathematische Beschreibungen wurden auch für "Chole Intelligence"-Symptome wie Neuralnetze und evolutionäre Algorithmen entwickelt. AI hinter den Kulissen Algorithms, die ursprünglich von AI-Forschern entwickelt wurden, begann als Teile größerer Systeme zu erscheinen. AI hatte viele sehr schwierige Probleme gelöst, und ihre Lösungen haben sich in der gesamten Technologiebranche als nützlich erwiesen, wie Data Mining, industrielle Robotik, Logistik, besondere Anerkennung, Banksoftware, medizinische Diagnose und Suchmaschine von Google. In den 90er und frühen 2000er Jahren erhielt der Bereich der AI wenig oder gar keinen Kredit. Viele der größten Neuerungen der AI wurden auf den Status eines einzigen weiteren Punkts in der Computerwissenschaft gesenkt. Nick Bostrom erläutert „Eine große Zahl an Randfängen hat in allgemeine Anwendungen gefiltert, oft ohne als AI bezeichnet zu werden, weil etwas sinnvoll genug und verbreitet genug ist, dass es nicht mehr als veretikettierte AI. " Viele Forscher in der AI in den 90er Jahren haben ihre Arbeit mit anderen Namen, wie Informatik, wissensbasierte Systeme, kognitive Systeme oder rechnerische Intelligenz, absichtlich aufgenommen. Zum Teil könnte dies geschehen, weil sie ihre Tätigkeit grundsätzlich von der AI unterscheiden, aber auch die neuen Namen helfen, die Finanzierung zu fördern. zumindest in der kommerziellen Welt gab es die versäumten Versprechen des AI Winters in den 2000er Jahren, da die New York Times 2005 berichtete: „Computer-Wissenschaftler und Software-Ingenieure vermeiden den Begriff künstliche Intelligenz, um Angst zu sehen, als Wildkörper angesehen zu werden“. Vorhersagen (oder "Wo ist HAL 9000?")In 1968 hatten Arthur C. Clarke und Stanley Kubrick gezeigt, dass bis zum Jahr 2001 eine Maschine mit einer Intelligenz existieren würde, die die Fähigkeiten des Menschen widerspiegelt oder übertroffen hat. HAL 9000 wurde auf einer von vielen führenden AI-Forschern geteilten Überzeugung gegründet, dass eine solche Maschine bis zum Jahr 2001 existiert. Im Jahr 2001 bat der AI Marvin Minsky "So die Frage ist warum wir 2001 nicht HAL bekommen?" Minsky glaubte, dass die Lösung darin besteht, dass die zentralen Probleme, wie die gemeinsame Grundhaltung, vernachlässigt wurden, während die meisten Forscher Dinge wie kommerzielle Anwendungen von Neuralnetzen oder genetischen Algorithmen verfolgen. John McCarthy hingegen schuldete das Qualifikationsproblem noch aus. Ray Kurzweil, das Thema ist Computerkraft und erwarte, dass Maschinen mit menschlicher Intelligenz bis 2029 erscheinen werden. Jeff Hawkins argumentierte, dass die neurale Netzforschung die wesentlichen Eigenschaften des menschlichen Kortex ignoriert und einfache Modelle bevorzugt, die bei der Lösung einfacher Probleme erfolgreich waren. Es gab viele andere Erklärungen und für jeden gab es ein entsprechendes Forschungsprogramm. Deep Learning, große Daten und künstliche allgemeine Intelligenz: 2011–2009 In den ersten Jahrzehnten des 21. Jahrhunderts wurde der Zugang zu großen Datenmengen (bekannt als „große Daten“), kostengünstigere und schnellere Computer und fortgeschrittene Maschinenbautechniken erfolgreich auf viele Probleme in der gesamten Wirtschaft angewendet. McKinsey Global Institute hat in ihrem berühmten Papier "Big Data: Die nächste Grenze für Innovation, Wettbewerb und Produktivität" geschätzt, dass "bis 2009 fast alle Wirtschaftszweige in den USA mindestens einen Durchschnitt von 200 terabyte gespeicherter Daten haben. Bis 2016 erreichte der Markt für AI-bezogene Produkte, Hardware und Software mehr als 8 Milliarden US-Dollar, und die New York Times berichtete, dass das Interesse an AI ein Frosch erreicht habe. Die Anwendungen großer Daten begannen, auch in andere Bereiche zu gelangen, wie beispielsweise Ausbildungsmodelle in der Ökologie und für verschiedene Anwendungen in der Wirtschaft. Fortschritte im tiefen Lernen (insbesondere tiefe konvolutionale Neuralnetze und wiederkehrende Neuralnetze) führten zu Fortschritten und Forschung in der Bild- und Videoverarbeitung, Textanalyse und sogar Spracherkennung. Deep Learning Deep Learning ist eine Branche des maschinenlesbaren Lernens, die hochaufwendige abstrakten Daten durch die Verwendung eines tiefen Graphik mit vielen Verarbeitungsschichten entwickelt. Laut der universellen Angleichung ist die Tiefe nicht notwendig für ein Neuralnetz, das willkürliche kontinuierliche Funktionen angleichen kann. Selbst so gibt es viele Probleme, die häufig zu flachen Netzen (wie der Überrüstung) gehören, die zu vermeiden sind. Insofern sind tiefgreifende Neuralnetze in der Lage, im Vergleich zu ihren flachen Kollegen viel komplexere Modelle zu erzeugen. Das tiefe Lernen hat jedoch Probleme. Ein gemeinsames Problem für die wiederkehrenden Neuralnetze ist das Problem der schwindenden Kluft, bei dem die zwischen den Schichten nach und nach schwindenden und wörtlich verschwindenden Verhältnisse, da sie auf Null abgerundet werden. Es wurden viele Methoden entwickelt, um dieses Problem anzugehen, wie z.B. langfristige Speichereinheiten. State-of-the-art tiefe Neural-Netzarchitekturen können manchmal sogar konkurrierende menschliche Genauigkeit in Bereichen wie Computervision, insbesondere was die MNIST-Datenbank betrifft, und Verkehrszeichenerkennung. Sprachverarbeitungsmotoren, die von intelligenten Suchmaschinen angetrieben werden, können den Menschen bei der Beantwortung allgemeiner Trivia-Fragen (wie IBM Watson) leicht entschädigen und die jüngsten Entwicklungen im tiefen Lernen haben zu positiven Ergebnissen im Wettbewerb mit dem Menschen geführt, unter anderem Go, und Justin (das ist ein erstes Spiel, das einige Kontroverse ausgelöst hat). Big Data Big Data bezieht sich auf eine Sammlung von Daten, die nicht erfasst, verwaltet und von herkömmlichen Software-Werkzeugen innerhalb eines bestimmten Zeitraums verarbeitet werden können. Es ist ein massiver Anteil an Entscheidungsprozessen, Einblick und Prozessoptimierungsfähigkeiten, die neue Verarbeitungsmodelle benötigen. In der Big Data Era, geschrieben von Victor Meyerlingenberg und Kenneth Cooke, bedeutet dies, dass statt der Zufallsanalyse (sample Survey) alle Daten zur Analyse verwendet werden. Kennzeichnend für große Daten (von IBM): Volumen, Geschwindigkeit, Variety, Wert, Veracity. strategische Bedeutung der großen Datentechnologie besteht nicht darin, enorme Dateninformationen zu beherrschen, sondern sich auf diese aussagekräftigen Daten zu spezialisieren. In anderen Worten, wenn große Daten einer Branche gleichgestellt werden, ist der Schlüssel zur Erzielung der Rentabilität in dieser Branche darin, die "Process-Fähigkeit" der Daten zu erhöhen und die "Wertschöpfung" der Daten durch die Verarbeitung zu verwirklichen. Künstliche allgemeine Intelligenz ist die Fähigkeit, Probleme zu lösen, anstatt eine Lösung für ein bestimmtes Problem zu finden. Künstliche allgemeine Intelligenz (oder AGI) ist ein Programm, das die Intelligenz auf eine Vielzahl von Problemen anwenden kann, in viel gleicher Weise kann der Mensch. Ben Goertze und andere sprachen in den frühen 2000er Jahren daran, dass die AI-Forschung weitgehend auf das ursprüngliche Ziel des Bereichs ausgerichtet war, künstliche allgemeine Intelligenz zu schaffen. AGI-Forschung wurde als separater Teilbereich gegründet und bis 2010 gab es akademische Konferenzen, Labors und Hochschulen, die der AGI-Forschung gewidmet sind, sowie private Konsortiume und neue Unternehmen. Künstliche allgemeine Intelligenz wird auch als "starke AI", "vollständige AI," oder synthetische Intelligenz im Gegensatz zu "weak AI" oder "narrow AI" bezeichnet. () Wissenschaftliche Quellen behalten "starke AI" vor, um auf Maschinen zu verweisen, die in der Lage sind, Bewusstsein zu erlangen. Beschreibung der künstlichen Intelligenz Fortschritte bei der künstlichen Intelligenz Zeit der künstlichen Intelligenz Geschichte der natürlichen Sprachverarbeitung Zeit des maschinenlesbaren Lernens Geschichte der Wissensvertretung und der damit verbundenen Grundkenntnisse (Referenzdaten)