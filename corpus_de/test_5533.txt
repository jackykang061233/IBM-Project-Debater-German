Eine zentrale Verarbeitungseinheit (CPU), die auch als zentraler Prozessor, Hauptprozessor oder nur Prozessor bezeichnet wird, ist die elektronische Schaltung, die Anweisungen ausführt, die ein Computerprogramm umfassen. Die CPU führt grundlegende arithmetische, logische, steuernde und Ein-/Ausgangsoperationen (I/O) durch, die durch die Anweisungen im Programm vorgegeben werden. Dies steht im Gegensatz zu externen Komponenten wie dem Hauptspeicher und der I/O-Schaltung und spezialisierten Prozessoren, wie z.B. Grafikverarbeitungseinheiten (GPUs). Die Form, das Design und die Implementierung von CPUs haben sich im Laufe der Zeit verändert, aber ihr grundlegender Betrieb bleibt nahezu unverändert. Zu den Hauptkomponenten einer CPU gehören die arithmetische Logikeinheit (ALU), die arithmetische und logische Operationen durchführt, Prozessorregister, die die Operanden der ALU liefern und die Ergebnisse der ALU-Operationen speichern, und eine Steuereinheit, die das Fetching (vom Speicher,) dekodieren und Ausführung von Anweisungen durch die Leitung der koordinierten Operationen der ALU, Register und andere Komponenten orchestriert. Die meisten modernen CPUs sind auf integrierten Schaltungen (IC) Mikroprozessoren implementiert, mit einer oder mehreren CPUs auf einem einzigen Metalloxid-Halbleiter (MOS) IC-Chip. Mikroprozessorchips mit mehreren CPUs sind Multi-Core-Prozessoren. Die einzelnen physikalischen CPUs, Prozessorkerne, können auch multithreaded werden, um zusätzliche virtuelle oder logische CPUs zu erstellen. Ein IC, das eine CPU enthält, kann auch Speicher, periphere Schnittstellen und andere Komponenten eines Computers enthalten; solche integrierten Geräte werden auf einem Chip (SoC) verschiedentlich Mikrocontroller oder Systeme genannt. Array-Prozessoren oder Vektor-Prozessoren haben mehrere Prozessoren, die parallel arbeiten, ohne Einheit als zentral betrachtet. Virtuelle CPUs sind eine Abstraktion dynamischer aggregierter Rechenressourcen. Geschichte Frühe Computer wie die ENIAC mussten physisch neu verdrahtet werden, um verschiedene Aufgaben zu erfüllen, die diese Maschinen als "Fixed-Programm-Computer" nannten. Der Begriff "Zentralverarbeitungseinheit" ist seit 1955 im Einsatz. Da der Begriff CPU in der Regel als Gerät für die Software (Computerprogramm)-Ausführung definiert ist, kamen die frühesten Geräte, die zu Recht als CPUs bezeichnet werden könnten, mit dem Aufkommen des Speicherprogrammrechners zusammen. Die Idee eines Speicherprogramm-Computers war bereits in der Gestaltung von J. Presper Eckert und John William Mauchlys ENIAC vorhanden, wurde aber zunächst weggelassen, so dass es früher fertig sein konnte. Am 30. Juni 1945 hat der Mathematiker John von Neumann vor ENIAC den ersten Entwurf eines Berichts über den EDVAC verteilt. Es war der Umriss eines Speicherprogramm-Computers, der im August 1949 abgeschlossen werden würde. EDVAC wurde entwickelt, um eine bestimmte Anzahl von Anweisungen (oder Operationen) verschiedener Arten durchzuführen. Deutlicherweise sollten die für EDVAC geschriebenen Programme nicht durch die physikalische Verdrahtung des Computers im Hochgeschwindigkeits-Computerspeicher gespeichert werden. Damit wurde eine starke Einschränkung von ENIAC überwunden, was die beträchtliche Zeit und Mühe war, den Computer neu zu konfigurieren, um eine neue Aufgabe durchzuführen. Mit dem Design von Neumann konnte das Programm, das EDVAC lief, einfach geändert werden, indem der Inhalt des Speichers geändert wird. EDVAC war jedoch nicht der erste Speicherprogramm-Computer; das Manchester Baby, ein kleiner experimenteller Speicherprogramm-Computer, lief sein erstes Programm am 21. Juni 1948 und das Manchester Mark 1 lief sein erstes Programm in der Nacht vom 16.–17. Juni 1949. Frühe CPUs waren kundenspezifische Designs, die als Teil eines größeren und manchmal unverwechselbaren Computers verwendet wurden. Diese Methode zur Auslegung von benutzerdefinierten CPUs für eine bestimmte Anwendung hat jedoch weitgehend die Entwicklung von Mehrzweckprozessoren in großen Mengen ermöglicht. Diese Standardisierung begann im Zeitalter von diskreten Transistor-Mainframes und Minicomputern und beschleunigte sich mit der Popularisierung der integrierten Schaltung (IC). Der IC ermöglichte zunehmend komplexe CPUs auf Toleranzen in der Größenordnung von Nanometern zu entwerfen und herzustellen. Sowohl die Miniaturisierung als auch die Standardisierung von CPUs haben die Präsenz digitaler Geräte im modernen Leben weit über die begrenzte Anwendung dedizierter Rechenmaschinen hinaus gesteigert. Moderne Mikroprozessoren erscheinen in elektronischen Geräten, von Automobilen bis zu Mobiltelefonen und manchmal sogar in Spielzeug. Während von Neumann wegen seines Designs von EDVAC am häufigsten mit dem Design des Speicherprogramm-Computers ausgezeichnet wird und das Design als von Neumann-Architektur bekannt wurde, hatten andere vor ihm, wie Konrad Zuse, ähnliche Ideen vorgeschlagen und umgesetzt. Die so genannte Harvard-Architektur des Harvard Mark I, die vor EDVAC fertiggestellt wurde, nutzte auch ein gespeichertes Programmdesign mit gestanztem Papierband anstatt elektronischem Speicher. Der wesentliche Unterschied zwischen den von Neumann- und Harvard-Architekturen besteht darin, dass diese die Speicherung und Behandlung von CPU-Anweisungen und -Daten trennt, während sie für beide den gleichen Speicherplatz verwendet. Die meisten modernen CPUs sind in erster Linie von Neumann im Design, aber auch CPUs mit der Harvard-Architektur werden gesehen, vor allem in Embedded-Anwendungen; zum Beispiel die Atmel AVR Mikrocontroller sind Harvard-Architekturprozessoren. Relais und Vakuumröhren (Thermoröhren) wurden häufig als Schaltelemente verwendet; ein nützlicher Computer benötigt Tausende oder Zehntausende von Schaltgeräten. Die Gesamtgeschwindigkeit eines Systems ist von der Drehzahl der Schalter abhängig. Vakuumröhren-Computer wie EDVAC neigten im Durchschnitt acht Stunden zwischen Fehlern, während Relais-Computer wie die (slower, aber früher)Harvard Mark I scheiterte sehr selten. Schließlich wurden röhrenbasierte CPUs dominant, weil die signifikanten Geschwindigkeitsvorteile die Zuverlässigkeitsprobleme im Allgemeinen überwiegen. Die meisten dieser frühen Synchron-CPUs liefen mit niedrigen Taktraten im Vergleich zu modernen mikroelektronischen Designs. Taktsignalfrequenzen von 100 kHz bis 4 MHz waren zu dieser Zeit sehr häufig, begrenzt weitgehend durch die Geschwindigkeit der Schaltgeräte, mit denen sie gebaut wurden. Transistor CPUs Die Design-Komplexität von CPUs erhöhte sich, da verschiedene Technologien den Aufbau kleinerer und zuverlässiger elektronischer Geräte erleichterten. Die erste solche Verbesserung kam mit dem Aufkommen des Transistors. Transistorisierte CPUs in den 1950er und 1960er Jahren mussten nicht mehr aus sperrigen, unzuverlässigen und zerbrechlichen Schaltelementen wie Vakuumröhren und Relais aufgebaut werden. Mit dieser Verbesserung wurden komplexere und zuverlässige CPUs auf eine oder mehrere Leiterplatten mit diskreten (individuellen) Komponenten aufgebaut. 1964 führte IBM seine IBM System/360 Computerarchitektur ein, die in einer Reihe von Computern verwendet wurde, die in der Lage sind, dieselben Programme mit unterschiedlicher Geschwindigkeit und Leistung auszuführen. Dies war in einer Zeit von Bedeutung, in der die meisten elektronischen Computer miteinander unvereinbar waren, auch die des gleichen Herstellers. Um diese Verbesserung zu erleichtern, nutzte IBM das Konzept eines Mikroprogramms (oft Mikrocode genannt), das noch weit verbreitete Nutzung in modernen CPUs sieht. Die System/360-Architektur war so populär, dass sie seit Jahrzehnten den Computer-Mainframe-Markt dominierte und ein Vermächtnis hinterlassen hat, das noch von ähnlichen modernen Computern wie der IBM zSeries fortgeführt wird. Im Jahr 1965 führte die Digital Equipment Corporation (DEC) einen weiteren einflussreichen Computer ein, der auf die wissenschaftlichen und Forschungsmärkte, die PDP-8, zielte. Transistorbasierte Computer hatten gegenüber ihren Vorgängern mehrere deutliche Vorteile. Neben der Erleichterung der erhöhten Zuverlässigkeit und des geringeren Stromverbrauchs erlaubten Transistoren auch CPUs aufgrund der kurzen Schaltzeit eines Transistors im Vergleich zu einem Rohr oder Relais mit viel höheren Drehzahlen zu arbeiten. Die erhöhte Zuverlässigkeit und dramatisch erhöhte Geschwindigkeit der Schaltelemente (die bis zu dieser Zeit fast ausschließlich Transistoren waren); CPU-Taktraten in den Zehnen von Megahertz wurden in dieser Zeit leicht erhalten. Zusätzlich, während diskrete Transistor und IC-CPUs in schwerem Gebrauch waren, begannen neue Hochleistungs-Designs wie SIMD (Single Instruction Multiple Data) Vektor-Prozessoren zu erscheinen. Diese frühen experimentellen Entwürfe führten später zur Ära spezialisierter Supercomputer wie die von Cray Inc und Fujitsu Ltd. Small-Skala Integration CPUs In dieser Zeit wurde eine Methode zur Herstellung von vielen miteinander verbundenen Transistoren in einem kompakten Raum entwickelt. Die integrierte Schaltung (IC) erlaubte es, eine Vielzahl von Transistoren auf einem einzigen Halbleiter-basierten Werkzeug oder Chip herzustellen." Zunächst wurden nur sehr einfache nicht spezialisierte digitale Schaltungen wie NOR-Gatter in ICs miniaturisiert. CPUs auf Basis dieser "Baustein"-ICs werden im Allgemeinen als "kleine Integration" (SSI)-Geräte bezeichnet. SSI-ICs, wie die im Apollo Guidance Computer verwendeten, enthielten in der Regel bis zu einigen Dutzend Transistoren. Um eine ganze CPU aus SSI ICs zu bauen, benötigte Tausende von einzelnen Chips, verbrauchte aber noch viel weniger Platz und Leistung als früher diskrete Transistordesigns. IBM's System/370, Anschluss an das System/360, verwendet SSI-ICs anstatt Solid Logic Technology diskrete-Transistor-Module. Auch die PDP-8/I und KI10 PDP-10 von den einzelnen Transistoren, die von den PDP-8 und PDP-10 auf SSI ICs verwendet werden, schalten aus, und ihre äußerst populäre PDP-11-Leitung wurde ursprünglich mit SSI ICs aufgebaut, wurde aber schließlich mit LSI-Komponenten umgesetzt, sobald diese praktisch wurden. Große Integration CPUs Lee Boysel veröffentlichte einflussreiche Artikel, darunter ein 1967er Manifest, das beschreibt, wie das Äquivalent eines 32-Bit-Mainframe-Computers aus einer relativ geringen Anzahl von großformatigen Integrationsschaltungen (LSI) aufgebaut werden kann. Die einzige Möglichkeit, LSI-Chips zu bauen, die Chips mit hundert oder mehr Gates sind, bestand darin, diese mit einem MOS-Halbleiterherstellungsprozess aufzubauen (entweder PMOS-Logik, NMOS-Logik oder CMOS-Logik). Einige Unternehmen bauten jedoch weiterhin Prozessoren aus bipolaren Transistor-Transistor-Logik (TTL)-Chips, da bipolare Verbindungstransistoren bis in die 1970er Jahre schneller waren als MOS-Chips (einige Unternehmen wie Datapoint bauten bis Anfang der 1980er Jahre Prozessoren aus TTL-Chips weiter). In den 1960er Jahren waren MOS-ICs langsamer und zunächst nur bei Anwendungen, die geringe Leistung erforderten, nützlich. Nach der Entwicklung der Silizium-Gate-MOS-Technologie von Federico Faggin auf Fairchild Semiconductor im Jahr 1968 ersetzten MOS ICs in den frühen 1970er Jahren die bipolare TTL als Standard-Chip-Technologie. Als die mikroelektronische Technologie voranging, wurde eine zunehmende Anzahl von Transistoren auf ICs gelegt, wodurch die Anzahl der einzelnen ICs verringert wurde, die für eine komplette CPU benötigt werden. MSI und LSI ICs haben den Transistor auf Hunderte und dann Tausende erhöht. 1968 wurde die Anzahl der ICs, die für den Aufbau einer kompletten CPU benötigt wurden, auf 24 ICs von acht verschiedenen Typen reduziert, wobei jeder IC etwa 1000 MOSFETs enthält. Im Gegensatz zu seinen SSI- und MSI-Vorläufern enthielt die erste LSI-Implementierung des PDP-11 eine CPU aus nur vier LSI-integrierten Schaltungen. Mikroprozessoren Fortschritte in der MOS-IC-Technologie führten zu der Erfindung des Mikroprozessors in den frühen 1970er Jahren. Seit der Einführung des ersten kommerziell erhältlichen Mikroprozessors, des Intel 4004 im Jahre 1971 und des ersten weit verbreiteten Mikroprozessors, des Intel 8080 im Jahre 1974, hat diese Klasse von CPUs fast alle anderen zentralen Verarbeitungseinheit-Implementierungsverfahren vollständig überholt. Mainframe- und Minicomputer-Hersteller der Zeit starteten proprietäre IC-Entwicklungsprogramme, um ihre älteren Computer-Architekturen zu aktualisieren, und produzierten schließlich kompatible Mikroprozessoren, die mit ihrer älteren Hardware und Software rückwärtskompatibel waren. In Kombination mit dem Aufkommen und dem eventuellen Erfolg des allgegenwärtigen Personalcomputers wird nun der Begriff CPU fast ausschließlich auf Mikroprozessoren angewendet. Mehrere CPUs (denotierte Kerne) können in einem einzigen Verarbeitungschip kombiniert werden. Frühere Generationen von CPUs wurden als diskrete Komponenten und zahlreiche kleine integrierte Schaltungen (ICs) auf einer oder mehreren Leiterplatten realisiert. Mikroprozessoren hingegen sind CPUs, die auf einer sehr geringen Anzahl von ICs hergestellt werden; meist nur eins. Die insgesamt kleinere CPU-Größe, da sie auf einer einzigen Matrize implementiert wird, bedeutet aufgrund physikalischer Faktoren wie verminderter Gateparasitärkapazität eine schnellere Schaltzeit. Dadurch konnten synchrone Mikroprozessoren Taktraten von zehn Megahertz bis zu mehreren Gigahertz haben. Darüber hinaus hat die Fähigkeit, extrem kleine Transistoren auf einem IC zu bauen, die Komplexität und Anzahl der Transistoren in einer einzigen CPU erhöht viele Falten. Dieser weithin beobachtete Trend wird durch Moore's Gesetz beschrieben, die sich bis 2016 als ziemlich genaue Vorhersage des Wachstums von CPU (und anderen IC) Komplexität erwiesen hatte. Während sich die Komplexität, Größe, Konstruktion und allgemeine Form von CPUs seit 1950 enorm verändert haben, hat sich das Grunddesign und die Funktion gar nicht verändert. Fast alle gängigen CPUs können heute sehr genau als von Neumann-Speicherprogrammmaschinen beschrieben werden. Da Moore's Gesetz nicht mehr hält, haben sich Sorgen um die Grenzen der integrierten Schaltungstransistortechnologie ergeben. Die extreme Miniaturisierung von elektronischen Toren bewirkt, dass die Effekte von Phänomenen wie Elektromigration und Subthreshold-Leckage viel wichtiger werden. Diese neueren Bedenken gehören zu den vielen Faktoren, die Forscher dazu veranlassen, neue Methoden des Computings wie den Quantencomputer zu untersuchen, sowie die Nutzung von Parallelismus und anderen Methoden, die die Nützlichkeit des klassischen von Neumann Modells erweitern. Betrieb Der grundsätzliche Betrieb der meisten CPUs ist, unabhängig von der physikalischen Form, die sie ergreifen, eine Folge von gespeicherten Anweisungen auszuführen, die als Programm bezeichnet wird. Die auszuführenden Anweisungen werden in einem Computerspeicher aufbewahrt. Fast alle CPUs folgen dem Fetch, decodieren und ausführen Schritte in ihrem Betrieb, die gemeinsam als Instruktionszyklus bekannt sind. Nach der Ausführung einer Instruktion wiederholt sich der gesamte Prozess, wobei der nächste Instruktionszyklus aufgrund des inkrementierten Wertes im Programmzähler normalerweise den nächsten In-Sequenzbefehl abruft. Wenn eine Sprunganweisung ausgeführt wurde, wird der Programmzähler modifiziert, um die Adresse der Anweisung zu enthalten, auf die gesprungen wurde und die Programmausführung normalerweise fortsetzt. In komplexeren CPUs können mehrere Anweisungen gleichzeitig abgeholt, decodiert und ausgeführt werden. Dieser Abschnitt beschreibt, was allgemein als "klassische RISC-Pipeline" bezeichnet wird, was unter den einfachen CPUs, die in vielen elektronischen Geräten (oft Mikrocontroller genannt) verwendet werden, durchaus üblich ist. Es ignoriert die wichtige Rolle des CPU-Caches und damit die Zugriffsphase der Pipeline weitgehend. Einige Anweisungen manipulieren den Programmzähler, anstatt Ergebnisdaten direkt zu erzeugen; solche Anweisungen werden in der Regel als Sprünge bezeichnet und erleichtern Programmverhalten wie Schlaufen, bedingte Programmausführung (durch die Verwendung eines bedingten Sprungs) und Existenz von Funktionen. In einigen Prozessoren ändern einige andere Anweisungen den Zustand der Bits in einem Flags-Register. Diese Fahnen können verwendet werden, um zu beeinflussen, wie sich ein Programm verhält, da sie oft das Ergebnis verschiedener Operationen angeben. Beispielsweise wertet in solchen Prozessoren eine Vergleichsanweisung zwei Werte aus und setzt bzw. räumt Bits im Flags-Register aus, um anzuzeigen, welche größer oder gleich sind; eine dieser Fahnen könnte dann durch eine spätere Sprunganweisung zur Bestimmung des Programmflusses verwendet werden. Gebühren Der erste Schritt, fetch, beinhaltet das Abrufen eines Befehls (der durch eine Anzahl oder Folge von Zahlen dargestellt ist) aus dem Programmspeicher. Der Befehlsort (Adresse) im Programmspeicher wird durch den Programmzähler (PC; genannt "Instruction Pointer" in Intel x86 Mikroprozessoren) bestimmt, der eine Zahl speichert, die die Adresse der nächsten zu holenden Anweisung identifiziert. Nach dem Starten einer Anweisung wird der PC um die Länge der Anweisung inkrementiert, so dass er die Adresse der nächsten Anweisung in der Sequenz enthalten wird. Oft muss die zu holende Anweisung aus relativ langsamem Speicher abgerufen werden, wodurch die CPU gestoppt wird, während die Anweisung zurückgegeben wird. Dieses Problem wird in modernen Prozessoren durch Caches und Pipeline-Architekturen (siehe unten) weitgehend angesprochen. Decode Der Befehl, den die CPU aus dem Speicher holt, bestimmt, was die CPU tun wird. Im Decodierschritt, der durch binäre Decoderschaltung, die als Befehlsdekoder bekannt ist, ausgeführt wird, wird der Befehl in Signale umgewandelt, die andere Teile der CPU steuern. Die Art und Weise, wie der Befehl interpretiert wird, wird durch die Anleitungs-Set-Architektur (ISA) der CPU definiert. Oft gibt eine Gruppe von Bits (d.h. ein Feld) innerhalb des Befehls, genannt der Opcode, an, welche Operation durchzuführen ist, während die übrigen Felder in der Regel ergänzende Informationen für die Operation, wie die Operanden, liefern. Diese Operanden können als konstanter Wert (sogenannter sofortiger Wert) oder als Ort eines Wertes, der ein Prozessorregister oder eine Speicheradresse sein kann, wie durch einen Adressierungsmodus bestimmt werden. In einigen CPU-Entwürfen wird der Befehlsdekoder als festverdrahtete, unveränderliche binäre Decoderschaltung ausgeführt. In anderen wird ein Mikroprogramm verwendet, um Anweisungen in Sätze von CPU-Konfigurationssignalen zu übersetzen, die sequentiell über mehrere Taktimpulse angelegt werden. In einigen Fällen ist der Speicher, der das Mikroprogramm speichert, umschreibbar, so dass es möglich ist, die Art und Weise zu ändern, in der die CPU Anweisungen decodiert. Durchführung Nach den Fetch- und Decodierschritten wird der Ausführungsschritt durchgeführt. Je nach CPU-Architektur kann dies aus einer einzigen Aktion oder einer Folge von Aktionen bestehen. Während jeder Aktion können Steuersignale verschiedene Teile der CPU elektrisch aktivieren oder deaktivieren, so dass sie ganz oder teilweise den gewünschten Betrieb ausführen können. Die Wirkung wird dann beendet, typischerweise auf einen Takt. Sehr oft werden die Ergebnisse in ein internes CPU-Register für schnellen Zugriff durch nachfolgende Anweisungen geschrieben. In anderen Fällen können Ergebnisse auf langsamer, aber weniger teuer und höhere Kapazität Hauptspeicher geschrieben werden. Soll beispielsweise eine Additionsanweisung ausgeführt werden, werden Register mit Operanden (zu summierende Zahlen) aktiviert, ebenso die Teile der arithmetischen Logikeinheit (ALU), die Addition durchführen. Bei Auftreten des Taktimpulses fließen die Operanden aus den Quellregistern in die ALU und die Summe erscheint am Ausgang. Bei nachfolgenden Taktimpulsen werden andere Komponenten aktiviert (und deaktiviert), um den Ausgang (die Summe der Operation) zum Speichern (z.B. Register oder Speicher) zu bewegen. Ist die resultierende Summe zu groß (d.h. sie ist größer als die Ausgangswortgröße der ALU), wird eine arithmetische Überlauffahne gesetzt, die die nächste Operation beeinflusst. Struktur und Implementierung Hardwired in eine CPU-Schaltung ist eine Reihe von grundlegenden Operationen, die es ausführen kann, genannt ein Befehlsset. Solche Operationen können z.B. zwei Zahlen addieren oder subtrahieren, zwei Zahlen vergleichen oder auf einen anderen Teil eines Programms springen. Jede Anweisung wird durch eine einzigartige Kombination von Bits dargestellt, die als Maschinensprache opcode bekannt sind. Bei der Bearbeitung einer Anweisung dekodiert die CPU den Opcode (über einen binären Decoder) in Steuersignale, die das Verhalten der CPU inszenieren. Eine vollständige maschinelle Sprachanweisung besteht aus einem Opcode und in vielen Fällen zusätzliche Bits, die Argumente für die Operation angeben (z.B. die bei einer Additionsoperation zu addierenden Zahlen). Ein Maschinenspracheprogramm ist eine Sammlung von Maschinensprachenanweisungen, die die CPU ausführt. Die eigentliche mathematische Operation für jede Anweisung wird durch eine Kombinationslogikschaltung innerhalb des Prozessors der CPU, der als Recheneinheit oder ALU bezeichnet wird, durchgeführt. In der Regel führt eine CPU eine Anweisung aus, indem sie sie aus dem Speicher herausholt, mit ihrer ALU eine Operation durchführt und dann das Ergebnis in den Speicher speichert. Neben den Anweisungen für ganzzahlige Mathematik und Logikoperationen existieren verschiedene andere Maschinenanweisungen, wie z.B. zum Laden von Daten aus dem Speicher und zum Zurückspeichern von Daten, Verzweigungen und mathematischen Operationen auf Floating-Point-Nummern, die von der Floating-Point-Einheit der CPU (FPU) ausgeführt werden.Steuereinheit Die Steuereinheit (CU) ist eine Komponente der CPU, die den Betrieb des Prozessors leitet. Es erzählt den Speicher des Computers, die Recheneinheit und die Eingabe- und Ausgabegeräte, wie man auf die Anweisungen reagiert, die an den Prozessor gesendet wurden. Sie leitet den Betrieb der anderen Einheiten durch die Bereitstellung von Takt- und Steuersignalen. Die meisten Computerressourcen werden von der CU verwaltet. Es leitet den Datenfluss zwischen der CPU und den anderen Geräten. John von Neumann umfasste die Steuereinheit als Teil der von Neumann Architektur. Bei modernen Computer-Designs ist die Steuereinheit typischerweise ein interner Teil der CPU mit ihrer Gesamtrolle und ihrem Betrieb seit ihrer Einführung unverändert. Arithmetische Logikeinheit Die arithmetische Logikeinheit (ALU) ist eine digitale Schaltung innerhalb des Prozessors, die ganzzahlige arithmetische und bitweise Logikoperationen durchführt. Bei den Eingängen der ALU handelt es sich um die zu betreibenden Datenworte (sog. Operanden,) Statusinformationen aus früheren Operationen und um einen Code der Steuereinheit, der angibt, welche Operation durchzuführen ist. Je nach Ausführung des Befehls können die Operanden aus internen CPU-Registern oder externem Speicher stammen, oder es können Konstanten sein, die von der ALU selbst erzeugt werden. Wenn alle Eingangssignale durch die ALU-Schaltung abgeglichen und propagiert sind, erscheint das Ergebnis der durchgeführten Operation an den Ausgängen der ALU. Das Ergebnis besteht sowohl aus einem Datenwort, das in einem Register oder Speicher abgelegt werden kann, als auch aus Statusinformationen, die typischerweise in einem speziellen, internen CPU-Register gespeichert sind, das hierfür reserviert ist. Adressenerzeugungseinheit Adressenerzeugungseinheit (AGU), manchmal auch Adressrechner (ACU) genannt, ist eine Ausführungseinheit innerhalb der CPU, die von der CPU verwendete Adressen berechnet, um auf den Hauptspeicher zuzugreifen. Durch die parallel zum Rest der CPU betriebenen Adressberechnungen können die für die Ausführung verschiedener Maschinenanweisungen erforderlichen CPU-Zyklen reduziert werden, wodurch Leistungsverbesserungen erzielt werden. Bei der Durchführung verschiedener Operationen müssen CPUs Speicheradressen berechnen, die für das Abrufen von Daten aus dem Speicher erforderlich sind; zum Beispiel müssen in-memory-Positionen von Array-Elementen berechnet werden, bevor die CPU die Daten von aktuellen Speicherplätzen abrufen kann. Diese Adreß-Generations-Berechnungen beinhalten verschiedene ganzzahlige arithmetische Operationen, wie Addition, Subtraktion, Modulo-Operationen oder Bitverschiebungen. Oft beinhaltet die Berechnung einer Speicheradresse mehr als eine allgemeine Maschinenanweisung, die nicht notwendigerweise schnell decodiert und ausgeführt wird. Durch die Einbindung einer AGU in ein CPU-Design, zusammen mit der Einführung von spezialisierten Anweisungen, die die AGU verwenden, können verschiedene Adreß-Generations-Berechnungen vom Rest der CPU abgeladen werden und können oft schnell in einem einzigen CPU-Zyklus ausgeführt werden. Fähigkeiten einer AGU hängen von einer bestimmten CPU und ihrer Architektur ab. So implementieren und entlarven einige AGUs mehr Adress-Berechnungsoperationen, während einige auch erweiterte spezialisierte Anweisungen enthalten, die auf mehreren Operanden zu einer Zeit arbeiten können. Darüber hinaus umfassen einige CPU-Architekturen mehrere AGU, so dass mehr als eine Adreß-Berechnungsoperation gleichzeitig ausgeführt werden kann, wodurch weitere Leistungsverbesserungen durch Kapitalisierung auf die superskalare Natur fortschrittlicher CPU-Designs erzielt werden. Zum Beispiel enthält Intel mehrere AGU in seine Sandy Bridge und Haswell Mikroarchitectures, die die Bandbreite des CPU-Speicher-Subsystems erhöhen, indem mehrere Speicherzugriffsanweisungen parallel ausgeführt werden können. Speicherverwaltungseinheit (MMU)Viele Mikroprozessoren (in Smartphones und Desktop, Laptop, Server-Computern) verfügen über eine Speicherverwaltungseinheit, die logische Adressen in physikalische RAM-Adressen überträgt, Speicherschutz und Paging-Fähigkeiten bietet, die für den virtuellen Speicher nützlich sind. Einfachere Prozessoren, insbesondere Mikrocontroller, enthalten in der Regel keine MMU. Cache Ein CPU-Cache ist ein Hardware-Cache, der von der zentralen Verarbeitungseinheit (CPU) eines Computers verwendet wird, um die durchschnittlichen Kosten (Zeit oder Energie) zu reduzieren, um auf Daten aus dem Hauptspeicher zugreifen zu können. Ein Cache ist ein kleinerer schneller Speicher, näher an einem Prozessorkern, der Kopien der Daten von häufig verwendeten Hauptspeicherstellen speichert. Die meisten CPUs haben unterschiedliche unabhängige Caches, einschließlich Befehls- und Datencaches, wobei der Datencache in der Regel als Hierarchie von mehr Cache-Pegeln (L1, L2, L3, L4, etc.) organisiert wird. Alle modernen (schnellen) CPUs (mit wenigen spezialisierten Ausnahmen) haben mehrere CPU-Caches. Die ersten CPUs, die einen Cache verwendet haben, hatten nur eine Cache-Ebene; im Gegensatz zu späteren Level 1 Caches wurde er nicht in L1d (für Daten) und L1i (für Anweisungen) aufgeteilt. Fast alle aktuellen CPUs mit Caches haben einen Split L1 Cache. Sie haben auch L2 Caches und für größere Prozessoren auch L3 Caches. Der L2-Cache ist in der Regel nicht geteilt und fungiert als gemeinsames Repository für den bereits gespaltenen L1-Cache. Jeder Kern eines Multi-Core-Prozessors hat einen dedizierten L2-Cache und wird in der Regel nicht zwischen den Kernen geteilt. Die L3-Cache und die höheren Caches werden zwischen den Kernen geteilt und nicht aufgeteilt. Ein L4-Cache ist derzeit ungewöhnlich, und ist im Allgemeinen auf dynamischem Zufalls-Access-Speicher (DRAM), anstatt auf statischem Zufalls-Access-Speicher (SRAM,) auf einem separaten Werkzeug oder Chip. Das war auch historisch mit L1 der Fall, während größere Chips die Integration von ihm und in der Regel alle Cache Levels erlaubt haben, mit der möglichen Ausnahme der letzten Ebene. Jeder zusätzliche Cache-Spiegel neigt dazu, größer zu sein und anders optimiert zu werden. Andere Arten von Caches existieren (die nicht auf die "Cache-Größe" der wichtigsten oben genannten Caches gezählt werden), wie der Translation Lookaside Puffer (TLB), der Teil der Speicherverwaltungseinheit (MMU), die die meisten CPUs haben. Die Caches sind in der Regel in zwei Kräften: 4, 8, 16 usw. bemessen. KiB oder MiB (für größere Nicht-L1) Größen, obwohl der IBM z13 einen 96 KiB L1 Instruktionscache hat. Uhrzeit am meisten CPUs sind Synchronschaltungen, d.h. sie verwenden ein Taktsignal, um deren Folgeoperationen zu beschleunigen. Das Taktsignal wird von einer externen Oszillatorschaltung erzeugt, die jeweils eine konstante Anzahl von Impulsen in Form einer periodischen Rechteckwelle erzeugt. Die Frequenz der Taktimpulse bestimmt die Rate, mit der eine CPU Befehle ausführt und damit je schneller der Takt, desto mehr Anweisungen wird die CPU jede Sekunde ausführen. Um einen ordnungsgemäßen Betrieb der CPU zu gewährleisten, ist die Taktperiode länger als die maximale Zeit, die für alle Signale benötigt wird, um die CPU (Bewegung) zu propagieren. Bei der Einstellung der Taktperiode auf einen Wert weit über der schlimmsten Laufzeit ist es möglich, die gesamte CPU und die Art, wie sie Daten um die Ränder des ansteigenden und fallenden Taktsignals bewegt. Dies hat den Vorteil, die CPU deutlich zu vereinfachen, sowohl aus konstruktiver Perspektive als auch aus komponentenzählerischer Perspektive. Es trägt aber auch den Nachteil, dass die gesamte CPU auf ihren langsamsten Elementen warten muss, obwohl einige Teile davon viel schneller sind. Diese Einschränkung wurde weitgehend durch verschiedene Methoden der Erhöhung der CPU-Parallelismus kompensiert (siehe unten). Architekturverbesserungen allein lösen jedoch nicht alle Nachteile global synchroner CPUs. Beispielsweise unterliegt ein Taktsignal den Verzögerungen eines anderen elektrischen Signals. Höhere Taktraten in zunehmend komplexer CPUs machen es schwieriger, das Taktsignal während der gesamten Einheit in Phase (synchronisiert) zu halten. Dies hat viele moderne CPUs dazu geführt, dass mehrere identische Taktsignale benötigt werden, um zu vermeiden, dass ein einziges Signal deutlich genug verzögert wird, um die CPU zu stören. Ein weiteres wichtiges Problem, da die Taktraten dramatisch zunehmen, ist die Wärmemenge, die von der CPU abgeführt wird. Die ständig wechselnde Uhr bewirkt, dass viele Komponenten unabhängig davon wechseln, ob sie zu diesem Zeitpunkt verwendet werden. Im allgemeinen verwendet ein Schaltelement mehr Energie als ein Element in einem statischen Zustand. Mit zunehmender Taktrate wird also der Energieverbrauch erhöht, wodurch die CPU mehr Wärmeableitung in Form von CPU-Kühllösungen erfordert. Ein Verfahren zur Behandlung des Schaltens nicht benötigter Komponenten wird als Taktgeber bezeichnet, der das Taktsignal auf nicht benötigte Komponenten abschaltet (effektiv deaktivieren). Dies gilt jedoch oft als schwierig zu implementieren und sieht daher außerhalb sehr leistungsarmer Designs keinen gemeinsamen Gebrauch. Ein bemerkenswertes jüngstes CPU-Design, das umfangreiche Taktschläge verwendet, ist das IBM PowerPC-basierte Xenon in der Xbox 360 verwendet; so werden die Leistungsanforderungen der Xbox 360 stark reduziert. Eine weitere Methode, einige der Probleme mit einem globalen Taktsignal zu adressieren, ist die Entfernung des Taktsignals insgesamt. Während das Entfernen des globalen Taktsignals den Designprozess in vielerlei Hinsicht erheblich komplexer macht, tragen asynchrone (oder taktlose) Designs im Vergleich zu ähnlichen synchronen Designs deutliche Vorteile bei Stromverbrauch und Wärmeableitung. Während etwas ungewöhnlich, ganz asynchron CPUs wurden ohne Verwendung eines globalen Taktsignals gebaut. Zwei bemerkenswerte Beispiele hierfür sind der ARM konforme AMULET und der MIPS R3000 kompatible MiniMIPS. Anstatt das Taktsignal vollständig zu entfernen, erlauben einige CPU-Entwürfe, bestimmte Teile des Gerätes asynchron zu sein, z.B. mit asynchronen ALUs in Verbindung mit superscalarer Pipelining, um einige arithmetische Leistungsgewinne zu erzielen. Es ist zwar nicht ganz klar, ob völlig asynchrone Designs auf vergleichbarer oder besserer Ebene als ihre synchronen Gegenstücke ausgeführt werden können, aber es ist offensichtlich, dass sie bei einfacheren Mathematikoperationen zumindest übertreffen. Dies, kombiniert mit ihren ausgezeichneten Leistungsaufnahme- und Wärmeableitungseigenschaften, macht sie sehr geeignet für Embedded-Computer. Spannungsreglermodul Viele moderne CPUs verfügen über ein integriertes Leistungsmanagement-Modul, das die On-Demand-Spannungsversorgung der CPU-Schaltkreise regelt, um das Gleichgewicht zwischen Leistung und Stromverbrauch zu halten. Integer-Bereich Jede CPU stellt Zahlenwerte auf eine bestimmte Weise dar. Beispielsweise repräsentierten einige frühe digitale Computer Zahlen als bekannte Dezimal (Basis 10) Ziffernsystemwerte, und andere haben ungewöhnlichere Darstellungen wie ternary (Basis drei) verwendet. Fast alle modernen CPUs stellen Zahlen in binärer Form dar, wobei jede Zahl durch eine zweiwertige physikalische Größe wie eine hohe oder niedrige Spannung repräsentiert wird. In Bezug auf numerische Darstellung ist die Größe und Präzision von ganzzahligen Zahlen, die eine CPU darstellen kann. Im Falle einer binären CPU wird dies durch die Anzahl der Bits (signifikante Stellen einer binären kodierten Ganzzahl) gemessen, die die CPU in einem Arbeitsgang verarbeiten kann, der allgemein Wortgröße, Bitbreite, Datenpfadbreite, Ganzzahlgenauigkeit oder Ganzzahlgröße genannt wird. Die ganzzahlige Größe einer CPU bestimmt den Bereich der ganzzahligen Werte, auf denen sie direkt arbeiten kann. Beispielsweise kann eine 8-Bit-CPU direkt Zahlen, die durch acht Bits dargestellt sind, manipulieren, die einen Bereich von 256 (28) diskrete Ganzzahlwerte aufweisen. Integer-Bereich kann auch die Anzahl der Speicherplätze beeinflussen, die CPU kann direkt ansprechen (eine Adresse ist ein ganzzahliger Wert, der einen bestimmten Speicherort repräsentiert). Wenn zum Beispiel eine binäre CPU 32 Bit verwendet, um eine Speicheradresse darzustellen, dann kann sie direkt 232 Speicherplätze ansprechen. Um diese Einschränkung und aus verschiedenen anderen Gründen zu umgehen, verwenden einige CPUs Mechanismen (z.B. Bank-Schaltung), die einen zusätzlichen Speicher ansprechen lassen. CPUs mit größeren Wortgrößen benötigen mehr Schaltkreise und sind folglich physisch größer, kosten mehr und verbrauchen mehr Leistung (und erzeugen daher mehr Wärme). Dadurch werden in modernen Anwendungen häufig kleinere 4- oder 8-Bit-Mikrocontroller eingesetzt, obwohl CPUs mit viel größeren Wortgrößen (wie 16, 32, 64, sogar 128-Bit) zur Verfügung stehen. Wenn jedoch eine höhere Leistung erforderlich ist, können die Vorteile einer größeren Wortgröße (größere Datenbereiche und Adressräume) die Nachteile überwiegen. Eine CPU kann interne Datenpfade kürzer als die Wortgröße haben, um Größe und Kosten zu reduzieren. Zum Beispiel, obwohl der IBM System/360 Befehlssatz ein 32-Bit-Anweisungssatz war, hatten das System/360 Modell 30 und Modell 40 8-Bit-Datenpfade in der arithmetischen logischen Einheit, so dass ein 32-Bit-Add benötigte vier Zyklen, eine für jede 8 Bit der Operanden, und zwar der Motorola 68000 Serie Befehlssatz war ein 32-Bit-Anweisungssatz, die Motorola 68000 und Motorola 68010 Um einige der Vorteile zu gewinnen, die sich sowohl durch niedrigere als auch höhere Bitlängen ergeben, haben viele Befehlssätze unterschiedliche Bitbreiten für ganzzahlige und schwebende Daten, so dass CPUs, die diesen Befehlssatz implementieren, unterschiedliche Bitbreiten für verschiedene Teile des Gerätes aufweisen. Zum Beispiel war der IBM System/360 Instruktionssatz in erster Linie 32 Bit, unterstützte aber 64-Bit-Schwebepunktwerte, um eine größere Genauigkeit und Reichweite in Schwimmpunktzahlen zu erleichtern. Das System/360 Modell 65 hatte einen 8-Bit-Addierer für Dezimal- und Fixpunkt-Binärarithmetic und einen 60-Bit-Addierer für Floating-Point-Arithmetic. Viele spätere CPU-Designs verwenden ähnliche gemischte Bitbreite, vor allem, wenn der Prozessor für den allgemeinen Gebrauch bestimmt ist, wo eine vernünftige Balance von ganzem und schwimmenden Punktfähigkeit erforderlich ist. Parallelität Die Beschreibung des grundsätzlichen Betriebs einer im vorherigen Abschnitt angebotenen CPU beschreibt die einfachste Form, die eine CPU nehmen kann. Diese Art von CPU, in der Regel als Subscalar bezeichnet, arbeitet an und führt eine Anweisung auf ein oder zwei Stücke von Daten zu einem Zeitpunkt, das ist weniger als eine Anweisung pro Taktzyklus (IPC < 1). Dieser Prozess führt zu einer inhärenten Ineffizienz in Subscalar CPUs. Da nur eine Anweisung zu einer Zeit ausgeführt wird, muss die gesamte CPU darauf warten, dass diese Anweisung abgeschlossen wird, bevor sie zur nächsten Anweisung geht. Dadurch wird die Subscalar-CPU auf Befehle "aufgehängt", die mehr als einen Taktzyklus benötigen, um die Ausführung abzuschließen. Selbst das Hinzufügen einer zweiten Ausführungseinheit (siehe unten) verbessert die Leistung nicht viel; anstatt einen Pfad aufgehängt zu werden, werden nun zwei Pfade aufgehängt und die Anzahl der ungenutzten Transistoren erhöht. Dieses Design, wobei die Ausführungsressourcen der CPU auf nur einer Anweisung zu einer Zeit arbeiten können, kann nur möglicherweise Skalarleistung erreichen (eine Anweisung pro Taktzyklus, IPC = 1). Die Leistung ist jedoch fast immer subscalar (weniger als eine Anweisung pro Taktzyklus, IPC < 1). Versuche, Skalar und bessere Leistung zu erreichen, haben zu einer Vielzahl von Design-Methoden geführt, die die CPU weniger linear und mehr parallel verhalten. Beim Bezug auf Parallelismus in CPUs werden in der Regel zwei Begriffe verwendet, um diese Konstruktionstechniken zu klassifizieren: Instruktion-Level-Parallelismus (ILP), die versucht, die Rate zu erhöhen, mit der Instruktionen innerhalb einer CPU ausgeführt werden (d.h., um die Verwendung von On-die-Execution-Ressourcen zu erhöhen); Task-Level-Parallelismus (TLP,), die die die dazu dient, die Anzahl der Fäden oder Prozesse zu erhöhen, die eine CPU gleichzeitig ausführen kann. Jede Methodik unterscheidet sich sowohl in der Art und Weise, in der sie implementiert werden, als auch in der relativen Wirksamkeit, die sie bei der Erhöhung der Leistung der CPU für eine Anwendung bieten. Anweisungsebene Parallelismus Eines der einfachsten Methoden für die verstärkte Parallelität ist, die ersten Schritte des Unterrichts fetching und decodieren zu beginnen, bevor die vorherige Anweisung beendet die Ausführung. Dies ist eine Technik, die als Instruktionspipelining bekannt ist und in fast allen modernen Universal-CPUs verwendet wird. Pipelining ermöglicht es, mehrere Anweisungen zu einem Zeitpunkt durch Brechen der Ausführung Weg in diskrete Stufen ausgeführt werden. Diese Trennung kann mit einer Montagelinie verglichen werden, in der an jeder Stufe eine Anweisung abgeschlossen wird, bis sie die Durchführungspipeline verlässt und in den Ruhestand geht. Pipelining führt jedoch die Möglichkeit für eine Situation ein, in der das Ergebnis der vorherigen Operation benötigt wird, um die nächste Operation abzuschließen; eine Bedingung oft als Datenabhängigkeitskonflikt bezeichnet. Daher müssen Pipeline-Prozessoren auf diese Art von Bedingungen überprüfen und gegebenenfalls einen Teil der Pipeline verzögern. Ein Pipeline-Prozessor kann sehr fast skalar werden, nur durch Pipeline-Stände gehemmt (eine Anleitung, die mehr als einen Taktzyklus in einer Phase ausgibt). Verbesserungen bei der Instruktionspipelinierung führten zu weiteren Abnahmen in der Ruhezeit von CPU-Komponenten. Als Superscalar bezeichnete Ausführungen umfassen eine lange Anweisungspipeline und mehrere identische Ausführungseinheiten, wie Lastspeichereinheiten, arithmetisch-logische Einheiten, Schwimmpunkteinheiten und Adresserzeugungseinheiten. In einer Superscalar-Pipeline werden Anweisungen gelesen und an einen Versender weitergeleitet, der entscheidet, ob die Anweisungen parallel (simultan) ausgeführt werden können. Wenn ja, werden sie an Ausführungseinheiten versandt, was zu ihrer gleichzeitigen Ausführung führt. Im Allgemeinen ist die Anzahl der Anweisungen, die eine Superscalar-CPU in einem Zyklus abschließt, abhängig von der Anzahl der Anweisungen, die sie gleichzeitig an Ausführungseinheiten versenden kann. Die meisten Schwierigkeiten bei der Konstruktion einer superscalar CPU-Architektur liegen darin, einen effektiven Dispatcher zu schaffen. Der Versender muss schnell feststellen können, ob Anweisungen parallel ausgeführt werden können, sowie diese so versenden, dass möglichst viele Ausführungseinheiten beschäftigt bleiben. Dies erfordert, dass die Befehlspipeline möglichst oft gefüllt ist und erhebliche Mengen an CPU-Cache benötigt. Es macht auch risikovermeidende Techniken wie Branchenvorhersage, spekulative Ausführung, Registerumbenennung, Out-of-order Ausführung und Transaktionsspeicher entscheidend, um hohe Leistungsstufen zu erhalten. Durch den Versuch, vorherzusagen, welche Abzweigung (oder Pfad) eine bedingte Anweisung dauern wird, kann die CPU die Anzahl der Zeiten minimieren, die die gesamte Pipeline warten muss, bis eine bedingte Anweisung abgeschlossen ist. Spekulative Ausführung bietet oft bescheidene Leistungssteigerungen, indem Teile von Code ausgeführt werden, die nach Beendigung einer bedingten Operation nicht benötigt werden können. Die Out-of-order Ausführung verändert die Reihenfolge, in der Anweisungen ausgeführt werden, um Verzögerungen aufgrund von Datenabhängigkeiten zu reduzieren. Auch bei einem einzelnen Befehlsstrom kann ein mehrfacher Datenstrom - ein Fall, wenn viele Daten desselben Typs verarbeitet werden müssen - moderne Prozessoren Teile der Pipeline deaktivieren, so dass die CPU bei mehrfacher Ausführung einer einzelnen Anweisung die Fetch- und Decode-Phasen überspringt und damit die Leistung bei bestimmten Gelegenheiten, insbesondere bei hoch monotonen Programmmotoren wie Video-Erstellungssoftware und Fotoverarbeitung, stark erhöht. Für den Fall, dass nur ein Teil der CPU superscalar ist, leidet der Teil, der aufgrund von Scheduling Ständen keine Leistungsstrafe erleidet. Das Intel P5 Pentium hatte zwei Superscalar ALUs, die je eine Anweisung pro Takt annehmen konnten, aber seine FPU konnte nicht. So war die P5 ganzzahlig superscalar, aber nicht schwimmende Punkt superscalar. Intels Nachfolger der P5-Architektur, P6, fügte seinen Floating Point-Funktionen superscalar Fähigkeiten hinzu. Einfaches Pipelining und superscalar Design erhöhen die ILP der CPU, indem es es Anleitungen zu Geschwindigkeiten ausführen, die eine Anweisung pro Taktzyklus übersteigen. Die meisten modernen CPU-Designs sind zumindest etwas superscalar, und fast alle allgemeinen Zweck CPUs, die im letzten Jahrzehnt entworfen wurden, sind superscalar. In späteren Jahren wurde ein Teil der Betonung bei der Gestaltung von High-ILP-Computern aus der CPU-Hardware und in seine Software-Schnittstelle oder Instruktionsset-Architektur (ISA) verschoben. Die Strategie des sehr langen Befehlswortes (VLIW) bewirkt, dass einige ILP direkt von der Software impliziert werden, wodurch die Arbeit der CPU bei der Förderung von ILP reduziert wird und damit die Komplexität des Designs reduziert wird. Aufgabenebene Parallelismus Eine weitere Strategie, Leistung zu erzielen, ist, mehrere Fäden oder Prozesse parallel auszuführen. Dieser Forschungsbereich ist als Parallelrechner bekannt. In der Taxonomie von Flynn wird diese Strategie als Mehrfachanweisungsstrom, Mehrfachdatenstrom (MIMD) bezeichnet. Eine hierfür verwendete Technologie war die Multiverarbeitung (MP.)Der ursprüngliche Geschmack dieser Technologie ist als symmetrische Multiverarbeitung (SMP) bekannt, bei der eine geringe Anzahl von CPUs eine kohärente Ansicht ihres Speichersystems teilen. In diesem System hat jede CPU zusätzliche Hardware, um eine ständig aktuelle Ansicht des Speichers zu erhalten. Durch die Vermeidung von stale-Ansichten des Speichers können die CPUs auf demselben Programm zusammenwirken und Programme können von einer CPU zur anderen migrieren. Um die Anzahl der kooperierenden CPUs über eine Handvoll hinaus zu erhöhen, wurden in den 1990er Jahren Systeme wie ungleichmäßiger Speicherzugriff (NUMA) und verzeichnisbasierte Kohärenzprotokolle eingeführt. SMP-Systeme sind auf eine kleine Anzahl von CPUs beschränkt, während NUMA-Systeme mit Tausenden von Prozessoren gebaut wurden. Zunächst wurde die Multiverarbeitung mit mehreren diskreten CPUs und Boards aufgebaut, um die Verbindung zwischen den Prozessoren zu implementieren. Wenn die Prozessoren und deren Verbindungsschaltung alle auf einem einzigen Chip implementiert sind, ist die Technologie als Chip-Level Multiprocessing (CMP) und der einzelne Chip als Multi-Core-Prozessor bekannt. Später wurde erkannt, dass feinere Kornparallelität mit einem einzigen Programm bestand. Ein einziges Programm kann mehrere Threads (oder Funktionen) aufweisen, die separat oder parallel ausgeführt werden können. Einige der frühesten Beispiele dieser Technologie implementierten Ein-/Ausgangsverarbeitung, wie z.B. direkten Speicherzugriff als separates Gewinde vom Rechengewinde. Ein allgemeinerer Ansatz dieser Technologie wurde in den 1970er Jahren eingeführt, als die Systeme dazu ausgelegt waren, mehrere Rechenfäden parallel zu führen. Diese Technologie ist als Multi-Threading (MT) bekannt. Dieser Ansatz gilt als kostengünstiger als die Multiverarbeitung, da nur eine geringe Anzahl von Komponenten innerhalb einer CPU repliziert wird, um MT im Gegensatz zur gesamten CPU bei MP zu unterstützen. In MT werden die Ausführungseinheiten und das Speichersystem einschließlich der Caches unter mehreren Threads geteilt. Die Unterseite von MT ist, dass die Hardware-Unterstützung für Multithreading für Software sichtbarer ist als die von MP und so Supervisor-Software wie Betriebssysteme müssen größere Änderungen der Unterstützung MT. Eine Art MT, die implementiert wurde, ist als zeitliches Multithreading bekannt, bei dem ein Thread ausgeführt wird, bis er auf Daten warten muss, um aus dem externen Speicher zurückzukehren. In diesem Schema würde die CPU dann schnell auf einen anderen, laufbereiten Faden umschalten, der Schalter oft in einem CPU-Taktzyklus, wie dem UltraSPARC T1. Eine andere Art von MT ist die gleichzeitige Multithreading, wobei Anweisungen aus mehreren Threads parallel innerhalb eines CPU-Taktzyklus ausgeführt werden. Seit mehreren Jahrzehnten von den 1970er- bis Anfang 2000er-Jahren konzentrierte sich der Fokus auf die Gestaltung von Hochleistungs-allgemeinen Zweck CPUs auf die Erzielung hoher ILP durch Technologien wie Pipelining, Caches, superscalar Ausführung, Out-of-order Ausführung, etc. Dieser Trend gipfelte in großen, energiehungrigen CPUs wie dem Intel Pentium 4. In den frühen 2000er Jahren wurden CPU-Designer durch die zunehmende Ungleichheit zwischen CPU-Betriebsfrequenzen und Hauptspeicher-Betriebsfrequenzen sowie die eskalierende CPU-Leistungsableitung aufgrund von esoterischen ILP-Techniken davon abgehalten, höhere Leistung aus ILP-Techniken zu erzielen. CPU-Designer leihten sich dann Ideen aus kommerziellen Computing-Märkten wie Transaktionsverarbeitung, wo die Gesamtleistung mehrerer Programme, auch als Durchsatz-Computing bekannt, wichtiger war als die Leistung eines einzigen Threads oder Prozesses. Diese Umkehr der Betonung wird durch die Verbreitung von dualen und mehr Kernprozessoren-Designs bewiesen und vor allem die neueren Designs von Intel ähneln seiner weniger superscalaren P6-Architektur. Späte Designs in mehreren Prozessorfamilien zeigen CMP, einschließlich der x86-64 Opteron und Athlon 64 X2, die SPARC UltraSPARC T1, IBM POWER4 und POWER5, sowie mehrere Videospiel-Konsole CPUs wie das Triple-Core PowerPC Design von Xbox 360 und den 7-Core Cell Mikroprozessor von PlayStation 3. Datenparallelität Ein weniger häufiger, aber immer wichtigeres Paradigma von Prozessoren (und in der Tat, Computing im Allgemeinen) beschäftigt sich mit Datenparallelismus. Die zuvor diskutierten Prozessoren werden alle als eine Art Skalargerät bezeichnet. Wie der Name schon sagt, beschäftigen Vektorprozessoren mehrere Datenstücke im Kontext einer Anweisung. Dies steht im Gegensatz zu Skalarprozessoren, die sich mit einem Stück Daten für jede Anweisung befassen. Mit Flynns Taxonomie werden diese beiden Systeme des Umgangs mit Daten im Allgemeinen als Einzelanweisungsstrom, Mehrfachdatenstrom (SIMD) und Einzelanweisungsstrom, Einzeldatenstrom (SISD) bezeichnet. Der große Nutzen bei der Erstellung von Prozessoren, die mit Vektoren von Daten umgehen, besteht darin, Aufgaben zu optimieren, die dazu neigen, den gleichen Betrieb (z.B. eine Summe oder ein Punktprodukt) auf einem großen Datensatz durchzuführen. Einige klassische Beispiele für diese Arten von Aufgaben sind Multimedia-Anwendungen (Bilder, Video und Sound), sowie viele Arten von wissenschaftlichen und technischen Aufgaben. Während ein Skalarprozessor den gesamten Prozess des Abrufens, Decodierens und Ausführens jeder Anweisung und des Wertes in einem Datensatz abschließen muss, kann ein Vektorprozessor auf einem vergleichsweise großen Datensatz mit einer Anweisung eine einzige Operation durchführen. Dies ist nur möglich, wenn die Anwendung in der Regel viele Schritte erfordert, die eine Operation auf eine große Menge von Daten anwenden. Die meisten frühen Vektorprozessoren, wie die Cray-1, waren fast ausschließlich mit wissenschaftlichen Forschungs- und Kryptographieanwendungen verbunden. Da sich Multimedia jedoch weitgehend auf digitale Medien verlagert hat, ist die Notwendigkeit einer gewissen Form von SIMD in Universalprozessoren signifikant geworden. Kurz nach der Aufnahme von Floating-Point-Einheiten begann, gemeinsam zu werden in allgemeine Prozessoren, Spezifikationen für und Implementierungen von SIMD-Ausführungseinheiten begann auch für allgemeine Prozessoren. Einige dieser frühen SIMD-Spezifikationen - wie HPs Multimedia Acceleration eXtensions (MAX) und Intels MMX - waren ganzzahlig. Dies erwies sich als eine signifikante Behinderung für einige Software-Entwickler, da viele der Anwendungen, die von SIMD profitieren in erster Linie mit Floating-Point-Nummern. Progressively, Entwickler verfeinert und neu gemacht diese frühen Designs in einige der gemeinsamen modernen SIMD-Spezifikationen, die in der Regel mit einer Anleitungsarchitektur (ISA) verbunden sind. Einige bemerkenswerte moderne Beispiele sind Intels Streaming-SIMD-Erweiterungen (SSE) und das PowerPC-bezogene AltiVec (auch bekannt als VMX). Virtuelle CPUs Cloud Computing kann dazu führen, dass CPU-Betrieb in virtuelle zentrale Verarbeitungseinheiten (vCPUs) unterteilt wird. Ein Host ist das virtuelle Äquivalent einer physischen Maschine, auf der ein virtuelles System arbeitet. Wenn es mehrere physikalische Maschinen gibt, die in Tandem arbeiten und als Ganzes verwaltet werden, bilden die gruppierten Rechen- und Speicherressourcen einen Cluster. In einigen Systemen ist es möglich, dynamisch einen Cluster hinzuzufügen und zu entfernen. Ressourcen auf Host- und Clusterebene können in Ressourcenpools mit feiner Granularität verteilt werden. PerformanceDie Leistung oder Geschwindigkeit eines Prozessors hängt unter anderem von der Taktrate (in der Regel in Vielfachen von Hertz) und den Anweisungen pro Takt (IPC) ab, die zusammen die Faktoren für die Anweisungen pro Sekunde (IPS) sind, die die CPU ausführen kann. Viele gemeldete IPS-Werte haben Spitzenausführungsraten auf künstlichen Befehlssequenzen mit wenigen Zweigen dargestellt, während realistische Workloads aus einer Mischung von Anweisungen und Anwendungen bestehen, von denen einige länger dauern als andere. Die Leistung der Speicherhierarchie wirkt sich auch stark auf die Prozessorleistung aus, ein Problem, das in MIPS-Berechnungen kaum berücksichtigt wird. Aufgrund dieser Probleme wurden verschiedene standardisierte Tests entwickelt, die oft als Benchmarks für diesen Zweck bezeichnet wurden – wie z.B. SPECint –, um die reale Leistungsfähigkeit in gängigen Anwendungen zu messen. Die Verarbeitungsleistung von Computern wird durch den Einsatz von Multicore-Prozessoren erhöht, die im wesentlichen zwei oder mehrere Einzelprozessoren (in diesem Sinne als Kerne bezeichnet) in eine integrierte Schaltung einstecken. Idealerweise wäre ein Doppelkernprozessor fast doppelt so leistungsstark wie ein einzelner Kernprozessor. In der Praxis ist der Leistungsgewinn durch unvollkommene Softwarealgorithmen und Implementierung weit kleiner, nur etwa 50,%. Die Erhöhung der Anzahl der Kerne in einem Prozessor (d.h. Dual-Core, Quad-Core, etc.) erhöht die Arbeitsbelastung, die man handhaben kann. Das bedeutet, dass der Prozessor nun zahlreiche asynchrone Ereignisse, Unterbrechungen usw. behandeln kann, die bei überwältigender CPU eine Maut auf die CPU nehmen können. Diese Kerne können in einer Verarbeitungsanlage als unterschiedliche Böden betrachtet werden, wobei jeder Boden eine andere Aufgabe übernimmt. Manchmal werden diese Kerne die gleichen Aufgaben behandeln wie Kerne, die ihnen benachbart sind, wenn ein einziger Kern nicht ausreicht, um die Informationen zu handhaben. Aufgrund der spezifischen Fähigkeiten moderner CPUs, wie z.B. gleichzeitiges Multithreading und Uncore, die den Austausch von tatsächlichen CPU-Ressourcen beinhalten und gleichzeitig auf eine verstärkte Auslastung, Überwachung von Leistungsniveaus und Hardwarenutzung abzielen, wurde eine komplexere Aufgabe. Als Antwort implementieren einige CPUs eine zusätzliche Hardware-Logik, die die tatsächliche Verwendung von verschiedenen Teilen einer CPU überwacht und verschiedene Zähler für die Software zugänglich macht; ein Beispiel ist die Intel Performance Counter Monitor-Technologie. Siehe auch Hinweise ReferenzenExterne Links Wie Mikroprozessoren Arbeiten bei HowStuffWorks.25 Mikrochips, die die Welt erschüttern – ein Artikel des Instituts für Elektro- und Elektronikingenieure.