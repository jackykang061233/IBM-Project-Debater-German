In der Informatik ist die Analyse von Algorithmen der Prozess, die rechnerische Komplexität von Algorithmen zu finden – die Menge an Zeit, Speicher oder andere Ressourcen, die für die Ausführung benötigt werden. Üblicherweise wird dabei eine Funktion ermittelt, die die Länge eines Algorithmus-Eingangs auf die Anzahl der Schritte bezieht, die er (ihre Zeitkomplexität) oder die Anzahl der Speicherplätze, die er verwendet (ihre Raumkomplexität). Ein Algorithmus soll effizient sein, wenn die Werte dieser Funktion klein sind, oder im Vergleich zu einem Wachstum in der Größe der Eingabe langsam wachsen. Verschiedene Eingänge gleicher Länge können dazu führen, dass der Algorithmus unterschiedliches Verhalten hat, so dass am besten, schlimmste und durchschnittliche Fallbeschreibungen von praktischem Interesse sein könnten. Wenn nicht anders angegeben, ist die die Leistung eines Algorithmus beschreibende Funktion in der Regel eine obere Grenze, die von den schlimmsten Falleingängen zum Algorithmus bestimmt wird. Der Begriff "Analyse von Algorithmen" wurde von Donald Knuth geprägt. Algorithm-Analyse ist ein wichtiger Teil einer breiteren Rechenkomplexitätstheorie, die theoretische Schätzungen für die Ressourcen liefert, die von jedem Algorithmus benötigt werden, der ein gegebenes Rechenproblem löst. Diese Schätzungen geben einen Einblick in vernünftige Suchrichtungen für effiziente Algorithmen. Bei der theoretischen Analyse von Algorithmen ist es üblich, ihre Komplexität im asymptotischen Sinne zu schätzen, d.h. die Komplexitätsfunktion für willkürlich große Eingaben abzuschätzen. Zu diesem Zweck werden Big O-Notation, Big-omega-Notation und Big-theta-Notation verwendet. Zum Beispiel soll die Binärsuche in einer Anzahl von Stufen ablaufen, die proportional zum Logarithmus der Länge der gesuchten Liste sind, oder in O(log(n,) kolloquial "in logarithmischer Zeit". Üblicherweise werden asymptotische Schätzungen verwendet, da sich unterschiedliche Implementierungen desselben Algorithmus in der Effizienz unterscheiden können. Die Wirksamkeiten jeder zwei vernünftigen Implementierungen eines bestimmten Algorithmus werden jedoch durch einen konstanten multiplikativen Faktor, der als versteckte Konstante bezeichnet wird, verwandt. Genaue (nicht asymptotische) Maßnahmen der Effizienz können manchmal berechnet werden, aber sie erfordern in der Regel bestimmte Annahmen über die jeweilige Implementierung des Algorithmus, genannt Modell der Berechnung. Ein Berechnungsmodell kann in Bezug auf einen abstrakten Rechner, z.B. Turing-Maschine, definiert werden und/oder indem nachgeahmt wird, dass bestimmte Vorgänge in der Zeiteinheit ausgeführt werden. Wenn z.B. die sortierte Liste, auf die wir binäre Suche anwenden, n Elemente hat, und wir können garantieren, dass jeder Blick auf ein Element in der Liste in der Zeiteinheit erfolgen kann, dann werden maximal log2 n + 1 Zeiteinheiten benötigt, um eine Antwort zurückzugeben. Kostenmodelle Zeiteffizienz-Schätzungen hängen davon ab, was wir als Schritt definieren. Damit die Analyse sinnvoll der tatsächlichen Ausführungszeit entspricht, muss die Zeit, die für die Durchführung eines Schrittes erforderlich ist, über eine Konstante begrenzt werden. Man muss hier vorsichtig sein; zum Beispiel zählen einige Analysen eine Addition von zwei Zahlen als einen Schritt. Diese Annahme kann in bestimmten Kontexten nicht gerechtfertigt werden. Wenn beispielsweise die an einer Berechnung beteiligten Zahlen beliebig groß sein können, kann die durch eine einzige Addition benötigte Zeit nicht mehr als konstant angenommen werden. Es werden in der Regel zwei Kostenmodelle verwendet: Das einheitliche Kostenmodell, auch als einheitliche Kostenmessung (und ähnliche Variationen) bezeichnet, ordnet jedem Maschinenbetrieb einen konstanten Kosten zu, unabhängig von der Größe der Zahlen, die das logarithmische Kostenmodell, auch logarithmisch-Kosten-Messung (und ähnliche Variationen,) beteiligt sind, gibt jedem Maschinenbetrieb proportional zur Anzahl der Bitsinvolviert Letzteres ist umständlicher zu verwenden, so wird es nur bei Bedarf eingesetzt, beispielsweise bei der Analyse von willkürlich-präzisen arithmetischen Algorithmen, wie sie in der Kryptographie verwendet werden. Ein wichtiger Punkt, der oft übersehen wird, ist, dass veröffentlichte niedrigere Grenzen für Probleme oft für ein Modell der Berechnung, die mehr eingeschränkt ist als die Menge der Operationen, die Sie in der Praxis verwenden könnten, und daher gibt es Algorithmen, die schneller sind als das, was naiv gedacht werden könnte. Laufzeitanalyse Die Laufzeitanalyse ist eine theoretische Klassifizierung, die den Anstieg der Laufzeit (oder Laufzeit) eines Algorithmus mit zunehmender Eingangsgröße (in der Regel mit n bezeichnet) vorausschätzt und erwartet. Laufzeiteffizienz ist ein Thema von großem Interesse in der Informatik: Ein Programm kann Sekunden, Stunden oder sogar Jahre dauern, um die Ausführung zu beenden, je nachdem, welcher Algorithmus es implementiert. Während Software-Profiling-Techniken verwendet werden können, um die Laufzeit eines Algorithmus in der Praxis zu messen, können sie keine Zeitdaten für alle unendlich vielen möglichen Eingänge bereitstellen; letztere können nur durch die theoretischen Methoden der Laufzeitanalyse erreicht werden.Da Algorithmen plattformunabhängig sind (d.h. ein vorgegebener Algorithmus in einer beliebigen Programmiersprache auf einem beliebigen Computer mit einem beliebigen Betriebssystem implementiert werden kann), gibt es zusätzliche wesentliche Nachteile, um einen empirischen Ansatz zu verwenden, um die Vergleichsleistung eines bestimmten Algorithmensatzes zu messen. Nehmen Sie als Beispiel ein Programm, das einen bestimmten Eintrag in einer sortierten Liste der Größe n ansieht. Angenommen, dieses Programm wurde auf Computer A implementiert, eine hochmoderne Maschine, mit einem linearen Suchalgorithmus, und auf Computer B, eine viel langsamere Maschine, mit einem binären Suchalgorithmus. Benchmark-Tests auf den beiden Computern, die ihre jeweiligen Programme ausführen, könnten wie folgt aussehen: Basierend auf diesen Metriken, wäre es einfach, zu dem Schluss zu springen, dass Computer A einen Algorithmus läuft, der weit überlegen in der Effizienz zu dem von Computer B. Wird jedoch die Größe der Eingabeliste auf eine ausreichende Anzahl erhöht, so wird diese Schlussfolgerung dramatisch als Fehler nachgewiesen: Computer A, das lineare Suchprogramm ausführt, zeigt eine lineare Wachstumsrate. Die Laufzeit des Programms ist direkt proportional zu seiner Eingangsgröße. Die Verdoppelung der Eingangsgröße verdoppelt die Laufzeit, vervierfacht die Eingangsgröße die Laufzeit und so weiter. Andererseits zeigt Computer B, der das binäre Suchprogramm betreibt, eine logarithmische Wachstumsrate. Die Verdoppelung der Eingangsgröße erhöht die Laufzeit nur um einen konstanten Betrag (in diesem Beispiel 50.000 ns.) Obwohl Computer A ostensibel eine schnellere Maschine ist, wird Computer B zwangsläufig Computer A in Laufzeit übertreffen, weil es einen Algorithmus mit einer viel langsameren Wachstumsrate läuft. Informell kann ein Algorithmus gesagt werden, eine Wachstumsrate in der Größenordnung einer mathematischen Funktion zu zeigen, wenn über eine bestimmte Eingangsgröße n hinaus die Funktion f (n) {\displaystyle f(n}) mal eine positive Konstante eine obere Grenze für die Laufzeit dieses Algorithmus liefert. Mit anderen Worten, für eine bestimmte Eingangsgröße n größer als einige n0 und eine Konstante c wird die Laufzeit dieses Algorithmus nie größer sein als c × f (n ) {\displaystyle c\times f(n}). Dieses Konzept wird häufig mit Big O-Notation ausgedrückt. Da z.B. die Laufzeit der Insertionssorte mit zunehmender Eingangsgröße quadratisch wächst, kann ein Insertionssort der Ordnung O(n2) genannt werden. Groß O-Notation ist eine bequeme Möglichkeit, das schlimmste Szenario für einen bestimmten Algorithmus auszudrücken, obwohl es auch verwendet werden kann, um den Durchschnittsfall auszudrücken - zum Beispiel das schlimmste Szenario für Quicksort ist O(n2,) aber die durchschnittliche Laufzeit ist O(n log n). Empirische Wachstumsordnung Die Ausführungsdauer folgt der Leistungsregel t ≈ k na, der Koeffizient a kann durch empirische Messungen der Laufzeit gefunden werden { t 1 , t 2 } {\displaystyle t_{1},t_{2 an einigen Problemgrößenpunkten { n 1 , n 2 } {\displaystyle t_{1},t_{2 an einigen Problemgrößenpunkten n_{1},n_{2 , und Berechnung t 2 / t 1 = ( n 2 / n 1 ) a {\displaystyle t_{2}/t_{1}=(n_{2}/n_{1})^{a so dass a = log kenn ( t 2 / t 1 ) / log kenn ( n 2 / n 1 ) {\displaystyle a=\log(t_{2}/t_{1})/\log(n_{2}/n_{1})Mit anderen Worten, dies misst die Steigung der empirischen Linie auf dem log-log-Plot der Ausführungszeit vs. Problemgröße, an einem bestimmten Größe Punkt. Wenn die Reihenfolge des Wachstums in der Tat der Machtregel folgt (und so die Linie auf log-log-Plot ist in der Tat eine gerade Linie,) bleibt der empirische Wert eines Testaments konstant in verschiedenen Bereichen, und wenn nicht, es wird sich ändern (und die Linie ist eine gebogene Linie) - aber könnte noch für den Vergleich von zwei gegebenen Algorithmen zu ihren empirischen lokalen Ordnungen des Wachstumsverhaltens dienen. Angewandt auf die obige Tabelle: Es ist deutlich zu erkennen, dass der erste Algorithmus nach der Leistungsregel eine lineare Wachstumsfolge aufweist. Die empirischen Werte für den zweiten sind rasant abnehmend, was darauf hindeutet, dass es einer anderen Wachstumsregel folgt und in jedem Fall viel niedrigere lokale Wachstumsordnungen (und sich weiter weiter verbessern,) empirisch als die erste. Auswertung der Laufzeitkomplexität Die Laufzeit-Komplexität für das schlimmste Szenario eines bestimmten Algorithmus kann manchmal durch die Untersuchung der Struktur des Algorithmus ausgewertet werden und einige vereinfachende Annahmen. Betrachten Sie den folgenden Pseudocode: 1 erhalten eine positive ganze Zahl n vom Eingang 2 wenn n > 10 3 Druck "Dies könnte eine Weile dauern." 4 für i = 1 bis n 5 für j = 1 bis i 6 Druck i * j 7 Druck Fertig Ein bestimmter Computer nimmt eine diskrete Zeit, um jede der Anweisungen auszuführen, die mit der Durchführung dieses Algorithmus beteiligt sind. Die spezifische Zeit, um eine bestimmte Anweisung durchzuführen, variiert je nachdem, welche Anweisung ausgeführt wird und welcher Computer sie ausführt, aber auf einem herkömmlichen Computer wird dieser Betrag deterministisch sein. Sagen Sie, dass die in Schritt 1 durchgeführten Aktionen als Zeitverzehr T1, Schritt 2 verwendet Zeit T2, und so weiter betrachtet werden. Im obigen Algorithmus werden die Schritte 1, 2 und 7 nur einmal ausgeführt. Bei einer schlimmsten Bewertung sollte davon ausgegangen werden, dass Schritt 3 auch ausgeführt wird. So beträgt die Gesamtzeit für die Durchführung der Schritte 1-3 und 7: T 1 + T 2 + T 3 + T 7 . {\displaystyle T_{1}+T_{2}+T_{3}+T_{7.,\ Die Schleifen in den Schritten 4, 5 und 6 sind schwieriger auszuwerten. Der äußere Schleifentest in Schritt 4 führt (n + 1 ) mal (Anmerkung, dass ein zusätzlicher Schritt erforderlich ist, um die Schleife zu beenden, also n + 1 und nicht n Ausführungen), die T4(n + 1 ) Zeit verbraucht. Die innere Schleife hingegen wird durch den Wert j bestimmt, der von 1 bis i iteriert. Beim ersten Durchgang durch die äußere Schlaufe iteriert j von 1 bis 1: Die innere Schleife macht einen Durchgang, so dass der innere Schleifenkörper (Schritt 6) verbraucht T6 Zeit, und der innere Schleifentest (Schritt 5) verbraucht 2T5 Zeit. Beim nächsten Durchlauf durch die äußere Schlaufe iteriert j von 1 bis 2: die innere Schlaufe zwei Durchgänge, so dass der innere Schlaufenkörper (Schritt 6) 2T6 Zeit verbraucht, und der innere Schlaufentest (Schritt 5) 3T5 Zeit verbraucht. Insgesamt kann die Gesamtzeit, die zum Laufen des inneren Schleifenkörpers benötigt wird, als arithmetische Progression ausgedrückt werden: T 6 + 2 T 6 + 3 T 6 + ⋯ + ( n - 1 ) T 6 + n T 6 {\displaystyle T_{6}+2T_{6}+3T_{6}+\cdots (n-1)T_{6}+nT_{6, die als T 6 faktorierbar sind [ 1 + 2 + 3 + ⋯ + (n - 1 ) + n ] = T 6 [ 1 2 (n 2 + n ) ]{\displaystyle T_{6}\left[1+2+3+\cdots (n-1)+n\right]=T_{6}\left[{\frac 1}{2}(n^{2}+n)\right] Die Gesamtzeit, die für die Durchführung des äußeren Schleifentests benötigt wird, kann analog ausgewertet werden: 2 T 5 + 3 T 5 + 4 T 5 + (n - 1 ) T 5 + n T 5 + (n + 1 ) T 5 = T 5 + 2 T 5 + 3 T 5 + 4 T 5 + ⋯ (n - 1 ) T 5 + n T 5 + (style n + 1 align) T 5 - T 5 n-1)T_{5}+nT_{5}+(n+1)T_{5=\ &T_{5}+3T_{5}+4T_{5}+\cdots n-1)T_{5}+nT_{5}+(n+1)T_{5}-T_{5}\end{ausgerichtet die als T 5 faktorierbar sind [ 1 + 2 + 3 +  − + ( n - 1 ) + n + ( n + 1 ) ] - T 5 = [ 1 2 (n 2 + n) ] T 5 + (n + 1 )T 5 - T 5 = T 5 [1 2 (n 2 + n) ] + n T 5 = [1 2 (n 2 + 3 n)] T 5 {\displaystyle start{align}&T_{5}\left[1+2+3+\cdots+(n-1)+n+(n+1)\right ]-T_{5}\\=&\left[{\frac 1}{2}}(n^{2}+n)\right]T_{5}+(n+1) T_{5}-T_{5}\\\\\=&T_{5}\left[{\frac 1}}(n^{2}+n)\right]+nT_{5}\=&\left[{\frac 1}{2}(n^2}+3n)\right]T_{5}\end{just{2}\ Die Gesamtlaufzeit für diesen Algorithmus ist: f (n) = T 1 + T 2 + T 3 + T 7 + (n + 1 ) T 4 + (n 2 + n ) T 6 + (n 2 + 3 n )******************************************* In der Regel von Thumb kann davon ausgegangen werden, dass der höchste Begriff in einer bestimmten Funktion seine Wachstumsrate dominiert und damit seine Laufzeitordnung definiert. In diesem Beispiel ist n2 der höchste Begriff, so dass man schließen kann, dass f(n) = O(n2.) Dies kann formal wie folgt nachgewiesen werden: )* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * k eine Konstante größer oder gleich [T1.T7] T 6 (n 2 + n ) + T 5 (n 2 + 3 n ) + (n + 1 ) T 4 + T 1 + T 2 + T 3 + T 7 ≤ k (n 2 + n ) + k (n 2 + 3 n ) + k n + 5 k = 2 k n 2 + 5 k n + 5 k n + 5 k ≤ 2 k n 2 + 5 k n 2 + 5 k n 2 (für n ≥ 1)) Ein eleganterer Ansatz zur Analyse dieses Algorithmus wäre, zu erklären, dass [T1..T7] alle gleich einer Einheit der Zeit sind, in einem System von Einheiten gewählt, so dass eine Einheit größer oder gleich den tatsächlichen Zeiten für diese Schritte ist. Dies würde bedeuten, dass die Laufzeit des Algorithmus wie folgt abbricht: 4 + Σ i = 1 n i ≤ 4 + Σ i = 1 n = 4 + n 2 ≤ 5 n 2 (für n ≥ 1 ) = O (n 2 ) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 5n^{2}\ (\text{für }n\geq 1)=O(n^{2}. Veränderungsrate Analyse anderer Ressourcen Die Methodik der Laufzeitanalyse kann auch zur Vorhersage anderer Wachstumsraten genutzt werden, wie zum Beispiel der Verbrauch von Speicherplatz. Betrachten Sie als Beispiel den folgenden Pseudocode, der die Speichernutzung durch ein Programm verwaltet und realisiert, basierend auf der Größe einer Datei, die dieses Programm verwaltet: während die Datei noch geöffnet ist: Lassen Sie n = Größe der Datei für alle 100.000 Kilobytes der Erhöhung der Dateigröße doppelt soviel Speicher reserviert In diesem Fall wird mit zunehmender Dateigröße n der Speicher mit einer exponentiellen Wachstumsrate verbraucht, die der Ordnung O(2n) entspricht. Dies ist eine äußerst schnelle und höchstwahrscheinlich unmanageable Wachstumsrate für den Verbrauch von Speicherressourcen. Relevanz Algorithm-Analyse ist in der Praxis wichtig, da der versehentliche oder unbeabsichtigte Einsatz eines ineffizienten Algorithmus die Systemleistung erheblich beeinflussen kann. In zeitsensitiven Anwendungen kann ein zu langer Algorithmus seine Ergebnisse veraltet oder nutzlos machen. Ein ineffizienter Algorithmus kann auch enden, dass eine unwirtschaftliche Menge an Rechenleistung oder Speicher benötigt, um zu laufen, wieder macht es praktisch nutzlos.Konstante Faktoren Die Analyse von Algorithmen konzentriert sich typischerweise auf die asymptotische Leistung, insbesondere auf der elementaren Ebene, aber in praktischen Anwendungen sind konstante Faktoren wichtig, und reale Daten sind in der Praxis immer in der Größe begrenzt. Die Grenze ist typischerweise die Größe des adressierbaren Speichers, also auf 32-Bit-Maschinen 232 = 4 GiB (größer, wenn segmentierter Speicher verwendet wird) und auf 64-Bit-Maschinen 264 = 16EiB. So kann bei begrenzter Größe eine Wachstumsfolge (Zeit oder Raum) durch einen konstanten Faktor ersetzt werden, und in diesem Sinne sind alle praktischen Algorithmen O(1) für eine genügend große Konstante oder für kleine genug Daten. Diese Interpretation ist in erster Linie für extrem langsam wachsende Funktionen nützlich: (binäre) iterierte Logarithm (log*) ist weniger als 5 für alle praktischen Daten (265536 Bit;) (binäre) Log-log (log log n) ist weniger als 6 für praktisch alle praktischen Daten (264 Bit;) und binäres Log (log n) ist weniger als 64 für praktisch alle praktischen Daten (264 Bit). Ein Algorithmus mit nicht-konstanter Komplexität kann dennoch effizienter sein als ein Algorithmus mit konstanter Komplexität auf praktischen Daten, wenn der Überkopf des konstanten Zeitalgorithmus zu einem größeren konstanten Faktor führt, z.B. kann man K > k log ≠ n {\displaystyle K>k\log \log n} so lange haben, wie K / k > 6 {\displaystyle K/k>6} und n < 2 6 = 2 64 {\displaystyle n<2^^^{2^{6}=2^{64 .Für große Daten können lineare oder quadratische Faktoren nicht ignoriert werden, aber für kleine Daten kann ein asymptotisch ineffizienter Algorithmus effizienter sein. Dies wird insbesondere in Hybridalgorithmen, wie Timsort, verwendet, die einen asymptotisch effizienten Algorithmus verwenden (hier Zusammenführungsort, mit Zeitkomplexität n log ≠ n {\displaystyle n\log n}), aber auf einen asymptotisch ineffizienten Algorithmus wechseln (hier Insertionsort, mit Zeitkomplexität n 2 {\displaystyle n^{2} ), da der einfachere Algorithmus auf kleinen Daten schneller ist. Siehe auch Amortisierte Analyse Analyse paralleler Algorithmen Asymptotische Rechenkomplexität Am besten, schlimmsten und durchschnittlichen Fall Big O notation Computational Komplexität Theorie Master Theorem (Analyse von Algorithmen) NP-Complete Numerische Analyse Polynomial time Programmoptimierung Profiling (Computerprogrammierung)Skalierbarkeit Beruhigte Analyse Terminationsanalyse — das Unterproblem der Überprüfung, ob ein Programm zu jeder Zeitkomplexität enden wird — beinhaltet Tabelle der Wachstumsaufträge für gemeinsame Algorithmen Informationsbasierte Komplexität Anmerkungen Referenzen Sedgewick, Robert; Flajolet, Philippe (2013). Eine Einführung in die Analyse von Algorithmen (2. ed.). Addison-Wesley.ISBN 978-0-321-90575-8.Greene, Daniel A;. Knuth, Donald E. (1982). Mathematik für die Analyse von Algorithmen (Second ed.). Birkhäuser.ISBN 3-7643-3102-X. Cormen, Thomas H;. Leiserson, Charles E;. Rivest, Ronald L. & Stein, Clifford (2001). Einführung in Algorithmen. Kapitel 1: Stiftungen (Second ed.). Cambridge, MA: MIT Presse und McGraw-Hill.pp.3–122.ISBN 0-262-03293-7.Sedgewick, Robert (1998). Algorithmen in C, Teile 1-4: Grundlagen, Datenstrukturen, Sortierung, Suche (3. ed.). Lesung, MA: Addison-Wesley Professional.ISBN 978-0-201-31452-6.Knuth, Donald. Die Kunst der Computerprogrammierung.Addison-Wesley.Goldreich, Oded (2010). Berechnung Komplexität: Eine konzeptuelle Perspektive.Cambridge University Press. ISBN 978-0-521-88473-0. Externe Links Medien im Zusammenhang mit der Analyse von Algorithmen bei Wikimedia CommonsOnline Mendelian Inheritance in Man (OMIM) ist ein kontinuierlich aktualisierter Katalog von menschlichen Genen und genetischen Störungen und Eigenschaften, mit einem besonderen Fokus auf die Gen-Phenotyp-Beziehung. Ab dem 28. Juni 2019 repräsentierten etwa 9.000 der über 25.000 Einträge in OMIM Phänotypen; die übrigen repräsentierten Gene, von denen viele mit bekannten Phänotypen verwandt waren. Versionen und Geschichte OMIM ist die online Fortsetzung von Dr. Victor A. McKusick's Mendelian Inheritance in Man (MIM), die in 12 Ausgaben zwischen 1966 und 1998 veröffentlicht wurde. Fast alle 1.486 Einträge in der ersten Ausgabe von MIM diskutierten Phenotypen.MIM/OMIM wird an der Johns Hopkins School of Medicine (JHUSOM) produziert und kuratiert. OMIM wurde 1987 unter der Leitung der Welch Medical Library am JHUSOM mit finanzieller Unterstützung des Howard Hughes Medical Institute im Internet verfügbar. Von 1995 bis 2010 war OMIM im World Wide Web mit Informatik und finanzieller Unterstützung des National Center for Biotechnology Information erhältlich. Die aktuelle OMIM-Website (OMIM.org), die mit Finanzierung von JHUSOM entwickelt wurde, wird von der Johns Hopkins University mit finanzieller Unterstützung vom National Human Genome Research Institute gepflegt. Erfassungsprozess und Nutzung Der Inhalt von MIM/OMIM basiert auf der Auswahl und Überprüfung der veröffentlichten peer-reviewed biomedical Literatur. Die Aktualisierung der Inhalte wird von einem Team von wissenschaftlichen Autoren und Kuratoren unter Leitung von Dr. Ada Hamosh am McKusick-Nathans Institut für Genetische Medizin der Johns Hopkins Universität durchgeführt. Während OMIM für die Öffentlichkeit frei verfügbar ist, ist es für die Verwendung in erster Linie von Ärzten und anderen Gesundheitsberufen, die mit genetischen Störungen, von Genetikern und von fortgeschrittenen Studenten in Wissenschaft und Medizin beschäftigt. Die Datenbank kann als Ressource zur Ortung von Literatur verwendet werden, die für geerbte Bedingungen relevant ist, und ihr Numeriersystem ist in der medizinischen Literatur weit verbreitet, um einen einheitlichen Index für genetische Erkrankungen bereitzustellen. MIM-Klassifikationssystem MIM-Nummern Jeder OMIM-Eintrag erhält eine eindeutige sechsstellige Kennung wie unten zusammengefasst: 100000–299999: Autosomal loci oder Phänotypen (Ergebnisse erstellt vor 15. Mai 1994) 300000–3999: X-verknüpfte Loci- oder Phenotypen 400000–4999: Y-verknüpfte Loci oder Phenotypen 500000–5999: Mitochondrial loci oder phenotypes 600000 und höher: Autosomal loci oder phenotypes (entries erstellt nach 15. Mai 1994) In Fällen der allerlischen Heterogenität folgt der MIM-Nummer des Eintrags ein Dezimalpunkt und eine eindeutige 4-stellige Zahl, die die Variante angibt. So sind beispielsweise allelic Varianten im HBB-Gen (141900) mit 141900.0001 bis 141900.0538 nummeriert. Da OMIM für die Einstufung und Benennung von genetischen Störungen verantwortlich ist, sind diese Zahlen stabile Kennungen der Störungen. Symbole vor MIM-Nummern Symbole vor MIM-Nummern geben die Eintragskategorie an: Ein Sternchen (*) vor einer Eintragsnummer gibt ein Gen an. Ein Zahlensymbol (#) vor einer Eingabenummer gibt an, dass es sich um einen beschreibenden Eintrag handelt, in der Regel um einen Phänotyp, und stellt keinen einzigartigen Ort dar. Der Grund für die Verwendung des Nummernsymbols ist im ersten Absatz des Eintrags angegeben. Die Diskussion aller Gene, die mit dem Phenotyp zusammenhängen, befindet sich in einem anderen Eintrag (oder Einträgen), wie im ersten Absatz beschrieben. Ein Pluszeichen (+) vor einer Eintragsnummer gibt an, dass der Eintrag die Beschreibung eines Gens bekannter Sequenz und eines Phenotyps enthält. Ein Prozentzeichen (%) vor einer Eintragsnummer gibt an, dass der Eintrag einen bestätigten Mendelian-Phänotyp oder phenotypischen Ort beschreibt, für den die zugrunde liegende molekulare Basis nicht bekannt ist. Kein Symbol vor einer Eintragsnummer gibt in der Regel eine Beschreibung eines Phänotyps an, für den die Mendelian-Basis, obwohl vermutet, nicht eindeutig festgelegt wurde oder dass die Abgrenzung dieses Phänotyps von dem in einem anderen Eintrag unklar ist. Ein Caret (^) vor einer Eingabenummer bedeutet, dass der Eintrag nicht mehr existiert, weil er aus der Datenbank entfernt wurde oder wie angedeutet in einen anderen Eintrag verschoben wurde. Siehe auch Mendelian Erbschaft Online Mendelian Erbschaft in Tiere Medizinische Klassifikation Comparative Toxicogenomics Datenbank, eine Datenbank, die Chemikalien und Gene mit menschlichen Erkrankungen, einschließlich OMIM-Daten, integriert. DECIPHER, eine Datenbank mit chromosomalem Ungleichgewicht und damit verbundenem Phänotyp beim Menschen, mit Ensembl-Ressourcen. MARRVEL, eine Website, die OMIM als eine der sechs menschlichen genetischen Datenbanken und sieben Modellorganismusdatenbanken verwendet, um Informationen zu integrieren. Referenzen Externe Links OMIM Homepage