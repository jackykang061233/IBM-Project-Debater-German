Statistik: Der k-norest-Nachbargorithmus (k-NN) ist eine nicht-metrische Klassifikationsmethode, die zunächst von Evelyn Fix und Joseph Hringes im Jahr 1951 entwickelt wurde und später von Thomas Cover erweitert wurde. Es wird zur Klassifizierung und Regression verwendet. In beiden Fällen besteht der Input aus den k engsten Schulungsbeispielen für Daten. Die Produktion hängt davon ab, ob k-NN zur Einstufung oder Regression verwendet wird: In k-NN-Klassifikation ist das Ergebnis eine Klassenmitgliedschaft. Ein Objekt wird durch eine pluralistische Abstimmung seiner Nachbarn eingestuft, wobei das Ziel, das der Klasse zugeordnet wird, die am häufigsten unter den k nächsten Nachbarn liegt (k ist eine positive Zahl, in der Regel kleine). Liegt k = 1, dann wird das Objekt einfach der Klasse dieser einzigen nahesten Nachbarn zugeordnet. In k-NN-Regression ist das Ergebnis der Immobilienwert für das Objekt. Dieser Wert ist der Mittelwert der Werte der k nächstgelegenen Nachbarn. k-NN ist eine Art von Klassifizierung, bei der die Funktion nur vor Ort angeglichen ist und alle Berechnungen bis zur Bewertung der Funktion vergeben werden. Da dieser Algorithmus auf die Entfernung zur Einstufung angewiesen ist, wenn die Merkmale unterschiedliche physische Einheiten darstellen oder in sehr unterschiedlichen Größen kommen, dann kann die Normalisierung der Ausbildungsdaten ihre Genauigkeit drastisch verbessern. Sowohl für die Einstufung als auch für die Regression kann eine nützliche Technik sein, um Gewicht auf die Beiträge der Nachbarn zuzuweisen, so dass die nahen Nachbarn mehr zum Durchschnitt beitragen als die weiter entfernten. Zum Beispiel besteht eine gemeinsame Gewichtungsregelung darin, jedem Nachbarn ein Gewicht von 1/d zu geben, wo d die Entfernung zu den Nachbarn ist. Die Nachbarn werden aus einer Reihe von Gegenständen stammen, für die die Klasse (für die k-NN-Klassifikation) oder der Objektwert (für die K-NN-Regression) bekannt ist. Man kann davon ausgehen, dass die Ausbildung für den Algorithmus, obwohl kein expliziter Ausbildungsschritt erforderlich ist. Kennzeichnend für den k-NN-Algorithmus ist, dass er empfindlich auf die lokale Struktur der Daten ist. Statistische Festlegung Kennzeichen (X 1 , Y 1 ) , ( X 2 , Y 2 ) , ... , ( X n , Y n ) Memestyle X_1},Y_{1}, (X_{2},Y_{2}),\dots (X_{n},Y_{n} Die Ausbildungsphase des Algorithmus besteht nur aus der Lagerung der Merkmalsvektoren und Klassenetiketten der Trainingsproben. In der Klassifizierungsphase ist k eine benutzerfreundliche Dauer, und ein unlabelter Vektor (ein Abfrage- oder Testpunkt) wird durch die Zuteilung des Etiketts klassifiziert, das am häufigsten unter den k-Ausbildungsproben in der Nähe dieses Abfragepunkts liegt. In der Regel wird die Entfernung von Euclidean verwendet. Für Einzelvariablen, wie zum Beispiel für die Textklassifikation, kann ein anderer Parameter verwendet werden, wie z.B. die Überschneidung (oder die Entfernung). K-NN wurde im Zusammenhang mit Gen Expression Mikroarray-Daten beispielsweise mit Korrelationskoeffizienten wie Pearson und Spearman als Parameter beschäftigt. Häufig kann die Einstufungsgenauigkeit von k-NN erheblich verbessert werden, wenn die Fernmessung mit Spezialgorithmen wie großflächige Grenz- oder Nachbarschaftskomponentenanalyse gelernt wird. Ein Rückschlag auf die grundlegende "Hauptabstimmung"-Klassifikation findet statt, wenn die Klassenverteilung skewed ist. Beispiele einer häufigeren Klasse dominieren in der Regel die Vorhersage des neuen Beispiels, weil sie wegen ihrer großen Zahl in den k nächsten Nachbarn üblich sind. Ein Weg, dieses Problem zu überwinden, ist das Gewicht der Klassifizierung unter Berücksichtigung der Entfernung vom Testpunkt zu jedem der k nächsten Nachbarn. Die Klasse (oder der Wert, in Regressionsproblemen) der einzelnen k nächsten Punkte wird mit einem Gewicht multipliziert, der proportional zum Kehrwert der Entfernung von diesem Punkt zum Testpunkt ist. Ein weiterer Weg zur Überwindung des Skeletts ist die Entnahme in der Datenvertretung. Beispielsweise ist jeder Node in einer Selbstorganisationskarte (SOM) ein Vertreter eines Clusters ähnlicher Punkte, unabhängig von ihrer Dichte in den ursprünglichen Schulungsdaten. K-NN kann dann auf die SOM angewendet werden. Parameterauswahl Die beste Auswahl an k hängt von den Daten ab; im Allgemeinen werden größere Werte von k die Wirkung des Lärms auf die Einstufung verringern, aber Grenzen zwischen Klassen weniger unterscheiden. Eine gute k kann von verschiedenen he touristischen Techniken (siehe Hyperparameteroptimierung) ausgewählt werden. Besonderer Fall, in dem die Klasse vorhergesagt wird, die Klasse der engsten Probe (d. h. wenn k = 1) wird der nächste benachbarte Algorithmus genannt. Die Genauigkeit des k-NN-Algorithmus lässt sich durch das Vorhandensein von lauten oder irrelevanten Merkmalen stark erniedrigen oder wenn die Merkmalsgrößen nicht mit ihrer Bedeutung übereinstimmen. Viele Forschungsanstrengungen wurden in die Auswahl oder Verkleinerung von Merkmalen zur Verbesserung der Klassifikation gestellt. Ein besonders populärer Ansatz ist die Verwendung von evolutionären Algorithmen zur Optimierung der Merkmalskalation. Ein weiterer populärer Ansatz besteht darin, durch die gegenseitige Information der Ausbildungsdaten mit den Ausbildungsklassen zu erweitern. In binären (zwei Klassen) Klassifikationsproblemen ist es hilfreich, k zu wählen, um eine ungeheuerte Zahl zu sein, da dies keine gebundenen Stimmen vermeidet. Ein populärer Weg, um die empirische optimale k in dieser Einrichtung zu wählen, ist über die Sprint-Methode. 1-Nearest benachbarte Klasseifier Kennzeichnend für die nächste Generation ist der nächste Nachbarklasser, der einen Punkt x der Klasse seiner engsten Nachbarn im Bereich des Merkmals zugewiesen, der C n 1 n ( x ) = Y ( 1 ) faser C_{n}^{1n}(x)=Y_[1 ] Je nach Größe der infinity festgelegten Ausbildungsdaten garantiert der nächstwichtigste Partner-Klasser eine Fehlerquote, die nicht schlechter ist als doppelt so hoch ist wie die Fehlerquote der Buchten (die minimale erreichbare Fehlerquote aufgrund der Datenverteilung). Gewicht in der Nähe der höchsten Nachbarklasse Die k-norest-Nachbarschaftsklasse kann als Zuordnung der k nächstgelegenen Nachbarn ein Gewicht 1 / k Memestyle 1/k} und alle anderen 0 Gewichts angesehen werden. Dies kann allgemeinisiert werden, um in der Nähe der höchsten Nachbarn zu gewichtet werden. Das ist, wenn der naheste Nachbarn ein Gewicht w n i {\displaystyle w_{ni} mit  with i = 1 n w n i = 1 {\displaystyle \sum i=1}^{n}w_{ni}=1 . Ein ähnliches Ergebnis ist auch die starke Konsistenz gewichteter benachbarter Klassenstellen. Lassen Sie C n n faserdisplaystyle C_{n}^{wn den gewichteten in der Nähe der höchsten Klasse mit Gewicht { w n i } i = 1 n {\displaystyle w_{ni__{i=1}^{n . Unter Berücksichtigung der Bedingungen für die Klassenverteilung hat das überschüssige Risiko folgende Asymptotic Expansion R (C n n n ) − R (C B a y e s ) = ( B 1 s n 2 + B 2 t n 2 n 2 ) { 1 + o ( 1 ) }, {\displaystyle KINGcal R__Get Mathematik R)(C_{n}^{wn} R((CBayBayes})=\left(B_{1} {n22}+B_{2}t_{n}^{2)right)11+o(1, für konstanten B 1 {\displaystyle B_{1} und B 2 {\displaystyle B_{2}, wo s n 2 = s 2 i = 1 n w n i 2 Memestyle s_{n}^{2} i=1}n}w_{ni}^{2 und t n = n − 2 / d  i i = 1 n w n i { i 1 + 2 / d − (i − 1 ) 1 + 2 / d } displaystyle t_{n}=n2/-2/d.sum_{i=1}^{n}w_{ni^{i11+2/d}-(i-1)11+2/d .Die optimale Gewichtungsregelung { w n i  i } i = 1 n {\displaystyle w_{ni**__{i=1}^{n , die den beiden oben genannten Bedingungen entsprechen, wird wie folgt angegeben: Set k  = =  B B n 4 d + 4 ⌋ HANAstyle k**} 4+d+4 rboden }, w n i  1 = 1 k ∗ [ 1 + d 2 − d 2 k ∗ 2 / d { i 1 + 2 / d − (i − 1 ) 1 + 2 / d } ] w_{ni**} 1}{k**[-Link[1+ Finanzfrac  d2{\-Barfrac d22{k**2/2/d2/i11+2/d}-(i-1) i1+2/d}\}\right] für i = 1 , 2 , ... , k ∗ {\displaystyle i=1,2,\dots ,k**} und w n i  i = 0 {\displaystyle w_{ni*=0 for i =  ... + 1 , ... n style i=k*,\dots ,k O*) und w n i n i · hv. h. h. h. h. h. h. h. h. Eigenschaften k-NN sind ein Sonderfall einer variablen Bandbreite, der kerndichte Ballon-Erreger mit einem einheitlichen Kern. Die naive Variante des Algorithmus ist einfach, die Entfernungen vom Testbeispiel für alle gespeicherten Beispiele zu berechnen, aber es ist rechnerisch intensiv für große Ausbildungssets. Mit einem ungefährsten benachbarten Suchgorithmus macht k-NN rechnerisch auch für große Datensets nutzbar. Viele naheliegende Suchalgorithmen wurden über die Jahre hinweg vorgeschlagen; dies strebt generell an, die Zahl der tatsächlich durchgeführten Fernbewertungen zu verringern. k-NN hat einige starke Konsistenzergebnisse. Je nach Umfang der Datenansätze infinity wird der zweistufige k-NN-Algorithmus garantiert, eine Fehlerquote zu erzielen, die nicht schlechter ist als doppelt so hoch ist wie die Fehlerquote der Buchten (die minimale erreichbare Fehlerquote aufgrund der Datenverteilung). Verschiedene Verbesserungen der k-NN-Geschwindigkeit sind möglich durch die Nutzung von Grenzdiagrammen. Mehrklassen k-NN-Klassifikation, Abdeckung und Johan Hart (2009) beweist eine höchste gebundene Fehlerquote von R  k ≤ R k N ≤ R ∗ ( 2 − M R  1 M − 1 ) Memestyle R*}{^\ \leq \ R_{k\ Mathematik {NN} }\ \ R R R** (2-Sparacf{MR*M-1right), wo R* ∗ KING R** \style R** \leq \ R* \ R* \, der Fehler in der N[N] ist der Fehler in der N-Nummer M = 2 Memedisplaystyle M=2} und da die Bayesische Fehlerquote R   WELLstyle R**} Ansätze Null hat, verringert sich diese Grenze auf "nicht mehr als das Doppelte der Bayesischen Fehlerquote". Fehlerquoten Es gibt viele Ergebnisse zur Fehlerquote der k nächstgelegenen Nachbarn. K-norest-Nachbarschaftsklasser ist stark (d. h. für jede gemeinsame Verteilung auf (X , Y ) Memestyle (X,Y)} )konsistent, sofern k := k n {\displaystyle k:=k_{n} Divergenzen und k n / n displaystyle k_{n}/n converges to Null as n →   KINGstyle n\to \infty } .Lassen Sie C n k n n faserdisplaystyle C_{n}^{knn den k nächstgelegenen Nachbarn-Klassenator auf der Grundlage einer Ausbildungsreihe von Größe n. Unter bestimmten Voraussetzungen ergibt sich das Überschussrisiko folgende Asymptotic Expansion R (C n k n n ) − R (C B a y e s ) = { B 1 k + B 2 ( k n ) 4 / d } { 1 + o ( 1 ) } , Memestyle KING KING R__Get Mathematik R)(C_{n}^{knn} R)(CBayBayes}=\leftBB_{1}{\frac) 1sk++B_{2sleft(Getfrac k}{n}}\nsright)11+o(1, für einige konstanten B 1 {\displaystyle B_{1} und B 2 {\displaystyle B_{2} .Die Wahl k  = =  B B n 4 d + 4 ⌋ 7.8displaystyle k**}=\lboden Bn^{\frac 4 4d+4.rboden } bietet einen Handel zwischen den beiden Bedingungen der oben genannten Anzeige, für die die k {\ {\displaystyle knea*} -nearest-Nachbarschaftsfehler s to the Bayes Fehler bei der optimalen (minimax) Rate O ( n − 4 d + 4 ) faserstyle O O(n^{-Firac 4}{d+4) . Lernen Die Leistung der K-norest-Nachbarschaftsklassifikation kann oft durch (überprüftes) metrisches Lernen erheblich verbessert werden. Volksbezogene Algorithmen sind die Analyse von Siedlungskomponenten und die große Marge nahe der Nachbarn. Supervised-metrische Lernalgorithmen verwenden die Etiketteninformationen, um eine neue Parameter- oder Pseudo-metrisch zu lernen. Materialgewinnung Wenn die Eingabedaten zu einem Algorithmus zu groß sind, um verarbeitet zu werden, und es wird vermutet, dass er entlassen wird (z.B. die gleiche Messung in beiden Fuß- und Metern), die Inputdaten werden in ein reduziertes Maß an Merkmalen umgewandelt (auch genannter Vektor). Die Umwandlung der Inputdaten in die Reihe von Merkmalen wird als Merkmalsgewinnung bezeichnet. Wenn die gewonnenen Merkmale sorgfältig ausgewählt werden, wird davon ausgegangen, dass die bereitgestellten Merkmale die entsprechenden Informationen aus den Inputdaten extrahieren, um die gewünschte Aufgabe unter Verwendung dieser reduzierten Darstellung anstelle des vollständigen Beitrags zu erfüllen. Materialgewinnung erfolgt vor der Anwendung des k-NN-Algorithmus auf die veränderten Daten in der Positionsfläche. Beispiel für eine typische Computer-Vision Rechenpipeline für Gesichtserkennung unter Verwendung von k-NN, einschließlich der charakteristischen Extraktions- und Massnahmen zur Verringerung der Vorverarbeitung (normalerweise mit OpenCV): Material Gesichtserkennung Mean-Verschiebungsanalyse PCA oder Fisher LDA in Merkmalsfläche, gefolgt von k-NN-Dimensionsminderung für hochdimensionale Daten (z.B. mit Anzahl der Abmessungen mehr als 10) wird in der Regel vor Anwendung des k-NN-Algorithmus durchgeführt, um die Auswirkungen des Problems der Dimension zu vermeiden. Die Tragbarkeit im k-NN-Kontext bedeutet im Wesentlichen, dass Euclidean Entfernung in hohen Dimensionen nicht hilfreich ist, da alle Vektoren fast equidistant auf den Abfragevektoren sind (sie sind mehrere Punkte, die mehr oder weniger auf einem Kreis mit der Abfragestelle im Zentrum liegen; die Entfernung von der Abfrage bis zu allen Datenpunkten im Suchraum ist fast gleich. Materialgewinnung und -verringerung können in einem Schritt kombiniert werden, wobei die Hauptkomponentenanalyse (PCA), lineare Diskriminierende Analyse (LDA) oder canonische Korrelationsanalyse (CCA) als Vorverarbeitungsschritt kombiniert werden können, gefolgt von einer Zusammenlegung von k-NN auf Merkmalsvektoren in reduzierter Größe. In diesem Prozess wird auch eine geringdimensionale Integration gefordert. Bei sehr hochdimensionalen Datensätzen (z.B. bei der Durchführung einer ähnlichen Suche auf Live-Videos, DNA-Daten oder hochdimensionale Zeitreihen), die eine schnelle, ungefähre k-NN-Suche unter Verwendung von Empfindlichkeitsempfindlichkeitsempfindlicher Häftlinge durchführen, "Sammelprojektionen," oder andere hochdimensionale ähnliche Suchtechniken aus der VLDB-Toolbox könnten die einzige Lösung sein. Entscheidungsgrenzen Nearest benachbarte Vorschriften in der Tat implizit die Entscheidungsgrenze berechnen. Es ist auch möglich, die Entscheidungsgrenze explizit zu berechnen und so effizient zu tun, damit die rechnerische Komplexität eine Funktion der Grenzkomplexität ist. Datenreduzierung ist eines der wichtigsten Probleme für die Arbeit mit riesigen Datensets. In der Regel sind nur einige der Datenpunkte für eine genaue Einstufung erforderlich. Diese Daten werden als Prototypen bezeichnet und können wie folgt abgerufen werden: Auswahl der Klassen-Outliers, d. h. Ausbildungsdaten, die von k-NN (für eine bestimmte k) falsch eingestuft werden, spalten die restlichen Daten in zwei Sätze: (i) die Prototypen, die für die Einstufungsentscheidungen verwendet werden, und (ii) die von k-NN korrekt mit Prototypen klassifizierten Aufnahmepunkte. Die aufgenommenen Punkte können dann von der Ausbildungseinrichtung entfernt werden. Auswahl der Klassen Ein Ausbildungsbeispiel, das durch Beispiele anderer Klassen umgeben ist, wird als Klassenabriss bezeichnet. Ursachen von Klassenausbrüchen sind: zufällig unzureichende Ausbildungsbeispiele dieser Klasse (ein isoliertes Beispiel scheint anstelle eines Clusters) fehlende wichtige Merkmale (die Klassen sind in anderen Dimensionen getrennt, die wir nicht kennen) zu viele Ausbildungsbeispiele anderer Klassen (unausgewogene Klassen), die einen feindlichen Hintergrund für die bestimmte kleine Klasse-Klassen mit k-NN-Lärm erzeugen. Sie können für künftige Analysen entdeckt und getrennt werden. Angesichts zweier natürlicher Nummern, k>r>0, wird ein Ausbildungsbeispiel als (k,r)NN-Ausgangsart bezeichnet, wenn seine k nächsten Nachbarn mehr als r Beispiele anderer Klassen umfassen. Konnektiv für die Datenverringerung in der Nähe der Nachbarn (CNN, Hart-Algorithmus) ist ein Algorithmus, der die Daten für die k-NN-Klassifikation reduzieren soll. Er wählt das Set von Prototypen aus den Schulungsdaten aus, so dass 1NN mit U die Beispiele fast so genau wie 1NN mit dem gesamten Datensatz ausweisen kann. CNN arbeitet in Anbetracht einer Ausbildungseinrichtung X und arbeitet sie für: Einstellung aller Elemente von X, Suche nach einem Element x, dessen nächster Prototyp aus U ein anderes Label als x enthält. X von X entfernen und hinzufügen, bis keine weiteren Prototypen an U.Use U anstelle von X für die Einstufung hinzugefügt werden. Die Beispiele, die keine Prototypen sind, werden aufgenommen. Es ist effizient, die Ausbildungsbeispiele zu überprüfen, um das Grenzverhältnis zu verringern. Das Grenzverhältnis eines Ausbildungsbeispiels x ist definiert als(x)=|x'-y|x-y|x-y|where x-y ist die Entfernung zum nächsten Beispiel y mit einer anderen Farbe als x, und x'-y ist die Entfernung von y zu ihrem nächsten Beispiel x' mit dem gleichen Etikett wie x. Das Grenzverhältnis liegt im Intervall [0,1], weil x'-y|never über x-y|never liegt. Diese Bestellung bevorzugt die Grenzen der Klassen für die Aufnahme in die Reihe von Prototypen U. Ein Punkt eines anderen Etiketts als x wird von außen zu x. Die Berechnung des Grenzwerts wird durch die Zahl an dem Recht veranschaulicht. Kennzeichnend für die Daten sind Farben: der erste Punkt ist x und sein Etikett ist rot. Externe Punkte sind blau und grün. Näher zu x externer Punkt ist y. Kurzumpunkt ist x' .Die Grenzquote a(x) =x'-y / x-y|is das Attribut des ersten Punkts x. unten ist ein Beispiel für CNN in einer Reihe von Zahlen. Es gibt drei Klassen (rot, grün und blau). Abb.1: zunächst gibt es 60 Punkte in jeder Klasse. Abb.2 zeigt die 1NN-Klassifikationskarte: Jedes Pixel wird von 1NN mit allen Daten klassifiziert. Abb.3 zeigt die 5NN-Klassifikationskarte. Weiße Gebiete entsprechen den nicht klassifizierten Regionen, in denen die 5NN-Entscheidung gebunden ist (z.B. wenn es zwei Grün, zwei rote und ein blaue Punkte unter 5 nächsten Nachbarn gibt). Fig.4 zeigt die reduzierten Daten. Die Kreuzungen sind die von der (3,2)NN-Regel (alle drei nächstgelegenen Nachbarn dieser Instanzen gehören zu anderen Klassen); die Quadrate sind die Prototypen, und die leeren Kreise sind die Aufnahmepunkte. Die linken Ecke zeigt die Anzahl der Klassen-Ausbrüche, Prototypen und absorbierten Punkte für alle drei Klassenklassen. Die Zahl der Prototypen variiert von 15% bis 20% für verschiedene Klassen in diesem Beispiel. Abb.5 zeigt, dass die 1NN-Klassifikationskarte mit den Prototypen sehr ähnlich ist wie mit den ursprünglichen Daten. Die Zahlen wurden unter Verwendung der Mirkes Apfelt hergestellt. KNN-Modellreduktion für k-NN-Klassenifiers k-NN-Regression In k-NN-Regression wird der k-NN-Algorithmus verwendet, um die Dauervariablen zu minimieren. Ein solcher Algorithmus verwendet einen gewichteten Durchschnitt der k nächstgelegenen Nachbarn, gewichtet durch den umgekehrten Abstand. Dieser Algorithmus funktioniert wie folgt: Compute the Euclidean oder Touchanobis Entfernung vom Abfragebeispiel zu den gekennzeichneten Beispielen. Reihenfolge der gekennzeichneten Beispiele durch zunehmende Entfernung. Suchen Sie auf der Grundlage von RMSE eine touristische optimale Zahl k der nächsten Nachbarn. Dies geschieht mit einer Cross-validierung. kalkuliert ein umgekehrter durchschnittlicher Abstand mit den multivariate Nachbarn in k-norest. k-NN Man kann auch als lokale Besatzungsschätzung betrachtet werden und ist somit auch ein populärer, unomaler Fortschritt bei der Erkennung. Je größer die Entfernung zur k-NN, desto geringer ist die lokale Bevölkerungsdichte, desto wahrscheinlicher ist die Abfragestelle. Obwohl es recht einfach ist, funktioniert dieses herausragende Modell zusammen mit einer anderen klassischen Datengewinnungsmethode, der lokale Aufholfaktor, im Vergleich zu neueren und komplexeren Ansätzen, nach einer groß angelegten experimentellen Analyse. Validierung der Ergebnisse Eine Verwirrungsmatrix oder eine "Abstimmungsmatrix" wird häufig als Instrument zur Validierung der Genauigkeit der k-NN-Klassifikation verwendet. robustere statistische Methoden wie die Wahrscheinlichkeitsprüfung können ebenfalls angewendet werden. Siehe auch die Next Centroid-Klassenifier Schließenspaar der Punkte Problemreferenzen Weiteres Lesen Dasarathy, Belur V, ed.(1991). Künftige (NN) Norms: NN Musterklassifikation Techniken.ISBNUNG DES0-8186-8930-7.Shakhnarovich, Gregory, Trevor; Indyk, Piotr, eds. (2005). Next-Neighbor-Methoden in Lernen und Vision. MIT Presse.ISBN AM0-262-19547-8.