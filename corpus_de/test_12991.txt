Die Softmax-Funktion, auch als softargmax oder normierte exponentielle Funktion bekannt, ist eine Verallgemeinerung der logistischen Funktion auf mehrere Dimensionen. Es wird in multinomaler logistischer Regression verwendet und wird oft als letzte Aktivierungsfunktion eines neuronalen Netzes verwendet, um die Ausgabe eines Netzes auf eine Wahrscheinlichkeitsverteilung über vorhergesagte Ausgabeklassen zu normalisieren, basierend auf Luces Wahlarxiom. Die Softmax-Funktion nimmt als Eingabe einen Vektor z von K realen Zahlen und normalisiert ihn in eine Wahrscheinlichkeitsverteilung bestehend aus K-Probabilities, die den Exponentien der Eingangszahlen proportional sind. Das heißt, vor dem Auftragen von softmax könnten einige Vektorkomponenten negativ oder größer als eins sein; und möglicherweise nicht summieren auf 1; aber nach dem Auftragen von softmax wird jede Komponente im Intervall [0, 1] {\displaystyle [0,1}] liegen und die Komponenten werden zu 1 addieren, so dass sie als Wahrscheinlichkeiten interpretiert werden können. Ferner entsprechen die größeren Eingangskomponenten größeren Wahrscheinlichkeiten. Standard (Einheit) softmax Funktion σ : R K → [0 , 1 ] K {\displaystyle \sigma :\mathbb {R} {^K}\to [0,1]^{K} wird durch die Formel σ (z ) definiert i) e z i Σ j = 1 K e z j für i = 1 , ... , K und z = ( z 1 , ... , z K ) ε R K . {\displaystyle \sigma (\mathbf {z} )_i}={\frac ) j=1{K}e^{z_{j\ \ \ \ \ \text für }i=1,\dotsc ,K{\text und }\mathbf {z} (=z_{1},\dotsc ,z_{K})\in \mathbb {R}^K}. In einfachen Worten gilt es für jedes Element z i {\displaystyle z_{i} des Eingangsvektors z {\displaystyle \mathbf {z} die standardmäßige exponentielle Funktion und normalisiert diese Werte durch Division durch die Summe aller dieser Exponentials; diese Normalisierung sorgt dafür, dass die Summe der Komponenten des Ausgangsvektors σ (z ) {\displaystyle \sigma (} ist Anstelle von e, eine andere Basis b > 0 kann verwendet werden. Wenn 0 < b < 1 kleinere Eingangskomponenten zu größeren Ausgangswahrscheinlichkeiten führen und der Wert b zu Wahrscheinlichkeitsverteilungen, die um die Positionen der kleinsten Eingangswerte konzentrierter sind. Umgekehrt werden bei b > 1 größere Eingangskomponenten zu größeren Ausgangswahrscheinlichkeiten führen und die Erhöhung des Wertes von b zu Wahrscheinlichkeitsverteilungen führen, die um die Positionen der größten Eingangswerte stärker konzentriert sind. Schreiben b = e β {\displaystyle b=e^{\beta } oder b = e - β {\displaystyle b=e^{-\beta } (für real β) ergibt die Ausdrücke: σ (z )i = e β z i Σ j = 1 K e β z j oder σ (z ) i) e - β z i Σ j = 1 K e - β z j für i = 1 , ... , K . ) (\mathbf} ) z_{j}}{\text für }i=1,\dotsc ,K}. In einigen Feldern wird die Basis entsprechend einer festen Skala fixiert, während in anderen der Parameter β variiert wird. Interpretationen glatt arg max Der Name softmax ist irreführend; die Funktion ist nicht ein glattes Maximum (eine glatte Annäherung an die maximale Funktion), sondern eine glatte Annäherung an die arg max Funktion: die Funktion, deren Wert der Index das Maximum hat. Tatsächlich wird der Begriff softmax auch für die eng verwandten LogSumExp-Funktion verwendet, die ein glattes Maximum ist. Aus diesem Grund bevorzugen einige den genaueren Begriff softargmax, aber der Begriff softmax ist im maschinellen Lernen üblich. Dieser Abschnitt verwendet den Begriff softargmax, um diese Interpretation zu betonen. ,,, n {\displaystyle 1,\dots ,n} (entsprechend dem Index)Beispielsweise ist bei dieser Kodierung eine r g m a x ‡ ( 1 , 5 , 10 ) = ( 0 , 0 , 1 ) , {\displaystyle \operatorname {arg\,max} 1,5,10)=(0,0,1} da das dritte Argument das Maximum ist. Dies kann auf mehrere arg max -Werte (mehrfach gleich z i {\displaystyle z_{i} ist das Maximum) verallgemeinert werden, indem die 1 zwischen allen max args geteilt wird; formal 1/k, wobei k die Anzahl der Argumente, die das Maximum annehmen. Z.B. eine r g m a x ‡ ( 1 , 5 , 5 ) = ( 0 , 1 / 2 , 1 / 2 ) , {\displaystyle \operatorname {arg\,max} 1,5,5)=(0,1/2,1/2} da das zweite und dritte Argument sowohl das Maximum sind. Für den Fall, dass alle Argumente gleich sind, ist dies einfach eine r g m a x ‡ (z , ... , z ) = ( 1 / n , ... , 1 / n ) . {\displaystyle \operatorname {arg\,max} (z,\dots z)=(1/n,\dots ,1/n}.) Punkte z mit mehreren Arg-Max-Werten sind Einzelpunkte (oder Singularitäten, und bilden den Singularsatz) – dies sind die Punkte, an denen arg max diskontinuierlich ist (mit einer Sprungunfähigkeit) – während Punkte mit einem einzigen Arg max als nicht-singuläre oder regelmäßige Punkte bekannt sind. Mit dem letzten in der Einführung gegebenen Ausdruck ist softargmax nun eine glatte Annäherung von arg max: wie β → ∞ {\displaystyle \beta \to \infty }, softargmax konvergiert zu arg max. Es gibt verschiedene Begriffe der Konvergenz einer Funktion; softargmax konvergiert zu arg max punktweise, d.h. für jeden festen Eingang z als β → ∞ {\displaystyle \beta \to \infty }, σ β (z ) → a r g m a x ♦ . {\displaystyle \sigma {\_beta }(\mathbf {z} )\to \operatorname {arg\,max} (\mathbf {z} } Softargmax konvergiert jedoch nicht gleichmäßig auf arg max, was intuitiv bedeutet, dass unterschiedliche Punkte mit unterschiedlichen Raten konvergieren und willkürlich langsam konvergieren können. In der Tat ist softargmax kontinuierlich, aber arg max ist nicht kontinuierlich am Singular-Set, wo zwei Koordinaten gleich sind, während die gleichmäßige Grenze der kontinuierlichen Funktionen kontinuierlich ist. Der Grund, warum es nicht gleichmäßig konvergiert ist, ist, dass bei Eingängen, bei denen zwei Koordinaten fast gleich sind (und einer das Maximum ist), der arg max der Index des einen oder des anderen ist, so dass eine kleine Eingangsänderung eine große Ausgangsänderung ergibt. Beispiel: σ β ( 1 , 1.0001 ) → ( 0 , 1 ) , {\displaystyle \sigma {\_beta }(1,1.0001)\to (0,1}) aber σ β ( 1 , 0.9999 ) → ( 1\, 0 )Softargmax konvergiert jedoch kompakt auf dem nicht-singularen Satz. Umgekehrt, wie β → - ∞ {\displaystyle \beta \to -\infty }, weichargmax konvergiert auf arg min in der gleichen Weise, wo hier der Singularsatz ist Punkte mit zwei arg min Werte. In der Sprache der tropischen Analyse ist der softmax eine Verformung oder Quantisierung von arg max und arg min, entsprechend der Verwendung des Log-Halbrings anstelle des max-plus-Halbrings (jeweils min-plus-Halbring,) und die Wiederherstellung der arg max oder arg min durch die Einnahme der Grenze wird Tropenisierung oder Dequantisierung genannt". Es ist auch der Fall, dass für jede feste β, wenn ein Eingang z i {\displaystyle z_{i} viel größer ist als die anderen relativ zur Temperatur, T = 1 / β {\displaystyle T=1/\beta }, der Ausgang etwa die arg max ist. Beispielsweise ist ein Unterschied von 10 groß gegenüber einer Temperatur von 1: σ (0, 10 ) := σ 1 (0, 10 ) = ( 1 / ( 1 + e 10 ) , e 10 / ( 1 + e 10 ) ≈ (0,000\, 0.99995 )\displaystyle \sigma (0,10):=\sigma _1}(0,0,95) Ist die Differenz jedoch klein gegenüber der Temperatur, so liegt der Wert nicht nahe dem Arg max. Eine Differenz von 10 ist z.B. klein gegenüber einer Temperatur von 100: σ 1 / 100 ( 0, 10 ) = ( 1 / ( 1 + e 1 / 10 ), e 1 / 10 / ( 1 + e 1 / 10 ) ≈ ( 0.475 , 0.525 ) . Als β → ∞ {\displaystyle \beta \to \infty } geht die Temperatur auf Null, T = 1 / β → 0 {\displaystyle T=1/\beta \to 0}, soeventual werden alle Unterschiede groß (bezogen auf eine Schrumpftemperatur), was eine weitere Interpretation für das Grenzverhalten gibt. Wahrscheinlichkeitstheorie In der Wahrscheinlichkeitstheorie kann der Ausgang der Softargmax-Funktion verwendet werden, um eine kategorische Verteilung darzustellen – also eine Wahrscheinlichkeitsverteilung über K verschiedene mögliche Ergebnisse. Statistische Mechanik Bei statistischen Mechaniken ist die Softargmax-Funktion als Boltzmann-Verteilung (oder Gibbs-Verteilung:) der Indexsatz 1 , ..., k {\displaystyle {1,\dots ,k} die Mikrostate des Systems; die Eingänge z i {\displaystyle z_{i} sind die Energien dieses Zustandes; der Nenner ist als die Partitionsfunktion bekannt, oft mit β bezeichnet.Anwendungen Die Softmax-Funktion wird in verschiedenen Multi-Klasse-Klassifikationsverfahren verwendet, wie multinomial logistische Regression (auch bekannt als Softmax Regression) [1], multi-Klasse lineare Diskriminante Analyse, naive Bayes Klassifikatoren und künstliche neuronale Netzwerke. Konkret ist bei multinomaler logistischer Regression und linearer diskriminierender Analyse der Eingang zur Funktion das Ergebnis von K eindeutigen linearen Funktionen, und die prognostizierte Wahrscheinlichkeit für die j'th Klasse bei einem Probenvektor x und einem Gewichtungsvektor w ist: P ( y = j ∣ x ) = e x T w j Σ k = 1 K e x T w k {\displaystyle P(y=j\mid \mathbf {x} {=)\frac e^{\mathbf {x} {\\mathf}\mathbf {w} * ) {\\mathf}\mathbf {w} (k) Dies kann als die Zusammensetzung von K linearen Funktionen x ↦ x T w 1 , ..., x ↦ x T w K {\displaystyle \mathbf {x} \mapsto \mathbf {x}^\mathsf {T}\mathbf {w},\ldots ,\mathbf {x} \mapsto \math ^____________________________ Neurale Netze Die Softmax-Funktion wird häufig in der Endschicht eines neuronalen Netzwerk-basierten Klassifikators verwendet. Solche Netzwerke werden häufig unter einem log loss (oder cross-entropy) Regime trainiert, was eine nichtlineare Variante der multinomalen logistischen Regression gibt. Da die Funktion einen Vektor und einen bestimmten Index i {\displaystyle i} auf einen realen Wert abbildet, muss das Derivat den Index berücksichtigen: ∂ q k σ ( q, i) = σ ( q, i) ( δi k - σ ( q , k ) . {\displaystyle {\frac {\partial {}\partial q_{k}}}\sigma {(\textbf {q},i)=\sigma {\textbf {q},i)(\delta {_ik}-\sigma {\textbf {q},k) Dieser Ausdruck ist in den Indexen i, k {\displaystyle i,k} symmetrisch und kann somit auch als ∂ q k σ (q , i ) = σ (q , k ) (δ i k - σ (q , i ) ) ausgedrückt werden. {\displaystyle {\frac {\partial {}\partial q_{k}}}\sigma {(\textbf {q},i)=\sigma {\textbf {q},k)(\delta {_ik}-\sigma {\textbf {q},i) Hier wird die Kronecker delta zur Einfachheit verwendet (\textbf}. Wird die Funktion mit dem Parameter β {\displaystyle \beta } skaliert, so müssen diese Ausdrücke mit β {\displaystyle \beta } multinomial logit für ein Wahrscheinlichkeitsmodell multipliziert werden, das die Softmax Aktivierungsfunktion verwendet. Verstärktes Lernen Im Bereich des Verstärkungslernens kann eine Softmax-Funktion verwendet werden, um Werte in Aktionswahrscheinlichkeiten umzuwandeln. Die gebräuchliche Funktion ist: P t (a) = exp ‡ ( q t (a ) / τ ) i = 1 n exp ≠ ( q t (i ) / τ ) , {\displaystyle P_{t}(a)={\frac exp(q_{t}(a)/\tau {})\sum i=1}\exp(q_ Bei hohen Temperaturen ( τ → ∞ {\displaystyle \tau bis\infty }) haben alle Aktionen fast die gleiche Wahrscheinlichkeit und je niedriger die Temperatur, desto mehr erwartete Belohnungen beeinflussen die Wahrscheinlichkeit. Für eine niedrige Temperatur ( τ → 0 + {\displaystyle \tau \to 0{}^+ ) neigt die Wahrscheinlichkeit der Aktion mit der höchsten erwarteten Belohnung zu 1. Eigenschaften Geometrisch stellt die Softmax-Funktion den Vektorraum R K {\displaystyle \mathbb {R} ^{K} auf die Grenze des Standards (K - 1 ) {\displaystyle (K-1}) -simplex, Schneiden der Dimension um eins (der Bereich ist ein (K - 1 )\displaystyle (K-1}) -dimensional simplex in K {\displaystyle K} -dimensionale BedeutungEntlang der Hauptdiagonale (x,x,\dots x}) ist softmax nur die gleichmäßige Verteilung an Ausgängen, ( 1 / n, ..., 1 / n ) {\displaystyle (1/n,\dots ,1/n}) : gleiche Punkte ergeben gleiche Wahrscheinlichkeiten. In der Regel ist softmax in der Übersetzung durch den gleichen Wert in jeder Koordinaten: Hinzufügen von c = (c, ..., c ) {\displaystyle \mathbf {c} (=c,\dots ,c}) zu den Eingängen z {\displaystyle \mathbf {z} gibt σ (z + c style) = σ (z) (weil e z i + c = e z i ∙ e c {\displaystyle e^{z_{i}+c}=e^{z_{i}\cdot e^c} ), so ändern sich die Verhältnisse nicht: σ (z + c) j = e z + c Σ k = 1 K e z k + c = e z j ⋅ e c Σ k = 1 K e z k ∙ e c = σ (z ) j . {\displaystyle \sigma (\mathbf {z} \+mathbf {c} ) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ******* (c) ****________ e^{c}}=\sigma (\mathbf {z} {)_j}. Geometrisch ist softmax entlang Diagonalen konstant: Dies ist die Dimension, die eliminiert wird, und entspricht der Softmax-Ausgang unabhängig von einer Übersetzung in den Eingangspunkten (eine Wahl von 0 Punkt). Man kann Eingabepunkte normalisieren, indem man annimmt, dass die Summe Null ist (subtrahieren Sie den Durchschnitt: c {\displaystyle \mathbf {c}, wobei c = 1 n Σ i {\textstyle c={\frac 1}{n}\sum z_{i} ), und dann nimmt der Softmax das Hyperplane von Punkten, die auf Null addieren, Σ z i = 0 {\textstyle \sum z_{i}=0, to the open simplex of positive value that sum to 1 Σ σ (z ) i = 1 {\textstyle \sum \sigma (\mathbf {z})_i}=1 , analog zu wie der Exponent 0 bis 1, e 0 = 1 {\displaystyle e^{0}=1 und ist positiv. Im Gegensatz dazu ist softmax nicht invariant unter Skalierung. Zum Beispiel σ ( ( 0 , 1 ) = ( 1 / ( 1 + e) , e / ( 1 + e) ) {\displaystyle \sigma {\bigl }(0,1){\bigr {=})\bigl }1/(1+e),e/(1+e){\bigr )} aber σ ( 0, 2 ) ) = ( 1 / ( 1 + e 2 ) e 2 / ( 1 + e 2 ) ) ) {\displaystyle \sigma {\bigl }(0,2){\bigr {=})\bigl 1/(1+e^{2}),e^}/(1+e^{2}){\bigr}}}}} Die Standard-Logistikfunktion ist der Sonderfall für eine 1-dimensionale Achse im 2-dimensionalen Raum, d.h. die x-Achse in der (x, y) Ebene. Eine Variable ist bei 0 festgelegt (Z 2 = 0 {\displaystyle z_{2}=0 ), so e 0 = 1 {\displaystyle e^{0}=1 , und die andere Variable kann variieren, bedeutet sie z 1 = x {\displaystyle z_{1}=x , so e z 1 / Σ k = 1 2 e z k = e x / ( e x + 1 ) ******** k=1{2}e^{z_{k}=e^{x}/(e^{x}+1,) die Standardlogistikfunktion und e z 2 / Σ k = 1 2 e z k = 1 / ( e x + 1 ) , {\textstyle ******************* k=1^{2}e^{z_{k}}=1/(e^{x}+1,) seine Ergänzung (d.h. sie addieren sich zu 1). Die 1-dimensionale Eingabe könnte alternativ als Zeile ( x / 2 , - x / 2 ) {\displaystyle (x/2,-x/2}) mit Ausgängen e x / 2 / ( e x / 2 + e- x / 2 ) = e x / ( e x + 1 ) {\displaystyle e^{x/2}/(e^{x/2}+e^{-x/2})=e^{x}/(e^{x}+1 und e - x / 2 / ( e x / 2 + e - x / 2 ) = 1 / ( e x + 1 ) e^{-x/2}/(e^{x/2}+e^{-x/2})=1/(e^{x}+1. Die Softmax-Funktion ist auch der Gradient der LogSumExp-Funktion, ein glattes Maximum: ∂ z i LSE ‡ (z ) = exp ‡ z i ≠ j = 1 K exp ‡ z j = σ (z ) i , für i = 1 , ... , K, z = ( z 1 , ... , z K ) ε K , z_{i}\operatorname {LSE} (\mathbf {z} {=)\frac {\exp} ) z_{j}=\sigma (\mathbf {z} {)_i},\quad \text für }i=1,\dotsc K,\quad \mathbf {z} (=z_{1},\dotsc ,z_{K})\in \mathbb {R} {^K}, wobei die LogSumExp-Funktion als LSE-Kennzeichen definiert ist (z 1 , ... , z n ) = log kennzeichen + ⋯ + exp Geschichte Die Softmax-Funktion wurde in der statistischen Mechanik als Boltzmann-Verteilung im Grundpapier Boltzmann (1868) formalisiert und im einflussreichen Lehrbuch Gibbs (1902) populär gemacht. Die Verwendung des Softmax in der Entscheidungstheorie wird an Luce (1959) gewürdigt, der das Axiom der Unabhängigkeit von irrelevanten Alternativen in der rationalen Wahltheorie verwendet, um den Softmax in Luces Wahlarxiom für relative Präferenzen zu entwerfen. Im maschinellen Lernen wird der Begriff softmax an John S. Bridle in zwei Konferenzpapieren von 1989, Bridle (1990a:) und Bridle (1990b) gutgeschrieben: Wir beschäftigen uns mit zukunftsweisenden nichtlinearen Netzwerken (Multilayer-Perceptrons oder MLPs) mit mehreren Ausgängen. Wir möchten die Ausgänge des Netzes als Wahrscheinlichkeiten von Alternativen (z.B. Musterklassen) behandeln, die auf die Eingänge konditioniert sind.Wir suchen nach geeigneten Ausgangs-Nichtlinearitäten und nach geeigneten Kriterien für die Anpassung der Parameter des Netzes (z.B. Gewichte). Wir erklären zwei Modifikationen: Wahrscheinlichkeitssscoring, die eine Alternative zur quadratischen Fehlerminimierung ist, und eine normierte exponentielle (softmax) Multi-Input-Verallgemeinerung der logistischen Nichtlinearität. Für jeden Eingang müssen die Ausgänge alle positiv sein und sie müssen zu Einheit zusammenfassen. Bei einem Satz von unkontrainierten Werten, V j ( x ) {\displaystyle V_{j}(x) , können wir beide Bedingungen durch Verwendung einer normierten Exponential-Transformation sicherstellen: Q j (x ) = e V j (x ) / Σ k e V k (x ) Diese Transformation kann als Multi-Input-Verallgemeinerung der auf der gesamten Ausgangsschicht arbeitenden Logistik betrachtet werden. Es bewahrt die Rangfolge seiner Eingangswerte und ist eine differenzierbare Verallgemeinerung der „Winner-take-all“-Operation zur Erfassung des Maximalwertes. Aus diesem Grund möchten wir es als softmax bezeichnen. Beispiel Wenn wir einen Eingang von [1, 2, 3, 4, 1, 2, 3] einnehmen, ist dessen Softmax [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. Die Leistung hat den größten Teil ihres Gewichts, wo die 4 im ursprünglichen Eingang. Dies ist, was die Funktion normalerweise verwendet wird für: die größten Werte hervorzuheben und Werte zu unterdrücken, die deutlich unter dem Maximalwert liegen. Anmerkung: softmax ist nicht maßstabsinvariant, also wenn der Eingang [0.1, 0.2, 0.4, 0.1, 0.2, 0.3] (welche Summen zu 1.6) wäre der softmax [0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153]. Dies zeigt, dass bei Werten zwischen 0 und 1 Softmax tatsächlich der Maximalwert de-emphasiert wird (Anmerkung, dass 0,169 nicht nur kleiner als 0,475 ist, sondern auch kleiner als der Anfangsanteil von 0,4/1.6=0,25). Berechnung dieses Beispiels mit Python-Code: Hier ist ein Beispiel von Julia Code: Hier ein Beispiel für R-Code: Hier ein Beispiel von Elixir-Code: Siehe auch Softplus Multinomial logistic regression Dirichlet Distribution – eine alternative Möglichkeit zur Probe kategorische Verteilungen Partitionsfunktion Notes == Referenzen =Hypostatische Abstraktion in mathematischer Logik, auch als Hypostasis oder subjektive Abstraktion bekannt, ist eine formale Operation, die ein Prädikat in eine Beziehung verwandelt; zum Beispiel "Honey is sweet" wird in "Honey has sweetness" umgewandelt. Die Beziehung wird zwischen dem ursprünglichen Subjekt und einem neuen Begriff geschaffen, der die durch das ursprüngliche Prädikat ausgedrückte Eigenschaft darstellt. Hypostasis ändert eine propositionale Formel der Form X Y zu einer anderen der Form X hat die Eigenschaft, Y zu sein oder X Y-ness hat. Die logische Funktion des zweiten Objekts Y-ness besteht allein in den Wahrheitswerten jener Sätze, die die entsprechende abstrakte Eigenschaft Y als Prädikat aufweisen. Das auf diese Weise eingeführte Gedankenobjekt kann als hypostatisches Objekt und in gewisser Weise ein abstraktes Objekt und ein formales Objekt bezeichnet werden. Die obige Definition wird von der von Charles Sanders Peirce (CP 4.235, "The Simplest Mathematics" (1902,) in Collected Papers, CP 4.227–323) gegeben. Wie Peirce es beschreibt, ist der Hauptpunkt für die formale Operation der hypostatischen Abstraktion, soweit sie auf formalen sprachlichen Ausdrücken arbeitet, dass sie ein predizatives Adjektiv oder Prädikat in ein zusätzliches Subjekt umwandelt und damit um einen die Anzahl der Subjektslots – die Arität oder Widmmut – des Hauptprädikats erhöht. Die Umwandlung von "Honey is sweet" in "Honey besitzt Süße" kann auf verschiedene Weise betrachtet werden: Die grammatische Spur dieser hypostatischen Transformation ist ein Prozess, der die Adjektivsüße aus dem Prädikat "ist süß", ersetzt es durch ein neues, erhöhtes Säureprädikat besitzt, und als Nebenprodukt der Reaktion, wie es war, die materielle Süße als zweites Subjekt des neuen Prädikats ausfällt.Die Abstraktion der Hypostase nimmt den konkreten physischen Sinn des Geschmacks, der in "Honey is sweet" gefunden wurde, und gibt ihm formale metaphysische Eigenschaften in "Honey has sweetness". Siehe auch Referenzen Peirce, C.S, Gesammelte Schriften von Charles Sanders Peirce, Bd.1–6 (1931–1935), Charles Hartshorne und Paul Weiss, Hd., Bd.7–8 (1958), Arthur W. Burks, hd., Cambridge, Massachusetts, Harvard University Press. Externe Links J. Jay Zeman, Peirce on Abstraction