Deep Learning (auch als tief strukturiertes Lernen bezeichnet) ist Teil einer breiteren Familie von maschinellen Lernmethoden basierend auf künstlichen neuronalen Netzwerken mit Repräsentationslern. Lernen kann beaufsichtigt, semi-supervised oder unupervised. Deep-learning-Architekturen wie tiefe neuronale Netzwerke, Deep-Glaube-Netzwerke, tiefe Verstärkung Lernen, wiederkehrende neuronale Netzwerke und konvolutionale neuronale Netzwerke wurden auf Felder wie Computer Vision, Spracherkennung, natürliche Sprachverarbeitung, maschinelle Übersetzung, Bioinformatik, Drogendesign, medizinische Bildanalyse, Materialinspektion und Brettspielprogramme angewendet, wo sie vergleichbare Ergebnisse mit und in einigen Fällen übertreffende menschliche Experten hervorgebracht haben. Künstliche neuronale Netze (ANNs) wurden von Informationsverarbeitung und verteilten Kommunikationsknoten in biologischen Systemen inspiriert. ANNs haben verschiedene Unterschiede von biologischen Gehirnen. Konkret neigen neuronale Netze zu statisch und symbolisch, während das biologische Gehirn der meisten lebenden Organismen dynamisch (plastisch) und analog ist. Das Adjektiv tief im Deep Learning bezieht sich auf den Einsatz mehrerer Schichten im Netzwerk. Frühe Arbeiten zeigten, dass ein linearer Perceptron kein universeller Klassifikator sein kann, sondern dass ein Netzwerk mit einer nicht polynomialen Aktivierungsfunktion mit einer versteckten Schicht ungebundener Breite kann. Deep Learning ist eine moderne Variation, die sich mit einer ungebundenen Anzahl von Schichten gebundener Größe beschäftigt, die eine praktische Anwendung und optimierte Implementierung ermöglicht, wobei die theoretische Universalität unter milden Bedingungen erhalten bleibt. Im Tiefenlernen dürfen die Schichten auch heterogen sein und weit von biologisch fundierten Verbindungsmodellen abweichen, um der Effizienz, Zugänglichkeit und Verständlichkeit zu dienen, wenn der strukturierte Teil. Definition Deep Learning ist eine Klasse von maschinellen Lernalgorithmen, die mehrere Schichten verwendet, um fortschrittliche Funktionen von der Roheingabe zu extrahieren. Beispielsweise können bei der Bildverarbeitung untere Schichten Kanten identifizieren, während höhere Schichten die für einen Menschen relevanten Konzepte wie Ziffern oder Buchstaben oder Gesichter identifizieren können. Die meisten modernen Deep Learning-Modelle basieren auf künstlichen neuronalen Netzwerken, speziell auf konvolutionalen neuronalen Netzwerken (CNN), obwohl sie auch propositionale Formeln oder latente Variablen umfassen können, die schichtweise in tiefen generativen Modellen organisiert werden, wie die Knoten in tiefen Glaubensnetzwerken und tiefen Boltzmann-Maschinen. Im Deep Learning lernt jede Ebene, ihre Eingabedaten in eine etwas abstraktere und zusammengesetzte Darstellung zu transformieren. Bei einer Bilderkennungsapplikation kann der Roheingang eine Matrix von Pixeln sein; die erste Darstellungsschicht kann die Pixel und Coderänder abstrahieren; die zweite Schicht kann Anordnungen von Kanten komponieren und kodieren; die dritte Schicht kann eine Nase und Augen codieren; und die vierte Schicht kann erkennen, dass das Bild ein Gesicht enthält. Wichtig ist, dass ein tiefgreifender Lernprozess lernen kann, welche Funktionen optimal in welcher Ebene selbst platziert werden. Dies eliminiert nicht völlig die Notwendigkeit der Handabstimmung; beispielsweise können unterschiedliche Schichtenzahlen und Schichtgrößen unterschiedliche Abstraktionsgrade bieten. Das Wort tief im "Deep Learning" bezieht sich auf die Anzahl der Schichten, durch die die Daten transformiert werden. Genauer gesagt haben tiefe Lernsysteme eine wesentliche Kreditzuweisungspfad (CAP) Tiefe. Die GAP ist die Kette der Transformationen von Input zu Output. CAPs beschreiben potenziell kausale Verbindungen zwischen Eingang und Ausgang. Für ein zukunftsweisendes neuronales Netz ist die Tiefe der CAPs die des Netzes und die Anzahl der versteckten Schichten plus eine (wie auch die Ausgangsschicht parametriert ist). Bei wiederkehrenden neuronalen Netzen, in denen ein Signal mehr als einmal durch eine Schicht propagieren kann, ist die CAP-Tiefe möglicherweise unbegrenzt. Keine allgemein vereinbarte Schwelle der Tiefe teilt sich flaches Lernen aus dem tiefen Lernen, aber die meisten Forscher sind sich darin einig, dass tiefes Lernen die Tiefe der GAP über 2 beinhaltet. Die GAP der Tiefe 2 hat sich als universeller Annäher in dem Sinne erwiesen, daß sie jede Funktion nachempfinden kann. Darüber hinaus addieren sich mehr Schichten nicht zu der Funktion Anreizfähigkeit des Netzwerks. Tiefe Modelle (CAP > 2) sind in der Lage, bessere Funktionen als flache Modelle zu extrahieren und somit zusätzliche Schichten helfen, die Funktionen effektiv zu lernen. Deep Learning-Architekturen können mit einer gierigen Schicht-für-Schicht-Methode aufgebaut werden. Deep Learning hilft, diese Abstraktionen zu entschärfen und herauszufinden, welche Funktionen die Leistung verbessern. Für beaufsichtigte Lernaufgaben eliminieren Deep Learning-Methoden die Feature-Engineering, indem die Daten in kompakte Zwischendarstellungen bezogen auf Hauptkomponenten übersetzt und Schichtstrukturen abgeleitet werden, die Redundanz in der Darstellung entfernen. Deep Learning Algorithmen können auf unübertroffene Lernaufgaben angewendet werden. Dies ist ein wichtiger Vorteil, denn unmarkierte Daten sind häufiger als die markierten Daten. Beispiele für tiefe Strukturen, die unübertroffen ausgebildet werden können, sind neuronale Geschichtskompressoren und tiefe Glaubensnetzwerke. InterpretationenDeep-Neuralnetze werden in der Regel in Bezug auf die universelle Approximationstheorem oder probabilistische Inferenz interpretiert. Das klassische universelle Approximationstheorem bezieht sich auf die Kapazität von Feedforward-Neural-Netzwerken mit einer einzigen versteckten Schicht von endlicher Größe, um kontinuierliche Funktionen anzunähern. 1989 wurde der erste Beweis von George Cybenko für sigmoide Aktivierungsfunktionen veröffentlicht und 1991 von Kurt Hornik zu fütternden Mehrschichtarchitekturen verallgemeinert. Neuere Arbeiten zeigten auch, dass die universelle Approximation auch für nichtgebundene Aktivierungsfunktionen wie die gleichgerichtete Lineareinheit gilt. Das universelle Approximationstheorem für tiefe neuronale Netze betrifft die Kapazität von Netzwerken mit begrenzter Breite, aber die Tiefe darf wachsen. Lu et al.proved, dass, wenn die Breite eines tiefen neuronalen Netzes mit ReLU-Aktivierung streng größer ist als die Eingangsdimension, das Netzwerk kann jede Lebesgue-Integrable-Funktion angenähern; Ist die Breite kleiner oder gleich der Eingangsdimension, dann ist tiefes neuronales Netz nicht ein universaler Atmator. Die probabilistische Interpretation ergibt sich aus dem Bereich des maschinellen Lernens. Es verfügt über Inferenz sowie die Optimierungskonzepte von Schulungen und Tests im Zusammenhang mit der Montage bzw. Verallgemeinerung. Insbesondere betrachtet die probabilistische Interpretation die Aktivierungs-Nichtlinearität als kumulative Verteilungsfunktion. Die probabilistische Interpretation führte zur Einführung von Dropout als Regularizer in neuronalen Netzwerken. Die probabilistische Interpretation wurde von Forschern wie Hopfield, Widrow und Narendra eingeführt und in Umfragen wie dem Bischof populär gemacht. Geschichte Einige Quellen weisen darauf hin, dass Frank Rosenblatt alle grundlegenden Bestandteile der tiefen Lernsysteme von heute entwickelt und erforschte. Er beschrieb es in seinem Buch "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms", herausgegeben von Cornell Aeronautical Laboratory, Inc. Cornell University 1962. Der erste allgemeine, arbeitende Lernalgorithmus für beaufsichtigte, tiefe, fütternde, mehrschichtige Perceptrons wurde 1967 von Alexey Ivakhnenko und Lapa veröffentlicht. Ein 1971er Papier beschreibt ein tiefes Netzwerk mit acht Schichten, die durch die Gruppenmethode der Datenverarbeitung ausgebildet werden. Andere tief lernende Arbeitsarchitekturen, speziell jene, die für die Computer-Vision gebaut wurden, begannen mit dem von Kunihiko Fukushima 1980 eingeführten Neocognitron. Der Begriff Deep Learning wurde 1986 von Rina Dechter in die Machine Learning Community eingeführt, sowie von Igor Aizenberg und Kollegen im Jahr 2000 im Rahmen von Booleschen Schwellenneuronen. 1989, Yann LeCun et al. den Standard-Backpropagationsalgorithmus, der seit 1970 als umgekehrter Modus der automatischen Differenzierung gewesen war, auf ein tiefes neuronales Netz mit dem Ziel, handschriftliche ZIP-Codes auf Post zu erkennen. Während der Algorithmus arbeitete, erforderte das Training 3 Tage. 1991 wurden solche Systeme zur Erkennung isolierter 2-D-Handschriften verwendet, während das Erkennen von 3-D-Objekten durch die Anpassung von 2-D-Bildern mit einem handgefertigten 3-D-Objektmodell erfolgte. Weng et al. schlug vor, dass ein menschliches Gehirn kein monolithisches 3-D-Objektmodell verwendet und 1992 Cresceptron veröffentlichte, ein Verfahren zur Durchführung von 3-D-Objekterkennung in zerstreuten Szenen. Da es direkt natürliche Bilder verwendet, begann Cresceptron den Beginn des allgemeinen visuellen Lernens für natürliche 3D-Welten. Cresceptron ist eine Kaskade von Schichten ähnlich Neocognitron. Während Neocognitron jedoch einen menschlichen Programmierer benötigte, um die Funktionen zu handhaben, erlernte Cresceptron eine offene Anzahl von Features in jeder Schicht ohne Aufsicht, wobei jede Funktion durch einen Faltenkern repräsentiert wird. Cresceptron segmentierte jedes gelernte Objekt aus einer betäubten Szene durch Rückanalyse durch das Netzwerk. Max-Pooling, jetzt oft von tiefen neuronalen Netzwerken (z.B. ImageNet-Tests) angenommen, wurde zunächst in Cresceptron verwendet, um die Positionsauflösung um einen Faktor (2x2) bis 1 durch die Kaskade für eine bessere Verallgemeinerung zu reduzieren. 1994 veröffentlichte André de Carvalho zusammen mit Mike Fairhurst und David Bisset experimentelle Ergebnisse eines mehrschichtigen booleschen neuronalen Netzes, auch als gewichtsloses neuronales Netz bekannt, bestehend aus einem 3-Schichten-selbstorganisierenden Merkmalsextraktion neuronalen Netzwerkmodul (SOFT) und einem mehrschichtigen Klassifizierungs-Neuralnetzwerkmodul (GSN), das unabhängig ausgebildet wurde. Jede Schicht im Merkmal Extraktionsmodul extrahiert Funktionen mit wachsender Komplexität in Bezug auf die vorherige Schicht. 1995, Brendan Frey zeigte, dass es möglich war, ein Netzwerk zu trainieren (über zwei Tage), das sechs voll verbundene Schichten und mehrere hundert versteckte Einheiten mit dem Wake-Sleep-Algorithmus, mit Peter Dayan und Hinton entwickelt. Viele Faktoren tragen zur langsamen Geschwindigkeit bei, einschließlich des verschwindenden Gradientenproblems, das 1991 von Sepp Hochreiter analysiert wurde. Seit 1997 erweiterte Sven Behnke den ernährungsweise hierarchischen Faltungsansatz in der Neural Abstraction-Pyramide durch seitliche und rückwärtige Verbindungen, um den Kontext flexibel in Entscheidungen zu integrieren und lokale Mehrdeutigkeiten iterativ zu lösen. Einfachere Modelle, die aufgabenspezifische handwerkliche Merkmale wie Gabor-Filter und Support-Vektor-Maschinen (SVMs) verwenden, waren eine beliebte Wahl in den 1990er und 2000er Jahren, wegen der rechnerischen Kosten des künstlichen neuronalen Netzes (ANN) und des mangelnden Verständnisses, wie das Gehirn seine biologischen Netzwerke verdrahtet. Sowohl flaches als auch tiefes Lernen (z.B. wiederkehrende Netze) von ANNs wurden seit vielen Jahren erforscht. Diese Methoden übertrafen nie die ungleichmäßige Innenhandcrafting Gaussian Mischung Modell / Hidden Markov Modell (GMM-HMM) Technologie auf Basis generativer Modelle der Sprache diskriminativ trainiert. Wesentliche Schwierigkeiten wurden analysiert, einschließlich Gradientenabbau und schwache zeitliche Korrelationsstruktur in neuronalen Vorhersagemodellen. Weitere Schwierigkeiten waren die fehlenden Trainingsdaten und die begrenzte Rechenleistung. Die meisten Spracherkennungsforscher zogen von neuronalen Netzen weg, um generative Modellierung zu verfolgen. Eine Ausnahme war Ende der 1990er Jahre bei SRI International. Gefördert von der NSA und DARPA der US-Regierung, untersuchte SRI tiefe neuronale Netzwerke in der Sprach- und Sprechererkennung. Das Sprecher-Erkennungsteam unter der Leitung von Larry Heck berichtete mit tiefen neuronalen Netzwerken in der Sprachverarbeitung im National Institute of Standards and Technology Speaker Recognition Evaluation von 1998. Das tiefe neuronale Netzwerk von SRI wurde dann im Nuance Verifier eingesetzt, was die erste große industrielle Anwendung von Deep Learning darstellt. Das Prinzip der Erhöhung der Rohmerkmale über die handwerkliche Optimierung wurde zunächst erfolgreich in der Architektur des tiefen Autoencoders auf dem Rohspektrogramm oder linearen Filterbank-Funktionen in den späten 1990er Jahren untersucht, was seine Überlegenheit über die Mel-Cepstral-Funktionen zeigt, die Stadien der festen Transformation aus Spektrogrammen enthalten. Die Rohmerkmale von Sprache, Wellenformen, später produzierte ausgezeichnete größere Ergebnisse. Viele Aspekte der Spracherkennung wurden von einer Deep Learning Methode, genannt Long Short-term Memory (LSTM,) übernommen, einem rezidiven neuronalen Netzwerk, das 1997 von Hochreiter und Schmidhuber veröffentlicht wurde. LSTM RNs vermeiden das verschwindende Gradientenproblem und können "Sehr Deep Learning"-Aufgaben lernen, die Erinnerungen an Ereignisse erfordern, die vor Tausenden von diskreten Zeitschritten geschehen, was für die Rede wichtig ist. 2003 begann LSTM mit traditionellen Spracherkennern auf bestimmten Aufgaben wettbewerbsfähig zu werden. Später wurde es mit der verbindungistischen zeitlichen Klassifizierung (CTC) in Stapeln von LSTM RNs kombiniert. Im Jahr 2015 erlebte Googles Spracherkennung einen dramatischen Leistungssprung von 49 % durch CTC-trained LSTM, den sie über Google Voice Search zur Verfügung gestellt haben. Im Jahr 2006 zeigten Publikationen von Geoff Hinton, Ruslan Salakhutdinov, Osindero und Teh, wie ein vielschichtiges neurales Netz zu einer Zeit effektiv vortrainiert werden konnte, jede Schicht wiederum als ununterbrochene eingeschränkte Boltzmann-Maschine behandelt und anschließend mit überwachter Backpropagation verfeinert werden konnte. Die Papiere bezogen sich auf das Lernen für tiefe Glaubensnetze. Deep Learning ist Teil modernster Systeme in verschiedenen Disziplinen, insbesondere der Computervision und der automatischen Spracherkennung (ASR). Die Ergebnisse von allgemein verwendeten Bewertungssets wie TIMIT (ASR) und MNIST (Bildklassifikation) sowie eine Reihe von großformatigen Spracherkennungsaufgaben haben sich stetig verbessert. Konvolutionale neuronale Netze (CNNs) wurden für ASR von CTC für LSTM überlagert. aber sind erfolgreicher in der Computer-Vision. Die Auswirkungen des tiefen Lernens in der Industrie begannen in den frühen 2000er Jahren, als CNNs bereits schätzungsweise 10% bis 20% aller in den USA geschriebenen Kontrollen verarbeiteten, so Yann LeCun. Um 2010 begannen industrielle Anwendungen des tiefen Lernens zur groß angelegten Spracherkennung. Der NIPS-Workshop 2009 über Deep Learning for Speech Recognition wurde durch die Einschränkungen von tiefen generativen Sprachmodellen motiviert und die Möglichkeit gegeben, dass leistungsfähigere Hardware- und großformatige Datensätze, die tiefe neuronale Netze (DNN) praktisch werden könnten. Es wurde angenommen, dass vortraining DNNs mit generativen Modellen von tiefen Glaubensnetzen (DBN) die Hauptschwierigkeiten von neuronalen Netzen überwinden würde. Es wurde jedoch festgestellt, dass bei der Verwendung von DNs mit großen, kontextabhängigen Ausgangsschichten durch den Austausch von Vortrainingsdaten große Mengen an Trainingsdaten für eine unkomplizierte Backpropagation deutlich niedrigere Fehlerraten als damals-state-of-the-art Gaussian Mischungsmodell (GMM)/Hidden Markov Model (HMM) und auch als erweiterte generative modellbasierte Systeme erzeugt wurden. Die Art der von den beiden Arten von Systemen erzeugten Erkennungsfehler war charakteristisch unterschiedlich und bietet technische Einblicke in die Integration des tiefen Lernens in das bestehende hocheffiziente, durch alle großen Spracherkennungssysteme eingesetzte Sprachdekodiersystem zur Laufzeit. Analyse um 2009–2010, Kontrast zum GMM (und anderen generativen Sprachmodellen) gegen DNN-Modelle, stimulierte frühe industrielle Investitionen in das tiefe Lernen zur Spracherkennung und führte schließlich zu einem pervasiven und dominanten Einsatz in dieser Branche. Diese Analyse erfolgte mit vergleichbarer Leistung (weniger als 1,5 % der Fehlerquote) zwischen diskriminierenden DN und generativen Modellen. Im Jahr 2010 erweiterten die Forscher das tiefe Lernen von TIMIT auf eine große Vokabeln Spracherkennung, indem sie große Ausgangsschichten der DNN auf Basis kontextabhängiger HMM-Staaten, die von Entscheidungsbäumen aufgebaut sind, übernommen haben. Fortschritte in der Hardware haben ein neues Interesse an Deep Learning ausgelöst. 2009 beteiligte sich Nvidia an dem sogenannten „großen Schlag“ des tiefen Lernens, „als tiefgreifende neuronale Netzwerke mit Nvidia-Grafikenverarbeitungseinheiten (GPUs) ausgebildet wurden“. Dieses Jahr, Andrew Ng ermittelte, dass GPUs die Geschwindigkeit von Tieflötsystemen um etwa 100 Mal erhöhen könnten. Insbesondere sind GPUs für die am maschinellen Lernen beteiligten Matrix/Vektor-Rechnungen gut geeignet. GPUs beschleunigen Trainingsalgorithmen nach Größenordnungen und reduzieren Laufzeiten von Wochen bis Tagen. Weitere spezialisierte Hardware- und Algorithmusoptimierungen können zur effizienten Verarbeitung von Tiefenlernmodellen verwendet werden. Tiefe Lernrevolution 2012 gewann ein Team unter der Leitung von George E. Dahl die "Merck Molecular Activity Challenge" mit multi-task tiefen neuronalen Netzwerken, um das biomolekulare Ziel eines Medikaments vorherzusagen. Im Jahr 2014 nutzte Hochreiters Gruppe tiefes Lernen, um Off-Target- und Gifteffekte von Umweltchemikalien in Nährstoffen, Haushaltsprodukten und Medikamenten zu erkennen und gewann die "Tox21 Data Challenge" von NIH, FDA und NCATS. Von 2011 bis 2012 waren deutliche zusätzliche Auswirkungen auf die Bild- oder Objekterkennung zu spüren. Obwohl CNNs, die von der Backpropagation geschult wurden, schon seit Jahrzehnten gewesen waren, und GPU-Implementierungen von NNs für Jahre, einschließlich CNNs, wurden schnelle Implementierungen von CNNs auf GPUs benötigt, um auf der Computer Vision voranzukommen. 2011 erreichte dieser Ansatz zum ersten Mal übermenschliche Leistung in einem visuellen Mustererkennungswettbewerb. Auch 2011 gewann es den ICDAR chinesischen Handschriftwettbewerb, und im Mai 2012 gewann es den ISBI-Bildsegmentierungswettbewerb. Bis 2011 spielten die CNNs bei Computer-Vision-Konferenzen keine große Rolle, aber im Juni 2012 zeigte ein Referat von Ciresan et al.at der führenden Konferenz CVPR, wie die max-pooling CNNs auf GPU viele Vision-Benchmark-Daten drastisch verbessern kann. Im Oktober 2012, ein ähnliches System von Krizhevsky et al.won the groß angelegten ImageNet Wettbewerb durch eine signifikante Marge über flache maschinelle Lernmethoden. Im November 2012 gewann Ciresan et al.'s System auch den ICPR-Wettbewerb zur Analyse großer medizinischer Bilder zur Krebserkennung und im Folgejahr auch die MICCAI Grand Challenge zum selben Thema. In den Jahren 2013 und 2014 wurde die Fehlerquote bei der ImageNet-Aufgabe mit Deep Learning nach einem ähnlichen Trend bei der groß angelegten Spracherkennung weiter reduziert. Die Bildklassifikation wurde dann auf die schwierigere Aufgabe erweitert, Beschreibungen (Kapitel) für Bilder zu erzeugen, oft als Kombination von CNNs und LSTMs. Einige Forscher behaupten, dass der ImageNet-Sieg von Oktober 2012 den Start einer "tiefe Lernrevolution" verankert hat, die die KI-Branche transformiert hat. Im März 2019 wurden Yoshua Bengio, Geoffrey Hinton und Yann LeCun mit dem Turing Award für konzeptionelle und technische Durchbrüche ausgezeichnet, die tiefe neuronale Netzwerke zu einem kritischen Bestandteil des Computings gemacht haben. Künstliche neuronale Netze Künstliche neuronale Netze (ANNs) oder Verbindungssysteme sind Rechensysteme, die von den biologischen neuronalen Netzen inspiriert sind, die tierische Gehirne bilden. Solche Systeme lernen (progressiv ihre Fähigkeit verbessern) Aufgaben zu erledigen, indem man Beispiele betrachtet, im Allgemeinen ohne aufgabenspezifische Programmierung. Beispielsweise können sie bei der Bilderkennung lernen, Bilder zu identifizieren, die Katzen enthalten, indem sie Beispielbilder analysieren, die manuell als Katze oder "kein Katze" markiert wurden und die analytischen Ergebnisse verwenden, um Katzen in anderen Bildern zu identifizieren. Sie haben die meiste Verwendung in Anwendungen gefunden, die mit einem traditionellen Computeralgorithmus schwer auszudrücken sind, indem sie regelbasierte Programmierung verwenden. Eine ANN basiert auf einer Sammlung von vernetzten Einheiten namens künstliche Neuronen (analog zu biologischen Neuronen in einem biologischen Gehirn). Jede Verbindung (Synapse) zwischen Neuronen kann ein Signal an ein anderes Neuron übertragen. Das empfangende (postsynaptische) Neuron kann das/die Signal(en) verarbeiten und anschließend nachgeschaltete Neuronen signalisieren. Neuronen können Zustand haben, in der Regel durch reale Zahlen dargestellt, typischerweise zwischen 0 und 1. Neuronen und Synapsen können auch ein Gewicht haben, das bei Lernvorgängen variiert, die die Stärke des Signals erhöhen oder verringern können, das es nachgeschaltet sendet. Typischerweise werden Neuronen in Schichten organisiert. Verschiedene Schichten können unterschiedliche Arten von Transformationen an ihren Eingängen ausführen. Signale gelangen von der ersten (Eingang,) zur letzten (Ausgangs-)Schicht, möglicherweise nach mehrfachem Durchlaufen der Schichten. Das ursprüngliche Ziel des neuronalen Netzwerkansatzes war es, Probleme in der gleichen Weise zu lösen, wie ein menschliches Gehirn würde.Im Laufe der Zeit konzentrierte sich die Aufmerksamkeit auf die Anpassung bestimmter geistiger Fähigkeiten, was zu Abweichungen von Biologie wie Backpropagation oder Weitergabe von Informationen in umgekehrter Richtung und Anpassung des Netzes, um diese Informationen zu reflektieren. Neurale Netzwerke wurden auf einer Vielzahl von Aufgaben verwendet, einschließlich Computer Vision, Spracherkennung, maschinelle Übersetzung, Social Network Filtering, Spielbrett und Videospiele und medizinische Diagnose. Ab 2017 haben neuronale Netzwerke typischerweise ein paar tausend bis ein paar Millionen Einheiten und Millionen Verbindungen. Obwohl diese Zahl mehrere Größenordnungen kleiner ist als die Anzahl der Neuronen an einem menschlichen Gehirn, können diese Netzwerke viele Aufgaben auf einer Ebene jenseits der des Menschen (z.B. das Erkennen von Gesichtern, das Spielen von Go) ausführen. Tiefe neuronale Netze Ein tiefes neuronales Netz (DNN) ist ein künstliches neuronales Netz (ANN) mit mehreren Schichten zwischen Eingangs- und Ausgangsschichten. Es gibt verschiedene Arten von neuronalen Netzwerken, aber sie bestehen immer aus den gleichen Komponenten: Neuronen, Synapsen, Gewichte, Bias und Funktionen. Diese Komponenten funktionieren ähnlich dem menschlichen Gehirn und können wie jeder andere ML-Algorithmus trainiert werden. Zum Beispiel wird ein DNN, der ausgebildet ist, Hunderassen zu erkennen, über das gegebene Bild gehen und die Wahrscheinlichkeit berechnen, dass der Hund im Bild eine bestimmte Rasse ist. Der Benutzer kann die Ergebnisse überprüfen und auswählen, welche Wahrscheinlichkeiten das Netzwerk anzeigen soll (über eine bestimmte Schwelle, etc.) und das vorgeschlagene Label zurückgeben. Jede mathematische Manipulation als solche gilt als Schicht, und komplexe DNN haben viele Schichten, also den Namen tiefe Netzwerke. DNNs können komplexe nichtlineare Beziehungen modellieren. DNN-Architekturen erzeugen kompositorische Modelle, bei denen das Objekt als Schichtkomposition von Primitiven ausgedrückt wird. Die zusätzlichen Schichten ermöglichen die Zusammensetzung von Merkmalen aus tieferen Schichten, potenzielle Modellierung komplexer Daten mit weniger Einheiten als ein ähnlich funktionierendes flaches Netzwerk. So hat sich gezeigt, dass sparse multivariate Polynome exponentiell leichter mit DNNs anzunähern sind als mit flachen Netzen. Tiefenarchitekturen umfassen viele Varianten von einigen grundlegenden Ansätzen. Jede Architektur hat in bestimmten Bereichen Erfolg gefunden. Es ist nicht immer möglich, die Leistung mehrerer Architekturen zu vergleichen, es sei denn, sie wurden auf denselben Datensätzen ausgewertet. Typischerweise sind DNNs vorspeisende Netze, in denen Daten von der Eingangsschicht zur Ausgangsschicht ohne Rückschleifen fließen. Zunächst erstellt die DNN eine Karte von virtuellen Neuronen und ordnet zufällige Zahlenwerte oder Gewichte an Verbindungen zwischen ihnen zu. Die Gewichte und Eingänge werden multipliziert und geben einen Ausgang zwischen 0 und 1 zurück. Wenn das Netzwerk ein bestimmtes Muster nicht genau erkannt hat, würde ein Algorithmus die Gewichte anpassen. Auf diese Weise kann der Algorithmus bestimmte Parameter einflussreicher machen, bis er die korrekte mathematische Manipulation ermittelt, um die Daten vollständig zu verarbeiten. Für Anwendungen wie Sprachmodellierung werden wiederkehrende neuronale Netzwerke (RNs) verwendet, in denen Daten in beliebiger Richtung fließen können. Für diesen Einsatz ist ein Langzeitspeicher besonders effektiv. Konvolutionale tiefe neuronale Netzwerke (CNNs) werden in der Computervision verwendet. CNNs wurden auch auf akustische Modellierung zur automatischen Spracherkennung (ASR) angewendet. Herausforderungen Wie bei ANNs können viele Probleme mit naiv geschulten DNN entstehen. Zwei häufige Probleme werden überholt und Rechenzeit. DNNs sind wegen der zusätzlichen Abstraktionsschichten anfällig, die es ihnen ermöglichen, seltene Abhängigkeiten in den Trainingsdaten zu modellieren. Regulierverfahren wie Ivakhnenko's Unit pruning or weight decay (l 2 {\displaystyle \ell {_2} -regularization) oder Sparsity (l 1 {\displaystyle \ell {_1} -regularization) können während des Trainings zur Bekämpfung von Overfitting angewendet werden. Alternativ gibt die Dropout-Regalisation zufällig Einheiten aus den versteckten Schichten während des Trainings. Dies hilft, seltene Abhängigkeiten auszuschließen. Schließlich können Daten über Methoden wie Ernten und Drehen so erweitert werden, dass kleinere Trainingseinheiten vergrößert werden können, um die Chancen der Überarbeitung zu reduzieren. DNNs müssen viele Trainingsparameter berücksichtigen, wie die Größe (Anzahl der Schichten und Anzahl der Einheiten pro Schicht), die Lernrate und die Anfangsgewichte. Durch den Parameterraum für optimale Parameter kann aufgrund der zeitlichen und rechnerischen Ressourcen nicht durchführbar sein. Verschiedene Tricks, wie z.B. das Batch (mit dem Gradienten auf mehreren Trainingsbeispielen gleichzeitig statt einzelne Beispiele) beschleunigen die Berechnung. Große Verarbeitungskapazitäten von Vielkern-Architekturen (wie GPUs oder Intel Xeon Phi) haben aufgrund der Eignung solcher Verarbeitungsarchitekturen für die Matrix- und Vektorrechnungen signifikante Geschwindigkeiten im Training erzeugt. Alternativ können Ingenieure andere Arten von neuronalen Netzwerken mit einfacheren und konvergenten Trainingsalgorithmen suchen. CMAC (cerebellar model artikulation Controller) ist eine solche Art von neuronalem Netzwerk. Es erfordert keine Lernraten oder randomisierte Anfangsgewichte für CMAC. Der Trainingsprozess kann in einem Schritt mit einem neuen Datensatz konvergieren und die rechnerische Komplexität des Trainingsalgorithmus ist bezüglich der Anzahl der beteiligten Neuronen linear. Hardware Seit den 2010er Jahren haben Fortschritte in beiden maschinellen Lernalgorithmen und Computerhardware zu effizienteren Methoden für die Ausbildung von tiefen neuronalen Netzwerken geführt, die viele Schichten von nichtlinearen versteckten Einheiten und eine sehr große Ausgangsschicht enthalten. Bis 2019 hatten grafische Verarbeitungseinheiten (GPUs), oft mit KI-spezifischen Erweiterungen, CPUs als dominante Methode der Ausbildung großräumiger kommerzieller Cloud-KI vertrieben. OpenAI schätzte die Hardware-Rechnung, die in den größten Deep Learning-Projekten von AlexNet (2012) bis AlphaZero (2017) verwendet wird, und fand eine 300.000-fache Erhöhung der benötigten Rechensumme mit einer Laufzeit von 3,4 Monaten. Anwendungen Automatische Spracherkennung Großformatige automatische Spracherkennung ist der erste und überzeugendste erfolgreiche Fall des Deep Learning. LSTM RNs können "Sehr Deep Learning"-Aufgaben lernen, die mehrere Sekunden dauern, die Sprachereignisse enthalten, die durch Tausende diskreter Zeitschritte getrennt sind, wobei ein Zeitschritt etwa 10 ms entspricht. LSTM mit vergessenen Toren ist wettbewerbsfähig mit traditionellen Spracherkennern auf bestimmten Aufgaben. Der erste Erfolg der Spracherkennung basierte auf kleinen Anerkennungsaufgaben auf TIMIT. Der Datensatz enthält 630 Lautsprecher aus acht großen Dialekten des amerikanischen Englischen, wobei jeder Lautsprecher 10 Sätze liest. Seine kleine Größe lässt viele Konfigurationen versucht werden. Wichtiger ist, dass die TIMIT-Task die Telefon-Sequenz-Erkennung betrifft, die im Gegensatz zu Wort-Sequenz-Erkennung schwache Telefon Bigram-Sprachmodelle erlaubt. Dadurch lässt sich die Stärke der akustischen Modellierungsaspekte der Spracherkennung leichter analysieren. Die unten aufgeführten Fehlerraten, einschließlich dieser frühen Ergebnisse und gemessen als Prozent-Telefonfehlerraten (PER,) wurden seit 1991 zusammengefasst. Das Debüt von DNNs für Sprechererkennung in den späten 1990er Jahren und Spracherkennung um 2009-2011 und von LSTM um 2003-2007, beschleunigte Fortschritte in acht großen Bereichen: Scale-up/out und beschleunigte DNN-Training und Dekodierung Sequenzdiskriminative Ausbildung Feature-Prozession durch tiefe Modelle mit solidem Verständnis der zugrunde liegenden Mechanismen Anpassung von DNNs und verwandte tiefe Modelle Multi-Task und Transfer-Learning von CNNs Andere Arten von Tiefmodellen, einschließlich Tensor-basierte Modelle und integrierte tiefe generative/diskriminative Modelle. Alle wichtigen kommerziellen Spracherkennungssysteme (z.B. Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu und iFlyTek Sprachsuche und eine Reihe von Nuance Sprachprodukten, etc.) basieren auf Deep Learning. Bilderkennung Ein gemeinsamer Bewertungssatz für die Bildklassifikation ist der MNIST-Datenbankdatensatz. MNIST besteht aus handschriftlichen Ziffern und umfasst 60.000 Trainingsbeispiele und 10.000 Testbeispiele. Wie bei TIMIT, seine kleine Größe ermöglicht Benutzer testen mehrere Konfigurationen. Eine umfassende Liste der Ergebnisse auf diesem Set ist verfügbar. Die tiefe lernbasierte Bilderkennung ist übermenschlich geworden, was genauere Ergebnisse als menschliche Anwärter hervorbringt. Dies geschah im Jahr 2011 in Anerkennung von Verkehrszeichen, und im Jahr 2014, mit Anerkennung der menschlichen Gesichter. Überwindung der menschlichen Ebene Gesichtserkennung Deep Learning-trained Fahrzeuge interpretieren jetzt 360° Kameraansichten. Ein weiteres Beispiel ist die Facial Dysmorphology Novel Analysis (FDNA) zur Analyse von Fällen menschlicher Fehlbildungen, die mit einer großen Datenbank genetischer Syndrome verbunden sind. Visuelle Kunstbearbeitung Nahe im Zusammenhang mit dem Fortschritt, der in der Bilderkennung gemacht wurde, ist die zunehmende Anwendung von Deep Learning Techniken auf verschiedene visuelle Kunstaufgaben. DNNs haben sich zum Beispiel in der Lage, a) die Stilperiode eines bestimmten Gemäldes zu identifizieren, b) Neural Style Transfer – den Stil eines bestimmten Kunstwerkes zu erfassen und es visuell ansprechend auf ein beliebiges Foto oder Video anzuwenden, und c) auffällige Bilder basierend auf zufälligen visuellen Eingabefeldern zu erzeugen. Natürliche Sprachverarbeitung Neural-Netzwerke wurden seit Anfang der 2000er Jahre zur Umsetzung von Sprachmodellen verwendet. LSTM half, die maschinelle Übersetzung und Sprachmodellierung zu verbessern. Andere Schlüsseltechniken in diesem Bereich sind negative Abtastung und Worteinbettung. Worteinbettung, wie Wort2vec, kann als Repräsentationsschicht in einer tiefen Lernarchitektur gedacht werden, die ein Atomwort in eine Positionsdarstellung des Wortes relativ zu anderen Worten im Datensatz transformiert; die Position ist als Punkt in einem Vektorraum dargestellt. Durch die Verwendung von Worteinbettung als RNN-Eingangsschicht kann das Netzwerk Sätze und Phrasen mit einer effektiven Kompositionsvektor Grammatik parsieren. Eine kompositorische Vektorgrammatik kann als probabilistische Kontext freie Grammatik (PCFG) gedacht werden, die von einem RNN implementiert wird. Rekursive Auto-Encoder, die ein Top-Wort-Einbettungen gebaut, können Satz-ähnlichkeit bewerten und Paraphrasierung erkennen. Tiefen neuronalen Architekturen bieten die besten Ergebnisse für Wahlparsing, Stimmungsanalyse, Informationsabruf, gesprochenes Sprachverständnis, maschinelle Übersetzung, Kontext-Entitäts-Linking, Schriftstilerkennung, Textklassifikation und andere. Neuere Entwicklungen verallgemeinern Worteinbettung zu Satzeinbettung. Google Translate (GT) verwendet ein großes End-to-End-Langzeit-Speicher (LSTM)-Netzwerk. Google Neural Machine Translation (GNMT) verwendet eine exemplarische maschinelle Übersetzungsmethode, in der das System "aus Millionen von Beispielen herauslöst". Es übersetzt "whole Sätze zu einer Zeit, anstatt Stücke. Google Translate unterstützt über hundert Sprachen. Das Netzwerk kodiert die "Semantik des Satzes, anstatt einfach Phrasen-Übersetzungen zu merken". GT verwendet Englisch als Zwischenprodukt zwischen den meisten Sprachpaaren. Drogenentdeckung und Toxikologie Ein großer Prozentsatz an Kandidatendrogen gewinnt keine Zulassung. Diese Ausfälle werden durch unzureichende Wirksamkeit (on-target-Effekt), unerwünschte Interaktionen (off-target-Effekte) oder unvorhergesehene toxische Effekte verursacht. Forschung hat den Einsatz von tiefem Lernen untersucht, um die biomolekularen Ziele, Off-Targets und toxischen Auswirkungen von Umweltchemikalien in Nährstoffen, Haushaltsprodukten und Medikamenten vorherzusagen. AtomNet ist ein tiefes Lernsystem für strukturbasiertes rationales Drogendesign. AtomNet wurde verwendet, um neue Kandidaten-Biomoleküle für Krankheitsziele wie das Ebola-Virus und Multiple Sklerose vorherzusagen. Im Jahr 2017 wurden erstmals graphische neuronale Netzwerke verwendet, um verschiedene Eigenschaften von Molekülen in einem großen Toxikologie-Datensatz vorherzusagen. Im Jahr 2019 wurden generative neuronale Netzwerke verwendet, um Moleküle herzustellen, die experimentell bis in Mäuse validiert wurden. Kundenbetreuung Mit dem Tiefenverstärkungslernen wurde der Wert von möglichen Direktmarketing-Aktionen, die in Bezug auf RFM-Variablen definiert sind, angenähert. Die geschätzte Wertfunktion hat sich als Kundenlebenswert als natürliche Interpretation erwiesen. Empfehlungssysteme Empfehlungssysteme haben Deep Learning genutzt, um sinnvolle Features für ein latentes Faktormodell für inhaltliche Musik- und Journalempfehlungen zu extrahieren. Multi-View Deep Learning wurde für das Lernen von Benutzereinstellungen aus mehreren Domänen angewendet. Das Modell verwendet einen hybriden kollaborativen und inhaltlichen Ansatz und verbessert Empfehlungen in mehreren Aufgaben. Bioinformatik Ein Autoencoder ANN wurde in der Bioinformatik verwendet, um Gen-Onlogie-Annotationen und Gen-Funktionsbeziehungen vorherzusagen. In der medizinischen Informatik wurde Deep Learning verwendet, um die Schlafqualität basierend auf Daten aus Wearables und Vorhersagen von gesundheitlichen Komplikationen aus elektronischen Gesundheitsdaten vorherzusagen. Medical Image AnalysisDeep Learning wurde gezeigt, um wettbewerbsfähige Ergebnisse in der medizinischen Anwendung wie Krebszellenklassifikation, Läsionserkennung, Organsegmentierung und Bildverbesserung zu produzieren. Mobile Werbung Die Suche nach dem passenden mobilen Publikum für mobile Werbung ist immer schwierig, da viele Datenpunkte berücksichtigt und analysiert werden müssen, bevor ein Zielsegment erstellt und in einer Anzeige verwendet werden kann, die von jedem Ad-Server bedient wird. Das Deep Learning wurde verwendet, um große, vieldimensionierte Werbedatensätze zu interpretieren. Viele Datenpunkte werden während des Anfrage- / Reservierungs/Klick Internet-Werbezyklus erhoben. Diese Informationen können die Grundlage des maschinellen Lernens bilden, um die Anzeigenauswahl zu verbessern. Deep Learning wurde erfolgreich auf inverse Probleme wie Denoising, Super-Resolution, Inpainting und Filmfärbung angewendet. Diese Anwendungen umfassen Lernmethoden wie "Shrinkage Fields for Effective Image Restoration", die auf einem Bilddatensatz trainiert, und Deep Image Prior, die auf dem Bild trainiert, das Restaurierung benötigt. FinanzbetrugserkennungDeep Learning wird erfolgreich auf die Erkennung von Finanzbetrug, die Erkennung von Steuerhinterziehungen und die Bekämpfung von Geldwäsche angewendet. Militär Das Verteidigungsministerium der Vereinigten Staaten nutzte tiefes Lernen, um Roboter in neuen Aufgaben durch Beobachtung zu trainieren. Beziehung zur menschlichen kognitiven und Gehirnentwicklung Deep Learning ist eng mit einer Klasse von Theorien der Gehirnentwicklung (spezifisch, neokortikale Entwicklung) von kognitiven Neurowissenschaftlern in den frühen 1990er Jahren vorgeschlagen. Diese entwicklungstheoretischen Theorien wurden in rechnerischen Modellen augenblicklich dargestellt, so dass sie Vorgänger von tiefen Lernsystemen. Diese Entwicklungsmodelle teilen die Eigenschaft, dass verschiedene vorgeschlagene Lerndynamik im Gehirn (z.B. eine Welle des Nervenwachstumsfaktors) die Selbstorganisation etwas analog zu den neuronalen Netzwerken unterstützen, die in tiefen Lernmodellen genutzt werden. Wie der Neokortex verwenden neuronale Netzwerke eine Hierarchie von schichtförmigen Filtern, in denen jede Schicht Informationen aus einer vorherigen Schicht (oder der Betriebsumgebung) betrachtet und dann ihren Ausgang (und ggf. den ursprünglichen Eingang) auf andere Schichten übergibt. Dieses Verfahren liefert einen selbstorganisierenden Stapel von Wandlern, deren Arbeitsumgebung gut abgestimmt ist. "Das Gehirn des Säuglings scheint sich unter dem Einfluss von Wellen so genannter Trophäen zu organisieren ... verschiedene Bereiche des Gehirns werden sequentiell miteinander verbunden, mit einer Schicht aus Gewebe vor dem anderen und so weiter, bis das ganze Gehirn reif ist. " Zur Untersuchung der Plausibilität von Tiefenlernmodellen aus neurobiologischer Perspektive wurden verschiedene Ansätze verwendet. Zum einen wurden mehrere Varianten des Backpropagationsalgorithmus vorgeschlagen, um seinen Verarbeitungsrealismus zu erhöhen. Andere Forscher haben argumentiert, dass unübertroffene Formen des tiefen Lernens, wie die auf hierarchischen Generativen Modellen und tiefen Glaubensnetzwerken basierend, näher an der biologischen Realität sein können. In dieser Hinsicht sind generative neuronale Netzwerkmodelle mit neurobiologischen Nachweisen über die Probenahme-basierte Verarbeitung in der cerebralen Cortex verbunden. Obwohl noch kein systematischer Vergleich zwischen der menschlichen Gehirnorganisation und der neuronalen Kodierung in tiefen Netzwerken festgestellt wurde, wurden mehrere Analogien gemeldet. Beispielsweise könnten die Berechnungen, die von tiefen Lerneinheiten durchgeführt werden, denen der tatsächlichen Neuronen und neuronalen Populationen ähnlich sein. In ähnlicher Weise sind die von Deep Learning Modellen entwickelten Darstellungen ähnlich denen, die sowohl auf der Einheit als auch auf der Bevölkerungsebene im Primat-Visualisierungssystem gemessen werden. Geschäftstätigkeit Das AI-Lab von Facebook erfüllt Aufgaben wie automatisch markiert hochgeladene Bilder mit den Namen der Menschen in ihnen. Googles DeepMind Technologies entwickelte ein System, das lernen kann, wie man Atari Videospiele mit nur Pixeln als Dateneingabe spielt. 2015 zeigten sie ihr AlphaGo-System, das das Spiel von Go gut genug gelernt, um einen professionellen Go-Spieler zu schlagen. Google Translate verwendet ein neuronales Netzwerk, um zwischen mehr als 100 Sprachen zu übersetzen. Im Jahr 2015 zeigte Blippar eine mobile Augmented-Reality-Anwendung, die Deep Learning verwendet, um Objekte in Echtzeit zu erkennen. Im Jahr 2017 wurde Covariant.ai gestartet, der sich auf die Integration von Deep Learning in Fabriken konzentriert. Ab 2008 entwickelten Forscher der University of Texas at Austin (UT) einen Maschinenlern-Framework namens Training an Agent Manual per Evaluative Verstärkung oder TAMER, das neue Methoden für Roboter oder Computerprogramme vorgeschlagen, um zu lernen, wie Aufgaben durch Interaktion mit einem menschlichen Instruktor ausgeführt werden können. Zuerst als TAMER entwickelt, wurde 2018 ein neuer Algorithmus namens Deep TAMER in Zusammenarbeit zwischen US Army Research Laboratory (ARL) und UT-Forschern vorgestellt. Deep TAMER nutzte tiefes Lernen, um einem Roboter die Möglichkeit zu bieten, neue Aufgaben durch Beobachtung zu lernen. Mit Deep TAMER lernte ein Roboter eine Aufgabe mit einem menschlichen Trainer, beobachtete Videostreams oder beobachtete einen Menschen eine Aufgabe persönlich. Der Roboter praktizierte später die Aufgabe mit Hilfe eines Coachings vom Trainer, der Feedback wie „gute Arbeit“ und „schlechte Arbeit“ lieferte. Kritik und Kommentar Deep Learning hat sowohl Kritik als auch Kommentar angezogen, in einigen Fällen von außerhalb der Informatik. Theorie Eine Hauptkritik betrifft die mangelnde Theorie, die einige Methoden umgibt. Lernen in den häufigsten tiefen Architekturen wird mit gut verstandenen Gradientenabstieg durchgeführt. Die Theorie, die andere Algorithmen umgibt, wie kontrastierende Divergenz ist jedoch weniger klar (z. Konvergiert es? Wenn ja, wie schnell? Was ist es?) Deep Learning Methoden werden oft als schwarze Box betrachtet, mit den meisten Bestätigungen empirisch statt theoretisch durchgeführt. Andere weisen darauf hin, dass das tiefe Lernen als Schritt zur Verwirklichung einer starken KI betrachtet werden sollte, nicht als eine allumfassende Lösung. Trotz der Kraft der tiefen Lernmethoden fehlt ihnen noch viel an der Funktionalität, die für die Verwirklichung dieses Ziels benötigt wird. Forschungspsychologe Gary Marcus bemerkte: "Realistisch ist Deep Learning nur Teil der größeren Herausforderung, intelligente Maschinen zu bauen. Solche Techniken fehlen Möglichkeiten, ursächliche Beziehungen (...) zu repräsentieren, haben keine offensichtlichen Möglichkeiten, logische Inferenzen durchzuführen, und sie sind auch noch ein langer Weg, abstraktes Wissen zu integrieren, wie Informationen über das, wofür sie sind, und wie sie typischerweise verwendet werden. Die mächtigsten A.I-Systeme, wie Watson (...) verwenden Techniken wie tiefes Lernen als nur ein Element in einem sehr komplizierten Ensemble von Techniken, von der statistischen Technik der Bayesischen Inferenz bis hin zu deduktiven Argumenten." In weiterer Bezugnahme auf die Idee, dass künstlerische Sensibilität in relativ niedrigen Ebenen der kognitiven Hierarchie vererbt werden könnte, zeigt eine veröffentlichte Serie von grafischen Darstellungen der internen Zustände von tiefen (20-30 Schichten) neuronalen Netzwerken, die versuchen, innerhalb von im Wesentlichen zufälligen Daten zu erkennen, die Bilder, auf denen sie trainiert wurden, zeigen einen visuellen Appell: die ursprüngliche Forschungsankündigung erhielt gut über 1.000 Kommentare, und war das Thema, was für eine Zeit der am häufigsten zugängliche.Fehler Einige Deep Learning-Architekturen zeigen problematische Verhaltensweisen, wie zuversichtlich klassifizieren unkennbare Bilder als Zugehörigkeit zu einer vertrauten Kategorie von gewöhnlichen Bildern und fehlklassifizierende minuskule-Perturbationen von korrekt klassifizierten Bildern. Goertzel unterschätzt, dass diese Verhaltensweisen auf Einschränkungen in ihren internen Darstellungen zurückzuführen sind und dass diese Einschränkungen die Integration in heterogene mehrkomponentige künstliche allgemeine Intelligenz (AGI)-Architekturen hemmen würden. Diese Probleme können möglicherweise von tiefen Lernarchitekturen angesprochen werden, die intern Zustände homolog zu bildgrammaren Zersetzungen von beobachteten Wesen und Ereignissen bilden. Das Erlernen einer Grammatik (visuell oder sprachlich) aus den Trainingsdaten wäre gleichwertig, um das System auf die Commonsense Argumentation zu beschränken, die auf Konzepten in Bezug auf grammatische Produktionsregeln arbeitet und ein grundlegendes Ziel sowohl der menschlichen Spracherwerb als auch der künstlichen Intelligenz (KI) ist. Cyber Bedrohung Während sich das tiefe Lernen vom Labor in die Welt bewegt, zeigt Forschung und Erfahrung, dass künstliche neuronale Netzwerke für Hacks und Täuschung anfällig sind. Durch die Identifizierung von Mustern, die diese Systeme verwenden, können Angreifer Eingaben an ANNs so ändern, dass die ANN eine Übereinstimmung findet, dass menschliche Beobachter nicht erkennen würden. Beispielsweise kann ein Angreifer subtile Veränderungen an einem Bild vornehmen, so dass die ANN eine Übereinstimmung findet, auch wenn das Bild einem Menschen nichts wie das Suchziel ansieht. Eine solche Manipulation wird als "adversarialer Angriff" bezeichnet. Im Jahr 2016 nutzten Forscher eine ANN, um Bilder in der Test- und Fehler-Mode zu behandeln, die Schwerpunkte eines anderen zu identifizieren und so Bilder zu generieren, die es täuschen. Die modifizierten Bilder sahen nicht anders aus als die menschlichen Augen. Eine andere Gruppe zeigte, dass Ausdrucke von promovierten Bildern dann erfolgreich ein Bildklassifikationssystem fotografiert. Eine Verteidigung ist umgekehrte Bildsuche, bei der ein mögliches gefälschtes Bild einer Seite wie TinEye vorgelegt wird, die dann andere Instanzen davon finden kann. Eine Weiterbildung besteht darin, nur Teile des Bildes zu suchen, um Bilder zu identifizieren, aus denen dieses Stück genommen werden kann. Eine andere Gruppe zeigte, dass bestimmte psychedelische Spektakel ein Gesichtserkennungssystem in das Denken gewöhnliche Menschen waren Prominenten, die eine Person erlaubt, eine andere zu verkörpern. Im Jahr 2017 haben Forscher Aufkleber hinzugefügt, um Zeichen zu stoppen und eine ANN zu fehlklassifizieren. ANNs kann jedoch weiter trainiert werden, um Versuche an Täuschung zu erkennen, potenziell führende Angreifer und Verteidiger in ein Waffenrennen ähnlich der Art, die bereits die Malware-Verteidigung Industrie definiert. ANNs wurden ausgebildet, um ANN-basierte Anti-Malware-Software zu besiegen, indem sie wiederholt eine Verteidigung mit Malware angriff, die durch einen genetischen Algorithmus ständig verändert wurde, bis es die Anti-Malware unter Beibehaltung seiner Fähigkeit, das Ziel zu beschädigen. Eine andere Gruppe zeigte, dass bestimmte Sounds das Google Now-Sprachbefehlssystem öffnen eine bestimmte Webadresse, die Malware herunterladen würde. Bei der „Datenvergiftung“ werden die falschen Daten ständig in das Trainingsset eines maschinellen Lernsystems geschmuggelt, um zu verhindern, dass es Meisterschaft schafft. Einhaltung menschlicher Mikroarbeit Die meisten Deep Learning Systeme verlassen sich auf Trainings- und Verifikationsdaten, die von Menschen generiert und/oder annotiert werden. Es wurde in der Medienphilosophie argumentiert, dass nicht nur Low-paid-Clickwork (z.B. auf Amazon Mechanical Turk) regelmäßig für diesen Zweck eingesetzt wird, sondern auch implizite Formen menschlicher Mikroarbeit, die oft nicht als solche erkannt werden. Der Philosoph Rainer Mühlhoff unterscheidet fünf Arten von "machinischen Erfassung" menschlicher Mikroarbeit, um Trainingsdaten zu generieren: (1) Gamification (die Einbettung von Annotations- oder Rechenaufgaben im Fluss eines Spiels,) (2) "Trapping and tracking" (z.B. CAPTCHAs für Bilderkennung oder Klick-Tracking auf Google-Suchergebnissen,) (3) Ausnutzung sozialer Motivationen (z.B. Markierungsflächen auf Google-Suche) Mühlhoff argumentiert, dass bei den meisten kommerziellen Endbenutzer-Anwendungen von Deep Learning wie Facebooks Gesichtserkennungssystem die Notwendigkeit von Trainingsdaten nicht aufhört, sobald eine ANN ausgebildet ist. Vielmehr besteht eine anhaltende Nachfrage nach human generierten Verifikationsdaten, um die ANN ständig zu kalibrieren und zu aktualisieren. Zu diesem Zweck stellte Facebook die Funktion vor, dass, sobald ein Benutzer automatisch in einem Bild erkannt wird, sie eine Benachrichtigung erhalten. Sie können wählen, ob sie nicht gerne öffentlich auf dem Bild markiert werden, oder Facebook sagen, dass es nicht sie im Bild ist. Diese Benutzeroberfläche ist ein Mechanismus, um "ein konstanter Strom von Verifikationsdaten" zu erzeugen, um das Netzwerk in Echtzeit weiterzubilden. Wie Mühlhoff argumentiert, ist die Einbeziehung menschlicher Nutzer zur Erstellung von Trainings- und Verifikationsdaten so typisch für die meisten kommerziellen Endbenutzer-Anwendungen von Deep Learning, dass solche Systeme als "menschlich-aided künstliche Intelligenz" bezeichnet werden können. Siehe auch Anwendungen der künstlichen Intelligenz Vergleich der Tiefen Lernsoftware Komprimierte Erfassung Differenzierbare Programmierung Echo-Zustandsnetz Liste der künstlichen Intelligenz Projekte Flüssigkeitszustandsmaschine Liste der Datensätze für die maschinelle LernforschungReservoir Computing Sparse Codierung Referenzen =Weiteres Lesen ==