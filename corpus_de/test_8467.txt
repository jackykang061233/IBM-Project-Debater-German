Natürliche Sprachverarbeitung (NLP) ist ein Unterfeld von Linguistik, Informatik und künstlicher Intelligenz, die sich mit den Interaktionen zwischen Computern und menschlicher Sprache befasst, insbesondere wie man Computer zur Verarbeitung und Analyse großer Mengen von natürlichen Sprachdaten programmiert. Ziel ist ein Computer, der in der Lage ist, die Inhalte von Dokumenten zu verstehen, einschließlich der kontextuellen Nuancen der Sprache in ihnen. Die Technologie kann dann die in den Dokumenten enthaltenen Informationen und Erkenntnisse genau entnehmen und die Dokumente selbst kategorisieren und organisieren. Herausforderungen in der natürlichen Sprachverarbeitung beinhalten häufig Spracherkennung, natürliches Sprachverständnis und natürliche Sprachgenerierung. Geschichte Die natürliche Sprachverarbeitung hat ihre Wurzeln in den 1950er Jahren. Bereits 1950 veröffentlichte Alan Turing einen Artikel mit dem Titel "Computing Machinery and Intelligence", der den sogenannten Turing-Test als Kriterium der Intelligenz vorschlug, eine Aufgabe, die die automatisierte Interpretation und Erzeugung von natürlicher Sprache beinhaltet, aber zur Zeit nicht als Problem von künstlicher Intelligenz artikuliert. Symbolische NLP (1950er – Anfang der 1990er Jahre) Die Prämisse des symbolischen NLP ist durch John Searles chinesisches Raumexperiment gut verkörpert: Bei einer Sammlung von Regeln (z.B. ein chinesisches Phrasenbuch, mit Fragen und passenden Antworten) emuliert der Computer das natürliche Sprachverständnis (oder andere NLP-Aufgaben), indem er diese Regeln auf die Daten, mit denen er konfrontiert wird, anwendet. 1950er Jahre:Das Georgetown-Experiment 1954 beteiligte sich an vollautomatischer Übersetzung von mehr als sechzig russischen Sätzen ins Englische. Die Autoren behaupteten, innerhalb von drei oder fünf Jahren wäre die maschinelle Übersetzung ein gelöstes Problem. Allerdings war der reale Fortschritt viel langsamer, und nach dem ALPAC-Bericht 1966, der festgestellt hat, dass die zehnjährige Forschung die Erwartungen nicht erfüllt hatte, wurde die Finanzierung der maschinellen Übersetzung drastisch reduziert. Erst in den späten 1980er Jahren, als die ersten statistischen maschinellen Übersetzungssysteme entwickelt wurden, wurden wenig weitere Untersuchungen in der maschinellen Übersetzung durchgeführt. 1960er Jahre: Einige besonders erfolgreiche natürliche Sprachverarbeitungssysteme, die in den 1960er Jahren entwickelt wurden, waren SHRDLU, ein natürliches Sprachsystem, das in beschränkten "Blocks-Welten" mit eingeschränkten Vokabularen arbeitet, und ELIZA, eine Simulation eines Rogerian Psychotherapeuten, geschrieben von Joseph Weizenbaum zwischen 1964 und 1966. Mit fast keiner Information über menschliches Denken oder Emotion lieferte ELIZA manchmal eine anfangs menschliche Interaktion. Wenn der Patient die sehr kleine Wissensbasis überstieg, könnte ELIZA eine generische Antwort geben, zum Beispiel auf "Mein Kopf tut weh" mit "Warum sagen Sie Ihren Kopf tut weh?".1970s: Während der 1970er Jahre begannen viele Programmierer, "Konzeptionelle Onlogien" zu schreiben, die reale Informationen in computerunterstützbare Daten strukturiert. Beispiele sind MARGIE (Schank, 1975,) SAM (Cullingford, 1978,) PAM (Wilensky, 1978,) TaleSpin (Meehan, 1976,) QUALM (Lehnert, 1977,) Politik (Carbonell, 1979,) und Plot Units (Lehnert 1981). Während dieser Zeit wurden die ersten vielen Chatterbots geschrieben (z.B. PARRY). 1980er Jahre: Die 1980er und Anfang der 1990er Jahre markieren den Tag symbolischer Methoden in NLP. Schwerpunkte der Zeit waren die Forschung über regelbasiertes Parsing (z.B. die Entwicklung von HPSG als rechnerische Operationalisierung von generativer Grammatik), Morphologie (z.B. zweistufige Morphologie), Semantik (z.B. Lesk-Algorithmus), Referenz (z.B. innerhalb der Centering Theory) und andere Bereiche des natürlichen Sprachverständnisses (z.B. im Rhetorischen Aufbau). Andere Forschungslinien wurden fortgesetzt, z.B. die Entwicklung von Chatterbots mit Racter und Jabberwacky. Eine wichtige Entwicklung (die schließlich zu der statistischen Wende in den 90er Jahren führte) war die steigende Bedeutung der quantitativen Bewertung in diesem Zeitraum. Statistische NLP (1990–2010) Bis in die 1980er Jahre basierten die meisten natürlichen Sprachverarbeitungssysteme auf komplexen Sätzen handschriftlicher Regeln. Ab Ende der 1980er Jahre gab es jedoch eine Revolution in der natürlichen Sprachverarbeitung mit der Einführung von maschinellen Lernalgorithmen für die Sprachverarbeitung. Dies war sowohl auf die stetige Zunahme der Rechenleistung (siehe Moore's Gesetz) als auch auf die allmähliche Verringerung der Dominanz der Chomskyan Theorien der Linguistik (z.B. Transformations-Grammatik), deren theoretische Grundlagen die Art von Korpus-Sprachen entmutigten, die dem maschinelllernenden Ansatz der Sprachverarbeitung zugrunde lag. Diese Systeme waren in der Lage, die bestehenden mehrsprachigen Textorgane zu nutzen, die vom Parlament Kanadas und der Europäischen Union durch Gesetze erzeugt worden waren, die die Übersetzung aller Regierungsverfahren in alle Amtssprachen der entsprechenden Regierungssysteme fordern. Die meisten anderen Systeme hängen jedoch von der speziell für die Aufgaben dieser Systeme entwickelten Unternehmensgruppe ab, die (und oft auch weiterhin) eine große Einschränkung des Erfolgs dieser Systeme war. Infolgedessen hat eine große Anzahl von Untersuchungen Methoden des effektiveren Lernens aus begrenzten Datenmengen gefunden. 2000er:Mit dem Wachstum des Internets sind seit Mitte der 1990er Jahre zunehmende Mengen an Roh- (unnotierten) Sprachdaten verfügbar. Die Forschung konzentriert sich somit zunehmend auf ununterbrochene und semi-supervisierte Lernalgorithmen. Solche Algorithmen können aus Daten, die nicht mit den gewünschten Antworten handbezeichnet wurden, oder unter Verwendung einer Kombination von annotierten und nicht annotierten Daten lernen. Im Allgemeinen ist diese Aufgabe viel schwieriger als beaufsichtigtes Lernen und erzeugt typischerweise weniger genaue Ergebnisse für eine bestimmte Menge von Eingabedaten. Es gibt jedoch eine enorme Menge an nicht angemeldeten Daten (unter anderem auch den gesamten Inhalt des World Wide Web), die oft für die minderwertigen Ergebnisse ausmachen können, wenn der verwendete Algorithmus eine geringe Zeitkomplexität aufweist, um praktisch zu sein. Neural NLP (heute)In den 2010er Jahren wurde die Darstellung von Lern- und tiefen neuralen netzartigen maschinellen Lernmethoden in der natürlichen Sprachverarbeitung verbreitet, da zum Teil ein Ergebnisfluss zeigt, dass solche Techniken hochmoderne Ergebnisse in vielen natürlichen Sprachaufgaben, zum Beispiel in der Sprachmodellierung, der Parsing, und vielen anderen erreichen können. Dies ist in der Medizin und im Gesundheitswesen immer wichtiger, wo NLP verwendet wird, um Notizen und Text in elektronischen Gesundheitsakten zu analysieren, die sonst für die Studie unzugänglich wären, wenn man die Pflege verbessern will. Methoden: Regeln, Statistiken, neuronale Netze In den frühen Tagen wurden viele Sprachverarbeitungssysteme durch symbolische Methoden, d.h. die Handcodierung einer Reihe von Regeln, gepaart mit einem Wörterbuch-Lookup, wie z.B. durch das Schreiben von Grammatiken oder die Erarbeitung heuristischer Regeln für das Stemmen entworfen. Neuere Systeme auf der Basis von maschinenlernenden Algorithmen haben viele Vorteile gegenüber handgemachten Regeln: Die während des maschinellen Lernens verwendeten Lernverfahren konzentrieren sich automatisch auf die häufigsten Fälle, während beim Schreiben von Regeln von Hand oft nicht offensichtlich ist, wo der Aufwand gerichtet werden soll. Automatische Lernverfahren können statistische Inferenzalgorithmen verwenden, um Modelle herzustellen, die robust sind, um nicht vertraute Eingaben (z.B. mit Wörtern oder Strukturen, die noch nicht gesehen wurden) und fehlerhafte Eingaben (z.B. mit verfehlten Wörtern oder Wörtern versehentlich weggelassen). Generell ist die Handhabung solcher Eingaben mit handschriftlichen Regeln oder allgemeiner, die Schaffung von Systemen von handschriftlichen Regeln, die weiche Entscheidungen treffen, extrem schwierig, fehleranfällig und zeitraubend. Systeme, die auf dem automatischen Lernen der Regeln basieren, können durch die Bereitstellung von mehr Eingabedaten genauer gemacht werden. Allerdings können Systeme, die auf handschriftlichen Regeln basieren, nur durch die Erhöhung der Komplexität der Regeln, die eine viel schwierigere Aufgabe ist, genauer gemacht werden. Insbesondere gibt es eine Begrenzung der Komplexität von Systemen, die auf handschriftlichen Regeln basieren, über die die Systeme immer unhandhabbarer werden. Die Erstellung von mehr Daten zur Eingabe in Maschinenlernsysteme erfordert jedoch einfach eine entsprechende Erhöhung der Anzahl der geleisteten Arbeitsstunden, in der Regel ohne nennenswerte Erhöhung der Komplexität des Annotationsprozesses. Trotz der Popularität des maschinellen Lernens in der NLP-Forschung werden symbolische Methoden immer noch (2020) häufig verwendet: wenn die Anzahl der Trainingsdaten nicht ausreicht, um maschinelle Lernmethoden erfolgreich anzuwenden, z.B. für die maschinelle Übersetzung niederer Ressourcensprachen wie das Apertium-System, für die Vorverarbeitung in NLP-Pipelines, z.B. für die Tokenisierung, oder für die Nachbearbeitung und die Transformation der Ausgabe von NLP-Pipeline, z.g, für Statistische Methoden Seit der so genannten "statistischen Revolution" in den späten 1980er und Mitte der 1990er Jahre hat sich eine viel natürliche Sprachverarbeitungsforschung stark auf das maschinelle Lernen angewiesen. Das maschinenlernende Paradigma fordert stattdessen die Verwendung statistischer Inferenzen dazu auf, solche Regeln automatisch durch die Analyse von Großkorporat (die Pluralform von Korpus, ist eine Reihe von Dokumenten, möglicherweise mit menschlichen oder Computer-Annotationen) von typischen real-world-Beispielen zu lernen. Viele verschiedene Klassen von maschinenlernenden Algorithmen wurden auf natursprachliche Verarbeitungsaufgaben angewendet. Diese Algorithmen nehmen als Eingabe eine große Anzahl von Funktionen auf, die aus den Eingabedaten generiert werden. In zunehmendem Maße hat sich die Forschung jedoch auf statistische Modelle konzentriert, die weiche, probabilistische Entscheidungen treffen, die auf der Befestigung von real bewerteten Gewichten an jedem Eingabemerkmal basieren (komplex bewertete Einbettungen und neuronale Netzwerke im Allgemeinen wurden auch für z.B. Sprache vorgeschlagen). Solche Modelle haben den Vorteil, dass sie die relative Sicherheit vieler verschiedener möglicher Antworten und nicht nur einer zum Ausdruck bringen können, wodurch zuverlässigere Ergebnisse erzielt werden, wenn ein solches Modell als Bestandteil eines größeren Systems aufgenommen wird. Einige der frühesten verwendeten maschinellen Lernalgorithmen, wie Entscheidungsbäume, produzierte Systeme von harten, wenn-dann Regeln ähnlich wie bestehende handschriftliche Regeln. Allerdings hat die part-of-speech tagging die Verwendung von versteckten Markov-Modellen zur natürlichen Sprachverarbeitung eingeführt, und zunehmend konzentriert sich die Forschung auf statistische Modelle, die weiche, probabilistische Entscheidungen treffen, basierend auf der Befestigung von real bewerteten Gewichten an den Merkmalen, die die Eingabedaten bilden. Die Cache-Sprachmodelle, auf die sich viele Spracherkennungssysteme jetzt verlassen, sind Beispiele für solche statistischen Modelle. Solche Modelle sind in der Regel robuster, wenn man nicht vertraute Eingaben erhält, insbesondere Eingaben, die Fehler (wie bei realen Daten sehr üblich) enthalten und zuverlässigere Ergebnisse bei der Integration in ein größeres System mit mehreren Subtasks erzeugen. Seit der neuronalen Wende wurden statistische Methoden in der NLP-Forschung weitgehend durch neuronale Netze ersetzt. Sie sind jedoch weiterhin relevant für Kontexte, in denen eine statistische Interpretation und Transparenz erforderlich ist. Neurale Netze Ein wesentlicher Nachteil statistischer Methoden ist, dass sie eine aufwendige Merkmalstechnik benötigen. Seit 2015 hat das Feld damit weitgehend statistische Methoden verlassen und in neuronale Netzwerke für maschinelles Lernen verschoben. Zu den beliebten Techniken gehören die Verwendung von Worteinbettungen, um semantische Eigenschaften von Wörtern zu erfassen, und eine Erhöhung des End-to-End-Erlernens einer übergeordneten Aufgabe (z.B. Fragebeantwortung) anstatt auf eine Pipeline von separaten Zwischenaufgaben (z.B. Teil-of-Sech-Tagging und Abhängigkeitsparsing). In einigen Bereichen hat diese Verschiebung erhebliche Veränderungen in der Gestaltung von NLP-Systemen zur Folge, so dass tiefe neurale netzbasierte Ansätze als ein neues Paradigma angesehen werden können, das von der statistischen natürlichen Sprachverarbeitung abweicht. Zum Beispiel betont der Begriff Neurale maschinelle Übersetzung (NMT) die Tatsache, dass tiefe lernbasierte Ansätze zur maschinellen Übersetzung direkt Sequenz-zu-Sequenz-Transformationen lernen, die Notwendigkeit von Zwischenschritten wie Wortausrichtung und Sprachmodellierung, die in der statistischen maschinellen Übersetzung (SMT) verwendet wurde. Letzte Arbeiten neigen dazu, nicht-technische Struktur einer bestimmten Aufgabe zu verwenden, um ein richtiges neuronales Netz aufzubauen. Gemeinsame NLP-Aufgaben Im Folgenden finden Sie eine Liste von einigen der am häufigsten erforschten Aufgaben in der natürlichen Sprachverarbeitung. Einige dieser Aufgaben haben direkte reale Anwendungen, während andere häufiger als Subtasks dienen, die zur Lösung größerer Aufgaben verwendet werden. Obwohl natürliche Sprachverarbeitungsaufgaben eng miteinander verbunden sind, können sie in Kategorien für Komfort unterteilt werden. Im folgenden wird eine grobe Teilung angegeben. Text- und Sprachverarbeitung Optische Zeichenerkennung (OCR) Bei einem Bild, das gedruckten Text darstellt, bestimmen Sie den entsprechenden Text. Spracherkennung Bei einem Klangclip einer Person oder eines Sprechens bestimmen Sie die textuelle Darstellung der Sprache. Dies ist das Gegenteil von Text zu Rede und ist eines der äußerst schwierigen Probleme, die kolloquial als AI-komplete bezeichnet werden (siehe oben). In natürlicher Sprache gibt es kaum Pausen zwischen aufeinanderfolgenden Wörtern, und so ist die Sprachsegmentierung ein notwendiger Teil der Spracherkennung (siehe unten). In den meisten gesprochenen Sprachen vermischen sich die Töne, die aufeinanderfolgende Buchstaben repräsentieren, in einem sogenannten Coarticulationsprozess, so dass die Umwandlung des analogen Signals zu diskreten Zeichen ein sehr schwieriger Prozess sein kann. Auch, da Wörter in der gleichen Sprache von Menschen mit unterschiedlichen Akzenten gesprochen werden, muss die Spracherkennungssoftware in der Lage sein, die Vielzahl der Eingaben so zu erkennen, wie sie in ihrem Textäquivalent miteinander identisch sind. Sprachsegmentierung Bei einem Soundclip einer Person oder Menschen, die sprechen, trennen Sie es in Worte. Ein Subtask der Spracherkennung und typischerweise gruppiert mit ihm. Text zu Buche In einem Text, transformieren Sie diese Einheiten und erzeugen Sie eine gesprochene Darstellung. Text-zu-Sprache kann verwendet werden, um die Sehbehinderung zu unterstützen. Wortsegmentierung (Tokenisierung) Trennen Sie ein Stück durchgängigen Text in separate Wörter. Für eine Sprache wie Englisch ist dies ziemlich trivial, da Wörter in der Regel durch Leerzeichen getrennt werden. Einige schriftliche Sprachen wie Chinesisch, Japanisch und Thai markieren jedoch keine Wortgrenzen so, und in diesen Sprachen ist Textsegmentierung eine bedeutende Aufgabe, die Kenntnisse über die Wortschatz und Morphologie der Wörter in der Sprache erfordert. Manchmal wird dieser Prozess auch in Fällen verwendet, wie z.B. in der Textsammlung (BOW). morphologische Analyse Lemmatisierung Die Aufgabe, nur Inflectional-Endings zu entfernen und die Basiswörterform eines Wortes zurückzugeben, das auch als Lemma bekannt ist. Lemmatisierung ist eine weitere Technik, um Wörter auf ihre normalisierte Form zu reduzieren. Aber in diesem Fall verwendet die Transformation tatsächlich ein Wörterbuch, um Wörter in ihre tatsächliche Form zu kartieren. Morphologische Segmentierung Getrennte Wörter in einzelne Morpheme und identifizieren die Klasse der Morpheme. Die Schwierigkeit dieser Aufgabe hängt stark von der Komplexität der Morphologie (d.h. der Struktur der Wörter) der betrachteten Sprache ab. Englisch hat ziemlich einfache Morphologie, insbesondere inflectionale Morphologie, und so ist es oft möglich, diese Aufgabe ganz zu ignorieren und einfach alle möglichen Formen eines Wortes (z.B. "offen, öffnen, öffnen, öffnen, öffnen") als separate Worte zu modellieren. In Sprachen wie Türkisch oder Meitei ist jedoch eine hoch agglutinierte indische Sprache nicht möglich, da jeder Wörterbucheintrag tausende von möglichen Wortformen hat. Teil-of-speech tagging Bei einem Satz bestimmen Sie den Teil der Rede (POS) für jedes Wort. Viele Wörter, besonders häufig, können als mehrere Teile der Rede dienen. Zum Beispiel kann Buch ein Noun ("das Buch auf dem Tisch") oder Verb ("ein Flug buchen)" sein; Set kann ein Noun, Verb oder Adjektiv sein; und out kann mindestens fünf verschiedene Teile der Rede sein. Stehlen Der Prozess der Verringerung von durchgebogenen (oder manchmal abgeleiteten) Wörtern zu einer Grundform (z.B. schließen wird die Wurzel für geschlossen, schließen, schließen, näher usw.). Stemming liefert ähnliche Ergebnisse als Lemmatisierung, aber tut dies aus Gründen der Regeln, nicht ein Wörterbuch. Syntaktische Analyse Grammatik Induktion Erzeugen Sie eine formale Grammatik, die die Syntax einer Sprache beschreibt. Sentence Breaking (auch bekannt als "Sentence Limit Disambiguation") Bei einem Stück Text finden Sie die Satzgrenzen. Sentence-Grenzen sind oft durch Perioden oder andere Pünktlichkeitszeichen gekennzeichnet, aber diese gleichen Zeichen können anderen Zwecken dienen (z.B. Markierungsabkürzungen). Parsing Bestimmen Sie den Parsebaum (Grammatikanalyse) eines bestimmten Satzes. Die Grammatik für natürliche Sprachen ist mehrdeutig und typische Sätze haben mehrere mögliche Analysen: vielleicht überraschend, für einen typischen Satz kann es Tausende von möglichen Parsen geben (die meisten werden völlig unsinnig für einen Menschen erscheinen). Es gibt zwei primäre Arten von Parsing: Abhängigkeitsparsing und Wahlparsing. Dependency Parsing konzentriert sich auf die Beziehungen zwischen Wörtern in einem Satz (Markierung von Dingen wie primäre Objekte und Prädikate), während Wahlparsing konzentriert sich auf den Aufbau des Parsebaums mit einer probabilistischen kontextfreien Grammatik (PCFG) (siehe auch stochastische Grammatik.) Lexische Semantik (von einzelnen Wörtern im Kontext)Lexische Semantik Was ist die rechnerische Bedeutung einzelner Wörter im Kontext? Vertriebssemantik Wie können wir semantische Darstellungen von Daten lernen? Namete Entity-Erkennung (NER)Gegeben einem Textstrom, bestimmen Sie, welche Elemente in der Textkarte zu richtigen Namen, wie Personen oder Orten, und welche Art von jedem solchen Namen ist (z.B. Person, Standort, Organisation). Obwohl die Kapitalisierung dazu beitragen kann, benannte Einheiten in Sprachen wie Englisch zu erkennen, können diese Informationen bei der Bestimmung der Art der benannten Einheit nicht helfen, und in jedem Fall ist oft ungenau oder unzureichend. Zum Beispiel wird auch der erste Brief eines Satzes kapitalisiert, und benannte Wesen erstrecken sich oft über mehrere Wörter, von denen nur einige kapitalisiert sind. Darüber hinaus haben viele andere Sprachen in nicht-westlichen Schriften (z.B. Chinesisch oder Arabisch) überhaupt keine Kapitalisierung, und sogar Sprachen mit Kapitalisierung dürfen sie nicht konsequent verwenden, um Namen zu unterscheiden. Zum Beispiel kapitalisiert Deutsch alle Nouns, unabhängig davon, ob sie Namen sind, und Französisch und Spanisch haben keine Namen, die als Adjektive dienen. Sentiment-Analyse (siehe auch Multimodale Sentiment-Analyse) Extrahieren subjektive Informationen in der Regel aus einer Reihe von Dokumenten, oft mit Online-Bewertungen, um Polarität über bestimmte Objekte zu bestimmen. Es ist besonders nützlich, Trends der öffentlichen Meinung in den sozialen Medien für Marketing zu identifizieren. Terminologie Extraktion Ziel der Terminologieextraktion ist es, relevante Begriffe automatisch aus einem bestimmten Korpus zu extrahieren. Wortsinn Verwirrung Viele Wörter haben mehr als eine Bedeutung; wir müssen die Bedeutung auswählen, die im Kontext am sinnvollsten ist. Für dieses Problem werden wir typischerweise eine Liste von Wörtern und zugehörigen Wortsinnen erhalten, z.B. aus einem Wörterbuch oder einer Online-Ressource wie WordNet. Beziehungsssemantik (Semantik einzelner Sätze)Beziehungsextraktion Bei einem Textstück identifizieren Sie die Beziehungen zwischen benannten Wesen (z.B. mit wem verheiratet). Semantische Parfümierung Bei einem Textstück (typischerweise ein Satz) wird eine formale Darstellung seiner Semantik, entweder als Graph (z.B. in AMR-Pasing) oder in Übereinstimmung mit einem logischen Formalismus (z.B. in DRT-Pasing) erstellt. Diese Herausforderung umfasst typischerweise Aspekte mehrerer elementarer NLP-Aufgaben aus der Semantik (z.B. semantische Rollenkennzeichnung, Wortsinndisambiguation) und kann auf eine vollwertige Diskursanalyse erweitert werden (z.B. Diskursanalyse, Coreference; siehe unten natürliche Sprachverständigung). Semantische Rollenkennzeichnung (siehe auch implizite semantische Rollenkennzeichnung unten)Gegeben einem einzigen Satz, identifizieren und disambiguate semantische Prädikate (z.B. verbale Rahmen,) dann identifizieren und klassifizieren die Rahmenelemente (semantische Rollen). Diskurs (Semantik über einzelne Sätze hinaus) Entschließung des Rates Bei einem Satz oder einem größeren Textstück bestimmen Sie, welche Wörter (Zemente) sich auf die gleichen Objekte (Zentren) beziehen.Anaphora-Auflösung ist ein konkretes Beispiel für diese Aufgabe und betrifft insbesondere die Anpassung der Pronomen mit den Nomen oder Namen, auf die sie sich beziehen. Die allgemeinere Aufgabe der Kernreferenzauflösung besteht auch darin, sogenannte "Brückenbeziehungen" zu identifizieren, die sich mit verweisenden Ausdrücken befassen. Zum Beispiel in einem Satz wie "Er betrat Johns Haus durch die Vordertür", "die Vordertür" ist ein verweisender Ausdruck und die Überbrückungsbeziehung zu identifizieren ist die Tatsache, dass die Tür bezeichnet wird, ist die Vordertür von Johns Haus (anstatt einer anderen Struktur, die auch genannt werden könnte). Diskursanalyse Diese Rubrik umfasst mehrere zusammenhängende Aufgaben. Eine Aufgabe ist das Diskursparsing, d.h. die Identifizierung der Diskursstruktur eines verbundenen Textes, d.h. der Art der Diskursbeziehungen zwischen Sätzen (z.B. Ausarbeitung, Erklärung, Kontrast). Eine weitere mögliche Aufgabe ist es, die Sprachhandlungen in einem Textstück zu erkennen und zu klassifizieren (z.B. ja-keine Frage, Inhaltsfrage, Erklärung, Behauptung usw.). Implizite semantische Rollenkennzeichnung In einem einzigen Satz identifizieren und disambiguieren semantische Prädikate (z.B. verbale Rahmen) und ihre expliziten semantischen Rollen im aktuellen Satz (siehe Semantische Rollenkennzeichnung oben). Dann identifizieren semantische Rollen, die nicht explizit im aktuellen Satz verwirklicht werden, klassifizieren sie in Argumente, die explizit anderswo im Text und diejenigen, die nicht spezifiziert werden, und lösen sie gegen den lokalen Text. Eine eng damit verbundene Aufgabe ist die Null-Anaphora-Auflösung, d.h. die Erweiterung der Coreference-Auflösung auf Pro-Drop-Sprachen. Erkennen von textueller Bedeutung Bei zwei Textfragmenten bestimmen Sie, ob das eine Wesen das andere mit sich bringt, die Negation des anderen mit sich bringt oder das andere entweder wahr oder falsch sein kann. Themensegmentierung und Erkennung Bei einem Textstück, trennen Sie es in Segmente, die jeweils einem Thema gewidmet sind, und identifizieren Sie das Thema des Segments. Argument Bergbau Ziel des Argumentbergbaus ist die automatische Extraktion und Identifizierung von argumentativen Strukturen aus dem natürlichen Sprachtext mit Hilfe von Computerprogrammen. Solche argumentativen Strukturen umfassen die Prämisse, Schlussfolgerungen, die Argumentation und die Beziehung zwischen dem Haupt- und Nebenargument oder die Haupt- und Gegenargument innerhalb des Diskurses. Höhere NLP-Anwendungen Automatische Zusammenfassung (Text-Zusammenfassung) Produzieren Sie eine lesbare Zusammenfassung eines Textes. Oftmals verwendet, um Zusammenfassungen des Textes einer bekannten Art, wie Forschungspapiere, Artikel im Finanzbereich einer Zeitung bereitzustellen. Büchergeneration Nicht eine NLP-Aufgabe, sondern eine Erweiterung der natürlichen Spracherzeugung und anderer NLP-Aufgaben ist die Erstellung von vollwertigen Büchern. Das erste maschinengenerierte Buch wurde 1984 von einem regelbasierten System erstellt (Racter, The Policeman's Bart ist halbkonstruiert). Die erste veröffentlichte Arbeit eines neuronalen Netzes wurde 2018 veröffentlicht, 1 die als Roman vermarktete Straße enthält sechzig Millionen Wörter. Beide Systeme sind im Grunde aufwendig, aber nicht-sensische (semantikfreie) Sprachmodelle. Das erste maschinengenerierte Wissenschaftsbuch wurde 2019 veröffentlicht (Beta Writer, Lithium-Ionen-Batterien, Springer, Cham). Im Gegensatz zu Racter und 1 der Straße wird dies auf sachlichem Wissen und auf der Grundlage der Textzusammenfassung begründet. Dialogmanagement Computersysteme, die mit einem Menschen umkehren sollen. Dokument AIA Die Document AI-Plattform befindet sich auf der NLP-Technologie, die es Nutzern ohne vorherige Erfahrung mit künstlicher Intelligenz, maschinellem Lernen oder NLP ermöglicht, schnell einen Computer zu trainieren, um die spezifischen Daten, die sie von verschiedenen Dokumententypen benötigen, zu extrahieren. NLP-powered Document AI ermöglicht es nicht-technischen Teams, schnell auf Informationen zuzugreifen, die in Dokumenten verborgen sind, beispielsweise Anwälte, Business Analysten und Buchhalter. Grammatische Fehlerkorrektur Grammatische Fehlererkennung und Korrektur beinhaltet eine große Bandbreite von Problemen auf allen Ebenen der sprachlichen Analyse (Phonologie/Orthographie, Morphologie, Syntax, Semantik, Pragmatik). Grammatikalische Fehlerkorrektur ist effektvoll, da es Hunderte von Millionen von Menschen betrifft, die Englisch als zweite Sprache verwenden oder erwerben. Sie unterliegt somit seit 2011 einer Reihe gemeinsamer Aufgaben. Was die Orthographie, Morphologie, Syntax und bestimmte Aspekte der Semantik betrifft, und aufgrund der Entwicklung leistungsfähiger neuraler Sprachmodelle wie GPT-2, kann dies nun (2019) als weitgehend gelöstes Problem angesehen werden und wird in verschiedenen kommerziellen Anwendungen vermarktet. Übersetzung der Maschine Übersetzen Sie automatisch Text von einer menschlichen Sprache in eine andere. Dies ist eines der schwierigsten Probleme, und ist ein Mitglied einer Klasse von Problemen, die kolloquial als AI-komplete bezeichnet werden, d.h., dass alle verschiedenen Arten von Wissen, die Menschen besitzen (Grammatik, Semantik, Fakten über die reale Welt, etc.) richtig zu lösen. Natürliche Spracherzeugung (NLG): Konvertieren Sie Informationen aus Computerdatenbanken oder semantischen Absichten in lesbare menschliche Sprache. Natürliches Sprachverständnis (NLU) Konvertieren Sie Texte in formalere Darstellungen wie Logikstrukturen erster Ordnung, die für Computerprogramme einfacher zu manipulieren sind. Natürliches Sprachverständnis beinhaltet die Identifizierung des beabsichtigten semantischen aus der multiplen möglichen Semantik, die aus einem natürlichen Sprachausdruck abgeleitet werden kann, der in der Regel in Form von organisierten Notationen von natürlichen Sprachkonzepten erfolgt. Einführung und Erstellung von Sprachmetamodel und Ontologie sind jedoch effiziente empirische Lösungen. Eine explizite Formalisierung der natürlichen Sprachsemantik ohne Verwirrung mit impliziten Annahmen wie Closed-world Annahme (CWA) gegen Open-World Annahme, oder subjektive Ja/No vs. objektivTrue/False wird für den Aufbau einer Basis der Semantik-Formalisierung erwartet. Frage beantworten Bei einer menschensprachigen Frage bestimmen Sie ihre Antwort. Typische Fragen haben eine bestimmte richtige Antwort (wie "Was ist die Hauptstadt von Kanada?"), aber manchmal werden offene Fragen betrachtet (wie "Was ist der Sinn des Lebens?"). Allgemeine Tendenzen und (mögliche) zukünftige Richtungen Aufgrund langjähriger Trends auf dem Gebiet ist es möglich, zukünftige Richtungen von NLP zu extrapolieren. Ab 2020 können drei Trends unter den Themen der langjährigen Reihe von CoNLL Shared Tasks beobachtet werden: Interesse an zunehmend abstrakten, kognitiven Aspekten der natürlichen Sprache (1999-2001: flache Parsing, 2002-03: benannte Entitätserkennung, 2006-09/2017-18: Abhängigkeitsssyntax, 2004-05/2008-09 semantische Rollenkennzeichnung, 2011-12 Coreference, 2015-16: Diskursparsing, 2019: semantic parsing). zunehmendes Interesse an Mehrsprachigkeit und potenziell multimodalität (Englisch seit 1999; Spanisch, Niederländisch seit 2002; Deutsch seit 2003; Bulgarisch, Dänisch, Japanisch, Portugiesisch, Slowenisch, Schwedisch, Türkisch seit 2006; Baskisch, Katalanisch, Chinesisch, Griechisch, Ungarisch, Italienisch, Türkisch seit 2007; Tschechisch seit 2009; Arabisch seit 2012; 2017: 40+ Sprachen; 2018: 60+/100 Sprachen Beseitigung symbolischer Repräsentationen (in regelbasierter Aufsicht auf schwach beaufsichtigte Methoden, Repräsentations- und End-to-End-Systeme) Kognition und NLP Die meisten hochrangigen NLP-Anwendungen beinhalten Aspekte, die intelligentes Verhalten und offensichtliches Verständnis der natürlichen Sprache emulieren. Die technische Operationalisierung von immer fortschrittlicheren Aspekten des kognitiven Verhaltens stellt eine der Entwicklungs-Trajektorien von NLP dar (siehe Trends bei den CoNLL-Gegenaufgaben oben). Die Erkenntnis bezieht sich auf "die mentale Handlung oder den Prozess des Wissens- und Verstehens durch Gedanken, Erfahrung und Sinne". Kognitive Wissenschaft ist die interdisziplinäre, wissenschaftliche Studie des Geistes und seiner Prozesse. Kognitive Linguistik ist ein interdisziplinärer Zweig der Linguistik, der Wissen und Forschung aus Psychologie und Linguistik verbindet. Vor allem im Alter der symbolischen NLP hielt der Bereich der rechnerischen Linguistik starke Verbindungen zu kognitiven Studien. Als Beispiel bietet George Lakoff eine Methodik, um natürliche Sprachverarbeitung (NLP) Algorithmen durch die Perspektive der kognitiven Wissenschaft zusammen mit den Erkenntnissen der kognitiven Linguistik zu bauen, mit zwei definierenden Aspekten: Bewerben Sie die Theorie der konzeptuellen Metapher, die von Lakoff als "das Verständnis einer Idee, in Bezug auf eine andere" erklärt, die eine Idee der Absicht des Autors bietet. Betrachten Sie beispielsweise das englische Wort „big“. Bei der Verwendung in einem Vergleich ("Das ist ein großer Baum",) ist die Absicht des Autors zu bedeuten, dass der Baum "physisch groß" gegenüber anderen Bäumen oder die Autoren Erfahrung ist. Bei der Verwendung metaphorisch (”Tomorrow ist ein großer Tag”,) die Absicht des Autors, “Importanz” implizieren. Die Absicht hinter anderen Nutzungen, wie in "Sie ist eine große Person" bleibt etwas mehrdeutig für eine Person und einen kognitiven NLP-Algorithmus gleichermaßen ohne zusätzliche Informationen. Zuordnung relativer Bedeutungsmaße zu einem Wort, Satz oder Textstück basierend auf den vor und nach der Analyse des Textstücks dargestellten Informationen, z.B. mittels einer probabilistischen kontextfreien Grammatik (PCFG). Die mathematische Gleichung für solche Algorithmen ist in US-Patent 9269353 dargestellt: R M ( t o k e n N ) = P M ( t o k e n N ) × 1 2 d ( Σ i = - d d ( P M ( t o k e n N - 1 ) × P F ( t o k e n N, t o k e n N - 1 ) i ) {\displaystyle RMM(token_{N})}={PMM(token_{N})}\times {\fra 1}{2d}\left(\sum) i=-d}{d}{((PMM(token_{N-1})}\times PF(token_{N},token_{N-1}))_{i}\right Ist, RMM, das relative Maß der Bedeutung von Token, ist jeder Block von Text, Satz, Satz oder Wort N, ist die Anzahl der Token analysiert PMM, ist das wahrscheinliche Maß der Bedeutung basierend auf einer Korporation d, ist der Ort des Tokens entlang der Sequenz von N-1 Tokens PF, ist die Probability Funktion spezifisch für eine Sprache Verbindungen mit kognitiven Linguistiken sind Teil des historischen Erbes der NLP, aber sie wurden seit der statistischen Wende in den 1990er Jahren weniger häufig angesprochen. Dennoch wurden Ansätze zur Entwicklung kognitiver Modelle zu technisch betriebsfähigen Gerüsten im Rahmen verschiedener Gerüste, wie z.B. der kognitiven Grammatik, der funktionalen Grammatik, der Baugrammatik, der rechnerischen Psycholinguistik und der kognitiven Neurowissenschaften (z.B. ACT-R) mit eingeschränkter Aufnahme in Mainstream NLP (gemessen durch Präsenz auf großen Konferenzen der ACL) verfolgt. In jüngster Zeit wurden Ideen der kognitiven NLP als Ansatz zur Erklärbarkeit wiederbelebt, z.B. unter dem Begriff "kognitive KI". Ebenso sind Ideen von kognitiven NLP inhärent neuronalen Modellen multimodal NLP (obwohl selten explizit gemacht). Siehe auch Referenzen =Weiterlesen ==