Lethal autonome Waffen (LAWs) sind eine Art autonomer militärischer Systeme, die unabhängig auf programmierten Zwängen und Beschreibungen suchen und Ziele setzen können. LAWs sind auch als tödliche autonome Waffensysteme (LAWS), autonome Waffensysteme (AWS), Roboterwaffen, Killerroboter oder Schlachthöfe bekannt. LAWs können in der Luft, an Land, im Wasser, im Wasser oder im Weltraum tätig sein. Die Autonomie der gegenwärtigen Systeme ab 2018 war im Sinne beschränkt, dass ein Mensch den endgültigen Befehl zum Angriff gibt, obwohl es Ausnahmen mit bestimmten defensiven Systemen gibt. autonom als Waffe autonom hat unterschiedliche Bedeutung in verschiedenen Studienbereichen. In der Technik kann sie auf die Fähigkeit der Maschine verweisen, ohne menschliche Beteiligung zu betreiben. Man kann auf eine Person hinweisen, die moralisch unabhängig ist. In der politischen Wissenschaft kann sie auf die Fähigkeit eines Gebiets verweisen, sich selbst zu entscheiden. In Bezug auf die Entwicklung militärischer Waffen ist die Identifizierung einer Waffe als autonom nicht so klar wie in anderen Bereichen. Die spezifische Norm, die im Begriff der autonomen Form enthalten ist, kann zwischen verschiedenen Wissenschaftlern, Nationen und Organisationen stark variieren. Verschiedene Menschen haben viele Definitionen, was eine tödliche autonome Waffe darstellt. Heather Roff, ein Schriftsteller für die West Reserve University School of Law, beschreibt autonome Waffensysteme als "armierte Waffensysteme, die in der Lage sind, ihre 'funktionstüchtigen Verhältnisse in der Umwelt, in der [sie sind], zu lernen und sich selbst zu entscheiden. Diese Definition autonomer Waffensysteme ist im Vergleich zu den Definitionen von Wissenschaftlern wie Peter Asaro und Mark Gubrud's Definitionen relativ hoch. Wissenschaftler wie Peter Asaro und Mark Gubrud versuchen, die Schwelle zu begrenzen und mehr Waffensysteme als autonom zu betrachten. Sie glauben, dass jedes Waffensystem, das in der Lage ist, ohne den Betrieb, die Entscheidung oder die Bestätigung einer menschlichen Aufsichtsbehörde eine tödliche Kraft zu setzen, als autonom gilt. Laut Gubrud gilt ein Waffensystem, das teilweise oder vollständig ohne menschliche Intervention betrieben wird. Er argumentiert, dass ein Waffensystem nicht in der Lage ist, Entscheidungen selbst vollständig zu treffen, um autonom zu werden. Stattdessen sollte sie als autonom behandelt werden, solange sie in einem oder mehreren Teilen des "Vorbereitungsprozesses" aktiv daran beteiligt ist, das Ziel endgültig zu bestimmen. Andere Organisationen setzen jedoch den Standard des autonomen Waffensystems in einer höheren Position ein. Das Verteidigungsministerium (Vereinigtes Königreich) definiert autonome Waffensysteme als "Systeme, die in der Lage sind, ein höheres Maß an Absicht und Richtung zu verstehen. Aus diesem Verständnis und seiner Wahrnehmung seiner Umwelt kann ein solches System geeignete Maßnahmen ergreifen, um einen gewünschten Zustand zu erreichen. Es ist in der Lage, einen Handlungskurs von einer Reihe von Alternativen zu wählen, unabhängig von der Überwachung und Kontrolle des Menschen - ein solches menschliches Engagement mit dem System kann dennoch vorhanden sein. Obwohl die Gesamtaktivität eines autonomen unbemannten Luftfahrzeugs vorhersehbar ist, können einzelne Maßnahmen nicht sein. " Infolgedessen erfordert die Zusammensetzung eines Vertrags zwischen den Staaten eine allgemein anerkannte Kennzeichnung, was genau eine autonome Waffe darstellt. automatische defensive Systeme Die älteste automatisch ausgelöste Lethalwaffe ist die seit mindestens 1600 verwendete Landgrube und Marineminen, die seit mindestens 1700 Jahren verwendet werden. Antipersonenminen sind in vielen Ländern durch den Ottawa-Vertrag von 1997 verboten, nicht die Vereinigten Staaten, Russland und den Nahen Osten. Manche aktuelle Beispiele von LAWs sind automatisierte schwerqualifizierte aktive Schutzsysteme, wie etwa ein Radar-gesteuerter CIWS-Systeme, die seit den siebziger Jahren eingesetzt wurden (z.B. das US-amerikanische Phalanx CIWS). Solche Systeme können autonom auf anstehenden Raketen, Raketen, Flugzeugen, Flugzeugen und Oberflächenschiffen nach Kriterien des menschlichen Betreibers identifiziert und angegriffen werden. Ähnliche Systeme gibt es für Tanks wie die russische Arena, die israelische Liga und die Deutsche AMAP-ADS. Mehrere Arten von stationären Entladungswaffen, die beim Menschen und bei Fahrzeugen zum Brand kommen, werden in Südkorea und Israel verwendet. Viele Raketenabwehrsysteme wie Iron Dome verfügen auch über autonome Einsatzfähigkeiten. Selbstbedienungsturreten, die auf Militärfahrzeugen installiert sind, werden als Fernwaffenstationen bezeichnet. Hauptgrund dafür, dass in diesen Systemen keine "menschliche Internetanschluss" besteht, ist die Notwendigkeit einer schnellen Reaktion. In der Regel wurden sie zum Schutz von Personal und Anlagen gegen künftige Projektile verwendet. autonome offensive Systeme mit höherem Maß an Autonomie würden Drohnen oder unbemannte Kampfflugzeuge umfassen, z.B.: "Die unwaffneten BAE Systems Taranis Jet-propelled Kampf Drohnen können zu einem künftigen Luftverkehrssystem führen, das autonome Suche, Identifizierung und Ansiedlung von Feinden ermöglichen kann, aber nur mit einem Ziel arbeiten kann, wenn es von der Mission genehmigt wird. Sie kann sich auch gegen Feindflugzeuge verteidigen (Heyns 2013, §45). Die Nordrop Grumman X-47B Drohnen können auf Flugzeugbetreibern (im Jahr 2014 genehmigt) landen und landen; es soll in ein System der unmanned Carrier-Launed-Luftüberwachung und Streik (UK) entwickelt werden. Laut dem Economist könnten künftige Anwendungen von unbemannten unterseeischen Fahrzeugen die Minenräumung, die Minenräumung, die Anti-Seesensor-Netze in streitigen Gewässern, die Patten mit aktiven Sonaren, die Wiederauffüllung von bemannten U-Booten und die Entwicklung kostengünstiger Raketenplattformen umfassen. Im Jahr 2018 behauptete die U.S Nuclear Posture Review, dass Russland ein „neues interkontinentales, nukleares, kernbetriebenes, unterseeeigenes Torrpedo“ entwickelt. Die Russische Föderation entwickelt künstlich intelligente Raketen, Drohnen, unbemannte Fahrzeuge, militärische Roboter und medizinische Roboter. Israeler Minister Ayoob Kara erklärte im Jahr 2017, dass Israel militärische Roboter entwickelt, darunter auch solche, die als Kleinkraft dienen. Oktober 2018, Zeng Yi, ein Senior Executive bei der chinesischen Verteidigungsgesellschaft Norinco, gab eine Rede, in der er sagte, dass "in künftigen Kampfgebieten keine Menschen sind," und dass die Verwendung von tödlichen autonomen Waffen in Kriegsführung unvermeidlich sei. „Im Jahr 2019 verlief der US-Verteidigungssekretär Mark Esper auf China, um Drohnen zu verkaufen, die ohne menschliche Aufsicht leben können. Die britische Armee hat 2019 neue unbemannte Fahrzeuge und militärische Roboter eingesetzt. Die US-Katastrophe entwickelt sich in unmannten Schiffen. Im Jahr 2020 hat ein Kargu 2 Drohnen ein menschliches Ziel in Libyen bejaht und angegriffen, laut einem Bericht der im März 2021 veröffentlichten Expertengruppe des UN-Sicherheitsrates für Libyen. Dies könnte zum ersten Mal ein autonomer Roboter mit tödlichen Waffenangriffen des Menschen gewesen sein. Konsolidierungs- und Rechtsfragen Standard, die in den aktuellen US-Politikstaaten verwendet werden: "Autonomous ... Waffensysteme sollen den Befehlsträgern und Betreibern die Möglichkeit geben, ein angemessenes Niveau des menschlichen Urteils über die Verwendung von Gewalt auszuüben". In der Politik ist jedoch vorgeschrieben, dass autonome Waffensysteme, die Menschen abtöten oder kinetische Kraft verwenden, Ziele ohne weiteres menschliches Eingreifen wählen und betreiben, als mit "geeigneten Ebenen" und anderen Normen bescheinigt werden, nicht, dass solche Waffensysteme diese Normen nicht erfüllen können und daher verboten sind. " Halbautonome Jäger-Höhner, die autonome Ermittlungs- und Angriffsziele festlegen, brauchen nicht einmal Zertifizierung. Deputy Defence Secretary Robert Work sagte im Jahr 2016, dass das Verteidigungsministerium "nicht befugt wäre, eine Maschine zu bestellen, um eine Entscheidung zu treffen", aber möglicherweise muss dies überprüfen, da "Autoritäre Regime" dies tun könnten. Kommissionspräsident Barack Obama erklärte im Oktober 2016, dass er in seiner Karriere davor war, eine Zukunft zu verfolgen, in der ein US-Präsident, der die Verwendung von Drängen vornimmt, "einen ständigen Kriegen auf der ganzen Welt und viel davon abdecken könnte, ohne dass Rechenschaftspflicht oder demokratische Debatte. In den USA ist die sicherheitsrelevante AI seit 2018. Oktober 2019 veröffentlichte das Verteidigungsministerium der Vereinigten Staaten den Entwurf eines Berichts, in dem fünf Grundsätze für die gepanzerte AI herausgearbeitet und 12 Empfehlungen für die ethische Nutzung künstlicher Erkenntnisse durch das Verteidigungsministerium abgegeben werden, die sicherstellen würden, dass ein menschlicher Anbieter immer in die "schwarze Box" blicken und den tödlichen Prozess verstehen kann. Hauptanliegen ist die Umsetzung des Berichts. mögliche Verletzungen von Ethik und internationalen Akten Stuart Russell, Professor für Informatik an der University of California, teilte mit, dass er mit LAWS besorgt sei, dass es unethisch und. sei. Hauptthema dieses Systems ist es schwer, zwischen Kämpfern und Nicht-Diskriminanten zu unterscheiden. Manche (z.B. Noel Haiey 2012) sind besorgt über die Frage, ob LAWs das humanitäre Völkerrecht verletzt, insbesondere den Grundsatz der Unterscheidung, der die Fähigkeit zur Diskriminierung von Kämpfern aus Nicht-Kombattanten und den Grundsatz der Verhältnismäßigkeit vorschreibt, wonach Schäden an Zivilisten im Verhältnis zum militärischen Ziel stehen. Diese Sorge wird häufig als Grund für das Verbot von "mittleren Robotern" genannt, aber es ist zweifelhaft, dass diese Besorgnis ein Argument gegen LAWs sein kann, die das humanitäre Völkerrecht nicht verletzen. Laut einem Bericht des US-Kongressionsforschungsdienstes von 2021 gibt es keine innerstaatlichen oder internationalen rechtlichen Verbote für die Entwicklung von LAWs, obwohl er die laufenden Gespräche im UN-Übereinkommen über bestimmte Waffen (CCW) anerkennt. LAWs werden von einigen gesagt, um die Grenzen zu verwischen, die für ein bestimmtes Töten verantwortlich sind. Philosopher Robert Sparrow argumentiert, autonome Waffen sind kausaler, aber nicht moralisch verantwortlich, ähnlich wie Kindersoldaten. In jedem Fall argumentiert er, es besteht die Gefahr von Gräueltaten, die ohne ein angemessenes, verantwortungsbewusstes Thema auftreten, das die Rechtsprechungen in bello verletzt. Thomas Simpson und Vincent Müller sprachen sich dafür aus, dass sie es leichter machen können, diejenigen zu erfassen, die den Befehl erteilt haben. Steven Umbrello, Phil Torres und Angelo F. De Bellis argumentieren, dass, wenn die technischen Kapazitäten von LAWs mindestens so genau wie menschliche Soldaten sind, angesichts der psychologischen Unzulänglichkeiten menschlicher Soldaten im Krieg gerechtfertigt sind, dass nur diese Arten von ethischen LAWs verwendet werden sollten. Ebenso schlagen sie vor, den Wert sensiblen Designansatz als mögliche Rahmenbedingungen für die Gestaltung dieser Gesetze zu nutzen, um die menschlichen Werte und das humanitäre Völkerrecht anzugleichen. Künftig sind potenzielle IHL-Konflikte von LAWs – nach Definition – nur in Konfliktsituationen anwendbar, die die Notwendigkeit betreffen, zwischen Kämpfern und Zivilisten zu unterscheiden. Jedes Konfliktszenario, das die Präsenz der Zivilbevölkerung – d. h. im Raum oder in den tiefen Meeren – verdeutlicht, würde nicht in die Hindernisse der IHL münden. Kampagnen zum Verbot von LAWs Die Möglichkeit von LAWs hat in naher oder weiter Zukunft eine bedeutende Debatte ausgelöst, vor allem über das Risiko von "Tonnen"-Roaming. Kampagne der Gruppe zur Einstellung von Robotern, die 2013 gebildet wurden. Im Juli 2015 unterzeichneten mehr als 1000 Experten in der künstlichen Intelligenz ein Mahnschreiben über die Bedrohung eines künstlichen Geheimwaffenrennens und forderten ein Verbot autonomer Waffen. In Buenos Aires wurde das Schreiben auf der 24. Internationalen Gemeinsamen Konferenz über Künstliche Intelligenz (IJCAI-15) vorgestellt und von Stephen Johnsoning, Elon Musk, Steve Wozniak, Noam Chomsky, Skype-Mitgründer Jaan Tallinn und Google DeepMind Mitgründer Demis Hassabis, unter anderem unterzeichnet. Laut PAX für den Frieden (eine der Gründungsorganisationen der Kampagne zur Einstellung von Robotern) werden vollautomatisierte Waffen (FAWs) den Schwellenwert des Krieges senken, da Soldaten aus dem Kampfplatz entfernt werden und die Öffentlichkeit vom Krieg entfernt ist, Politiker und andere Entscheidungsträger mehr Raum für die Entscheidung über den Zeitpunkt und den Weg zum Krieg. Man warnte, dass FAWs die demokratische Kontrolle des Krieges erschweren werden – etwas, das die Ermächtigung der Tötungsentscheidung – ein Roman zu diesem Thema – und IT-Spezial Daniel Suarez warnten auch davor: Nach seiner Macht könnte sie die Macht in sehr wenigen Händen dezentralisieren, indem sie sehr wenige Menschen auf Krieg bringen. Websites protestieren die Entwicklung von LAWs durch die Vorlage unerwünschter Folgen, wenn die Erforschung des Geräts der künstlichen Intelligenz zur Bezeichnung von Waffen fortgesetzt wird. Nachrichten zu ethischen und rechtlichen Fragen werden ständig aktualisiert, um mit den jüngsten Nachrichten über internationale Tagungen und Forschungsartikel über LAWs zu beginnen. Der Heiligen Stuhl hat die internationale Gemeinschaft aufgefordert, die Verwendung von LAWS mehrfach zu verbieten. Erzbischof Ivan Jurkovic, der Ständige Beobachter des Heiligen Sees bei den Vereinten Nationen, erklärte im November 2018, dass „um einen Waffenrennen und die Zunahme von Ungleichheiten und Instabilität zu verhindern, ist es eine zwingende Pflicht, rasch zu handeln: Jetzt ist es an der Zeit, LAWS zu verhindern, die Realität des Kriegs von morgen zu werden.“ Die Kirche befürchtet, dass diese Waffensysteme die Möglichkeit haben, die Art der Kriegsführung unwiderruflich zu verändern, Abschreckung von der menschlichen Agentur zu schaffen und die Menschheit der Gesellschaften in Frage zu stellen. Am 29. März 2019 befürworteten die meisten Regierungen, die auf einer UN-Tagung vertreten sind, um die Angelegenheit zu erörtern, ein Verbot für LAWS. Eine Minderheit von Regierungen, einschließlich derjenigen aus Australien, Israel, Russland, dem Vereinigten Königreich und den USA, lehnte ein Verbot ab. Keines Verbots, aber die Verordnung Ein dritter Ansatz konzentriert sich auf die Regelung der Verwendung autonomer Waffensysteme in einem Verbot. Militärische Kontrolle von Waffen wird wahrscheinlich die Institutionalisierung neuer internationaler Normen erfordern, die in wirksamen technischen Spezifikationen enthalten sind, zusammen mit aktiver Überwachung und informeller Diplomatie von Expertengemeinschaften und einem rechtlichen und politischen Überprüfungsprozess. Siehe auch die Liste der fiktiven militärischen Roboter Slaughterbots verweist auf weitere Lesung Heyns, Christof (2013), „Bericht des Sonderberichts über außergerichtliche, Zusammenfassung oder willkürliche Hinrichtungen“, UN-Generalversammlung, Menschenrechtsrat, 23 (3), A/HRC/23/47. Krishnan, Armin (2009), Messenger Roboter: Legalität und Ethik autonomer Waffen (Aldershot: Ashgate)Müller, Vincent C. (2016,) „Autonomous Killer Roboter sind wahrscheinlich gute Nachrichten“, in Ezio Di Nucci und Filippo Santoni de Sio (eds). Drones und Verantwortung: Legale, philosophische und sozio-technische Perspektiven für den Einsatz von ferngesteuerten Waffen, 67-81 (London: Ashgate). Douglas's Kampagne gegen LAWs Haiey, Noel E (2012), „Automating Warfare: Lehren aus den Drohnen, Journal of Law, Information & Science, 21 (2). Simpson, Thomas W und Müller, Vincent C. (2016,) „Justiz und Robotermord“ Philosophisches Viertel 66 (263,) 302–22. Singer, Peter (2009), Wired für Krieg: Robotik Revolution und Konflikt im 21. Jahrhundert(New York: Penguin) US Department of Defence (2012), „Richtlinie 3000.09, Autonomy in Waffensystemen“. Konsumroboter Policy Paper Final.dkli. US Department of Defence (2013), 'Unmanned Systems Integrated Road Map FY2013-2038'. Ethik der autonomen Waffensysteme (2014) Seminar in UPenn shttps://www.law.upenn.org/institutes/cerl/conferences/ethofweapons/schedule-required-readings.php.