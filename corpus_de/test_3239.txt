Im maschinellen Lernen werden Support-Vector-Maschinen (SVMs, auch Support-Vector-Netzwerke) mit zugehörigen Lernalgorithmen überwacht, die Daten zur Klassifikations- und Regressionsanalyse analysieren. Entwickelt bei AT&T Bell Laboratories von Vladimir Vapnik mit Kollegen (Boser et al,. 1992, Guyon et al,. 1993, Vapnik et al,. 1997: DIE ZAHLUNGEN SVMs sind eine der robustesten Vorhersagemethoden, die auf statistischen Lernrahmen oder VC-Theorie basiert, die von Vapnik (1982, 1995) und Chervonenkis (1974) vorgeschlagen wurden. Ein SVM-Trainingsalgorithmus baut bei einer Reihe von Trainingsbeispielen, die jeweils als einer von zwei Kategorien gekennzeichnet sind, ein Modell, das neue Beispiele einer Kategorie oder der anderen zuordnet, so dass es ein nicht-probabilistischer binärer linearer Klassifikator (obwohl Methoden wie Platt-Skalierung existieren, um SVM in einer probabilistischen Klassifikationseinstellung zu verwenden). SVM stellt Trainingsbeispiele auf Punkte im Raum dar, um die Breite der Lücke zwischen den beiden Kategorien zu maximieren. Neue Beispiele werden dann in denselben Raum abgebildet und vorhergesagt, zu einer Kategorie zu gehören, auf deren Seite sie fallen. Neben der linearen Klassifikation können SVMs eine nichtlineare Klassifikation mit dem sogenannten Kernel-Trick effizient durchführen, indem sie ihre Eingaben in hochdimensionale Merkmalsräume implizit abbilden. Wenn Daten unmarkiert sind, ist beaufsichtigtes Lernen nicht möglich, und es wird ein ununterbrochener Lernansatz benötigt, der versucht, die natürliche Clusterung der Daten zu Gruppen zu finden, und dann neue Daten zu diesen gebildeten Gruppen abzubilden. Der von Hava Siegelmann und Wladimir Vapnik erstellte Support-Vektor-Cluster-Algorithmus wendet die Statistik der im Support-Vektor-Maschinen-Algorithmus entwickelten Support-Vektoren an, um unmarkierte Daten zu kategorisieren und ist einer der am weitesten verbreiteten Cluster-Algorithmen in industriellen Anwendungen. Motivation Die Klassifizierung von Daten ist eine gemeinsame Aufgabe beim maschinellen Lernen. Angenommen, einige vorgegebene Datenpunkte gehören jeweils zu einer von zwei Klassen, und das Ziel ist zu entscheiden, welche Klasse ein neuer Datenpunkt sein wird. Im Falle von Support-Vector-Maschinen wird ein Datenpunkt als p {\displaystyle p} -dimensionaler Vektor (eine Liste von p {\displaystyle p}-Nummern) betrachtet, und wir möchten wissen, ob wir solche Punkte mit einem (p - 1) {\displaystyle (p-1)} trennen können -dimensionales Hyperplane. Dies wird als linearer Klassifikator bezeichnet. Es gibt viele Hyperplane, die die Daten klassifizieren könnten. Eine vernünftige Wahl als das beste Hyperplane ist diejenige, die die größte Trennung oder Marge zwischen den beiden Klassen darstellt. So wählen wir das Hyperplane, so dass der Abstand von ihm zum nächsten Datenpunkt auf jeder Seite maximiert wird. Wenn ein solches Hyperplan vorhanden ist, ist es als das maximal-margine Hyperplane und der lineare Klassifikator, den es definiert, als ein maximal-margin Klassifikator bekannt; oder gleichwertig ist der Perceptron optimaler Stabilität. Begriff Eine Stützvector-Maschine konstruiert formell ein Hyperplan oder eine Reihe von Hyperplanen in einem hoch- oder unendlichen Raum, der zur Klassifizierung, Regression oder anderen Aufgaben wie der Ausreißererkennung verwendet werden kann. Intuitiv wird eine gute Trennung durch das Hyperplane erreicht, das den größten Abstand zum nächsten Trainingsdatenpunkt jeder Klasse (sogenannter Funktionsrand) hat, da im allgemeinen die Marge umso größer ist, je geringer der Verallgemeinerungsfehler des Klassifikators ist. Während das ursprüngliche Problem in einem endlichen Raum angegeben werden kann, kommt es häufig vor, dass die zu diskriminierenden Sätze in diesem Raum nicht linear trennbar sind. Aus diesem Grund wurde vorgeschlagen, dass der ursprüngliche endliche Raum in einen viel höheren Raum abgebildet wird, wodurch die Trennung in diesem Raum vermutlich erleichtert wird. Um die rechnerische Belastung angemessen zu halten, sind die von SVM-Systemen verwendeten Mappings dazu ausgelegt, sicherzustellen, dass Punktprodukte von Paaren von Eingabedatenvektoren in Bezug auf die Variablen im Originalraum leicht berechnet werden können, indem sie in Bezug auf eine Kernelfunktion k (x, y )\displaystyle k(x,y)} definiert werden, die dem Problem entspricht. Die Hyperplane im höherdimensionalen Raum sind definiert als der Satz von Punkten, deren Punktprodukt mit einem Vektor in diesem Raum konstant ist, wobei ein solcher Satz von Vektoren ein orthogonaler (und damit minimaler) Satz von Vektoren ist, die ein Hyperplan definieren. Die die Hyperplane definierenden Vektoren können als lineare Kombinationen mit Parametern α i {\displaystyle \alpha {_i} von Bildern von Merkmalsvektoren x i {\displaystyle x_{i} gewählt werden, die in der Datenbank vorkommen. Bei dieser Wahl eines Hyperplans werden die Punkte x {\displaystyle x} im in das Hyperplane abgebildeten Merkmalsraum durch die Relation Σ i α i k ( x i, x ) = Konstante definiert. {\displaystyle \textstyle \sum {_i}\alpha i}k(x_{i},x)={\text{constant. Beachten Sie, dass wenn k (x, y ) {\displaystyle k(x,y)} klein wird, da y {\displaystyle y} weiter weg von x {\displaystyle x} wächst, jeder Begriff in der Summe den Grad der Nähe des Testpunktes x {\displaystyle x} zu dem entsprechenden Datenbasispunkt x i {\displaystyle x_{i} mißt. Beachten Sie, dass der Satz von Punkten x {\displaystyle x}, die in jedes Hyperplane abgebildet werden, dadurch ziemlich verwirrt werden kann, so dass viel komplexere Diskriminierung zwischen Sätzen, die überhaupt nicht konvex im ursprünglichen Raum sind. Anwendungen SVMs können verwendet werden, um verschiedene reale Probleme zu lösen: SVMs sind hilfreich bei der Text- und Hypertext-Kategorisierung, da ihre Anwendung den Bedarf an markierten Trainingsinstanzen sowohl in den standardmäßigen induktiven als auch transduktiven Einstellungen deutlich reduzieren kann. Einige Methoden zur flachen semantischen Parsierung basieren auf Stützvektormaschinen. Die Klassifizierung von Bildern kann auch mit SVMs durchgeführt werden. Experimentelle Ergebnisse zeigen, dass SVMs nach nur drei bis vier Runden Relevanz-Feedback deutlich höhere Suchgenauigkeit als herkömmliche Abfrageveredelungssysteme erreichen. Dies gilt auch für Bildsegmentierungssysteme, einschließlich derjenigen, die eine modifizierte Version SVM verwenden, die den privilegierten Ansatz verwendet, wie von Vapnik vorgeschlagen. Klassifizierung von Satellitendaten wie SAR-Daten mit überwachtem SVM. Handgeschriebene Zeichen können mit SVM erkannt werden. Der SVM-Algorithmus wurde in den biologischen und anderen Wissenschaften weit verbreitet. Sie wurden verwendet, um Proteine mit bis zu 90% der korrekt klassifizierten Verbindungen zu klassifizieren. Permutationstests auf Basis von SVM-Gewichten wurden als Mechanismus zur Interpretation von SVM-Modellen vorgeschlagen. Auch für die Interpretation von SVM-Modellen in der Vergangenheit wurden Gebrauchsmaschinengewichte verwendet. Posthoc-Interpretation von Support-Vector-Maschinenmodellen zur Identifizierung von Merkmalen, die vom Modell zur Vorhersage verwendet werden, ist ein relativ neuer Forschungsbereich mit besonderer Bedeutung in den biologischen Wissenschaften. Geschichte Der ursprüngliche SVM-Algorithmus wurde von Vladimir N. Vapnik und Alexey Ya.Chervonenkis 1963 erfunden. Im Jahr 1992 schlugen Bernhard Boser, Isabelle Guyon und Vladimir Vapnik einen Weg vor, nichtlineare Klassifikatoren zu schaffen, indem der Kernel-Trick auf maximal margine Hyperplanen angewendet wird. Die gegenwärtige Standardinkarnation (weiche Marge) wurde 1993 von Corinna Cortes und Vapnik vorgeschlagen und 1995 veröffentlicht. Linear SVMWir erhalten einen Trainingsdatensatz von n {\displaystyle n} Punkten des Formulars ( x 1 , y 1 ) , ... , ( x n , y n ) , {\displaystyle (\mathbf {x} 1},y_{1}),\ldots ,(\mathbf {x} n},y_{n) wobei die y i {\displaystyle y_{i} entweder 1 oder -1 sind, wobei jede die Klasse angibt, zu der der Punkt x i {\displaystyle \mathbf {x} {_i} gehört. Alle x i {\displaystyle \mathbf {x} {_i} ist ein p {\displaystyle p} -dimensionaler realer Vektor. Wir möchten das "maximum-margine Hyperplane" finden, das die Gruppe der Punkte x i {\displaystyle \mathbf {x} {_i} teilt, für die y i = 1 {\displaystyle y_{i}=1 aus der Gruppe der Punkte, für die y i = - 1 {\displaystyle y_{i}=-1 ist, die so definiert ist, dass der Abstand zwischen dem Hyperplane i und dem nächstgelegenen Jeder Hyperplan kann als Satz von Punkten x {\displaystyle \mathbf {x} } geschrieben werden, die w T x - b = 0, {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=0,} genügen, wobei w {\displaystyle \mathbf {w} } der (nicht notwendigerweise normierte) Normalvektor zum Hyperplan ist. Dies ist ähnlich wie Hessen normale Form, außer dass w {\displaystyle \mathbf {w} } nicht unbedingt ein Einheitsvektor ist. Der Parameter b zusammengestellt w {\displaystyle {\tfrac b}{\|\\\\\mathbf {w}}} bestimmt den Offset des Hyperplans vom Ursprung entlang des normalen Vektors w {\displaystyle \mathbf {w} . Hartmargin Wenn die Trainingsdaten linear trennbar sind, können wir zwei parallele Hyperplane auswählen, die die beiden Datenklassen voneinander trennen, so dass der Abstand zwischen ihnen möglichst groß ist. Die von diesen beiden Hyperplanen begrenzte Region wird die Marge genannt, und das Maximum-Margin Hyperplane ist das Hyperplane, das auf halbem Weg zwischen ihnen liegt. Bei einem normierten oder standardisierten Datensatz können diese Hyperplane durch die Gleichungen w T x - b = 1 {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=1} beschrieben werden (jeweils an oder oberhalb dieser Grenze ist eine Klasse, mit Label 1) und w T x - b = - 1 {\displaystyle \mathbf} Geometrisch ist der Abstand zwischen diesen beiden Hyperplanen 2 , die gebildet werden {\displaystyle {\tfrac 2}{\|\\\\\\\\\mathbf {w} \|}}}}, um den Abstand zwischen den Ebenen zu maximieren, die wir wollen, um zu minimieren  we\displaystyle |\mathbf {w} \|} .Die Entfernung wird anhand der Ebene berechnet. Wir müssen auch verhindern, dass Datenpunkte in die Marge fallen, wir fügen folgende Einschränkung hinzu: für jede i {\displaystyle i} entweder w T x i − b ≥ 1 {\displaystyle \mathbf {w} ^{T}\mathbf {x} ! 1 , wenn y i = 1 {\displaystyle y_{i}=1 ,or w T x i - b ≤ - 1 {\displaystyle \mathbf} {_i}-b\leq -1}, wenn y i = - 1 {\displaystyle * .Die Strafverfolgungen geben an, dass jeder Datenpunkt auf der richtigen Seite des Randes liegen muss. Dies kann als y i (w T x i - b ) ≥ 1 , für alle 1 ≤ i ≤ n . ( 1 ) {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} {_i}-b)\geq 1,\quad \text für alle }1\leqi\q nq Wir können dies zusammenstellen, um das Optimierungsproblem zu erhalten: "Minimize ̄w{\displaystyle |\mathbf {w} vorbehaltlich y i (w T x i − b ) ≥ 1 {\displaystyle y_{i}(\math\bf {w}\c} Eine wichtige Folge dieser geometrischen Beschreibung ist, dass das max-margine Hyperplane vollständig durch diejenigen x → i {\displaystyle {\vec x}_{i bestimmt wird, die ihm am nächsten liegen. Diese x i {\displaystyle \mathbf {x} {_i} werden als Stützvektoren bezeichnet. Weichmargin Um SVM auf Fälle auszudehnen, in denen die Daten nicht linear trennbar sind, ist die Scharnierverlustfunktion hilfreich max ( 0, 1 - y i (w T x i - b)). {\displaystyle \max links(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} {_i}-b)\right. Beachten Sie, dass y i {\displaystyle y_{i} das i-te Ziel ist (d.h. in diesem Fall 1 oder -1), und w T x i − b {\displaystyle \mathbf {w} ^{T}\mathbf {x} {_i}-b ist der i-te Ausgang. Diese Funktion ist Null, wenn die Strenge in (1) erfüllt ist, also, wenn x i {\displaystyle \mathbf {x} {_i} auf der richtigen Seite des Randes liegt. Bei Daten auf der falschen Seite des Randes ist der Wert der Funktion proportional zum Abstand vom Rand. Ziel der Optimierung ist es dann, [ 1 n Σ i = 1 n max ( 0 , 1 - y i ( w T x i - b )) ] + λ w^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \lVert \mathbf {w} \rVert ^{2,}, wo der Parameter λ {\displaystyle \lambda } den Trade-off zwischen der Erhöhung der Margengröße bestimmt und sicherstellt, dass die x i {\displaystyle \mathbf {x} {_i} auf der richtigen Seite des Randes liegen. So wird bei ausreichend kleinen Werten von λ {\displaystyle \lambda } der zweite Begriff in der Verlustfunktion vernachlässigbar werden, daher wird er sich ähnlich wie der Hard-Margin SVM verhalten, wenn die Eingangsdaten linear klassierbar sind, aber immer noch erlernen, wenn eine Klassifizierungsregel funktionsfähig ist oder nicht. Nichtlineare Klassifizierung Der ursprünglich von Vapnik 1963 vorgeschlagene, höchstmargige Hyperplane-Algorithmus baute einen linearen Klassifikator. Im Jahr 1992 schlugen Bernhard Boser, Isabelle Guyon und Vladimir Vapnik jedoch einen Weg vor, nichtlineare Klassifikatoren zu erstellen, indem der Kernel-Trick (ursprünglich von Aizerman et al.) auf maximal marginale Hyperplanen angewendet wird. Der resultierende Algorithmus ist formal ähnlich, außer dass jedes Punktprodukt durch eine nichtlineare Kernelfunktion ersetzt wird. Dies ermöglicht es dem Algorithmus, das Maximum-Margin Hyperplane in einem transformierten Feature-Raum zu passen. Die Transformation kann nichtlinear und der transformierte Raum hochdimensional sein; obwohl der Klassifikator ein Hyperplan im transformierten Merkmalsraum ist, kann es im ursprünglichen Eingaberaum nichtlinear sein. Es ist bemerkenswert, dass die Arbeit in einem höherdimensionalen Merkmalsraum den Verallgemeinerungsfehler von Support-Vektor-Maschinen erhöht, obwohl bei genügend Proben der Algorithmus noch gut ausführt. Einige gemeinsame Kerne umfassen: Polynom (homogene): k ( x i → , x j → ) = ( x i → ⋅ x j → ) dgl. {\displaystyle k({\vec x_{i}},{\vec x_{j}})=({\vec x_{i}}}\cdot {\vec x_{j}}})^{d .Polynomial (inhomogen): k ( x i → , x j → ) = ( x i → ∙ x j → + 1 ) d {\displaystyle k({\vec x_{i}}},{\vec x_{j}})=({\vec x_{i}}\cdot {\vec x_{j}}+1)^{d .Gaussian radiale Basisfunktion: k ( x i →, x j → ) = exp ( − γ ereignet x i → − x j → ^ 2 ereignet) {\displaystyle k({\vec x_{i}}},{\vec x_{j}}}=\exp(-\gamma {|\vec * γ > 0 {\displaystyle \gamma >0} .Manchmal parametrisiert mit γ = 1 / ( 2 σ 2 ) {\displaystyle \gamma =1/(2\sigma ^{2)} . Hyperbolic tangent: k ( x i → , x j → ) = tanh ‡ ( κ x i → x j → + c ) {\displaystyle k({\vec x_{i}}},{\vec x_{j}}})=\tanh(\kappa {\vec x_{i}}\cdot {\vec x_{j}+c) für einige (nicht alle) κ > 0 {\displaystyle \kappa >0} und c < 0 {\displaystyle c<0}.Der Kernel ist mit der Transformation φ (x i → )\displaystyle \varphi {(\vec x_{i}} durch die Gleichung k verbunden ( {\displaystyle k({\vec x_{i}}}},{\vec x_{j}})=\varphi {(\vec x_{i}})\cdot \varphi {(\vec x_{j)} . Der Wert w ist auch im transformierten Raum, mit w → = Σ i α i y i φ ( x → i ) {\displaystyle \textstyle} (w) ! i}y_{i}\varphi {(\vec x}_{i) .Dot-Produkte mit w zur Klassifizierung können wieder durch den Kernel-Trick berechnet werden, d.h. w → φ ( x → ) = Σ i α i i k ( x → i, x → ) {\displaystyle \textstyle {\vec {w}}\cdot \varphi {(\vec {x}})=\sum {_i}\alpha ,,{i}k({\vec x}_{i},{\vec {x)} . Computing der SVM Klassifikator Die Eingabe des (soft-margin) SVM-Klassifikators ist die Minimierung eines Ausdrucks des Formulars [1 n Σ i = 1 n max (0, 1 - y i (w T x i - b) ) ] + λ ) w φ Wir konzentrieren uns auf den Soft-Margin-Klassifikator, da, wie oben erwähnt, die Wahl eines ausreichend kleinen Wertes für λ {\displaystyle \lambda } den Hard-Margin-Klassifikator für linear klassierbare Eingangsdaten liefert. Im Folgenden wird der klassische Ansatz, der die Reduzierung von (2) auf ein quadratisches Programmierproblem beinhaltet, näher erläutert. Dann werden neuere Ansätze wie Subgradientenabstieg und Koordinatenabstieg diskutiert. Primal Minimierung (2) kann als ein eingeschränktes Optimierungsproblem mit einer differenzierbaren Zielfunktion auf folgende Weise neu geschrieben werden. Für jede i ε { 1 , ... , n } {\displaystyle i\in {1,\,\ldots ,n} stellen wir eine Variable ζ ein i = max ( 0, 1 - y i ( w T x i - b) ) {\displaystyle \zeta {_i}=\max links(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} {_i}-b)\right .Anmerken, dass ζ i {\displaystyle \zeta {_i} die kleinste nichtnegative Zahl ist, die y i (w T x i - b) ≥ 1 - ζ i erfüllt. {\displaystyle y_{i}(\mathbf} {_i.} So können wir das Optimierungsproblem wie folgt neu schreiben: Minimieren Sie 1 n Σ i = 1 n ζ i + λ w w ≠ 2 {\displaystyle \text{minimize }{\frac 1}{n}\sum * i=1{n}\zeta {_i}+\lambda |\mathbf {w} ^{|\2} vorbehaltlich y i (w T x i − b) ≥ 1 − ζ i und ζ i ≥ 0 , für alle i . {\displaystyle \text{subject to y_{i}(\mathbf {w} ^{T}\mathbf {x} {_i}-b)\geq 1-\zeta _i}\,{\text und },\zeta {_i}\geq 0\,{\text{für alle i. Das nennt man das Primalproblem. Dual Durch die Lösung des lagrangischen Doppels des obigen Problems erhält man das vereinfachte Problem maximiert f ( c 1 ... c n ) = Σ i) 1 n c i - 1 2 Σ i = 1 n Σ j = 1 n y i c i ( x i T x j ) y j c j , {\displaystyle text{maximize}\,\,f(c_{1}\ldots c_{n})=\sum i=1^{n}c_{i}-{\frac 1 * i=1{n}\sum j=1^{n}y_{i}c_{i}(\mathbf {x}_i}^{T}\mathbf {x} j})y_{j}c_{j, vorbehaltlich Σ i = 1 n c i = 0 und 0 ≤ c i ≤ 1 2 n λ für alle i. {\displaystyle \text{subject}\sum i=1}^{n}c_{i}y_{i}=0,\text{und }0\leq c_{i}\leq{\frac 1}{2n\lambda };\text{für alle i. Das nennt man das doppelte Problem. Da das duale Maximierungsproblem eine quadratische Funktion des c i {\displaystyle c_{i} unter linearen Zwängen ist, ist es durch quadratische Programmieralgorithmen effizient solvierbar. Dabei werden die Variablen c i {\displaystyle c_{i} so definiert, dass w = Σ i = 1 n c i x i i {\displaystyle \mathbf {w} = <= <================================================================================== {x} {_i} .Moreover, c i = 0 {\displaystyle c_{i}=0 genau dann, wenn x i {\displaystyle \mathbf {x} {_i} auf der richtigen Seite des Randes liegt, und 0 < c i < ( 2 n λ ) - 1 {\displaystyle 0<c_{i}(2n\lambda ^{)-1} wenn x i {\displaystyle \mathbf {x} {_i} an der Grenze liegt. Es folgt, dass w {\displaystyle \mathbf {w} } als lineare Kombination der Stützvektoren geschrieben werden kann. Der Offset, b {\displaystyle b} , kann durch das Auffinden einer x i {\displaystyle \mathbf {x} {_i} an der Grenze des Randes und Lösen y i (w T x i- b ) = 1 ⟺ b = w T x i - y i . {\displaystyle y_{i}(\mathbf} {_i}-b)=1\iff b=\mathbf i}-y_{i. (Anmerkung, dass y i - 1 = y i {\displaystyle y_{i}^{-1}=y_{i seit y i = ± 1 * 1}.) Kernel trick Angenommen, wir möchten nun eine nichtlineare Klassifikationsregel lernen, die einer linearen Klassifikationsregel für die transformierten Datenpunkte φ (x i) entspricht. {\displaystyle \varphi (\mathbf {x} {_i).} Außerdem erhält man eine Kernelfunktion k {\displaystyle k}, die k (x i, x j ) = φ ( x i ) φ ( x j ) {\displaystyle k(\mathbf {x} erfüllt. {_i},\mathbf {x} {_j})=\varphi (\mathbf {x} {_i})\cdot \varphi (\mathbf {x} {_j)} . Wir kennen den Klassifikationsvektor w {\displaystyle \mathbf {w} im transformierten Raum satisfis w = Σ i = 1 n c i φ ( x i ) , {\displaystyle \mathbf {w} = \sum i=1}^{n}c_{i}y_{i}\varphi (\mathbf {x} {_i),} wobei die c i {\displaystyle c_{i} durch Lösen des Optimierungsproblems maximize f (c 1 ... c n) = Σ i) 1 n c i - 1 2 Σ i = 1 n Σ j = 1 n y i c i i ( φ ( x i ) φ ( x j ) y j c j = Σ i = 1 n c i - 1 2 Σ i = 1 n Σ j = 1 n y i c i k ( x i, x j ) y j c j n σ n Σ n Σ j = 1 n Σ j = 1 n y i c i c i k ( x i , x j ) y j c j n σ n Σ n Σ j = 1 n Σ j = 1 n y n y i c i c i c i c i c i k ( x ( x , x n , x , x , x , x , x , x , x , x , x , x , x , x , x , x , x , x j , x ) y j ) y j ) y j ) y j ) y j ) y j ) y j ) y j eff n eff n eff n eff n eff n eff n eff n eff n eff n eff n eff n eff n eff n eff n eff n eff c_{n}&=\sum i=1}{n}c_{i}-{\fra\frac 1 * i=1{n}\sum j=1{n}y_{i}c_{i}(\varphi (\mathbf {x} {_i})\cdot \varphi (\mathbf {x} j})y_{j}c_{j}\&=\sum I=1{n}c_{i}-{\fra 1 * i=1{n}\sum j=1{n}y_{i}c_{i}k(\mathbf} {_i},\mathbf {x} j})y_{j}c_{j}\\\end{ausgerichtet vorbehaltlich Σ i = 1 n c i y i = 0 und 0 ≤ c i ≤ 1 2 n λ für alle i. {\displaystyle \text{subject}\sum i=1}^{n}c_{i}y_{i}=0,\text{und }0\leq c_{i}\leq{\frac 1}{2n\lambda };\text{für alle i. Die Koeffizienten c i {\displaystyle c_{i} können wie zuvor für die Verwendung quadratischer Programmierung gelöst werden. Auch hier können wir einen Index i {\displaystyle i} so finden, dass 0 < c i < ( 2 n λ ) - 1 {\displaystyle 0<c_{i}<(2n\lambda ^{)-1} , so dass φ ( x i ) {\displaystyle \varphi (\mathbf {x} {_i)} auf der Grenze des Randes im transformierten Raum liegt - y i = [ Σ j = 1 n c j φ ( x j φ) [ j = 1 n c j y j k ( x j , x i ) ] - y i . (\mathbf {x} {_j}\j}\varphi (\mathbf {x} {_j})\cdot \varphi (\mathbf {x} i})\right]-y_{i}\&=\left[\sum] j=1^{n}c_{j}y_{j}k(\mathbf {x},\mathbf {x} i})\right]-y_{i}.\end{align Schließlich z ↦ sgn ‡ ( w T φ (z ) - b ) = sgn ‡ ( [ Σ i = 1 n c i k ( x i, z ) ] - b ) . {\displaystyle \mathbf {z} \mapsto \operatorname {sgn}(\mathbf {w}\T}\varphi (\mathbf {z} -b)=\Operatorname {sgn} links(\left[\sum i=1}^{n}c_{i}y_{i}k(\mathbf {x} {_i},\mathbf {z} rechts]-b\right.} Moderne Methoden Die neuesten Algorithmen für das Finden des SVM-Klassifikators umfassen subgradiente Abstieg und Koordinatenabstieg. Beide Techniken haben sich als wesentliche Vorteile gegenüber dem traditionellen Ansatz erwiesen, wenn es sich um große, sparsame Datensätze handelt – Subgradiente Methoden sind besonders effizient, wenn es viele Trainingsbeispiele gibt, und koordinieren Abstieg, wenn die Dimension des Merkmalsraums hoch ist. Subgradiente absteigende Subgradiente Abstiegsalgorithmen für die SVM arbeiten direkt mit dem Ausdruck f (w, b) = [ 1 n Σ i = 1 n max ( 0 , 1 - y i ( w T x i - b ) ) ] + λ w zusammengestellt 2 . {\displaystyle f(\mathbf {w},b)=\left[{{\frac 1}{n}\sum_i=1}{n}\max links(0,1-y_{i}(\mathbf {w}\T}\mathbf {x}}_i}-b)\right]+\lambda\\ Beachten Sie, dass f {\displaystyle f} eine konvexe Funktion von w {\displaystyle \mathbf {w} und b {\displaystyle b} ist. Dadurch können herkömmliche Gradientenabstiegs- (oder SGD)-Methoden angepasst werden, wobei anstelle eines Schritts in Richtung des Gradienten der Funktion ein Schritt in Richtung eines aus dem Subgradienten der Funktion ausgewählten Vektors genommen wird. Dieser Ansatz hat den Vorteil, dass für bestimmte Implementierungen die Anzahl der Iterationen nicht mit n {\displaystyle n} skaliert, die Anzahl der Datenpunkte. Koordinatenabstiegsalgorithmen für die SVM-Arbeit aus dem dualen Problem maximieren f ( c 1 ... c n ) = Σ i) 1 n c i - 1 2 Σ i = 1 n Σ j = 1 n y i c i ( x i Θ x j ) y j c j , {\displaystyle text{maximize}\,\,f(c_{1}\ldots c_{n})=\sum i=1{n}c_{i}-{\frac 1 * i=1{n}\sum **___________ x_{j})y_{j}c_{j, vorbehaltlich Σ i = 1 n c i y i = 0, und 0 ≤ c i ≤ 1 2 n λ für alle i. {\displaystyle \text{subject}\sum i=1}^{n}c_{i}y_{i}=0,\text{und }0\leq c_{i}\leq{\frac 1}{2n\lambda };\text{für alle i. Für jedes i ε {1, ..., n } {\displaystyle i\in {1,\,\ldots ,n} wird iterativ der Koeffizient c i {\displaystyle c_{i} in Richtung ∂ f / ∂ c i {\displaystyle \partial f/\partial c_{i} eingestellt. Der resultierende Vektor der Koeffizienten (c 1', ..., c n') {\displaystyle (c_{1}',\ldots ,\c_{n)'} wird auf den nächsten Vektor der Koeffizienten projiziert, der die gegebenen Zwänge erfüllt.(Typische Euclide-Abstände werden verwendet.) Das Verfahren wird dann solange wiederholt, bis ein Nahoptimaler Vektor von Koeffizienten erhalten wird. Der resultierende Algorithmus ist in der Praxis extrem schnell, obwohl wenige Leistungsgarantien nachgewiesen wurden. Empirische Risikominimierung Die oben beschriebene Soft-Margin-Unterstützungsvektormaschine ist ein Beispiel für einen empirischen Risikominimierungsalgorithmus (ERM) für den Scharnierverlust. Auf diese Weise gehören Stützvektormaschinen zu einer natürlichen Klasse von Algorithmen für statistische Inferenz, und viele seiner einzigartigen Eigenschaften sind auf das Verhalten des Scharnierverlustes zurückzuführen. Diese Perspektive kann weitere Einblicke in die Funktionsweise von SVMs geben und es uns ermöglichen, ihre statistischen Eigenschaften besser zu analysieren. Risikominimierung Im beaufsichtigten Lernen erhält man eine Reihe von Ausbildungsbeispielen X 1 ... X n {\displaystyle X_{1}\displays X_{n} mit Etiketten y 1 ... y n {\displaystyle y_{1}\ldots y_{n} und will y n + 1 {\displaystyle y_{n+1} bei X n + 1\displaystyle X_{n+1} vorhersagen Wir möchten dann eine Hypothese wählen, die das erwartete Risiko minimiert: ε (f ) = E [ l ( y n + 1 , f ( X n + 1 ) ] . {\displaystyle \varepsilon (f)=\mathbb {E} links[\ell y_{n+1},f(X_{n+1}))\right]. In den meisten Fällen wissen wir nicht die gemeinsame Verteilung von X n + 1, y n + 1 {\displaystyle X_{n+1},\,y_{n+1 outright. In diesen Fällen besteht eine gemeinsame Strategie darin, die Hypothese zu wählen, die das empirische Risiko minimiert: ε ^ (f ) = 1 n Σ k = 1 n l ( y k, f (X k ) . {\displaystyle {\hat {\varepsilon (}f)={\frac 1}{n}}\sum _k=1}^{n}\ell y_{k},f(X_{k). Unter bestimmten Annahmen über die Abfolge von Zufallsvariablen X k, y k {\displaystyle X_{k},\,y_{k (z.B., dass sie durch einen endlichen Markov-Prozess erzeugt werden), wenn der Satz von Hypothesen als klein genug betrachtet wird, wird der Minimierer des empirischen Risikos den Minimierer des erwarteten Risikos genau annähern, da n {\displaystyle n} groß wird. Dieser Ansatz wird als empirische Risikominimierung oder ERM bezeichnet. Reglementierung und Stabilität Damit das Minimierungsproblem eine gut definierte Lösung hat, müssen wir Einschränkungen auf das Set H {\displaystyle {\mathcal {H} von Hypothesen setzen, die berücksichtigt werden. Wenn H {\displaystyle {\mathcal {H} ein normierter Raum ist (wie es für SVM der Fall ist), ist eine besonders effektive Technik, um nur diejenigen Hypothesen f {\displaystyle f} zu betrachten, für die Veröffentlichungen Vert. {H}<k .Dies entspricht der Einführung einer Regelstrafe R (f ) = λ k H {\displaystyle {\mathcal {R}}(f)=\lambda I Veröffentlichungen Vert. {H} und Lösung des neuen Optimierungsproblems f ^ = r g min f ε ^ (f ) + R (f ) . {\displaystyle}\hat {f}=\mathrm {arg} \min {_f\in\mathcal H}{\hat {\varepsilon (}f)+{\mathcal {R}(f) Dieser Ansatz wird als Tikhonov Regularisierung bezeichnet. Im allgemeinen kann R (f ) {\displaystyle {\mathcal {R}(f) ein Maß für die Komplexität der Hypothese f {\displaystyle f} sein, so dass einfachere Hypothesen bevorzugt werden. SVM und der Scharnierverlust n ^ , b : x ) sgn {\displaystyle left[{\frac 1}{n}}\sum _i=1}^{n}\max left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} -b)\right]+\lambda |\mathbf {w} ^{\\2}} Im Lichte der obigen Diskussion sehen wir, dass die SVM-Technik der empirischen Risikominimierung mit Tikhonov-Normalisierung entspricht, wobei in diesem Fall die Verlustfunktion der Scharnierverlust l ( y, z) = max ( 0, 1 - y z) ist. {\displaystyle \ell (y,z) =\max links(0,1-yz\right)} Aus dieser Perspektive ist SVM eng mit anderen grundlegenden Klassifizierungsalgorithmen wie regelmäßigen Kleinstquares und logistischen Regressionen verbunden. Der Unterschied zwischen den drei liegt in der Wahl der Verlustfunktion: normierte Kleinstquare beträgt empirische Risikominimierung mit der Quadratverluste, l s q ( y , z ) = ( y - z ) 2 {\displaystyle \ell _sq}(y,z)=(y-z)^{2; logistische Regression verwendet die log-loss, l log ( y, z n {\displaystyle \ell {_\log y,z)=\ln(1+e^{-yz. Zielfunktionen Der Unterschied zwischen dem Scharnierverlust und diesen anderen Verlustfunktionen wird am besten in Bezug auf Zielfunktionen angegeben - die Funktion, die das erwartete Risiko für ein bestimmtes Paar von Zufallsvariablen X, y {\displaystyle X,\,y} minimiert. Insbesondere, lassen Sie y x {\displaystyle y_{x} y {\displaystyle y} bedingt auf das Ereignis, dass X = x {\displaystyle X=x} . In der Klassifikationseinstellung haben wir: y x = { 1 mit Wahrscheinlichkeit p x - 1 mit Wahrscheinlichkeit 1 — p x y_{x}={\begin{cases}1&{\text{mit Wahrscheinlichkeit p_{x\\-1&{\text{mit Wahrscheinlichkeit 1-p_{x\end{cases Der optimale Klassifikator ist also: f Ψ ( x ) = { 1 wenn p x ≥ 1 / 2 − 1 andernfalls {\displaystyle f^{*}(x)={\begin{cases}1&{\text{if p_{x\geq 1/2\-1&{\text{otherwise}\end{cases Für den quadratischen Verlust ist die Zielfunktion die bedingte Erwartungsfunktion, f s q ( x ) = E [ y x ] {\displaystyle f_{sq}(x)=\mathbb {E} links[y_{x}\right] ; Für den logistischen Verlust ist es die logit-Funktion, f log ( x ) = ln ‡ ( p x / ( 1 - p x ) ) {\displaystyle f_{\log (}x)=\ln links(p_{x}/({1-p_{x}}}})\right. Tatsächlich geben sie uns genug Informationen, um die Verteilung von y x {\displaystyle y_{x} vollständig zu beschreiben. Andererseits kann man überprüfen, ob die Zielfunktion für den Scharnierverlust genau f Ψ\displaystyle f^{*} ist. In einem ausreichend reichen Hypothesenraum - oder gleichfalls für einen entsprechend ausgewählten Kernel - wird der SVM-Klassifikator auf die einfachste Funktion (in Bezug auf R\displaystyle {\mathcal {R}) konvergieren, die die Daten korrekt klassifiziert. Dies erweitert die geometrische Interpretation von SVM - zur linearen Klassifikation wird das empirische Risiko durch jede Funktion minimiert, deren Ränder zwischen den Stützvektoren liegen, und das einfachste davon ist der Max-Margin-Klassifikator. Eigenschaften SVMs gehören zu einer Familie von generalisierten linearen Klassifikatoren und können als Erweiterung des Perceptrons interpretiert werden. Sie können auch als Sonderfall der Tikhonov-Regulierung betrachtet werden. Eine besondere Eigenschaft ist, dass sie gleichzeitig den empirischen Klassifikationsfehler minimieren und den geometrischen Rand maximieren; daher sind sie auch als maximale Margen-Klassifikatoren bekannt. Ein Vergleich der SVM mit anderen Klassifikatoren wurde von Meyer, Leisch und Hornik vorgenommen. Auswahl der Parameter Die Wirksamkeit von SVM hängt von der Auswahl von Kernel, den Kernelparametern und dem WeichrandparameterC ab. Eine gemeinsame Wahl ist ein Gaussischer Kernel, der einen einzigen Parameter γ {\displaystyle \gamma } hat.Die beste Kombination von C und γ {\displaystyle \gamma } wird oft durch eine Rastersuche mit exponentiell wachsenden Sequenzen von C und γ {\displaystyle \gamma } ausgewählt, z.B. C εstyle { 2 - 5 , 2 - 3}, ... C\in 2^{-5},2^{-3},\dots 2^{13},2^{15\ ; γ ε 2 - 15 , 2 - 13 , ..., 2 1 , 2 3 } {\displaystyle \gamma \in 2^{-15},2^{-13},\dots 2^{1},2^{3\ Typischerweise wird jede Kombination von Parameterwahlen mit Kreuzvalidierung überprüft und die Parameter mit bester Quervalidierungsgenauigkeit ausgewählt. Alternativ kann die jüngste Arbeit in der Bayesischen Optimierung verwendet werden, um C und γ {\displaystyle \gamma } auszuwählen, die oft die Auswertung von wesentlich weniger Parameterkombinationen als die Rastersuche erfordern. Das endgültige Modell, das zum Testen und zur Klassifizierung neuer Daten verwendet wird, wird dann über die ausgewählten Parameter auf dem gesamten Trainingsset trainiert. Probleme Potenzielle Nachteile des SVM umfassen folgende Aspekte: Erfordert die vollständige Kennzeichnung von Eingabedaten Unkalibrierte Klassenmitgliedschaftswahrscheinlichkeiten –SVM stammt aus Vapniks Theorie, die keine Schätzung von Wahrscheinlichkeiten auf endlichen Daten verhindert Der SVM ist nur für zweistufige Aufgaben direkt anwendbar. Daher müssen Algorithmen angewendet werden, die die Multi-Class-Task auf mehrere binäre Probleme reduzieren; siehe Abschnitt Multi-Class SVM. Parameter eines gelösten Modells sind schwer zu interpretieren. Erweiterungen Support-Vector Clustering (SVC) SVC ist eine ähnliche Methode, die auch auf Kernel-Funktionen baut, aber für unsupervised Learning geeignet ist. Es wird als eine grundlegende Methode in der Datenwissenschaft betrachtet. Multiclass SVM Multiclass SVM zielt darauf ab, Etiketten mit Hilfe von Stützvector-Maschinen zuzuordnen, wo die Etiketten aus einem endlichen Satz von mehreren Elementen gezogen werden. Die dominante Herangehensweise dafür ist, das Problem der einzelnen Multiklassen in mehrere binäre Klassifikationsprobleme zu reduzieren. Gemeinsame Methoden für diese Reduktion umfassen: Aufbau von binären Klassifikatoren, die zwischen einem der Labels und dem Rest (ein-versus-all) oder zwischen jedem Paar von Klassen (ein-versus-one) unterscheiden. Die Klassifizierung neuer Instanzen für den Einzelfall erfolgt durch eine Sieger-Stakes-all-Strategie, bei der der Klassifikator mit der höchsten Ausgangsfunktion der Klasse zuordnet (es ist wichtig, dass die Ausgangsfunktionen kalibriert werden, um vergleichbare Punkte zu erzeugen).Für den ein-versus-one-Ansatz wird die Klassifizierung durch eine max-wins-Abstimmungsstrategie vorgenommen, in der jeder Klassifikator die Instanz einer der beiden Klassen zuordnet, dann wird die Abstimmung für die zugeordnete Klasse um eine Stimme erhöht, und schließlich bestimmt die Klasse mit den meisten Stimmen die Instanzenklassifikation. Direkte azyklische Graph SVM (DAGSVM) Fehlerkorrigierende AusgangscodesCrammer und Singer schlugen eine Multiclass SVM-Methode vor, die das Multiclass-Klassifikationsproblem in ein einziges Optimierungsproblem geworfen, anstatt es in mehrere binäre Klassifikationsprobleme zu zerlegen. Siehe auch Lee, Lin und Wahba und Van den Burg und Groenen. Transduktive Support-Vektor-Maschinen Transduktive Support-Vektor-Maschinen erweitern SVMs, indem sie auch teilweise markierte Daten im semi-supervised Lernen behandeln können, indem sie den Prinzipien der Transduktion folgen. Hier wird neben dem Trainingsset D {\displaystyle {\mathcal {D} auch der Lernende ein Set D ⋆ = { x → i ⋆ ∣ X → i ⋆ ε R p i = 1 k \\displaystyle {\mathcal D}{\star}{=\vec * - Nein. *}_{i}{\star}\in \mathbb {R} p}_{i=1}^{k der zu klassifizierenden Testbeispiele. Formal wird eine transduktive Stützvektormaschine durch das folgende Primaloptimierungsproblem definiert: Minimize (in w →, b , y ⋆ → {\displaystyle {\vec w},b,{\vec y^{\star}}}}}}} 1 2 zusammengestellt w → ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 1,} y j ⋆ (w → ⋅ x j ⋆ → - b ) ≥ 1 , {\displaystyle y_{j}{\star {(}\vec {w}}\cdot {\vec x_{j}{\star -}b)\geq 1,} und y j ⋆ ε { - 1 }. {\displaystyle ****___________________ 1998 wurden von Wladimir N. Vapnik transduktive Support-Vektor-Maschinen eingeführt. Strukturierte SVM SVMs wurden auf strukturierte SVMs verallgemeinert, wo der Etikettenraum strukturiert ist und möglicherweise unendlich groß ist. Regression Eine Version von SVM für Regression wurde 1996 von Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman und Alexander J. Smola vorgeschlagen. Diese Methode wird als Support-Vector Regression (SVR) bezeichnet. Das von der Support-Vector-Klassifikation (wie oben beschrieben) erzeugte Modell hängt nur von einer Teilmenge der Trainingsdaten ab, da die Kostenfunktion für den Aufbau des Modells keine über den Rand liegenden Trainingspunkte berücksichtigt. Analog hängt das von SVR produzierte Modell nur von einer Teilmenge der Trainingsdaten ab, da die Kostenfunktion für den Bau des Modells alle Trainingsdaten in der Nähe der Modellvorhersage ignoriert. Eine weitere SVM-Version, die als Kleinstquares-Unterstützungsvector-Maschine (LS-SVM) bekannt ist, wurde von Suykens und Vandewalle vorgeschlagen. Ausbildung der ursprünglichen SVR bedeutet das Lösen von Minimierung 1 2 gefunden w ^ 2 gefunden {\displaystyle {\frac 1}{2}\ww\^^{2 vorbehaltlich | y i − ⟨ w , x i ・ − b | ≤ ε {\displaystyle |y_{i}-\langle w,x_{i}\rangle -b|\leq \varepsilon } wobei x i {\displaystyle x_{i} eine Trainingsprobe mit dem Zielwert y i {\displaystyle y_{i} ist.Das innere Produkt plus Abschnitt ⟨ w, x i ・ + b {\displaystyle \langle w,x_{i}\rangle +b} ist die Prädiktion für diese Probe und ε\display Slack-Variablen werden in der Regel in die vorstehende hinzugefügt, um Fehler zu ermöglichen und eine Annäherung zu ermöglichen, falls das obige Problem unfehlbar ist. Bayesian SVMIn 2011 wurde von Polson und Scott gezeigt, dass die SVM eine Bayesische Interpretation durch die Technik der Datenvergrößerung zugibt. In diesem Ansatz wird der SVM als grafisches Modell (wo die Parameter über Wahrscheinlichkeitsverteilungen verbunden sind) angesehen. Diese erweiterte Ansicht ermöglicht die Anwendung der Bayesischen Techniken auf SVMs, wie flexible Feature-Modellierung, automatische Hyperparameter-Tuning und vorausschauende Unsicherheits-Quantifizierung. Vor kurzem wurde eine skalierbare Version des Bayesischen SVM von Florian Wenzel entwickelt, die die Anwendung von Bayesian SVMs auf große Daten ermöglicht. Florian Wenzel entwickelte zwei verschiedene Versionen, ein Variations-Inferenzsystem (VI) für die Bayesische Kernel-Unterstützungsvektormaschine (SVM) und eine stochastische Version (SVI) für die lineare Bayesian SVM. Durchführung Die Parameter des Maximum-Margin Hyperplans werden durch Lösen der Optimierung abgeleitet. Es gibt mehrere spezialisierte Algorithmen zur schnellen Lösung des quadratischen Programmier-Problems (QP), das aus SVMs entsteht, meist auf Heuristiken für das Abbrechen des Problems in kleinere, handhabbarere Stücke. Ein weiterer Ansatz ist die Verwendung einer Innen-Punkt-Methode, die Newton-ähnliche Iterationen verwendet, um eine Lösung der Karush-Kuhn-Tucker-Bedingungen der Primal- und Dual-Probleme zu finden. Anstatt eine Folge von aufgebrochenen Problemen zu lösen, löst dieser Ansatz direkt das Problem insgesamt. Um ein lineares System mit der großen Kernel-Matrix zu lösen, wird im Kernel-Trick oft eine geringe Annäherung an die Matrix verwendet. Ein weiteres gemeinsames Verfahren ist der sequentielle minimale Optimierungs-Algorithmus von Platt, der das Problem in analytisch gelöste 2-dimensionale Teilprobleme zerlegt und die Notwendigkeit eines numerischen Optimierungs-Algorithmus und Matrixspeichers beseitigt. Dieser Algorithmus ist konzeptionell einfach, einfach zu implementieren, im Allgemeinen schneller und hat bessere Skalierungseigenschaften für schwierige SVM-Probleme. Der spezielle Fall von linearen Support-Vector-Maschinen kann durch die gleiche Art von Algorithmen, die verwendet werden, um seine enge Cousine zu optimieren, logistische Regression; diese Klasse von Algorithmen umfasst sub-gradienten Abstieg (z.B. PEGASOS) und Koordinatenabstieg (z.B. LIBLINEAR). LIBLINEAR hat einige attraktive Trainings-Zeit-Eigenschaften. Jede Konvergenz Iteration braucht Zeit linear in der Zeit, um die Zugdaten zu lesen, und die Iterationen haben auch eine Q-lineare Konvergenz Eigenschaft, so dass der Algorithmus extrem schnell. Die allgemeinen Kernel-SVMs können auch mit Subgradientenabstieg (z.B. P-packSVM) effizienter gelöst werden, insbesondere wenn eine Parallelisierung erlaubt ist. Kernel SVMs sind in vielen Machine-learning-Toolkits erhältlich, darunter LIBSVM, MATLAB, SAS, SVMlight, Kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV und andere. Die Vorverarbeitung von Daten (Standardisierung) ist sehr empfehlenswert, um die Genauigkeit der Klassifizierung zu verbessern. Es gibt einige Methoden der Standardisierung, wie min-max, Normalisierung durch Dezimalskalierung, Z-Score. Subtraktion von Mittelwert und Division durch Varianz jeder Funktion wird in der Regel für SVM verwendet. Siehe auch In situ adaptive Tabulation Kernel Maschinen Fisher Kernel Platt Skalierung Polynom-Kernel Predictive analytics Regularisierungsperspektiven auf Support-Vector-Maschinen Relevanzvektor-Maschine, ein probabilistisches Sparse-Kernel-Modell identisch in funktioneller Form zu SVM Sequential minimale Optimierung Raum MappingWinnow (algorithm) Referenzen Weiter lesen Bennett, Kristin P;. Campbell, Colin (2000). " Support Vector Maschinen: Hype oder Hallelujah?" (PDF). SIGKDD Explorations.2 (2): 1–13.doi:10.1145/380995.380999.S2CID 207753020.Cristianini, Nello; Shawe-Taylor, John (2000). Eine Einführung in den Support Vector Machines und andere kernelbasierte Lernmethoden. Cambridge University Press. ISBN 0-521-78019-5.Fradkin, Dmitriy; Muchnik, Ilya (2006). "Support Vector Machines for Classification" (PDF). In Abello, J;. Carmode, G. (Hrsg.). Diskrete Methoden in der Epidemiologie. DIMACS Serie in Discrete Mathematics and Theoretical Computer Science.70.pp.13–20.Ivanciuc, Ovidiu (2007)."Applications of Support Vector Machines in Chemistry" (PDF). Bewertungen in Computational Chemistry.23: 291–400.doi:10.1002/9780470116449.ch6.ISBN 9780470116449.James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2013). "Support Vector Machines" (PDF). Einführung in das Statistische Lernen : mit Anwendungen in R. New York: Springer.pp.337–372.ISBN 978-1-4614-7137-0.Schölkopf, Bernhard; Smola, Alexander J. (2002). Lernen mit Kernels. Cambridge, MA: MIT Drücken. ISBN 0-262-19475-9.Steinwart, Ingo; Christmann, Andreas (2008). Unterstützung Vector Machines. New York: Springer.ISBN 978-0-387-77241-7.Theodoridis, Sergios; Koutroumbas, Konstantinos (2009). Mustererkennung (4. ed.). Akademische Presse. ISBN 978-1-59749-272-0. Externe Links libsvm, LIBSVM ist eine beliebte Bibliothek von SVM Lernenden liblinear ist eine Bibliothek für große lineare Klassifizierung einschließlich einige SVMs SVM Licht ist eine Sammlung von Software-Tools zum Lernen und Klassifizierung mit SVM SVMJS Live Demo ist ein GUI Demo für JavaScript-Implementierung von SVMs