In der Informatik ist die rechnerische Komplexität oder einfach Komplexität eines Algorithmus die Menge der Ressourcen, die benötigt werden, um es auszuführen. Ein besonderer Fokus liegt auf Zeit- und Speicheranforderungen. Die Komplexität eines Problems ist die Komplexität der besten Algorithmen, die die Lösung des Problems ermöglichen. Die Untersuchung der Komplexität von explizit gegebenen Algorithmen wird als Analyse von Algorithmen bezeichnet, während die Untersuchung der Komplexität von Problemen als rechnerische Komplexitätstheorie bezeichnet wird. Beide Bereiche sind hochverwandt, da die Komplexität eines Algorithmus immer eine obere Grenze der Komplexität des durch diesen Algorithmus gelösten Problems ist. Außerdem ist es für die Gestaltung effizienter Algorithmen oft grundlegend, die Komplexität eines bestimmten Algorithmus mit der Komplexität des zu lösenden Problems zu vergleichen. Auch in den meisten Fällen ist das einzige, was über die Komplexität eines Problems bekannt ist, dass es niedriger ist als die Komplexität der am effizientesten bekannten Algorithmen. Es besteht daher eine große Überschneidung zwischen der Analyse von Algorithmen und der Komplexitätstheorie. Da die zur Ausführung eines Algorithmus benötigte Ressourcenmenge in der Regel mit der Größe der Eingabe variiert, wird die Komplexität in der Regel als Funktion n → f(n) ausgedrückt, wobei n die Größe der Eingabe ist und f(n) entweder die schlimmste Komplexität (das Maximum der Ressourcenmenge, die über alle Eingaben der Größe n benötigt werden) oder die durchschnittliche Komplexität (das Mittel der Ressourcenmenge über alle Eingaben der Größe n). Die Zeitkomplexität wird in der Regel als Anzahl der benötigten Elementaroperationen an einem Eingang der Größe n ausgedrückt, wobei die Elementaroperationen angenommen werden, auf einem bestimmten Computer eine konstante Zeit zu nehmen und sich bei einem anderen Computer nur um einen konstanten Faktor zu ändern. Die Raumkomplexität wird in der Regel als Speichermenge ausgedrückt, die von einem Algorithmus auf einem Eingang der Größe n benötigt wird. Zeit der Ressourcen Die Ressource, die am häufigsten betrachtet wird, ist Zeit. Wenn Komplexität ohne Qualifikation verwendet wird, bedeutet dies in der Regel Zeitaufwand. Die üblichen Zeiteinheiten (z.B. Sekunden, Minuten usw.) werden in der Komplexitätstheorie nicht verwendet, weil sie zu von der Wahl eines bestimmten Computers und von der Entwicklung der Technologie abhängig sind. So kann ein Computer heute einen Algorithmus deutlich schneller ausführen als ein Computer aus den 1960er-Jahren; dies ist jedoch nicht ein Eigenmerkmal des Algorithmus, sondern eine Folge technologischer Fortschritte in der Computerhardware. Komplexitätstheorie versucht, die intrinsischen Zeitanforderungen von Algorithmen zu quantifizieren, d.h. die grundlegenden Zeitzwänge, die ein Algorithmus auf jedem Computer platzieren würde. Dies wird durch Zählen der Anzahl der Elementaroperationen erreicht, die während der Berechnung ausgeführt werden. Diese Operationen werden angenommen, um auf einer bestimmten Maschine konstante Zeit (d.h. nicht von der Größe der Eingabe betroffen) zu nehmen und werden oft als Schritte bezeichnet. SpaceAnother wichtige Ressource ist die Größe des Computerspeichers, die für die Ausführung von Algorithmen benötigt wird. Sonstige Die Anzahl der arithmetischen Operationen ist eine weitere Ressource, die häufig verwendet wird. In diesem Fall spricht man von arithmetischen Komplexität. Kennt man eine obere Grenze der Größe der binären Darstellung der bei einer Berechnung auftretenden Zahlen, so ist die Zeitkomplexität im allgemeinen das Produkt der arithmetischen Komplexität um einen konstanten Faktor. Für viele Algorithmen ist die Größe der bei einer Berechnung verwendeten Ganzzahlen nicht gebunden, und es ist nicht realistisch zu berücksichtigen, dass arithmetische Operationen eine konstante Zeit einnehmen. Daher kann die Zeitkomplexität, in diesem Zusammenhang allgemein Bitkomplexität genannt, viel größer sein als die arithmetische Komplexität. Beispielsweise ist die arithmetische Komplexität der Berechnung des Determinanten einer n x n Ganzzahlmatrix O (n 3 ) \{displaystyle O(n^{3}) für die üblichen Algorithmen (Gaussische Eliminierung). Die Bitkomplexität der gleichen Algorithmen ist in n exponentiell, da die Größe der Koeffizienten während der Berechnung exponentiell wachsen kann. Wenn dagegen diese Algorithmen mit multimodularer Recheneinheit gekoppelt sind, kann die Bitkomplexität auf O~(n4) reduziert werden. Formal bezieht sich die Bit-Komplexität auf die Anzahl der Operationen auf Bits, die zur Ausführung eines Algorithmus benötigt werden. Bei den meisten Berechnungsmodellen entspricht sie der Zeitkomplexität bis zu einem konstanten Faktor. Auf Computern ist auch die Anzahl der benötigten Operationen an Maschinenworten proportional zur Bitkomplexität. Die Zeitkomplexität und die Bitkomplexität sind also für realistische Berechnungsmodelle äquivalent Bei der Sortierung und Suche gilt die Ressource, die im Allgemeinen als Anzahl der Einträge Vergleiche betrachtet wird. Dies ist im allgemeinen ein gutes Maß für die Zeitkomplexität, wenn Daten entsprechend organisiert werden. Komplexität in Abhängigkeit von der Eingangsgröße Es ist unmöglich, die Anzahl der Schritte eines Algorithmus auf alle möglichen Eingänge zu zählen. Da die Komplexität im allgemeinen mit der Größe des Eingabes zunimmt, wird die Komplexität typischerweise in Abhängigkeit von der Größe n (in Bits) des Eingabes ausgedrückt, so dass die Komplexität eine Funktion von n ist. Die Komplexität eines Algorithmus kann jedoch für verschiedene Eingänge gleicher Größe dramatisch variieren. Daher werden häufig mehrere Komplexitätsfunktionen verwendet. Die schlimmste Komplexität ist das Maximum der Komplexität über alle Eingaben der Größe n, und die durchschnittliche Komplexität des Falles ist der Mittelwert der Komplexität über alle Eingaben der Größe n (das macht Sinn, da die Anzahl der möglichen Eingaben einer bestimmten Größe endlich ist). Im Allgemeinen, wenn Komplexität verwendet wird, ohne weiter spezifiziert zu werden, ist dies die schlimmste Zeit Komplexität, die betrachtet wird. Asymptotische Komplexität Es ist in der Regel schwierig, genau den schlimmsten Fall und die durchschnittliche Komplexität zu berechnen. Darüber hinaus bieten diese exakten Werte wenig praktische Anwendung, da jede Änderung des Computers oder des Rechenmodells die Komplexität etwas verändern würde. Darüber hinaus ist die Ressourcennutzung für kleine Werte von n nicht kritisch, und dies macht, dass für kleine n die einfache Implementierung im Allgemeinen interessanter ist als eine geringe Komplexität. Aus diesen Gründen konzentriert man sich in der Regel auf das Verhalten der Komplexität für große n, also auf ihr asymptotisches Verhalten, wenn n zur Unendlichkeit neigt. Daher wird die Komplexität in der Regel durch die Verwendung großer O-Notation ausgedrückt. Beispielsweise hat der übliche Algorithmus für die Ganzzahlmultiplikation eine Komplexität von O (n 2 ) , \{displaystyle O(n^{2}, was bedeutet, dass es eine Konstante c u \{displaystyle c_{u} gibt, so dass die Multiplikation von zwei Ganzzahlen von höchstens n Ziffern in einer Zeit unter c u n 2 erfolgen kann. \{displaystyle c_{u}n^{2. Diese Grenze ist in dem Sinne scharf, dass die schlimmste Komplexität und die durchschnittliche Komplexität der Fälle Ω (n 2 ) , \{displaystyle \Omega (n^{2} sind, was bedeutet, dass es eine Konstante c l \{displaystyle c_{l} gibt, so dass diese Komplexitäten größer als c l n 2 sind. \{displaystyle c_{l}n^{2. Die Radik erscheint in dieser Komplexität nicht, da die Radikänderung nur die Konstanten c u \{displaystyle c_{u} und c l ändert. \{displaystyle c_{l.} Modelle der Berechnung Die Auswertung der Komplexität beruht auf der Wahl eines Berechnungsmodells, das darin besteht, die in einer Zeiteinheit durchgeführten Grundoperationen zu definieren. Wenn das Berechnungsmodell nicht explizit angegeben ist, wird dies im Allgemeinen als Multitape Turing-Maschine bezeichnet. Determinante Modelle Ein deterministisches Berechnungsmodell ist ein Berechnungsmodell, so dass die aufeinanderfolgenden Zustände der Maschine und die durchzuführenden Vorgänge vollständig durch den vorhergehenden Zustand bestimmt werden. Historisch waren die ersten deterministischen Modelle wiederkehrende Funktionen, Lambda-Calculus und Turing-Maschinen. Das Modell von Zutrittsmaschinen (auch RAM-Maschinen genannt) ist auch weit verbreitet, als näheres Gegenstück zu realen Computern. Wenn das Berechnungsmodell nicht angegeben ist, wird allgemein davon ausgegangen, dass es sich um eine Multitape Turing-Maschine handelt. Für die meisten Algorithmen ist die Zeitkomplexität auf Multitape Turing-Maschinen wie auf RAM-Maschinen, obwohl einige Sorgfalt erforderlich sein kann, wie Daten im Speicher gespeichert werden, um diese Gleichwertigkeit zu erhalten. Nicht-deterministische Berechnung Bei einem nicht-deterministischen Berechnungsmodell, wie z.B. nicht-deterministischen Turing-Maschinen, können bei einigen Schritten der Berechnung einige Entscheidungen getroffen werden. In der Komplexitätstheorie betrachtet man alle möglichen Entscheidungen gleichzeitig, und die nicht-deterministische Zeitkomplexität ist die Zeit, die benötigt wird, wenn immer die besten Entscheidungen getroffen werden. Mit anderen Worten ist man der Auffassung, dass die Berechnung gleichzeitig auf möglichst vielen (identischen) Prozessoren erfolgt, und die nicht-deterministische Rechenzeit ist die Zeit, die der erste Prozessor, der die Berechnung beendet, verbraucht. Dieser Parallelismus ist zum Teil über überlagerte verschränkte Zustände bei laufender spezifischer Quantenalgorithmen, wie z.B. Shors Factorisierung von noch nur kleinen Zahlen (Stand März 2018: 21 = 3 × 7). Auch wenn ein solches Berechnungsmodell noch nicht realistisch ist, hat es theoretische Bedeutung, meist bezogen auf das P = NP-Problem, die die Identität der Komplexitätsklassen, die durch die Einnahme von "Polynomzeit" und "nicht-deterministische Polynomzeit" gebildet werden, als zumindest obere Grenzen hinterfragt. Die Simulation eines NP-Algorithmus auf einem deterministischen Computer dauert in der Regel "exponentielle Zeit". Problematisch ist die Komplexitätsklasse NP, wenn sie in der Polynomzeit auf einer nicht-deterministischen Maschine gelöst werden kann. Ein Problem ist NP-komplete, wenn es etwa in NP ist und nicht einfacher ist als jedes andere NP-Problem. Viele kombinatorische Probleme, wie das Knapsack-Problem, das Wander-Verkäufer-Problem und das Boolesche Befriedigungsproblem sind NP-komplete. Für all diese Probleme hat der bekannteste Algorithmus exponentielle Komplexität. Wenn eines dieser Probleme in der Polynomzeit auf einer deterministischen Maschine gelöst werden könnte, könnten auch alle NP-Probleme in der Polynomzeit gelöst werden und man hätte P = NP. Ab 2017 wird allgemein verworfen, dass P ≠ NP, mit der praktischen Implikation, dass die schlimmsten Fälle von NP-Problemen sind intrinsisch schwierig zu lösen, d.h. länger als jede vernünftige Zeitspanne (Decades!) für interessante Eingabelängen. Parallele und verteilte Berechnung Parallele und verteilte Berechnungen bestehen aus der Aufteilung der Berechnung auf mehrere Prozessoren, die gleichzeitig arbeiten. Der Unterschied zwischen dem unterschiedlichen Modell liegt hauptsächlich in der Art, Informationen zwischen Prozessoren zu übertragen. Typischerweise ist bei der parallelen Berechnung die Datenübertragung zwischen Prozessoren sehr schnell, während bei der verteilten Berechnung die Datenübertragung über ein Netzwerk erfolgt und daher viel langsamer ist. Die Zeit, die für eine Berechnung auf N Prozessoren benötigt wird, ist mindestens der Quotient von N der Zeit, die von einem einzigen Prozessor benötigt wird. In der Tat kann diese theoretisch optimale Bindung nie erreicht werden, weil einige Subtasks nicht parallelisiert werden können, und einige Prozessoren müssen möglicherweise ein Ergebnis von einem anderen Prozessor warten. Das Hauptkomplexitätsproblem besteht also darin, Algorithmen so auszugestalten, dass das Produkt der Rechenzeit durch die Anzahl der Prozessoren möglichst nahe an der Zeit ist, die für die gleiche Berechnung auf einem einzigen Prozessor benötigt wird. Quantum Computing Ein Quantencomputer ist ein Computer, dessen Berechnungsmodell auf der Quantenmechanik basiert. Die Church-Turing-Thesis gilt für Quantencomputer, d.h. jedes Problem, das durch einen Quantencomputer gelöst werden kann, kann auch durch eine Turing-Maschine gelöst werden. Allerdings können einige Probleme theoretisch mit einer viel geringeren Zeitkomplexität mit einem Quantencomputer gelöst werden, anstatt mit einem klassischen Computer. Dies ist im Moment rein theoretisch, da niemand weiß, wie man einen effizienten Quantenrechner baut. Quantenkomplexitätstheorie wurde entwickelt, um die Komplexitätsklassen von Problemen zu untersuchen, die mit Quantenrechnern gelöst werden. Es wird in der post-quantum-Kryptographie verwendet, die besteht darin, kryptographische Protokolle zu entwerfen, die gegen Angriffe durch Quantencomputer beständig sind. Problemkomplexität (untere Grenzen) Die Komplexität eines Problems ist das Unvermögen der Komplexitäten der Algorithmen, die das Problem lösen können, einschließlich unbekannter Algorithmen. So ist die Komplexität eines Problems nicht größer als die Komplexität eines jeden Algorithmus, der die Probleme löst. Daraus folgt, dass jede Komplexität, die mit großer O-Notation ausgedrückt wird, eine Komplexität des Algorithmus sowie des entsprechenden Problems ist. Andererseits ist es in der Regel schwierig, nicht triviale geringere Grenzen für die Problemkomplexität zu erhalten, und es gibt wenige Methoden, um solche niedrigeren Grenzen zu erhalten. Zur Lösung der meisten Probleme ist es erforderlich, alle Eingabedaten zu lesen, die normalerweise eine der Größe der Daten proportionale Zeit benötigen. So haben solche Probleme eine Komplexität, die zumindest linear ist, d.h. mit großer Omega-Notation, eine Komplexität Ω (n ) . \{displaystyle \Omega (n.}) Die Lösung einiger Probleme, typischerweise in Computer-Algebra und rechnerische algebraische Geometrie, kann sehr groß sein. In einem solchen Fall wird die Komplexität durch die maximale Größe der Ausgabe begrenzt, da die Ausgabe geschrieben werden muss. Beispielsweise kann ein System von n polynomialen Gleichungen des Grades d in n indeterminates bis zu d n \{displaystyle d^{n} komplexe Lösungen haben, wenn die Anzahl der Lösungen endlich ist (das ist Bézout's Theorem). Da diese Lösungen niedergeschrieben werden müssen, ist die Komplexität dieses Problems Ω (d n ) . \{displaystyle \Omega (d^{n}) Für dieses Problem ist ein Algorithmus der Komplexität d O (n ) \{displaystyle d^{O(n}) bekannt, der somit als asymptotisch quasi-optimal betrachtet werden kann. Für die Anzahl der für einen Sortieralgorithmus erforderlichen Vergleiche ist eine nichtlineare untere Grenze von Ω (n log ≠ n) \{displaystyle \Omega (n\log n}) bekannt. So sind die besten Sortieralgorithmen optimal, da ihre Komplexität O (n log ≠ n) ist. \{displaystyle O(n\log n.}) Diese untere Grenze ergibt sich aus der Tatsache, dass es n gibt! Möglichkeiten, n Objekte zu bestellen. Da jeder Vergleich in zwei Teilen dieses Satzes von n!orders aufgeteilt wird, muss die Anzahl der N der Vergleiche, die zur Unterscheidung aller Bestellungen benötigt werden, 2 N > n !, \{displaystyle 2^{N}>n! überprüfen, was N = Ω (n log ∂ n ), \{displaystyle N=\Omega (n\log n},) von Stirling's Formel bedeutet. Eine Standardmethode zur Erzielung niedrigerer Komplexitätsgrenzen besteht darin, ein Problem auf ein anderes Problem zu reduzieren. Genauer gesagt, dass man ein Problem A der Größe n in ein Unterproblem der Größe f(n) eines Problems B kodieren kann, und dass die Komplexität von A Ω (g (n )) ist. \{displaystyle \Omega g(n.}) Ohne Verlust der Allgemeinheit kann man annehmen, dass die Funktion f mit n zunimmt und eine inverse Funktion h hat. Dann ist die Komplexität des Problems B Ω (g (h (n ) ) . \{displaystyle \Omega g(h(n}) Dies ist die Methode, die verwendet wird, um zu beweisen, dass, wenn P ≠ NP (eine ungelöste Konjektur), die Komplexität jedes NP-komplete Problem ist Ω (n k ) , \{displaystyle \Omega (n^{k},) für jede positive ganze Zahl k. Verwendung in Algorithmus-Design Die Auswertung der Komplexität eines Algorithmus ist ein wichtiger Bestandteil der Algorithmus-Design, da dies nützliche Informationen über die Leistung gibt, die erwartet werden kann. Es ist eine häufige Missverständnisse, dass die Auswertung der Komplexität von Algorithmen aufgrund des Moore-Gesetzes weniger wichtig wird, was das exponentielle Wachstum der Macht moderner Computer widerspiegelt. Dies ist falsch, weil diese Leistungssteigerung die Arbeit mit großen Eingangsdaten (große Daten) ermöglicht. Zum Beispiel, wenn man eine Liste von einigen Hunderten von Einträgen, wie die Bibliographie eines Buches, alphabetisch sortieren will, sollte jeder Algorithmus in weniger als einer Sekunde gut funktionieren. Auf der anderen Seite würden für eine Liste von einer Million von Einträgen (z.B. Telefonnummern einer Großstadt) die elementaren Algorithmen, die O (n 2 ) \{displaystyle O(n^{2})-Vergleiche benötigen, eine Billion von Vergleichen, die etwa drei Stunden bei der Geschwindigkeit von 10 Millionen von Vergleichen pro Sekunde benötigen. Auf der anderen Seite erfordern die Schnellsortierung und Zusammenführung nur n log 2 ‡ n \{displaystyle n\log _{2}n Vergleiche (als durchschnittliche Komplexität für die ersteren, als schlimmste Komplexität für letztere). Für n = 1.000.000 ergibt sich damit etwa 30.000.000 Vergleiche, die bei 10 Millionen Vergleichen pro Sekunde nur 3 Sekunden dauern würden. So kann die Auswertung der Komplexität vor jeder Implementierung viele ineffiziente Algorithmen eliminieren. Dies kann auch zur Abstimmung komplexer Algorithmen verwendet werden, ohne alle Varianten zu testen. Durch die Ermittlung der kostspieligsten Schritte eines komplexen Algorithmus ermöglicht die Untersuchung der Komplexität auch die Konzentration auf diese Schritte den Aufwand zur Verbesserung der Effizienz einer Implementierung. Siehe auch rechnerische Komplexität der mathematischen Operationen Referenzen Arora, Sanjeev; Barak, Boaz (2009,) Computational Complexity: A Modern Approach, Cambridge, ISBN 978-0-521-42426-4, Zbl 1193.68112 Du. Ding-Zhu; Ko, Ker-I (2000,) Theorie der Computational Complexity, John Wiley & Sons, ISBN 978-0-471-34506-0 Garey, Michael R.; Johnson, David S. (1979,) Computer und Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman, ISBN 0-7167-1045-5 Goldreich, Oded Handed (2008, Computational Complex Presse, ISBN 978-0-444-88071-0 Papadimitriou, Christos (1994,) Computational Complexity (1. ed.,) Addison Wesley, ISBN 0-201-53082-1 Sipser, Michael (2006,) Einführung in die Computationstheorie (2. ed.,) USA: Thomson Course Technology, ISBN 0-534-95097-3