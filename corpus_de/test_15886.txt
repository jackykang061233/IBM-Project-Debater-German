Statistiken sind die Disziplin, die die Erhebung, Organisation, Analyse, Auslegung und Darstellung von Daten betrifft. Zur Anwendung von Statistiken auf ein wissenschaftliches, industrielles oder soziales Problem ist es üblich, mit einer statistischen Bevölkerung oder einem statistischen Modell zu beginnen. Bevölkerungsgruppen können unterschiedliche Gruppen von Menschen oder Gegenständen wie "alle Menschen, die in einem Land leben" oder "alle atom komosing akristall" sein. Statistik befasst sich mit jedem Aspekt der Daten, einschließlich der Planung der Datenerhebung im Hinblick auf die Gestaltung von Umfragen und Experimenten. Zählt man keine Volkszählungsdaten ein, sammeln die Statistiker Daten durch die Entwicklung spezifischer experimenteller Muster und Erhebungsproben. Mit der repräsentativen Probenahme wird sichergestellt, dass Auseinandersetzungen und Schlussfolgerungen von der Probenahme auf die Gesamtbevölkerung angemessen ausreichen können. In einer experimentellen Studie werden Messungen des untersuchten Systems, die Nutzung des Systems und anschließend zusätzliche Messungen mit demselben Verfahren durchgeführt, um festzustellen, ob die Manipulation die Werte der Messungen verändert hat. Jedoch beinhaltet eine Beobachtungsstudie keine experimentelle Manipulation. Zwei wesentliche statistische Methoden werden in der Datenanalyse verwendet: beschreibende Statistiken, die Daten aus einer Stichprobe mit Indexen wie der mittleren oder Standardabweichung und inferenziellen Statistiken zusammenfassen, die Schlussfolgerungen aus Daten ziehen, die zufällig variieren (z.B. Beobachtungsfehler, Probenahmeschwankungen). Deskriptive Statistiken sind am häufigsten mit zwei Arten von Eigenschaften einer Verteilung (Pflanzen oder Bevölkerung): Die zentrale Tendenz (oder Lage) zielt darauf ab, den zentralen oder typischen Wert der Verteilung zu bestimmen, während die Streuung (oder Variabilität) den Umfang, in dem die Mitglieder des Vertriebs von ihrem Zentrum und jedem anderen abfliegen. mathematische Statistiken werden im Rahmen der Wahrscheinlichkeitstheorie erstellt, die sich mit der Analyse von Zufallsphänomenen befasst. Standard statistisches Verfahren umfasst die Erhebung von Daten, die zur Prüfung der Beziehung zwischen zwei statistischen Datensets oder einer Datensammlung und synthetischen Daten aus einem idealisierten Modell führen. Eine Hypothese wird für die statistische Beziehung zwischen den beiden Datensätzen vorgeschlagen, und dies wird als Alternative zu einer idealisierten Nullhypothese ohne Beziehung zwischen zwei Datensets verglichen. Durch die Nichthypothesen werden statistische Tests durchgeführt, die den Sinn quantifizieren, in dem die Nichtigkeit nachgewiesen werden kann, da die Daten, die im Test verwendet werden, verwendet werden. Arbeiten aus einer Nullhypothese werden zwei grundlegende Fehlerarten anerkannt: Fehler der Typ I (Nullhypothesis ist falsch abgelehnt, weil sie ein "false positives" und Typ II Fehler (Nullhypothesis wird nicht abgelehnt und eine tatsächliche Beziehung zwischen den Bevölkerungsgruppen wird versäumt, "false negative" zu geben). Mehr Probleme sind mit diesem Rahmen verbunden, von der Erlangung einer ausreichenden Probegröße bis zur Angabe einer angemessenen Nullhypothese. Messverfahren, die statistische Daten generieren, unterliegen ebenfalls Fehlern. Viele dieser Fehler werden als Zufall (Lärm) oder systematisch (Zitel) eingestuft, aber andere Fehlerarten (z.B. Unterbrechung, beispielsweise wenn ein Analysten falsche Einheiten meldet) können ebenfalls auftreten. Mangelnde Daten oder Zensur können zu unvoreingenommenen Schätzungen und spezifischen Techniken führen, um diese Probleme zu lösen. Einführungsstatistiken sind ein mathematisches wissenschaftliches Gremium, das die Sammlung, Analyse, Auslegung oder Erklärung sowie die Darstellung von Daten oder als Zweig der Mathematik betrifft. Manche betrachten Statistiken als eine unterschiedliche mathematische Wissenschaft anstelle einer Mathematik. Obwohl viele wissenschaftliche Untersuchungen Daten verwenden, sind Statistiken mit der Nutzung von Daten im Zusammenhang mit Unsicherheit und Entscheidungsfindung im Zusammenhang mit Unsicherheit betroffen. Bei der Anwendung von Statistiken auf ein Problem ist es üblich, mit einer Bevölkerung zu beginnen oder zu untersuchen. Bevölkerungsgruppen können unterschiedliche Themen wie "alle Menschen, die in einem Land leben" oder "jede atom komosing akristall". Idealerweise sammeln Statistiker Daten über die gesamte Bevölkerung (eine Operation namens Volkszählung). Dies kann von staatlichen statistischen Ämtern organisiert werden. Deskriptive Statistiken können verwendet werden, um die Bevölkerungsdaten zu zusammenfassen. Numerical deskriptors umfassen eine mittlere und Standardabweichung für kontinuierliche Daten (wie Einkommen), während Häufigkeit und Prozentsatz bei der Beschreibung von kategorischen Daten (wie Bildung) sinnvoller sind. Wenn eine Volkszählung nicht möglich ist, wird eine ausgewählte Untergruppe der Bevölkerung untersucht. Nach Feststellung einer Probe, die für die Bevölkerung repräsentativ ist, werden die Daten für die Probemitglieder in einer Beobachtungs- oder experimentellen Umgebung erhoben. Mehr beschreibende Statistiken können verwendet werden, um die Stichprobendaten zu zusammenfassen. Jedoch enthält die Stichprobe ein Zufallselement; daher sind die numerischen Deskriptoren der Probe auch für Unsicherheiten anfällig. Um aussagekräftige Schlussfolgerungen über die gesamte Bevölkerung zu ziehen, sind unerhebliche Statistiken erforderlich. Es verwendet Muster in den Stichprobendaten, um Unterschiede in Bezug auf die Bevölkerung zu ziehen, die während der Erfassung der Zufallsheit vertreten sind. Diese Abweichungen können die Form der Beantwortung von ja/keinen Fragen zu den Daten (Hypothesis-Test), die Schätzung numerischer Merkmale der Daten (Schätzung), die Beschreibung von Vereinen innerhalb der Daten (Korrelation) und die Modellierung der Beziehungen innerhalb der Daten (z.B. bei der Regressionsanalyse). Gleichgültigkeit kann sich auf Prognose, Vorhersage und Schätzung von nichtobservierten Werten entweder in oder mit der untersuchten Bevölkerung erstrecken. Es kann auch Extrapolation und Interpolation von Zeitreihen oder räumlichen Daten sowie den Data Mining umfassen. Mathematische Statistiken sind die Anwendung von Mathematik auf Statistiken. mathematische Techniken, die für dies verwendet werden, umfassen mathematische Analyse, lineare Algenanalyse, krchtische Analyse, differenzierte Gleichungen und die Mess-theoretic-Kriteriumtheorie. Geschichte Kurzumfassungen zu statistischer Ausgewogenheit sind an arabischen Säugetieren und Kryptographen im islamischen Zeitalter zwischen dem 8. und 13. Jahrhundert zurückgetreten. Al-Khalil (717–786) schrieb das Buch der Kryptographie-Nachrichten, die die erste Verwendung von Permutationen und Kombinationen enthält, um alle möglichen arabischen Wörter mit und ohne Wowels aufzulisten. In seinem Buch gab Al-Kindi eine ausführliche Beschreibung der Verwendung von Frequenzanalysen zu deciphering verschlüsselten Nachrichten. Al-Kindi hat auch den frühesten Bekanntheitsgrad von statistischen Angaben gemacht, während er und später arabischen Kryptographen die frühen statistischen Methoden für die Entschlüsselung verschlüsselter Nachrichten entwickelt haben. Ibn Adlan (1187–1268) leistete später einen wichtigen Beitrag zur Verwendung der Stichprobengröße in der Frequenzanalyse. Das früheste europäische Schreiben über die Statistiken geht zurück auf 1663, wobei die Veröffentlichung von Natur- und Politischen Beobachtungen auf die Mortalitätsgesetze von John Graunt erfolgte. Frühzeitige Anwendung des statistischen Denkens setzt sich auf die Bedürfnisse der Staaten ein, die Politik auf dem Gebiet der demografischen und wirtschaftlichen Daten zu stützen, so dass die stat- etymologie. Umfang der Disziplin der Statistiken, die im frühen 19. Jahrhundert erweitert wurde, um die Erhebung und Analyse von Daten im Allgemeinen einzubeziehen. Statistiken werden heute in Regierungen, Unternehmen und Natur- und Sozialwissenschaften weit verbreitet. Die mathematischen Grundlagen moderner Statistiken wurden im 17. Jahrhundert mit der Entwicklung der Wahrscheinlichkeitstheorie von Gerolamo Cardano, Blaise Pascal und Pierre de Fermat geschaffen. mathematische Wahrscheinlichkeitstheorie wurde aus der Untersuchung von Glücksspielen hervorgegangen, obwohl das Konzept der Wahrscheinlichkeit bereits im mittelalterlichen Recht und von Philosophen wie Juan Caramuel untersucht wurde. Die Methode von mindestens Quadratmetern wurde erstmals von Adrien-Marie Legendre in 1805 beschrieben. Ende des 19. und frühen 20. Jahrhunderts in drei Phasen entstanden. Die erste Welle, die zum Ende des Jahrhunderts führte, wurde durch die Arbeit von Francis Galton und Karl Pearson geleitet, die Statistiken in eine strikte mathematische Disziplin umwandeln, die nicht nur in Wissenschaft, sondern auch in Industrie und Politik. Galtons Beiträge enthalten die Einführung der Konzepte der Standardabweichung, der Korrelation, der Regressionsanalyse und der Anwendung dieser Methoden auf die Untersuchung der Vielfalt menschlicher Merkmale – Größe, Gewicht, Sehlänge unter anderem. Pearson entwickelte den Pearson-Produkt-Modern-Koeffizient, definiert als Produkt-Moment, die Methode der Momente für den Einbau von Vertriebenen in Proben und die Pearson-Vertrieb, unter anderem. Galton und Pearson gründeten Biometrika als erste Zeitschrift für mathematische Statistiken und Biostatistische (die sogenannte Bimetrie) und gründeten die erste Abteilung für Universitätsstatistiken an der University College London. Ronald Fisher hat den Begriff Null hypothesis im Rahmen des Experiments von Lady verdeutlicht, der "es hat sich nie bewährt oder etabliert, aber im Laufe der Experimentation möglicherweise unprovisiert". William Sealy Gosset wurde die zweite Welle der 1860er und 20er Jahre eingeleitet und erreichte seinen Höhepunkt in den Erkenntnissen von Ronald Fisher, die die Lehrbücher schrieben, die die akademische Disziplin in Universitäten weltweit definieren sollen. Fishers wichtigste Veröffentlichungen waren sein Halbjahrespapier "The Correlation zwischen relatives on the Supposition of Mendelian Inheritance (das die erste zur Nutzung des statistischen Begriffs, der Varianz), seine klassische Arbeit statistischer Methoden für Forschungsarbeitnehmer und seine 1935 The Design of Experiments, wo er ein strenges Design von Experimenten entwickelt hat. Er stammt aus den Konzepten der Sufficiency, Nebenstatistiken, der linearen Unterscheidungskraft des Fischers und der Fischereiinformationen. In seinem 30er-Jahresbuch Die genetische Theorie der natürlichen Auswahl legte er Statistiken für verschiedene biologische Konzepte wie das Prinzip der Fischer (die A. F. Edwards nannten "probantes Argument in der evolutionären Biologie" und "Fischerei" ein Konzept in der sexuellen Auswahl über einen positiven Feedback-Skandal in der Entwicklung. Die letzte Welle, die vor allem die Raffinerie und Expansion früherer Entwicklungen erlebte, entstand aus der gemeinsamen Arbeit zwischen Egon Pearson und Jerzy Neyman in den 1930er Jahren. In ihnen wurden die Begriffe "Typ II"-Konflikt, Macht eines Test- und Vertrauensintervalls eingeführt. Jerzy Neyman im Jahr 1934 zeigte, dass die stratifizierten zufälligen Stichproben im Allgemeinen eine bessere Methode der Schätzung waren als die (Quoten) Probenahme. Heute werden statistische Methoden in allen Bereichen angewandt, die die Entscheidungsfindung einschließen, um genaue Anteilnahmen aus einem zusammengetragenen Datenbestand zu erzielen und Entscheidungen angesichts der Unsicherheit aufgrund statistischer Methoden zu treffen. Die Verwendung moderner Computer hat groß angelegte statistische Berechnungen beschleunigt und ermöglicht auch neue Methoden, die nicht praktikabel sind. Statistiken sind nach wie vor ein Bereich der aktiven Forschung, beispielsweise zum Problem der Analyse großer Daten. Statistische Datensammlung Sampling Wenn vollständige Volkszählungsdaten nicht erhoben werden können, erheben die Statistiker Stichprobendaten, indem sie spezielle experimentelle Entwürfe und Erhebungsproben entwickeln. Statistiken selbst bieten auch Instrumente zur Vorhersage und Vorhersage durch statistische Modelle. Zur Verwendung einer Probe als Leitfaden für eine ganze Bevölkerung ist es wichtig, dass sie die Gesamtbevölkerung tatsächlich repräsentiert. Mit der repräsentativen Probenahme wird sichergestellt, dass Gleichgültigkeit und Schlussfolgerungen von der Probe auf die gesamte Bevölkerung sicher ausgedehnt werden können. Ein größeres Problem besteht darin, festzustellen, inwieweit die Auswahl tatsächlich repräsentativ ist. Statistiken bieten Methoden zur Schätzung und Korrektur von Verzerrungen im Rahmen der Stichproben- und Datenerhebungsverfahren. experimentelles Design für Versuche gibt es auch Methoden, die diese Probleme zu Beginn einer Studie verringern und ihre Fähigkeit stärken, Wahrheiten über die Bevölkerung zu erkennen.. Theorie ist Teil der mathematischen Disziplin der Wahrscheinlichkeitstheorie. Probability wird in mathematischen Statistiken verwendet, um die Stichprobenverteilung von Stichprobenstatistiken und allgemein die Eigenschaften statistischer Verfahren zu untersuchen. Jede statistische Methode ist gültig, wenn das System oder die betroffene Bevölkerung die Annahmen der Methode erfüllt. Die Differenz zwischen der klassischen Wahrscheinlichkeitstheorie und der Probenahmetheorie ist etwa, dass die Wahrscheinlichkeitstheorie von den jeweiligen Parametern einer Gesamtbevölkerung beginnt, um die Wahrscheinlichkeit von Proben zu verringern. Statistische Ausgewogenheit bewegt sich jedoch in der entgegengesetzten Richtung – induktiverweise von Proben auf die Parameter einer größeren oder Gesamtbevölkerung. Versuchs- und Beobachtungsstudien Ein gemeinsames Ziel für ein statistisches Forschungsprojekt besteht darin, die Kausalalität zu untersuchen und insbesondere einen Abschluss der Veränderungen der Werte von Berechenten oder unabhängigen Variablen auf abhängige Variablen zu ziehen. Es gibt zwei große Arten von kausalen statistischen Studien: experimentelle Studien und Beobachtungsstudien. In beiden Arten von Studien werden die Auswirkungen einer unabhängigen Variablen (oder Variablen) auf das Verhalten der abhängigen Variablen beobachtet. Die Differenz zwischen den beiden Arten liegt darin, wie die Studie tatsächlich durchgeführt wird. Jeder kann sehr wirksam sein. In einer experimentellen Studie werden Messungen des untersuchten Systems, die Nutzung des Systems und anschließend zusätzliche Messungen mit demselben Verfahren durchgeführt, um festzustellen, ob die Manipulation die Werte der Messungen verändert hat. Jedoch beinhaltet eine Beobachtungsstudie keine experimentelle Manipulation. stattdessen werden Daten gesammelt und Korrelationen zwischen Berechtigten und Reaktion untersucht. Obwohl die Werkzeuge der Datenanalyse am besten auf Daten aus randomisierten Studien arbeiten, werden sie auch auf andere Arten von Daten – wie natürliche Experimente und Beobachtungsstudien – angewendet, für die ein Statistiker eine geänderte, strukturiertere Schätzungsmethode (z.B. Differenzenschätzung und maßgebliche Variablen, unter anderem) verwenden würde, die konsistente Ester produzieren. Experimente Die grundlegenden Schritte eines statistischen Experiments sind: Planung der Forschung, einschließlich der Feststellung der Anzahl der Nachbildungen der Studie unter Verwendung folgender Informationen: vorläufige Schätzungen über die Größe der Behandlungseffekte, alternative Hypothesen und die geschätzte experimentelle Variabilität. Prüfung der Auswahl experimenteller Themen und der Ethik der Forschung ist notwendig. Statistiker empfehlen, dass Versuche (mindestens) eine neue Behandlung mit Standardbehandlung oder Kontrolle vergleichen, um eine unvoreingenommene Schätzung des Unterschieds bei Behandlungseffekten zu ermöglichen. Entwurf von Experimenten, mit Blockierung, um den Einfluss von Konfoundierungsvariablen zu verringern, und zufällige Zuteilung von Behandlungen an Probanden, um unvorhergesehene Behandlungseffekte und experimentelle Fehler zu ermöglichen. In dieser Phase schreiben die Experimentier- und Statistiker das Versuchsprotokoll an, das die Leistung des Experiments steuern und die primäre Analyse der experimentellen Daten vorweisen wird. Durchführung des Experiments nach dem experimentellen Protokoll und Analyse der Daten nach dem Versuchsprotokoll. weitere Prüfung der in Sekundäranalysen enthaltenen Daten, um neue Hypothesen für die künftige Studie vorzuschlagen. Dokumentation und Darstellung der Ergebnisse der Studie. Versuche mit menschlichem Verhalten haben besondere Bedenken. In der berühmten Hamiltone-Studie wurden Änderungen am Arbeitsumfeld in der Hamiltone-Anlage der Western Electric Company untersucht. Forscher waren daran interessiert, festzustellen, ob die erhöhte Beleuchtung die Produktivität der Arbeiter der Montagelinie erhöhen würde. Die Forscher haben zunächst die Produktivität in der Anlage gemessen und dann die Beleuchtung in einem Gebiet der Anlage geändert und überprüft, ob die Veränderungen der Beleuchtung die Produktivität beeinflussen. Es hat sich herausgestellt, dass die Produktivität tatsächlich verbessert wurde (unter den experimentellen Bedingungen). Jedoch wird die Studie heute wegen Fehler in experimentellen Verfahren, insbesondere wegen fehlender Kontrollgruppe und Blindheit, stark kritisiert. Die Wirkung von Hacke bezieht sich darauf, dass ein Ergebnis (in diesem Fall die Arbeitsproduktivität) aufgrund der Beobachtung selbst verändert wurde. Letztere in der Hamilton-Studie wurde produktiver, nicht weil die Beleuchtung verändert wurde, sondern weil sie beobachtet wurden. Beobachtungsstudie Ein Beispiel für eine Beobachtungsstudie ist eine, die den Verband zwischen Rauchen und Lungenkrebs untersucht. Diese Art der Studie nutzt in der Regel eine Umfrage, um Beobachtungen über den Bereich des Interesses zu sammeln und anschließend statistische Analysen durchzuführen. In diesem Fall würden die Forscher Beobachtungen sowohl von Rauchern als auch von Nichtrauchern sammeln, vielleicht durch eine Kohortenstudie, und die Zahl der Fälle von Lungenkrebs in jeder Gruppe. Eine Fallkontrollstudie ist eine weitere Art von Beobachtungsstudie, in der Menschen mit und ohne das Ergebnis von Interesse (z.B. Lungenkrebs) eingeladen werden, sich zu beteiligen und ihre Expositionsgeschichte zu sammeln. Arten von Daten Verschiedene Versuche wurden unternommen, um eine Steueronomie der Messwerte zu erstellen. The Psychophysicist Stanley Smith Stevens definiert nominale, ordinale, zeitliche und verhältnismäßige Größen. Nominale Messungen verfügen nicht über eine sinnvolle Rangordnung unter Werten und ermöglichen eine einmalige (injizierte) Transformation. Ordinale Messungen haben unpräzise Unterschiede zwischen aufeinander folgenden Werten, haben aber eine sinnvolle Reihenfolge dieser Werte und ermöglichen eine geordnete Umgestaltung. Interval-Messungen haben erhebliche Entfernungen zwischen definierten Messungen, aber der Nullwert ist willkürlich (wie im Falle von Lang- und Temperaturmessungen in Celsius oder Fahrscheinheit) und ermöglichen eine lineare Umwandlung. Ratio-Messungen haben sowohl einen sinnvollen Nullwert als auch die Entfernungen zwischen verschiedenen Messungen festgelegt und ermöglichen eine Umformung. Da Variablen, die nur nominale ordinale Messungen entsprechen, nicht numerisch gemessen werden können, manchmal werden sie als kategorische Variablen zusammengefasst, während die Verhältnis- und Intervallmessungen aufgrund ihrer numerischen Beschaffenheit als quantitative Variablen zusammengefasst werden, die entweder einzeln oder kontinuierlich sein können. Solche Unterscheidungen lassen sich oft mit Datentyp in der Computerwissenschaft verkoppeln, da die Dichotomen kategorischen Variablen mit dem Datentyp, polytomen kategorischen Variablen mit willkürlich zugewiesenen Zahlen im festen Datentyp und kontinuierlichen Variablen mit dem realen Datentyp, der variablen Nummernberechnung enthält, vertreten sein können. Jedoch hängt die Kartierung von Computerdatentypen zu statistischen Datentypen davon ab, welche Kategorisierung dieser Daten erfolgt. Andere Kategorisierungen wurden vorgeschlagen. Beispiel: Mosteller und Tukey (1977) unterschiedene Besoldungsgruppen, Rankings, gezählte Fraktionen, Werte und Gleichgewichte. Nelder (1990) bezeichnete kontinuierliche Werte, kontinuierliche Verhältnisse, Zählquoten und kategorische Datenarten. (Siehe auch: Chrisman (1998), van den Berg (1991)). Die Frage, ob es sinnvoll ist, verschiedene Arten statistischer Methoden auf Daten anzuwenden, die aus verschiedenen Arten von Messverfahren gewonnen werden, ist kompliziert, wenn es um die Umwandlung von Variablen und die genaue Auslegung von Forschungsfragen geht. " Die Beziehung zwischen den Daten und dem, was sie beschreiben, spiegelt lediglich die Tatsache wider, dass bestimmte Arten statistischer Daten tatsächlich Werte haben, die nicht unter einigen Umbrüchen unvariabel sind. Ob oder nicht eine Transformation sinnvoll ist, hängt von der Frage ab, die man zu beantworten versucht. Methoden beschreibende Statistiken Eine beschreibende Statistik (im Zählbegriff) ist eine zusammenfassende Statistik, die die Merkmale einer Informationssammlung quantitativ beschreibt oder zusammenfasst, während beschreibende Statistiken im Massenbegriff der Prozess ist, diese Statistiken zu verwenden und zu analysieren. beschreibende Statistiken unterscheiden sich von den induktiven Statistiken (oder induktiven Statistiken), in denen beschreibende Statistiken eine Probe zusammenfassen wollen, anstatt die Daten zu nutzen, um über die Bevölkerung zu erfahren, dass die Datenstichprobe vertreten ist.Jährliche statistische Erfassung ist der Prozess der Datenanalyse, um Eigenschaften einer zugrunde liegenden Wahrscheinlichkeitsverteilung abzuschwächen. Inferiale statistische Analyse beeinträchtigt Eigenschaften einer Bevölkerung, beispielsweise durch Prüfung von Hypothesen und Abschätzungen. Es wird davon ausgegangen, dass die beobachteten Daten aus einer größeren Bevölkerung entnommen werden. Interferenzstatistiken lassen sich mit beschreibenden Statistiken widersetzen. beschreibende Statistiken betreffen ausschließlich die Eigenschaften der beobachteten Daten, und die Annahme, dass die Daten aus einer größeren Bevölkerung stammen. Terminologie und Theorie der inferenziellen Statistiken, Ester und entscheidende Mengen, die unabhängig voneinander verteilt sind (IID) Zufallsvariablen mit einer bestimmten Wahrscheinlichkeitsverteilung: Standard-Statistik und Schätzungstheorie definieren eine Zufallsstichprobe als Zufallsvektor der Spaltenvektoren dieser IID-variablen. Die untersuchte Bevölkerung wird durch eine wahrscheinliche Verteilung beschrieben, die möglicherweise unbekannte Parameter aufweisen kann. Eine Statistik ist eine Zufallsvariable, die eine Funktion der Zufallsstichprobe ist, aber keine Funktion unbekannter Parameter. Die Wahrscheinlichkeit der Verteilung der Statistiken kann jedoch unbekannte Parameter haben. betrachten Sie nun eine Funktion des unbekannten Parameters: Ein Ester ist eine Statistik, die zur Schätzung dieser Funktion verwendet wird. Häufig verwendete Ester umfassen Probenahmen, unvorhergesehene Probenvariationen und Probenkovarianz. Eine Zufallsvariable, die eine Funktion der Zufallsstichprobe und des unbekannten Parameters ist, deren Wahrscheinlichkeitsverteilung jedoch nicht von dem unbekannten Parameter abhängig ist, wird als eine entscheidende Menge oder Schlüssel bezeichnet. Großteil der verwendeten Schlüssel sind die z-score, die chi Quadrat-Statistik und deren t-Wert. Zwischen zwei Estern eines bestimmten Parameters wird der mit einem geringeren durchschnittlichen kalkulierten Fehler effizienter. Außerdem wird ein Estimator unvoreingenommen, wenn sein erwarteter Wert dem tatsächlichen Wert des geschätzten unbekannten Parameters entspricht und unvoreingenommen unvoreingenommen ist, wenn der erwartete Wert auf der Grenze zum tatsächlichen Wert des Parameters übereinstimmt. Andere wünschenswerte Eigenschaften für Ester: UMVUE Estimatoren, die die niedrigste Varianz für alle möglichen Werte des zu erwartenden Parameters aufweisen (dies ist in der Regel ein einfacheres Eigentum, als Effizienz zu überprüfen) und konsistente Estimatoren, die sich in der Wahrscheinlichkeit an den tatsächlichen Wert dieses Parameters annähern. Es bleibt noch die Frage, wie Estimatoren in einer bestimmten Situation erhalten und die Berechnung durchführen können, wurden mehrere Methoden vorgeschlagen: die Methode der Momente, die höchstmögliche Wahrscheinlichkeitsmethode, die am wenigsten Quadratemethode und die neuere Methode zur Schätzung von Gleichungen. Nichthypothesen und alternative Hypothesis Interpretation statistischer Informationen können oft die Entwicklung einer Nullhypothese beinhalten, die in der Regel (aber nicht notwendigerweise) ist, dass keine Beziehung zwischen Variablen besteht oder dass sich im Laufe der Zeit keine Veränderung ergeben. Das beste Beispiel für ein Novice ist die Vorführung eines Strafverfahrens. H0 behauptet, dass der Angeklagte unschuldig ist, während die alternative Hypothese, H1, behauptet, dass der Beklagte schuldig ist. Es geht um den Verdacht der Schuld. Der H0 (Status quo) steht im Widerspruch zu H1 und wird beibehalten, es sei denn, H1 wird durch Beweismittel "ein angemessener Zweifel" unterstützt. In diesem Fall ist jedoch nicht die Unschuld, sondern nur, dass die Beweismittel nicht ausreichen, um verurteilt zu werden. So akzeptiert die Jury nicht unbedingt H0, sondern lehnt H0 ab. Obwohl eine Nullhypothese nicht nachgewiesen werden kann, kann man prüfen, wie nah es mit einem Power-Test, der für Typ II Fehler getestet wird. Was die Statistiker eine alternative Hypothese fordern, ist einfach eine Hypothese, die der Nullhypothese entgegensteht. Fehler arbeiten an einer Nullhypothese, zwei große Fehlerkategorien werden anerkannt: Typ I Fehler, bei denen die Nullhypothese falsch abgelehnt wird, geben einen „falsen positiven“. Typ II Fehler, bei denen die Nullhypothese nicht abgelehnt wird und ein tatsächlicher Unterschied zwischen den Bevölkerungsgruppen verfehlt wird, was „falsche negative Auswirkungen“ bedeutet. Standardabweichung bezieht sich auf den Umfang, in dem einzelne Beobachtungen in einer Stichprobe von einem zentralen Wert abweichen, wie die Probe oder die Bevölkerung, während Standardfehler auf eine Schätzung der Differenz zwischen Stichprobe und Bevölkerung verweist. Ein statistischer Fehler ist der Betrag, durch den sich eine Beobachtung von ihrem erwarteten Wert unterscheidet, ein Rest ist der Betrag einer Beobachtung vom Wert, den der Erreger des erwarteten Werts auf einer bestimmten Probe (auch als Vorhersage bezeichnet) unterscheidet. Mean kalkulierter Fehler wird zur Erlangung effizienter Ester, einer weit verbreiteten Klasse von Estern verwendet. Grund: Quadratfehler ist einfach die Quadratwurzel des durchschnittlichen Quadratkilometern Fehlers. Viele statistische Methoden zielen darauf ab, die Restsumme der Quadrate zu minimieren, und dies sind "Methoden von mindestens Quadraten" im Gegensatz zu den absoluten Abweichungen von Least. Letztere verleiht kleinen und großen Fehlern gleich Gewicht, während der frühere große Fehler mehr Gewicht verleiht. Rückstandssumme von Quadraten ist ebenfalls unterschiedlich, was eine praktische Eigenschaft für die Regression bietet. Least Quadrate, die auf lineare Regression angewendet werden, sind gewöhnliche mindestens Quadrate und mindestens Quadrate, die auf nichtlineare Regression angewendet werden, werden als nichtlineare mindestens Quadrate bezeichnet. In einem linearen Regressionsmodell wird auch der nicht deterministische Teil des Modells als Fehlerzeit, Störung oder mehr reine Lärm bezeichnet. Sowohl lineare Regression als auch nicht-lineare Regression werden in polynomialen mindestens Quadraten angegangen, die auch die Varianz in einer Vorhersage der abhängigen variablen (y-Achse) als Funktion der unabhängigen variablen (x-Achse) und der Abweichungen (Terroren, Lärm, Störungen) von der geschätzten (ausgewogenen) Kurve beschreiben. Messverfahren, die statistische Daten generieren, unterliegen ebenfalls Fehlern. Viele dieser Fehler werden als Zufall (Lärm) oder systematisch (Zitel) eingestuft, aber andere Fehlerarten (z.B. Abschwächung, beispielsweise wenn ein Analysten falsche Einheiten meldet) können ebenfalls wichtig sein. Mangelnde Daten oder Zensur können zu unvoreingenommenen Schätzungen und spezifischen Techniken führen, um diese Probleme zu lösen. Interval Schätzung Die meisten Studien stellen nur einen Teil einer Bevölkerung dar, so dass die Gesamtbevölkerung nicht voll vertreten ist. Jeglichen Schätzungen, die aus der Probe entnommen wurden, entspricht nur dem Bevölkerungswert. Vertrauensintervalle ermöglichen es Statistikern, zu zeigen, wie eng die Stichprobe den wahren Wert der gesamten Bevölkerung entspricht. Häufig werden sie als 95%-Konvergenz ausgedrückt. Formell ist ein 95 %-Konvergenz für einen Wert eine Reihe, in der die Probenahme und Analyse unter denselben Bedingungen wiederholt wurden (Ein anderes Datenset), würde das Intervall den wahren (Population) Wert in 95 % aller möglichen Fälle umfassen. Dies bedeutet nicht, dass die Wahrscheinlichkeit, dass der tatsächliche Wert im Vertrauensintervall liegt, 95 % beträgt. Aus der häufigen Perspektive ist ein solcher Anspruch nicht einmal sinnvoll, da der wahre Wert keine Zufallsvariable ist. Entweder der wahre Wert ist oder nicht innerhalb des vorgegebenen Zeitraums. Es ist jedoch zu beachten, dass vor jeder Probenahme und einem Plan für den Aufbau des Vertrauensintervalls 95 % der Wahrscheinlichkeit, dass das bis zu berechnete Intervall den wahren Wert abdecken wird: zu diesem Zeitpunkt sind die Grenzen des Intervalls bis zu erhaltende Zufallsvariablen. Ein Ansatz, der ein gewisses Maß an Wahrscheinlichkeit, den wahren Wert zu enthalten, ergibt, ist die Verwendung eines glaubwürdigen Intervalls aus den Statistiken der Buchten: Dieser Ansatz hängt von einer anderen Art der Auslegung ab, was mit Wahrscheinlichkeit gemeint ist, d. h. als Wahrscheinlichkeit. Vertrauensintervalle können symmetrisch oder asymmetrische sein. Ein Intervall kann asymmetrisch sein, weil es für einen Parameter niedriger oder oberer Grenze (zwischenseitiges Intervall oder rechtsseitiges Intervall) arbeitet, aber es kann auch asymmetrische sein, weil das zweiseitige Intervall die Symmetrie rund um die Schätzung verursacht. Manchmal werden die Bindungen für ein Vertrauensintervall asymmetrischer Art erreicht, und diese werden verwendet, um die tatsächlichen Bindungen anzugleichen. Markenstatistiken geben selten eine einfache Ja/No-Typ Antwort auf die unter Analyse stehende Frage. Verdolmetschung kommt oft auf die Höhe der statistischen Bedeutung, die auf die Zahlen angewendet wird, und verweist oft auf die Wahrscheinlichkeit eines Wertes, der die Nullhypothese (einige Zeiten, die als p-Wert bezeichnet werden). Standardkonzept ist die Prüfung einer Nullhypothese gegen eine alternative Hypothese. Eine kritische Region ist die Reihe von Werten des Estimators, die dazu führen, die Nullhypothese zu überbrücken. Typ I Fehler ist daher die Wahrscheinlichkeit, dass der Estimator zu der kritischen Region gehört, da die Nullhypothese (statistische Bedeutung) ist und die Wahrscheinlichkeit des Typ-II-fehlers die Wahrscheinlichkeit ist, dass der Estimator nicht der kritischen Region gehört, da die alternative Hypothese wahr ist. Die statistische Leistung eines Tests ist die Wahrscheinlichkeit, dass es die Nullhypothese korrekt ablehnt, wenn die Nullhypothese falsch ist. In Bezug auf die statistische Bedeutung bedeutet nicht unbedingt, dass das Gesamtergebnis in Echtzeit signifikant ist. Beispielsweise kann in einer großen Untersuchung eines Arzneimittels gezeigt werden, dass das Medikament statistisch signifikante, aber sehr geringe positive Auswirkungen hat, so dass das Medikament nicht zu einer Benachrichtigung des Patienten beitragen kann. Obwohl grundsätzlich das annehmbare Niveau statistischer Bedeutung Gegenstand einer Debatte sein kann, ist die Bedeutung des größten p-Werts, der den Test zur Ablehnung der Nullhypothese ermöglicht. Dieser Test entspricht logischerweise der Aussage, dass der p-Wert die Wahrscheinlichkeit ist, wenn die Nullhypothese tatsächlich ist, mindestens so extrem wie die Teststatistik zu beobachten. Je geringer die Bedeutung ist, desto geringer ist die Wahrscheinlichkeit, den Typ I-Fehler zu begehen. Manche Probleme sind in der Regel mit diesem Rahmen verbunden (siehe Kritik an hypothesis test): Ein sehr statistisch signifikanter Unterschied kann immer noch keine praktische Bedeutung haben, aber es ist möglich, entsprechende Tests richtig zu formulieren. Eine Antwort bedeutet, dass über die Berichterstattung nur die Bedeutungsstufe hinausgeht, um den p-Wert bei der Meldung, ob eine Hypothese abgelehnt oder akzeptiert wird. Der p-Wert deutet jedoch nicht auf die Größe oder Bedeutung des beobachteten Effekts hin und kann auch die Bedeutung kleiner Unterschiede in großen Studien überschätzen. Ein besserer und zunehmend gemeinsamer Ansatz besteht darin, Vertrauensvorgaben zu melden. Obwohl dies aus denselben Berechnungen wie hypothesis-Tests oder p-Werten hergestellt wird, beschreiben sie sowohl die Größe der Wirkung als auch die Unsicherheit, die sie umgeben. Fallhaftigkeit der übergangenenen Konditionalität: Kritiken ergeben sich, weil das hypothesis-Testkonzept eine Hypothese (die Nullhypothese) befürwortet, da die Bewertung die Wahrscheinlichkeit des beobachteten Ergebnisses ist, da die Nullhypothese und nicht die Wahrscheinlichkeit der Nullhypothese aufgrund des beobachteten Ergebnisses. Eine Alternative zu diesem Ansatz wird von der Bayesian-Konferenz angeboten, obwohl sie eine Vorbedingung für die Festlegung einer Vorlaufzeit vorschreibt. Nicht automatisch beweist die Nullhypothese die alternative Hypothese. Wie alles in unfruchtbaren Statistiken, die es auf Stichprobengröße setzt, und daher unter Fett-Rückgangswerten kann ernsthaft missverstanden werden. Beispiele für bekannte statistische Tests und Verfahren sind: Sondierungs-Datenanalyse (EDA) ist ein Ansatz zur Analyse von Datensets zur Zusammenfassung ihrer wichtigsten Merkmale, oft mit visuellen Methoden. Ein statistisches Modell kann verwendet werden oder nicht, vor allem EDA ist es, zu sehen, was die Daten uns über die formale Modell- oder hypothesis-Testaufgabe informieren können. Irreführende Fehlverwendung von Statistiken kann verfälschende, aber ernste Fehler in der Beschreibung und Auslegung verursachen – in dem Sinne, dass selbst erfahrene Fachkräfte solche Fehler machen, und in dem Sinne, dass sie zu verheerenden Entscheidungsfehlern führen können. Sozialpolitik, medizinische Praxis und die Zuverlässigkeit von Strukturen wie Brücken alle hängen von der ordnungsgemäßen Verwendung von Statistiken ab. Selbst wenn statistische Techniken korrekt angewandt werden, können die Ergebnisse für diejenigen, die keine Expertise haben, schwer ausgelegt werden. Die statistische Bedeutung eines Trends in den Daten – die den Umfang, in dem ein Trend durch zufällige Variationen der Probe verursacht werden könnte – kann oder kann sich nicht mit einem intuitiven Sinne seiner Bedeutung einigen. Die Reihe grundlegender statistischer Fähigkeiten (und Skepsis), die die Menschen in ihrem Alltagsleben mit Informationen umgehen müssen, wird als statistische Kompetenz bezeichnet. Es gibt eine allgemeine Wahrnehmung, dass statistisches Wissen all-too- selten vorsätzlich missbräuchlich ist, indem Wege gefunden werden, um nur die Daten zu interpretieren, die für den jetzigen Anbieter günstig sind. Misstrauen und Missverständnis der Statistiken sind mit der Notierung verbunden, "Es gibt drei Arten von liegen: liegen, gebremst und Statistiken. Mangelnde Statistiken können sowohl unabsichtlich als auch vorsätzlich sein, und das Buch Wie in Lie mit Statistiken skizziert eine Reihe von Überlegungen. Versuch, die Verwendung und den Missbrauch von Statistiken zu beleuchten, werden statistische Techniken, die in bestimmten Bereichen verwendet werden, durchgeführt (z.B. Warne, Lazo, Ramos und Ritter (2012)). Wege, um den Missbrauch von Statistiken zu vermeiden, sind die Verwendung geeigneter Grafiken und Vermeidung von Verzerrungen. Missbräuche können auftreten, wenn Schlussfolgerungen überallgemeinisiert werden und als repräsentativ für mehr als sie wirklich sind, oft durch absichtliche oder unbewußte Einstellung der Probenahmen. Bar Graphen sind zweifellos die einfachsten Bilder, um zu verwenden und zu verstehen, und sie können entweder durch Hand oder mit einfachen Computerprogrammen hergestellt werden. Leider sehen die meisten Menschen keine Verzerrungen oder Fehler, so dass sie nicht bemerkt werden. Man kann daher oft glauben, dass etwas richtig ist, auch wenn es nicht gut vertreten ist. Zur Erhebung von Daten aus Statistiken muss die entnommene Probe repräsentativ sein. Laut Huff „Die Abhängigkeit einer Probe kann durch [bias.] selbst ein gewisses Maß an Skepsis zerstört werden. " Huff hat eine Reihe von Fragen vorgeschlagen, die in jedem Fall zu beantworten sind: Wer sagt dies? () Hält er/sie eine Unruhe?) Wie weiß er? () Kann er die Ressourcen haben, um die Fakten zu kennen?) Was fehlt? () Kann er uns ein vollständiges Bild geben?) Kann jemand das Thema ändern? () Kann er uns die richtige Antwort auf das falsche Problem geben?) Kann es sinnvoll sein?(I his/her Schlussfolgerung logisch und mit dem, was wir bereits wissen?) Fehlinterpretationen: Korrelation Das Konzept der Korrelation ist besonders bemerkenswert für die mögliche Verwirrung, die es verursachen kann. Statistische Analyse eines Datensatzes zeigt oft, dass zwei Variablen (Beschreibungen) der betroffenen Bevölkerung in der Regel unterschiedlich sind, wenn sie miteinander verbunden waren. Zum Beispiel könnte eine Studie über das jährliche Einkommen, die auch im Alter von Todesfällen aussieht, finden, dass arme Menschen eher kürzeres Leben als afflurierende Menschen haben. Die beiden Variablen werden als korreliert bezeichnet; sie können jedoch nicht die Ursache eines anderen sein. Die Korrelationsphänomene könnten durch ein drittes, bisher unübertretbares Phänomen verursacht werden, das als ein erschreckender variabler oder erschreckender Unterschied bezeichnet wird. Aus diesem Grund gibt es keinen Weg, sofort die Existenz eines Kausalzusammenhangs zwischen den beiden Variablen zu stören. Anwendungsbezogene Statistiken, theoretische Statistiken und mathematische Statistiken umfassen beschreibende Statistiken und die Anwendung von Feldstatistiken. Die theoretischen Statistiken betreffen die logischen Argumente, die die Rechtfertigung von Ansätzen zur statistischen Gleichgültigkeit sowie mathematische Statistiken betreffen. Mathematische Statistiken umfassen nicht nur die Manipulation von Wahrscheinlichkeitsverteilungen, die zur Ableitung von Ergebnissen im Zusammenhang mit Methoden der Schätzung und Gleichgültigkeit erforderlich sind, sondern auch verschiedene Aspekte der Rechenstatistiken und der Gestaltung von Experimenten. Statistische Berater können Organisationen und Unternehmen helfen, die für ihre jeweiligen Fragen keine interne Expertise besitzen. Maschinen- und Datenverarbeitungsmodelle sind statistische und probabilistische Modelle, die Muster in den Daten durch Verwendung von Rechengorithmen erfassen. Statistiken in akademischen Statistiken gelten für eine Vielzahl von akademischen Disziplinen, einschließlich Natur- und Sozialwissenschaften, Regierung und Wirtschaft. Wirtschaftsstatistiken gelten statistische Methoden in den Bereichen Ökonometrie, Audit und Produktion und Betrieb, einschließlich Verbesserung der Dienstleistungen und Marketingforschung. Eine Studie von zwei Fachzeitschriften in der tropischen Biologie ergab, dass die 12 häufigsten statistischen Tests: Analyse der Varianz (ANOVA), Chi-Square Test, Student’s T Test, Linear Regression, Pearsons Correlation Coeffizient, Mann-Whitney U Test, Kruskal-Wallis Test, Shannons Diversity Index, Tukey Test, Clusteranalyse, Spearman's Rank Correlation Test und Hauptanalyse. Ein typischer statistischer Kurs umfasst beschreibende Statistiken, Wahrscheinlichkeit, Bindemittel und normale Verteilungen, Prüfung von Hypothesen und Vertrauensintervallen, lineare Regression und Korrelation. Moderne grundlegende statistische Kurse für Unterabsolventinnen und Studenten konzentrieren sich auf die richtige Testauswahl, die Auswertung der Ergebnisse und die Nutzung freier Statistiken. Statistik Die schnelle und nachhaltige Erhöhung der Rechenleistung ab der zweiten Hälfte des 20. Jahrhunderts hat erhebliche Auswirkungen auf die Praxis der statistischen Wissenschaft. Frühe statistische Modelle waren fast immer von der Klasse linearer Modelle, aber leistungsfähige Computer, gekoppelt mit geeigneten numerischen Algorithmen, haben ein größeres Interesse an nichtlinearen Modellen (wie Neuralnetzen) sowie an der Schaffung neuer Arten, wie allgemeinisierte lineare Modelle und Multi-Level-Modelle. Erhöhte Rechenleistung hat auch zu einer zunehmenden Beliebtheit von rechnerisch intensiven Methoden geführt, die auf der Neubelebung basieren, wie z.B. Permutationstests und der Sprint, während Techniken wie Gibbs Probenahmen die Verwendung von Bayesian-Modellen praktikabel machen. Die Computerrevolution hat Auswirkungen auf die Zukunft der Statistiken mit einem neuen Schwerpunkt auf experimentellen und empirischen Statistiken. Inzwischen gibt es eine große Zahl von allgemeinem und besonderem Zweck statistische Software. Beispiele für verfügbare Software, die in der Lage ist, komplexe statistische Daten zu verarbeiten, sind Programme wie KPMG, SAS, SPSS und R. Business Statistics Statistiken sind ein allgemein genutztes Management- und Unterstützungsinstrument. Insbesondere wird sie in den Bereichen Finanzmanagement, Marketingmanagement und Produktion, Dienstleistungen und Betriebsmanagement angewendet. Statistiken werden auch in der Management-Rechnungsführung und -prüfung stark verwendet. Disziplin der Managementwissenschaft formalisiert die Verwendung von Statistiken und anderen Mathematikern in der Wirtschaft.() Econometrics ist die Anwendung statistischer Methoden auf wirtschaftliche Daten, um empirischen Inhalten den wirtschaftlichen Beziehungen zu ermöglichen.) Ein typischer "Business Statistics"-Kurs ist für die Geschäftsbereiche bestimmt und umfasst beschreibende Statistiken (Sammlung, Beschreibung, Analyse und Zusammenfassung der Daten), Wahrscheinlichkeit (in der Regel die verbindlichen und normalen Vertriebenen), Prüfung von Hypothesen und Vertrauensintervallen, lineare Regression und Korrelation; (Nachfolge) Kurse können Prognosen, Zeitreihen, Entscheidungs Bäume, mehrfach lineare Regression und andere Themen der Unternehmensanalyse allgemein umfassen. Siehe auch Business Mathematik § University. professionelle Zertifizierungsprogramme wie die CFA umfassen häufig Themen in Statistiken. Statistiken, die auf Mathematik oder Kunst traditionellerweise angewandt wurden, waren mit der Erstellung von Gleichgültigkeiten mit einer halbstandardisierten Methode, die in den meisten Wissenschaften erworben wurde. Diese Tradition hat sich mit der Verwendung von Statistiken in nicht-inferialen Kontexten verändert. Was einmal als trockener Gegenstand angesehen wurde, der in vielen Bereichen als Grad-Aufforderungen eingenommen wurde, wird nun begeistert gesehen.zunächst von einigen mathematischen Puristen abgeschnitten, gilt es nun als wesentliche Methode in bestimmten Bereichen. In der Numerologie können unterschiedliche Daten, die durch eine Vertriebsfunktion generiert werden, mit vertrauten Werkzeugen, die in Statistiken verwendet werden, um die zugrunde liegenden Muster zu erkennen, die dann zu Hypothesen führen können. Methoden der Statistiken, einschließlich prädikativer Methoden in der Prognose, werden mit der Chaostheorie und der Fracktal-Gleichheit kombiniert. The Process Art of Jackson Pollock stützte sich auf künstlerische Experimente, bei denen die zugrunde liegenden Verbreitungen in der Natur künstlerischer Art gezeigt wurden. Mit Computern wurden statistische Methoden angewandt, um solche verteilungsorientierten natürlichen Prozesse zu formalisieren und zu analysieren, um die Bewegung von Video-Arten zu gestalten. Methoden der Statistiken können vordringlich in Performance-Arten verwendet werden, wie in einer Karte, die auf einem Markov-Prozess basiert, der nur einige Zeit funktioniert, deren Ausstellung mittels statistischer Methoden vorhergesagt werden kann. Statistiken können verwendet werden, um Kunst vorzuschlagen, wie in der von Iannis Xenakis entwickelten statistischen oder krchtischen Musik, wo die Musik leistungsorientiert ist. Obwohl diese Art von Künstler nicht immer so erwartet wird, funktioniert sie in der Weise, die vorhersehbar und mit Statistiken übereinstimmen. Spezialisierte Fachrichtungen Statistiktechniken werden in einer Vielzahl von Arten von wissenschaftlichen und sozialen Forschung eingesetzt, darunter: Biostatistische, Rechenbiologie, Rechentechnik, Netzwerkbiologie, soziale Wissenschaft, Soziologie und soziale Forschung. Manche Untersuchungsfelder verwenden angewandte Statistiken so umfassend, dass sie eine spezielle Terminologie haben. Diese Disziplinen umfassen: Darüber hinaus gibt es spezielle Arten statistischer Analysen, die auch ihre spezielle Terminologie und Methodik entwickelt haben: Statistiken bilden ein wichtiges Basisinstrument für Unternehmen und Produktion. Es wird verwendet, um die Variabilität der Messsysteme zu verstehen, die Kontrollverfahren (wie die statistische Prozesskontrolle oder die SPC), die Zusammenfassung der Daten zu verstehen und datengesteuerte Entscheidungen zu treffen. In diesen Rollen ist es ein wichtiges Werkzeug und vielleicht das einzige zuverlässige Werkzeug. Siehe auch Stiftungen und wichtige Bereiche der Statistiken Lydia Denworth, "Ein wichtiges Problem: Standard-wissenschaftliche Methoden sind unter Brand. Kann nichts ändern,?" wissenschaftlicher Amerikaner, vol. 321, no. 4 (Oktober 2019), S.62–67." Die Verwendung von p-Werten für fast ein Jahrhundert [seit 1916] zur Bestimmung der statistischen Bedeutung von experimentellen Ergebnissen hat dazu beigetragen, dass in vielen wissenschaftlichen Bereichen Sicherheit und [to] Reducibility-Krise entsteht. Es wächst die Entschlossenheit, statistische Analysen zu reformieren... Manche [Forscher] schlägt Änderung statistischer Methoden vor, während andere mit einem Schwellenwert für die Definition signifikanter Ergebnisse umgehen würden." (p 63.)Barbara Illowsky; Susan Dean (2014). Einführung von Statistiken. OpenStax CNX.ISBNTIL938168208. Hamburger, David W. "Introductory Statistics: Konzepte, Modelle und Anwendungen". State University (3rd Web ed). Archiviert vom Original am 28. Mai 2020. OpenIntro Statistics Archivd 2019-06-16 auf dem Wegback-Maschine, dritte Ausgabe von Diez, Barr und Cetinkaya-Rundel Stephen Jones, 2010. Statistiken in der Psychologie: Übersichten ohne Equationen. Palgrave Macmillan.ISBNTIL137282392. Cohen, J (1990). " Dinge, die ich gelernt habe (so weit)" (PDF). American Psychologist.45 (12): 1304–1312.doi:10.1037/0003-066x.45.12.1304.Archived aus dem Original (PDF) für 2017-10-18. Gigerenzer, G (2004)." Journal of Socio-Economik.33 (5): 587-606. doi:106/j.socec.2004.09.033.Ioannidis, J.P.A (2005) "Die meisten veröffentlichten Forschungsergebnisse sind falsch". PLOS Medizin.2 (4): 696–701. doi:10.1371/journal.pmed.0040168.PMC 1855693.PMID 17456002. Externe Links (Elektronische Version): TIBCO Software Inc. (2020). Data Science Textbook. Online-Statistik Bildung: Ein interaktiver Multimedia-Studiengang. Entwicklung von Rice University (Lead Entwickler), University of Houston Clear Lake, Tufts University und National Science Foundation. UCLA Statistik-Eigenschaft der Statistik von der Wissenschaftsbibliothek der Philosophie