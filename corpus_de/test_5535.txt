Beim maschinellen Lernen ist Boosting ein Ensemble-Meta-Algorithmus zur primären Verringerung der Vorspannung, sowie Varianz im überwachten Lernen, und eine Familie von maschinellen Lernalgorithmen, die schwache Lernende zu starken konvertieren. Die Förderung basiert auf der Frage von Kearns und Valiant (1988, 1989): "Kann eine Reihe von schwachen Lernenden einen einzigen starken Lernenden schaffen? " Ein schwächer Lernender ist als Klassifikator definiert, der nur geringfügig mit der wahren Klassifikation korreliert (es kann Beispiele besser als zufälliges Erraten bezeichnen). Ein starker Lernender ist dagegen ein Klassifikator, der willkürlich gut mit der wahren Klassifikation korreliert. Robert Schapires bekräftigende Antwort in einem Bericht von 1990 zur Frage von Kearns und Valiant hatte erhebliche Auswirkungen auf das maschinelle Lernen und die Statistik, vor allem auf die Entwicklung der Förderung. Bei der ersten Einführung hat die Hypothese, die das Problem ankurbelt, einfach auf den Prozess der Umwandlung eines schwachen Lernenden in einen starken Lernenden verwiesen. "Informell fragt das Problem, ob ein effizienter Lernalgorithmus [...] eine Hypothese ausgibt, deren Leistung nur geringfügig besser ist als zufälliges Erraten [d.h. ein schwacher Lernender] das Vorhandensein eines effizienten Algorithmus, der eine Hypothese willkürlicher Genauigkeit ausgibt [dh ein starker Lernender]. " Algorithmen, die Hypothese zu erreichen, die die Steigerung schnell wurde einfach als Boosting bekannt". Freund und Schapire's Arcing (Adapt[at]ive Resampling and Combining) als allgemeine Technik, ist mehr oder weniger synonym für die Förderung von Freund. Erhöhen von Algorithmen Während die Förderung nicht algorithmisch eingeschränkt ist, bestehen die meisten stimulierenden Algorithmen aus iterativ lernen schwachen Klassifikatoren in Bezug auf eine Verteilung und Hinzufügen zu einem endgültigen starken Klassifikator. Wenn sie hinzugefügt werden, werden sie in einer Weise gewichtet, die mit der Genauigkeit der schwachen Lernenden zusammenhängt. Nach Zugabe eines schwachen Lernenden werden die Datengewichte neu eingestellt, bekannt als Nachgewichtung. Verschiedene Eingabedaten gewinnen ein höheres Gewicht und Beispiele, die korrekt klassifiziert werden, verlieren Gewicht. So konzentrieren sich zukünftige schwache Lernende mehr auf die Beispiele, die frühere schwache Lernende missklassifiziert haben. Es gibt viele stimulierende Algorithmen. Die von Robert Schapire (eine wiederkehrende Mehrheits-Gate-Formulierung) und Yoav Freund (beide mit Mehrheit) vorgeschlagenen Originale waren nicht adaptiv und konnten die schwachen Lernenden nicht voll ausnutzen. Schapire und Freund entwickelten dann AdaBoost, einen adaptiven Verstärkeralgorithmus, der den renommierten Gödel-Preis gewann. Nur Algorithmen, die in der wahrscheinlich ungefähr korrekten Lernformulierung nachweisbare Verstärkungsalgorithmen sind, können als Verstärkungsalgorithmen bezeichnet werden. Andere Algorithmen, die im Geiste ähnlich sind, um Algorithmen zu fördern, werden manchmal als "Lernalgorithmen" bezeichnet, obwohl sie manchmal auch falsch als Verstärkungsalgorithmen bezeichnet werden. Die Hauptvariation zwischen vielen stimulierenden Algorithmen ist ihre Methode der Gewichtung von Trainingsdatenpunkten und Hypothesen. AdaBoost ist sehr beliebt und die bedeutendste historisch, da es der erste Algorithmus war, der sich an die schwachen Lernenden anpassen konnte. Es ist oft die Grundlage der einleitenden Abdeckung der Förderung der universitären maschinellen Lernkurse. Es gibt viele neuere Algorithmen wie LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost und andere. Viele stimulierende Algorithmen passen in das AnyBoost-Framework, das zeigt, dass die Erhöhung der Gradientenabstieg in einem Funktionsraum mit einer konvexen Kostenfunktion führt. Objekt-Kategorisierung in der Computer-Vision Bei Bildern, die verschiedene bekannte Objekte der Welt enthalten, kann von ihnen ein Klassifikator gelernt werden, um die Objekte in zukünftigen Bildern automatisch zu klassifizieren. Einfache Klassifikatoren, die auf einigen Bildmerkmalen des Objekts aufgebaut sind, sind in der Kategorisierungsleistung eher schwach. Die Verwendung von Boosting-Methoden zur Objekt-Kategorisierung ist eine Möglichkeit, die schwachen Klassifikatoren auf eine spezielle Weise zu vereinheitlichen, um die Gesamtfähigkeit der Kategorisierung zu steigern. Problem der Objekt-Kategorisierung Objekt-Kategorisierung ist eine typische Aufgabe der Computer-Vision, die es beinhaltet zu bestimmen, ob ein Bild eine bestimmte Kategorie von Objekt enthält oder nicht. Die Idee ist eng mit Erkennung, Identifikation und Erkennung verbunden. Die Appearance basierte Objekt-Kategorisierung enthält typischerweise Feature-Extraktion, Lernen eines Klassifikators und Anwendung des Klassifikators auf neue Beispiele. Es gibt viele Möglichkeiten, eine Kategorie von Objekten darzustellen, z.B. aus der Formanalyse, aus Wörtern Modellen oder lokalen Deskriptoren wie SIFT, etc. Beispiele für beaufsichtigte Klassifikatoren sind Naive Bayes Klassifikatoren, Trägervektormaschinen, Mischungen von Gaussern und neuronalen Netzwerken. Die Forschung hat jedoch gezeigt, dass Objektkategorien und deren Standorte in Bildern auch ununterbrochen entdeckt werden können. Status quo für Objekt-Kategorisierung Die Erkennung von Objektkategorien in Bildern ist ein herausforderndes Problem in der Computersicht, insbesondere wenn die Anzahl der Kategorien groß ist. Dies ist auf eine hohe Intraklasse-Variabilität und die Notwendigkeit der Verallgemeinerung über Variationen von Objekten innerhalb derselben Kategorie zurückzuführen. Objekte innerhalb einer Kategorie können ganz anders aussehen. Auch das gleiche Objekt kann unter verschiedenen Gesichtspunkten, Skalen und Beleuchtung unähnlich erscheinen. Hintergrund-Cutter und teilweise Okklusion fügen auch Schwierigkeiten zur Erkennung hinzu. Menschen sind in der Lage, Tausende von Objekttypen zu erkennen, während die meisten der vorhandenen Objekterkennungssysteme ausgebildet sind, um nur wenige, z.B. menschliche Gesichter, Autos, einfache Objekte usw. zu erkennen. Die Forschung war sehr aktiv bei der Behandlung von mehr Kategorien und die Ermöglichung von inkrementellen Ergänzungen neuer Kategorien, und obwohl das allgemeine Problem ungelöst bleibt, wurden mehrere Mehrkategorien-Objektdetektoren (für bis zu Hunderte oder Tausende von Kategorien) entwickelt. Ein Mittel ist das Teilen von Funktionen und das Anheben. Boosting für binäre Kategorisierung AdaBoost kann als Beispiel für binäre Kategorisierung zur Gesichtserkennung verwendet werden. Die beiden Kategorien sind Gesichter gegen Hintergrund. Der allgemeine Algorithmus ist wie folgt: Gestalten Sie eine große Reihe von einfachen Features Initialisieren Sie Gewichte für Trainingsbilder Für T-Runden Normalisieren Sie die Gewichte Für verfügbare Features aus dem Set, trainieren Sie einen Klassifikator mit einer einzigen Funktion und bewerten Sie den Trainingsfehler Wählen Sie den Klassifikator mit dem niedrigsten Fehler Aktualisieren Sie die Gewichte der Trainingsbilder: erhöhen Sie, wenn dieser Klassifikator falsch klassifiziert wird, verringern Sie, falls zutreffend, den endgültigen starken Klassifikator als lineare Kombination der T-Klassifikatoren (koeffizient größer, wenn der Trainingsfehler klein ist) Nach dem Boosten könnte ein aus 200 Merkmalen aufgebauter Klassifikator eine 95 %ige Detektionsrate unter einer 10 - 5 {\displaystyle 10^{-5} falschen positiven Rate liefern. Eine weitere Anwendung der Förderung für binäre Kategorisierung ist ein System, das Fußgänger mit Mustern von Bewegung und Aussehen erkennt. Diese Arbeit ist die erste, die sowohl Bewegungsinformationen als auch Erscheinungsinformationen als Merkmale zu kombinieren, um eine Gehperson zu erkennen. Es geht um einen ähnlichen Ansatz im Rahmen der Viola-Jones Objekterkennung. Boosting für Multi-Klasse-Kategorisierung Im Vergleich zu binären Kategorisierung, Multi-Klasse-Kategorisierung sucht gemeinsame Funktionen, die über die Kategorien gleichzeitig geteilt werden können. Sie drehen sich um mehr generische Rand wie Features. Beim Lernen können die Detektoren für jede Kategorie gemeinsam ausgebildet werden. Im Vergleich zu der Ausbildung ist es besser, benötigt weniger Trainingsdaten und erfordert weniger Funktionen, um die gleiche Leistung zu erreichen. Der Hauptstrom des Algorithmus ist dem Binärfall ähnlich. Anders ist, dass ein Maß für den gemeinsamen Ausbildungsfehler im Voraus festgelegt wird. Während jeder Iteration wählt der Algorithmus einen Klassifikator eines einzigen Merkmals aus (Features, die von mehreren Kategorien geteilt werden können, werden gefördert). Dies kann durch Umwandeln der mehrstufigen Klassifizierung in einen binären (ein Satz von Kategorien gegen den Rest) oder durch Einführung eines Straffehlers aus den Kategorien erfolgen, die nicht das Merkmal des Klassifikators haben. In der Zeitung "Sharing visuelle Features für Multiclass- und Multiview-Objektdetektion", A. Torralba et al.used GentleBoost für die Steigerung und zeigte, dass das Lernen über die Freigabe von Funktionen eine viel bessere Arbeit als keine Freigabe, bei gleichen Auftriebsrunden. Auch bei einem gegebenen Leistungsniveau wird die Gesamtzahl der benötigten Merkmale (und damit die Laufzeitkosten des Klassifikators) für die Merkmalsfreigabedetektoren etwa logarithmisch mit der Anzahl der Klasse, d.h. langsamer als das lineare Wachstum im Nicht-Sharing-Fall beobachtet. Ähnliche Ergebnisse werden in der Zeitung "Incrementales Lernen von Objektdetektoren mit einem visuellen Form-Alphabet" gezeigt, doch die Autoren benutzten AdaBoost zur Steigerung. Convex vs. non-convex stimulierende Algorithmen Die Verstärkungsalgorithmen können auf konvexen oder nicht konvexen Optimierungsalgorithmen beruhen. Convex-Algorithmen, wie AdaBoost und LogitBoost, können durch zufälliges Rauschen besiegt werden, so dass sie keine grundlegenden und erlernbaren Kombinationen von schwachen Hypothesen lernen können. Diese Einschränkung wurde 2008 von Long & Servedio hervorgehoben. Bis 2009 zeigten jedoch mehrere Autoren, dass die Steigerung von Algorithmen auf Basis der nicht-konvexen Optimierung, wie BrownBoost, von lauten Datensätzen lernen und gezielt den zugrunde liegenden Klassifikator des Long-Servedio-Datensatzes lernen kann. Siehe auch Implementierungen Scikit-learn, eine Open Source Machine Learning Library für python Orange, eine kostenlose Data Mining Software Suite, Modul Orange. Ensemble Weka ist ein maschinelles Lernset von Werkzeugen, das vielfältige Implementierungen von Algorithmussteigerungen wie AdaBoost und LogitBoost R Paket GBM (Generalized Boosted Regression Models) implementiert Erweiterungen zu Freund und Schapire AdaBoost Algorithmus und Friedmans Gradienten-Boosting-Maschine. jboost;AdaBoost, LogitBoost, RobustBoost, Boostexter und alternierende Entscheidung Bäume R Paket adabag:Applies Multiclass AdaBoost.M1, AdaBoost-SAMME und Bagging R Paket xgboost: Eine Implementierung von Gradienten-Boosting für lineare und baumbasierte Modelle. Hinweise Referenzen Weiter lesen Yoav Freund und Robert E. Schapire (1997); Eine Entscheidung-Theoretische Verallgemeinerung des Online-Lernens und eine Anwendung auf Boosting, Journal of Computer and System Sciences, 55(1):119-139 Robert E. Schapire und Yoram Singer (1999); Verbesserte Erhöhung von Algorithmen Mit Confidence-Rated Predictors, Machine Learning, 37(3):297-336 Externe Links Robert E. Schapire (2003); The Boosting Approach to Machine Learning: Eine Übersicht, MSRI (Mathematical Sciences Research Institute) Workshop über nichtlineare Schätzung und Klassifizierung Zhou Zhi-Hua (2014)Boosting 25 Jahre, CCL 2014 Keynote. Zhou, Zhihua (2008). " Am Rande Erläuterung des Verstärkungsalgorithmus" (PDF). In: Proceedings of the 21st Annual Conference on Learning Theory (COLT'08): 479–490.Zhou, Zhihua (2013). " Zum Zweifel an der Margin-Erklärung des Boosting" (PDF). Künstliche Intelligenz. 203: 1–18.arXiv:1009.3613.doi:10.1016/j.artint.2013.07.002 S2CID 2828847.