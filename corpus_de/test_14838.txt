Die Cross-Validierung, manchmal als Rotation-Schätzung oder Out-of-Sample-Tests bezeichnet, ist jede von verschiedenen ähnlichen Modell-Validierungstechniken, um zu beurteilen, wie die Ergebnisse einer statistischen Analyse zu einem unabhängigen Datensatz verallgemeinert werden. Es wird hauptsächlich in Einstellungen verwendet, in denen das Ziel Vorhersage ist, und man will schätzen, wie genau ein Vorhersagemodell in der Praxis durchgeführt wird. Bei einem Prädiktionsproblem erhält man üblicherweise einen Datensatz bekannter Daten, an dem das Training durchgeführt wird (Training Dataset), und einen Datensatz unbekannter Daten (oder erst gesehene Daten), gegen die das Modell getestet wird (genannter Validierungsdatensatz oder Testsatz). Ziel der Cross-Validierung ist es, die Fähigkeit des Modells zu testen, neue Daten zu vorhersagen, die bei der Schätzung nicht verwendet wurden, um Probleme wie Überholen oder Selektions-Bias zu markieren und einen Einblick zu geben, wie das Modell zu einem unabhängigen Datensatz (d.h. einem unbekannten Datensatz, z.B. aus einem realen Problem) verallgemeinert wird. Eine Kreuzvalidierungsrunde beinhaltet die Partitionierung einer Datenprobe in komplementäre Teilmengen, die Analyse auf einer Teilmenge (das Trainingsset genannt) und die Validierung auf der anderen Teilmenge (das Validierungsset oder Testset). Um die Variabilität zu reduzieren, werden in den meisten Methoden mehrere Kreuzvalidierungsrunden mit unterschiedlichen Partitionen durchgeführt und die Validierungsergebnisse (z.B. gemittelt) über die Runden zusammengefasst, um eine Schätzung der Vorhersageleistung des Modells zu liefern. Zusammenfassend kombiniert Cross-validation (Durchschnitt) Maßnahmen der Fitness in der Prädiktion, um eine genauere Schätzung der Modellvorhersage Leistung abzuleiten. Motivation Angenommen, wir haben ein Modell mit einem oder mehreren unbekannten Parametern und einen Datensatz, auf den das Modell passt (der Trainingsdatensatz). Der Montageprozess optimiert die Modellparameter, um das Modell so gut wie möglich an die Trainingsdaten anzupassen. Wenn wir dann eine unabhängige Stichprobe von Validierungsdaten aus der gleichen Bevölkerung machen wie bei der Erfassung der Trainingsdaten, wird sich in der Regel herausstellen, dass das Modell nicht den Validierungsdaten entspricht und es die Trainingsdaten passt. Die Größe dieser Differenz ist wahrscheinlich groß, insbesondere wenn die Größe des Trainingsdatensatzes klein ist, oder wenn die Anzahl der Parameter im Modell groß ist. Die Cross-Validierung ist eine Möglichkeit, die Größe dieses Effektes zu schätzen. Bei linearer Regression haben wir reale Antwortwerte y1, ..., yn und n p-dimensionale Vektorkovariate x1, ..., xn. Die Komponenten des Vektors xi sind mit xi1, ..., xip bezeichnet. Wenn wir mit den Daten (xi, yi) 1 ≤ i ≤ n eine Funktion in Form eines Hyperplanes ≠ a + βTx verwenden, könnten wir die Passform dann mit dem mittleren quadratischen Fehler (MSE) bewerten. Die MSE für vorgegebene geschätzte Parameterwerte a und β auf dem Trainingssatz (xi, yi) 1 ≤ i ≤ n ist definiert als M S E = 1 n Σ i = 1 n ( y i - y ^ i ) 2 = 1 n Σ i = 1 n ( y i - a - β T x i) 2 = 1 n = 1 n ( y i - a - β 1 x i 1 -  − - β p x) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ **_{i}{2}={\frac 1}{n}}\sum i=1}^{n}(y_{i}-a-{\boldsymbol) {\cH00FF}\cH00FF}{2}{\cH00FF}\cH00FF}\cH00FF} (y) 1}x_{i1}-\dots -\beta p}x_{ip})^{2 Ist das Modell richtig spezifiziert, kann unter milden Annahmen gezeigt werden, dass der Erwartungswert des MSE für den Trainingssatz (n - p - 1)/(n + p + 1) < 1 mal der Erwartungswert des MSE für den Validierungssatz (der Erwartungswert wird über die Verteilung von Trainingseinheiten übernommen) beträgt. Wenn wir also das Modell anpassen und das MSE auf dem Trainingsset berechnen, erhalten wir eine optimistisch vorgespannte Bewertung, wie gut das Modell einem unabhängigen Datensatz passt. Diese voreingenommene Schätzung wird als Stichprobenvoranschlag der Passform bezeichnet, während die Kreuzvalidierungsschätzung eine Stichprobenvoranschlag ist. Da bei linearer Regression der Faktor (n - p - 1)/(n + p + 1) direkt berechnet werden kann, mit dem das Training MSE die Validierung MSE unter der Annahme unterschätzt, dass die Modellspezifikation gültig ist, kann zur Überprüfung, ob das Modell überrüstet wurde, Quervalidierung herangezogen werden, wobei der MSE im Validierungssatz seinen erwarteten Wert im Wesentlichen überschreitet. (Cross-validierung im Rahmen der linearen Regression ist auch nützlich, indem es verwendet werden kann, um eine optimal normierte Kostenfunktion auszuwählen.) In den meisten anderen Regressionsprozeduren (z.B. logistische Regression) gibt es keine einfache Formel, um den erwarteten Out-of-Sample-Pass zu berechnen. Die Cross-Validierung ist somit eine allgemein anwendbare Möglichkeit, die Leistungsfähigkeit eines Modells anhand von numerischen Berechnungen anstelle der theoretischen Analyse auf nicht verfügbare Daten vorherzusagen. Arten Zwei Arten von Kreuzvalidierung können unterschieden werden: erschöpfende und nicht erschöpfende Quervalidierung. Ausgeschöpfte Cross-Validation Ausgeschöpfte Cross-Validierungsverfahren sind Cross-Validierungsverfahren, die auf alle möglichen Weisen lernen und testen, um die ursprüngliche Probe in ein Training und ein Validierungsset zu teilen. Die Hinterbliebene Kreuzvalidierung (LpO CV) beinhaltet die Verwendung von P-Beobachtungen als Validierungssatz und die übrigen Beobachtungen als Ausbildungssatz. Dies wird auf allen Wegen wiederholt, um die ursprüngliche Probe auf einem Validierungssatz von p Beobachtungen und einem Trainingsset zu schneiden. LpO-Quervalidierung erfordert Ausbildung und Validierung des Modells C p n {\displaystyle C_{p}^{n mal, wobei n die Anzahl der Beobachtungen in der Originalprobe ist, und wobei C p n {\displaystyle C_{p}^{n der binomiale Koeffizient ist. Für p > 1 und für noch mäßig große n kann LpO CV rechnerisch unfehlbar werden. Beispielsweise mit n = 100 und p = 30, C 30 100 ≈ 3 × 10 25 . {\displaystyle C_{30}^{100}\ca. 3\Zeiten 10^{25}. Eine Variante der LpO-Quervalidierung mit p=2, die als Leave-pair-out-Quervalidierung bekannt ist, wurde als nahezu unvoreingenommenes Verfahren zur Schätzung der Fläche unter ROC-Kurve von binären Klassifikatoren empfohlen. Die Hinterbliebene Kreuzvalidierung (LOOCV) ist ein besonderer Fall der Ausbliebenenvalidierung mit p = 1. Der Prozess sieht jackknife ähnlich aus; bei Quervalidierung berechnet man jedoch eine Statistik auf der linken Probe(n), während man mit jackknifing nur eine Statistik aus den gespeicherten Proben berechnet. Die LOO-Quervalidierung erfordert weniger Rechenzeit als LpO-Quervalidierung, da es nur C 1 n = n {\displaystyle C_{1}{n}=n Pässe anstelle C p n {\displaystyle C_{p}^{n gibt.Pseudo-Code-Algorithm: Eingabe: x, {Längevektor N mit x-Werten von eingehenden Punkten} y, {Vector der Länge N mit y-Werten des erwarteten Ergebnisses} interpolate( x_in, y_in, x_out ), { gibt die Schätzung für Punkt x_out zurück, nachdem das Modell mit x_in-y_in Paaren geschult ist} Ausgabe: err, {Schätzung für den Vorhersagefehler} Schritte: err ← 0 für i ← 1, ..., N do // definieren die Cross-Validation Subsets x_in ← (x[1], ..., x[i − 1], x[i + 1], ..., x[N]) y_in ← (y[1], ..., y[i − 1], y[i + 1], ..., y[N]) x_out x[i] y_ Nicht erschöpfende Quervalidierungsverfahren berechnen nicht alle Arten der Spaltung der Originalprobe. Diese Methoden sind Annäherungen der Ausscheiden-P-out-Quervalidierung. k-fache Quervalidierung Bei k-facher Quervalidierung wird die ursprüngliche Probe zufällig in k gleich große Unterproben unterteilt. Von den k Subsamples wird als Validierungsdaten zum Testen des Modells ein einziger Subsample gespeichert und die restlichen k - 1 Subsamples werden als Trainingsdaten verwendet. Der Quervalidierungsprozess wird dann k-mal wiederholt, wobei jedes der k Subsamples genau einmal als Validierungsdaten verwendet wird. Die k-Ergebnisse können dann zu einer einzigen Schätzung gemittelt werden. Der Vorteil dieses Verfahrens bei wiederholter zufälliger Subsampling (siehe unten) besteht darin, dass alle Beobachtungen sowohl für die Ausbildung als auch für die Validierung verwendet werden und jede Beobachtung genau einmal zur Validierung verwendet wird. Üblicherweise wird eine 10-fache Kreuzvalidierung verwendet, im allgemeinen bleibt k jedoch ein unfixierter Parameter. Die Einstellung k = 2 ergibt beispielsweise eine 2-fache Kreuzvalidierung. Bei der 2-fachen Kreuzvalidierung stoßen wir den Datensatz zufällig in zwei Sätze d0 und d1, so dass beide Sätze gleich groß sind (dies wird in der Regel durch Schuppeln des Datenfeldes realisiert und dann in zwei geteilt). Wir trainieren dann auf d0 und validieren auf d1, gefolgt von Ausbildung auf d1 und Validierung auf d0. Wenn k = n (die Anzahl der Beobachtungen) k-fache Kreuzvalidierung gleichbedeutend mit der ausscheidenden Kreuzvalidierung ist. Bei stratifizierter k-facher Quervalidierung werden die Partitionen so gewählt, dass der mittlere Antwortwert in allen Partitionen annähernd gleich ist. Bei der binären Klassifikation bedeutet dies, dass jede Partition etwa die gleichen Anteile der beiden Typen von Klassenetiketten enthält. Bei wiederholter Quervalidierung werden die Daten mehrmals in k-Partitionen aufgeteilt. Dadurch kann die Leistung des Modells über mehrere Strecken gemittelt werden, was in der Praxis jedoch selten wünschenswert ist. Halteverfahren Bei der Holdout-Methode vergeben wir stichprobenartig Datenpunkte an zwei Sätze d0 bzw. d1, die üblicherweise als Trainingssatz bzw. Testsatz bezeichnet werden. Die Größe jedes der Sätze ist beliebig, obwohl typischerweise der Testsatz kleiner als der Trainingssatz ist. Anschließend trainieren wir auf d0 (Bauen eines Modells) und testen auf d1. Bei der typischen Kreuzvalidierung werden die Ergebnisse mehrerer Abläufe des Modelltests gemeinsam gemittelt; im Gegensatz dazu beinhaltet das Holdout-Verfahren isoliert einen einzigen Ablauf. Es sollte mit Vorsicht verwendet werden, da ohne eine solche Mittelung von Mehrfachläufen sehr irreführende Ergebnisse erzielt werden können. Der Indikator der Vorhersagegenauigkeit (F*) wird tendenziell instabil sein, da er nicht durch mehrere Iterationen geglättet wird (siehe unten). In ähnlicher Weise werden Indikatoren der spezifischen Rolle, die von verschiedenen Prädiktorvariablen (z.B. Werten von Regressionskoeffizienten) gespielt wird, instabil sein. Während das Holdout-Verfahren als "die einfachste Art der Kreuzvalidierung" gerahmt werden kann, klassifizieren viele Quellen stattdessen Holdout als eine Art einfacher Validierung, anstatt eine einfache oder degenerierte Form der Kreuzvalidierung. Wiederholte zufällige Unterprobenvalidierung Diese Methode, auch bekannt als Monte Carlo Cross-validation, erzeugt mehrere zufällige Spaltungen des Datensatzes in Trainings- und Validierungsdaten. Für jede solche Aufspaltung ist das Modell an die Trainingsdaten angepasst und anhand der Validierungsdaten wird die Prädiktivgenauigkeit bewertet. Die Ergebnisse werden dann über die Spalten gemittelt. Der Vorteil dieses Verfahrens (über k-fache Kreuzvalidierung) besteht darin, dass der Anteil der Trainings-/Validierungsspalte nicht von der Anzahl der Iterationen (d.h. der Anzahl der Partitionen) abhängig ist. Nachteilig bei diesem Verfahren ist, dass einige Beobachtungen im Validierungsunterstich niemals ausgewählt werden können, während andere mehr als einmal ausgewählt werden können. Mit anderen Worten können sich Validierungsuntermengen überlappen. Diese Methode zeigt auch Monte Carlo Variation, was bedeutet, dass die Ergebnisse variieren, wenn die Analyse mit verschiedenen zufälligen Spalten wiederholt wird.Da sich die Anzahl der zufälligen Spalten in Unendlichkeit nähert, neigt das Ergebnis einer wiederholten zufälligen Unterprobenvalidierung dazu, dass die Ausscheiden-P-out-Quervalidierung erfolgt. In einer stratifizierten Variante dieses Ansatzes werden die Zufallsmuster so erzeugt, dass der mittlere Ansprechwert (d.h. die abhängige Größe in der Regression) in den Trainings- und Testsets gleich ist. Dies ist insbesondere dann sinnvoll, wenn die Antworten mit einer unsymmetrischen Darstellung der beiden Antwortwerte in den Daten dichotom sind. Eingebettete Kreuzvalidierung Wenn die Quervalidierung gleichzeitig zur Auswahl der besten Hyperparameter und zur Fehlerschätzung (und Beurteilung der Verallgemeinerungskapazität) verwendet wird, ist eine geschachtelte Quervalidierung erforderlich. Es gibt viele Varianten. Es können mindestens zwei Varianten unterschieden werden: k*l-fache Quervalidierung Dies ist eine wirklich geschachtelte Variante (z.B. verwendet von cross_val_score in scikit-learn), die eine äußere Schlaufe von k-Sets und eine innere Schlaufe von l-Sets enthält. Der Gesamtdatensatz wird in k-Sets aufgeteilt. Eins nach dem anderen wird als (outer) Testset ein Set ausgewählt und die k - 1 anderen Sätze werden in das entsprechende äußere Trainingsset zusammengefasst. Dies wird für jeden der k-Sets wiederholt. Jeder äußere Trainingssatz ist weiter in L-Sets unterteilt. Eins nach dem anderen wird ein Satz als Innentest (Validierung)-Set gewählt und die l - 1 anderen Sätze werden in das entsprechende Innentrainingsset zusammengefaßt. Dies wird für jeden der L-Sets wiederholt. Die inneren Trainingseinheiten werden verwendet, um Modellparameter zu passen, während der äußere Testsatz als Validierungssatz verwendet wird, um eine unvoreingenommene Bewertung des Modellsitzes bereitzustellen. Typischerweise wiederholt sich dies bei vielen verschiedenen Hyperparametern (oder sogar unterschiedlichen Modelltypen) und der Validierungssatz dient zur Bestimmung des besten Hyperparametersatzes (und Modelltyp) für dieses innere Trainingsset. Danach ist ein neues Modell auf das gesamte äußere Trainingsset abgestimmt, das die besten Hyperparameter aus der inneren Quervalidierung verwendet. Die Leistung dieses Modells wird dann mit dem äußeren Testsatz ausgewertet. k-fache Quervalidierung mit Validierungs- und Testsatz Dies ist eine Art k*l-facher Quervalidierung, wenn l = k - 1, eine einzelne k-fache Quervalidierung sowohl mit einem Validierungs- als auch Testset verwendet wird. Der Gesamtdatensatz wird in k-Sets aufgeteilt. Eins nach dem anderen wird ein Set als Testset ausgewählt. Dann wird eines nach dem anderen als Validierungsset einer der übrigen Sätze verwendet und die anderen k - 2 Sätze werden als Trainingseinheiten verwendet, bis alle möglichen Kombinationen ausgewertet wurden. Ähnlich wie bei der k*l-fachen Kreuzvalidierung wird das Trainingsset für Modellbeschlag verwendet und das Validierungsset wird für jede der Hyperparametersätze zur Modellauswertung verwendet. Schließlich wird für den ausgewählten Parametersatz der Testsatz verwendet, um das Modell mit dem besten Parametersatz auszuwerten. Hier sind zwei Varianten möglich: entweder die Auswertung des Modells, das auf dem Trainingsset trainiert wurde, oder die Auswertung eines neuen Modells, das auf die Kombination von Zug und Validierungsset passte. Maßnahmen der Anpassung Ziel der Quervalidierung ist es, die erwartete Anpassung eines Modells an einen Datensatz zu schätzen, der unabhängig von den Daten ist, die für die Modellbildung verwendet wurden. Es kann verwendet werden, um alle quantitativen Maß an Passung zu schätzen, die für die Daten und Modell geeignet ist. Beispielsweise wird bei binären Klassifikationsproblemen jeweils im Validierungssatz entweder korrekt oder falsch vorhergesagt. In dieser Situation kann die Fehlklassifikationsfehlerrate zur Zusammenfassung der Passung herangezogen werden, obwohl auch andere Maßnahmen wie positiver Prädiktionswert verwendet werden könnten. Wenn der vorhergesagte Wert kontinuierlich verteilt wird, könnte der mittlere quadratische Fehler, der Wurzelmittel-Quadd-Fehler oder die mediane absolute Abweichung verwendet werden, um die Fehler zusammenzufassen. Vorherige Informationen verwenden Wenn Benutzer Quervalidierung anwenden, um eine gute Konfiguration zu wählen λ {\displaystyle \lambda }, dann könnten sie die kreuzvalidierte Wahl mit ihrer eigenen Schätzung der Konfiguration ausgleichen wollen. Auf diese Weise können sie versuchen, der Flüchtigkeit der Kreuzvalidierung entgegenzuwirken, wenn die Stichprobengröße klein ist und relevante Informationen aus früheren Untersuchungen enthalten. In einer prognostizierenden Kombinationsübung kann beispielsweise eine Kreuzvalidierung zur Schätzung der Gewichte angewendet werden, die jeder Prognose zugeordnet werden. Da eine einfache gleichgewichtige Prognose schwer zu schlagen ist, kann eine Strafe hinzugefügt werden, um von gleichen Gewichten abzuweichen. Oder, wenn Quervalidierung angewendet wird, um einzelne Gewichte zu Beobachtungen zuzuordnen, kann man Abweichungen von gleichen Gewichten bestrafen, um potenziell relevante Informationen zu vermeiden.Hoornweg (2018) zeigt, wie ein Tuning-Parameter γ {\displaystyle \gamma } definiert werden kann, so dass ein Benutzer intuitiv zwischen der Genauigkeit der Kreuzvalidierung und der Einfachheit des Festhaltens an einem vom Benutzer definierten Referenzparameter λ R {\displaystyle \lambda {_R} ausgleichen kann. Wenn λ i {\displaystyle \lambda {_i} die i t h {\displaystyle i^{th}-Kandidatkonfiguration bedeutet, die ausgewählt werden könnte, so kann die zu minimierende Verlustfunktion als L λ i = ( 1 - γ) definiert werden. Relative Genauigkeit i + γ relative Einfachheit i. {_i}=(1-\gamma )\mbox Relative Accuracy}}_{i}+\gamma \mbox Relative Simplicity}}_{i. Die relative Genauigkeit kann als MSE (λ i ) / MSE (λ R ) {\displaystyle mbox{MSE}(\lambda _i})/{\mbox{MSE}(\lambda {_R}) quantifiziert werden, so dass der mittlere quadratische Fehler eines Kandidaten λ i {\displaystyle \lambda {style {i} relativ zu dem eines Benutzers gemacht wird {_R} .Der relative Einfachheitsbegriff misst den Betrag, den λ i {\displaystyle \lambda {_i} von λ R {\displaystyle \lambda abweicht {_R} bezogen auf die maximale Abweichung von λ R {\displaystyle \lambda {_R} . Entsprechend kann die relative Einfachheit als (λ i - λ R ) 2 (λ max - λ R ) 2 {\displaystyle {\\frac {(\lambda {_i}-\lambda _R})^{2}{(\lambda\max }-\lambda_Rstyle}^^{2 } angegeben werden, wobei {_R} .Mit γ ε [0, 1 ] {\displaystyle \gamma \in [0,1}] bestimmt der Benutzer, wie hoch der Einfluss des Referenzparameters relativ zur Quervalidierung ist. Für mehrere Konfigurationen c = 1 , 2 , können relativ einfache Begriffe addiert werden.. . ., C {\displaystyle c=1,2,...,C} durch Angabe der Verlustfunktion als L λ i = relative Genauigkeit i + Σ c = 1 C γ c 1 - γ c re Einfachheit i , c L *i}={\mbox Relative Genauigkeit (c=1{C}{\fra) (Gamma) Relative Simplicity}_{i,c. Hoornweg (2018) zeigt, dass eine Verlustfunktion mit einer solchen Präzisionssimplicity-Tradeoff auch genutzt werden kann, um Schwindigkeitsschätzer wie die (adaptive) Lasso- und Bayesian / Gratregression intuitiv zu definieren. Klicken Sie zum Beispiel auf das Lasso. Statistische Eigenschaften Angenommen, wir wählen ein Maß für die Passform F und verwenden Quervalidierung, um eine Schätzung F* des erwarteten EF eines Modells zu einem unabhängigen Datensatz aus der gleichen Bevölkerung wie die Trainingsdaten zu erstellen. Wenn wir uns vorstellen, mehrere unabhängige Trainingseinheiten nach der gleichen Verteilung abzutasten, werden die resultierenden Werte für F* variieren. Aus dieser Variation ergeben sich die statistischen Eigenschaften von F*. Der Kreuzvalidierungsschätzer F* ist für EF fast unvoreingenommen. Der Grund dafür ist, dass der Trainingssatz in der Quervalidierung geringfügig kleiner ist als der eigentliche Datensatz (z.B. bei LOOCV beträgt die Trainingssatzgröße n - 1 bei n beobachteten Fällen). In fast allen Situationen wird die Wirkung dieser Bias konservativ sein, indem die geschätzte Passform in die Richtung leicht vorgespannt wird, die eine schlechtere Passform nahelegt. In der Praxis ist diese Bias selten ein Anliegen. Die Varianz von F* kann groß sein. Wenn aus diesem Grund zwei statistische Verfahren auf der Grundlage der Ergebnisse der Kreuzvalidierung verglichen werden, kann das Verfahren mit der besser geschätzten Leistung tatsächlich nicht das bessere der beiden Verfahren sein (d.h. es kann nicht den besseren Wert von EF haben). Es wurden einige Fortschritte bei der Erstellung von Vertrauensintervallen rund um die Schätzungen der Kreuzvalidierung erzielt, dies gilt jedoch als schwieriges Problem. Berechnungsfragen Die meisten Formen der Kreuzvalidierung sind einfach umzusetzen, solange eine Implementierung der zu untersuchenden Vorhersagemethode zur Verfügung steht. Insbesondere kann die Prädiktionsmethode eine "schwarze Box" sein – es ist nicht notwendig, Zugang zu den internen seiner Umsetzung zu haben. Ist das Prädiktionsverfahren zu trainieren aufwendig, kann die Quervalidierung sehr langsam sein, da das Training wiederholt durchgeführt werden muss. In einigen Fällen, wie zum Beispiel am wenigsten Quadrate und Kernelregression, kann die Cross-Validierung durch Vorkomputierung bestimmter Werte, die im Training wiederholt benötigt werden, oder durch die Verwendung von schnellen "updating rules" wie der Sherman-Morrison-Formel deutlich aufgespult werden. Man muss jedoch darauf achten, die "Total Blinding" des Validierungssatzes aus dem Training zu erhalten, andernfalls kann es zu Vorurteilen kommen. Ein extremes Beispiel für eine beschleunigte Quervalidierung tritt bei linearer Regression auf, wobei die Ergebnisse der Quervalidierung einen geschlossenen Ausdruck aufweisen, der als Prädiktions-Restfehlersumme von Quadraten (PRESS) bekannt ist. Einschränkungen und Missbrauch Die Cross-validation ergibt nur dann sinnvolle Ergebnisse, wenn der Validierungssatz und der Ausbildungssatz aus der gleichen Bevölkerung gezogen werden und nur wenn menschliche Vorurteile kontrolliert werden. In vielen Anwendungen der vorausschauenden Modellierung entwickelt sich die Struktur des untersuchten Systems im Laufe der Zeit (d.h. es ist nicht stationär). Beide können systematische Unterschiede zwischen den Ausbildungs- und Validierungseinheiten einführen. Wenn beispielsweise ein Modell zur Vorhersage von Bestandswerten für einen bestimmten Zeitraum von fünf Jahren auf Daten geschult wird, ist es unrealistisch, den darauffolgenden Zeitraum von fünf Jahren als Auszug aus der gleichen Bevölkerung zu behandeln. Als weiteres Beispiel wird angenommen, dass ein Modell entwickelt wird, um das Risiko eines Individuums für die Diagnose einer bestimmten Krankheit im nächsten Jahr vorherzusagen.Wenn das Modell anhand von Daten aus einer Studie, die nur eine bestimmte Bevölkerungsgruppe (z.B. Jugendliche oder Männer) umfasst, ausgebildet wird, dann aber auf die allgemeine Bevölkerung angewendet wird, könnte die Quervalidierung von dem Ausbildungssatz stark von der tatsächlichen Vorhersageleistung abweichen. In vielen Anwendungen können Modelle auch falsch spezifiziert und in Abhängigkeit von Modellierungsvorspannungen und/oder willkürlichen Entscheidungen variieren. Wenn dies geschieht, kann es eine Illusion geben, dass sich das System in externen Proben ändert, während der Grund ist, dass das Modell einen kritischen Prädiktor verpasst und/oder einen konfundierten Prädiktor enthalten hat. Neue Beweise sind, dass die Kreuzvalidierung von sich aus nicht sehr vorhersehbar ist von externer Gültigkeit, während eine Form der experimentellen Validierung, die als Swap-Probenahme bekannt ist, die Kontrolle für menschliche Vorurteile tut, viel vorausschauender von externer Gültigkeit sein kann. Wie durch diese große MAQC-II-Studie über 30.000 Modelle definiert, enthält Swap-Probenahme Cross-validierung in dem Sinne, dass Vorhersagen über unabhängige Trainings- und Validierungsproben getestet werden. Allerdings werden auch Modelle über diese unabhängigen Proben und Modellierer entwickelt, die gegeneinander blind sind. Wenn es in diesen Modellen eine Fehlanpassung gibt, die über diese verschlungenen Trainings- und Validierungsmuster entwickelt wird, wie es recht häufig geschieht, zeigt MAQC-II, dass dies viel voraussagekräftiger ist als herkömmliche Cross-validierung. Der Grund für den Erfolg der vertauschten Probenahme ist eine integrierte Kontrolle für menschliche Vorurteile im Modellbau. Neben zu viel Vertrauen in Vorhersagen, die über Modellierer variieren können und zu einer schlechten externen Gültigkeit führen, die durch diese confounding modeler Effekte, sind dies einige andere Möglichkeiten, dass Cross-Validation kann falsch verwendet werden: Indem eine erste Analyse durchgeführt wird, um die informativsten Merkmale mit dem gesamten Datensatz zu identifizieren – wenn durch das Modellierungsverfahren eine Auswahl oder Modellabstimmung erforderlich ist, muss dies auf jedem Trainingssatz wiederholt werden. Andernfalls werden Vorhersagen sicherlich nach oben vorgespannt sein. Wird die Quervalidierung verwendet, um zu entscheiden, welche Funktionen zu verwenden sind, muss eine innere Quervalidierung durchgeführt werden, um die Merkmalsauswahl auf jedem Trainingsset durchzuführen. Durch die Möglichkeit, einige der Trainingsdaten auch in den Testsatz einzubeziehen – dies kann durch eine Partnerschaft im Datensatz geschehen, wobei im Datensatz exakt gleiche oder nahezu identische Proben vorliegen. In gewissem Maße erfolgt die Partnerschaft auch in perfekt unabhängigen Trainings- und Validierungsproben. Dies liegt daran, dass einige der Trainingsmusterbeobachtungen fast identische Werte von Prädiktoren als Validierungsmusterbeobachtungen aufweisen werden. Und einige von ihnen werden mit einem Ziel in besserer als Chance in der gleichen Richtung in der Ausbildung und Validierung korrelieren, wenn sie tatsächlich von konfundierten Vorhersagen mit schlechter externer Gültigkeit angetrieben werden. Wenn ein solches kreuzvalidiertes Modell aus einem k-fachen Satz ausgewählt wird, wird die menschliche Bestätigungsvorspannung am Arbeitsplatz sein und feststellen, dass ein solches Modell validiert wurde. Aus diesem Grund muss die traditionelle Kreuzvalidierung mit Kontrollen für menschliche Vorurteile und konfundierte Modellspezifikationen wie Swap-Probenahme und prospektive Studien ergänzt werden. Kreuzvalidierung für Zeitreihenmodelle Da die Reihenfolge der Daten wichtig ist, kann Cross-validation für Zeitreihenmodelle problematisch sein. Ein geeigneterer Ansatz könnte sein, rollende Quervalidierung zu verwenden. Wenn die Leistung jedoch durch eine einzige Zusammenfassungsstatistik beschrieben wird, ist es möglich, dass der von Politis und Romano als stationäres Bootstrap beschriebene Ansatz funktionieren wird. Die Statistik des Bootstrap muss ein Intervall der Zeitreihe akzeptieren und die Zusammenfassung Statistik darauf zurückgeben. Der Aufruf zum stationären Bootstrap muss eine entsprechende mittlere Intervalllänge festlegen. Anwendungen Cross-validierung kann verwendet werden, um die Leistungen verschiedener Prädiktionsmodelle zu vergleichen. Nehmen wir zum Beispiel an, dass wir an der optischen Charaktererkennung interessiert sind, und wir erwägen, entweder die Hauptkomponentenanalyse (PCA) oder k-nearest Nachbarn (KNN) zu verwenden, um den wahren Charakter aus einem Bild eines handschriftlichen Charakters vorherzusagen. Mit Cross-Validierung könnten wir diese beiden Methoden objektiv in Bezug auf ihre jeweiligen Anteile falsch eingestufter Zeichen vergleichen. Wenn wir die Methoden auf Basis ihrer In-Sample-Fehlerraten einfach vergleichen, erscheint eine Methode wahrscheinlich besser, da sie gegenüber der anderen Methode flexibler und damit anfälliger ist. Eine Cross-Validierung kann auch bei variabler Auswahl verwendet werden.Angenommen, wir verwenden die Expressionspegel von 20 Proteinen, um vorherzusagen, ob ein Krebspatient auf ein Medikament reagieren wird. Ein praktisches Ziel wäre es, zu bestimmen, welche Teilmenge der 20 Merkmale verwendet werden sollte, um das beste Vorhersagemodell zu erzeugen. Für die meisten Modellierungsprozeduren, wenn wir Feature-Subsets mit den in-sample-Fehlerraten vergleichen, wird die beste Leistung auftreten, wenn alle 20 Funktionen verwendet werden. Das Modell mit der besten Passform wird jedoch unter Cross-Validierung in der Regel nur eine Teilmenge der Merkmale enthalten, die als wirklich informativ angesehen werden. Eine jüngste Entwicklung der medizinischen Statistik ist der Einsatz in der Metaanalyse. Es bildet die Grundlage der Validierungsstatistik, Vn, die verwendet wird, um die statistische Gültigkeit der Zusammenfassungsschätzungen der Metaanalyse zu testen. Es wurde auch in einem konventionelleren Sinne in der Meta-Analyse verwendet, um den wahrscheinlichen Vorhersagefehler der Meta-Analyseergebnisse abzuschätzen. Siehe auch Boosting (Maschinenlernen)Bootstrap-Aggregation (bagging)Out-of-bag-Fehler Bootstrapping (Statistik) Leakage (Maschinenlernen)ModellauswahlResampling (Statistik) Stabilität (Lerntheorie)Validity (Statistik) == Anmerkungen und Referenzen ==