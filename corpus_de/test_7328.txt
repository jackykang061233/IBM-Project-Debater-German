Sprachsynthese ist die künstliche Produktion von menschlicher Sprache. Ein hierfür verwendetes Computersystem wird als Sprachrechner oder Sprachsynthesizer bezeichnet und kann in Software- oder Hardwareprodukten implementiert werden. Ein Text-zu-Sprache-System (TTS) wandelt normalen Sprachtext in Sprache um; andere Systeme machen symbolische sprachliche Darstellungen wie phonetische Transkriptionen in Sprache. Synthesize-Rede kann durch das Zusammenfassen von in einer Datenbank gespeicherten Teilen von aufgezeichneten Rede erstellt werden. Systeme unterscheiden sich in der Größe der gespeicherten Spracheinheiten; ein System, das Telefone oder Diphone speichert, bietet den größten Ausgangsbereich, kann aber nicht Klarheit. Für bestimmte Nutzungsdomänen ermöglicht die Speicherung von ganzen Wörtern oder Sätzen eine hochwertige Ausgabe. Alternativ kann ein Synthesizer ein Modell des Gesangstrakts und andere menschliche Spracheigenschaften einfügen, um eine völlig synthetische Sprachausgabe zu erstellen. Die Qualität eines Sprachsynthesizers wird durch seine Ähnlichkeit mit der menschlichen Stimme und durch seine Fähigkeit, klar zu verstehen, beurteilt. Ein verständliches Text-zu-Sprach-Programm ermöglicht es Menschen mit Sehbehinderungen oder Lesebehinderungen, auf geschriebene Wörter auf einem Heimcomputer zu hören. Viele Computer-Betriebssysteme enthalten Sprachsynthesizer seit den frühen 1990er Jahren. Ein Text-Peech-System (oder Motor) besteht aus zwei Teilen: einem Frontend und einem Backend. Das Frontend hat zwei wichtige Aufgaben. Zuerst konvertiert er Rohtext mit Symbolen wie Zahlen und Abkürzungen in das Äquivalent der ausgeschriebenen Wörter. Dieser Prozess wird oft als Textnormalisierung, Vorverarbeitung oder Tokenisierung bezeichnet. Das Frontend ordnet dann jedem Wort phonetische Transkriptionen zu und teilt und markiert den Text in prosodische Einheiten, wie Phrasen, Klauseln und Sätze. Der Prozess der Zuordnung von phonetischen Transkriptionen zu Wörtern wird als text-to-phoneme oder grapheme-to-phoneme Conversion bezeichnet. Phonetische Transkriptionen und Prosody-Informationen bilden zusammen die symbolische sprachliche Darstellung, die vom Frontend ausgegeben wird. Das als Synthesizer bezeichnete Backend wandelt die symbolische sprachliche Darstellung in Klang um. In bestimmten Systemen umfasst dieser Teil die Berechnung der Zielprosody (Pitchkontur, Phonemdauer), die dann auf die Ausgaberede auferlegt wird. Geschichte Lange vor der Erfindung der elektronischen Signalverarbeitung versuchten einige Menschen, Maschinen zu bauen, um menschliche Sprache zu emulieren. Einige frühe Legenden der Existenz von "Brasen Heads" waren Papst Silvester II (d. 1003 AD,) Albertus Magnus (1198–1280), Roger Bacon (1214–1294). 1779 gewann der deutsch-dänische Wissenschaftler Christian Gottlieb Kratzenstein den ersten Preis in einem Wettbewerb, der von der Russischen Imperial Academy of Sciences and Arts für Modelle bekannt gegeben wurde, die er aus dem menschlichen Gesangstrakt gebaut hatte, der die fünf langen Vokalklänge erzeugen könnte (in International Phonetic Alphabet Notation: [aacryl], [eacryl], [iэ], [oэ] und [uý]). Es folgte die faltenbalgbetätigte "akustisch-mechanische Sprachmaschine" von Wolfgang von Kempelen von Pressburg, Ungarn, die in einem 1791 Papier beschrieben wurde. Diese Maschine fügte Modelle der Zunge und Lippen hinzu, so dass sie Konsonanten sowie Vokale produzieren. Im Jahre 1837 produzierte Charles Wheatstone eine "sprechende Maschine" auf Basis von von Kempelens Design und 1846 stellte Joseph Faber die Euphonia aus. Im Jahr 1923 Paget auferstanden Wheatstones Design. In den 1930er Jahren entwickelte Bell Labs den Vokoder, der die Sprache automatisch in seine Grundtöne und Resonanzen analysierte. Aus seiner Arbeit am Vocoder entwickelte Homer Dudley einen Keyboard-operierten Voice-Synthesizer namens The Voder (Voice Demonstrator), den er auf der New York World's Fair 1939 ausstellte. Dr. Franklin S. Cooper und seine Kollegen bei Haskins Laboratories bauten die Musterwiedergabe in den späten 1940er Jahren und beendeten sie 1950. Es gab mehrere verschiedene Versionen dieses Hardware-Geräts; nur einer überlebt derzeit. Die Maschine wandelt Bilder der akustischen Sprachmuster in Form eines Spektrogramms wieder in den Klang um. Mit diesem Gerät entdeckten Alvin Liberman und Kollegen akustische Ellen für die Wahrnehmung von phonetischen Segmenten (Konsonanten und Vokale). Elektronische Geräte Die ersten computergestützten Sprachsynthesesysteme stammen aus den späten 1950er Jahren. Noriko Umeda et al. hat 1968 das erste allgemeine englische Text-zu-Sprache-System am Elektrotechnischen Labor in Japan entwickelt. 1961 Physiker John Larry Kelly, Jr und seine Kollegin Louis Gerstman nutzten einen IBM 704 Computer, um Reden zu synthetisieren, ein Ereignis unter den prominentesten in der Geschichte von Bell Labs.Kellys Sprachrecorder-Synthesizer (vocoder) rekonstruierte den Song "Daisy Bell", mit musikalischer Begleitung von Max Mathews. Übrigens besuchte Arthur C. Clarke seinen Freund und Kollegen John Pierce im Bell Labs Murray Hill. Clarke war so beeindruckt von der Demonstration, dass er es in der klimatischen Szene seines Drehbuchs für seinen Roman verwendet 2001:Ein Space Odyssey, wo der HAL 9000 Computer das gleiche Lied singt, wie Astronaut Dave Bowman es zum Schlafen bringt. Trotz des Erfolgs der rein elektronischen Sprachsynthese setzt die Forschung zu mechanischen Sprachsynthesen fort. Lineare prädiktive Kodierung (LPC), eine Form der Sprachcodierung, begann die Entwicklung mit der Arbeit von Fumitada Itakura der Nagoya Universität und Shuzo Saito von Nippon Telegraph und Telefon (NTT) 1966. Weitere Entwicklungen in der LPC-Technologie wurden von Bishnu S. Atal und Manfred R. Schroeder in Bell Labs in den 1970er Jahren gemacht. LPC war später die Basis für frühe Sprachsynthesizerchips, wie die Texas Instruments LPC Speech Chips, die in den Speak & Spell Spielzeugen von 1978 verwendet wurden. 1975 entwickelte Fumitada Itakura die Linienspektralpaare (LSP)-Methode zur Hochkompressionsredecodierung, während bei NTT. Von 1975 bis 1981 untersuchte Itakura Probleme bei der Sprachanalyse und Synthese auf der Grundlage der LSP-Methode. 1980 entwickelte sein Team einen LSP-basierten Sprachsynthesizerchip. LSP ist eine wichtige Technologie für die Sprachsynthese und Codierung, und in den 1990er Jahren wurde von fast allen internationalen Sprachcodierungsstandards als wesentliche Komponente angenommen, was zur Verbesserung der digitalen Sprachkommunikation über mobile Kanäle und das Internet beiträgt. 1975 wurde MUSA freigelassen und war eines der ersten Sprachsynthesesysteme. Es bestand aus einer eigenständigen Computerhardware und einer spezialisierten Software, die es ermöglichte, Italienisch zu lesen. Eine 1978 veröffentlichte zweite Version konnte Italienisch auch im "a cappella"-Stil singen. Dominante Systeme in den 1980er und 1990er Jahren waren das DECtalk-System, das im Wesentlichen auf der Arbeit von Dennis Klatt am MIT und dem Bell Labs-System basierte; letzteres war eines der ersten mehrsprachigen sprachunabhängigen Systeme, die einen umfangreichen Einsatz natürlicher Sprachverarbeitungsmethoden ermöglichten. Handheld-Elektronik mit Sprachsynthese begann in den 1970er Jahren. Einer der ersten war der Telesensory Systems Inc. (TSI)Speech+ tragbarer Taschenrechner für die Blinde im Jahr 1976. Andere Geräte hatten vor allem Bildungszwecke, wie das 1978 von Texas Instruments produzierte Speak & Spell Spielzeug. Die Fidelity veröffentlichte 1979 eine Sprachversion ihres elektronischen Schachcomputers. Das erste Videospiel mit der Sprachsynthese war das 1980 Shoot 'em up Arcade-Spiel, Stratovox (bekannt in Japan als Speak & Rescue,) von Sun Electronics. Das erste persönliche Computerspiel mit Sprachsynthese war Manbiki Shoujo (Shoplifting Girl), das 1980 für das PET 2001 veröffentlicht wurde, für das der Entwickler des Spiels Hiroshi Suzuki eine "Nullkreuz" Programmiertechnik entwickelt hat, um eine synthetisierte Sprachwellenform zu erzeugen. Ein weiteres frühes Beispiel, die Arcade-Version von Berzerk, stammt auch aus 1980. Die Milton Bradley Company produzierte das erste Multiplayer-Elektronik-Spiel mit Sprachsynthese, Milton, im gleichen Jahr. Frühe elektronische Sprachsynthesizer klangen robotisch und waren oft kaum verständlich. Die Qualität der synthetisierten Rede hat sich stetig verbessert, aber ab 2016 ist die Ausgabe von zeitgenössischen Sprachsynthesesystemen deutlich von der tatsächlichen menschlichen Rede zu unterscheiden. Synthesische Stimmen klangen typischerweise männlich bis 1990, als Ann Syrdal bei AT&T Bell Laboratories eine weibliche Stimme schuf. Kurzweil prognostizierte im Jahr 2005, dass als Kosten-Leistungs-Verhältnis Sprachsynthesizer billiger und zugänglicher werden ließen, mehr Menschen würden von der Verwendung von Text-zu-Sprach-Programmen profitieren. Synthesizer Technologien Die wichtigsten Qualitäten eines Sprachsynthesesystems sind Natürlichkeit und Verständlichkeit. Natürlichkeit beschreibt, wie eng die Ausgabe wie menschliche Sprache klingt, während die Verständlichkeit die Leichtigkeit ist, mit der die Ausgabe verstanden wird. Der ideale Sprachsynthesizer ist sowohl natürlich als auch verständlich. Sprachsynthesesysteme versuchen in der Regel, beide Eigenschaften zu maximieren. Die beiden primären Technologien, die synthetische Sprachwellenformen erzeugen, sind eine konlatenative Synthese und Formantsynthese. Jede Technologie hat Stärken und Schwächen, und die beabsichtigte Verwendung eines Synthesesystems wird in der Regel bestimmen, welcher Ansatz verwendet wird. Concatenation Synthese Concatenative Synthese basiert auf der Verkettung (oder Verstrengung zusammen) von Segmenten der aufgenommenen Rede.In der Regel produziert die konlatenative Synthese die natürlich-schallendste synthetisierte Rede. Unterschiede zwischen natürlichen Sprachvariationen und der Art der automatisierten Techniken zur Segmentierung der Wellenformen führen jedoch manchmal zu hörbaren Glimmungen in der Ausgabe. Es gibt drei Hauptsubtypen der konlatenativen Synthese. Synthese der Auswahl Einheitenauswahlsynthese verwendet große Datenbanken der aufgezeichneten Sprache. Während der Datenbank-Erstellung wird jede aufgezeichnete Äußerung in einige oder alle der folgenden Segmente segmentiert: einzelne Telefone, Diphone, Halbphone, Silben, Morpheme, Wörter, Phrasen und Sätze. Typischerweise erfolgt die Aufteilung in Segmente unter Verwendung eines speziell modifizierten Spracherkenners, der auf einen "gezwungenen Ausrichtungsmodus" mit manueller Korrektur nachher eingestellt wird, unter Verwendung von visuellen Darstellungen wie der Wellenform und dem Spektrogramm. Ein Index der Einheiten in der Sprachdatenbank wird dann basierend auf den Segmentierungs- und akustischen Parametern wie der Grundfrequenz (Pitch,) Dauer, Position in der Silbe und benachbarten Telefonen erstellt. Zur Laufzeit wird die gewünschte Zielauswertung durch Bestimmung der besten Kette von Kandidateneinheiten aus der Datenbank (Einheitsauswahl) erstellt. Dieses Verfahren wird typischerweise mit einem speziell gewichteten Entscheidungsbaum erreicht. Die Einheitenauswahl bietet die größte Natürlichkeit, da sie nur eine geringe Menge an digitaler Signalverarbeitung (DSP) auf die aufgenommene Sprache anwendet. DSP macht oft aufgezeichnete Redegeräusche weniger natürlich, obwohl einige Systeme eine kleine Menge an Signalverarbeitung an der Stelle der Verkettung verwenden, um die Wellenform zu glätten. Der Ausgang der besten Unit-Selektions-Systeme ist oft von realen menschlichen Stimmen, vor allem in Kontexten, für die das TTS-System abgestimmt wurde, nicht zu unterscheiden. Die maximale Natürlichkeit erfordert jedoch typischerweise eine sehr große Auswahl an Sprachdatenbanken, in einigen Systemen, die in die Gigabyte der aufgezeichneten Daten reichen, was Dutzende von Stunden Rede darstellt. Auch sind Einheitsauswahlalgorithmen bekannt, Segmente von einem Ort auszuwählen, der zu weniger als idealer Synthese führt (z.B. kleinere Wörter werden unklar), auch wenn eine bessere Wahl in der Datenbank besteht. Vor kurzem haben Forscher verschiedene automatisierte Methoden vorgeschlagen, um unnatürliche Segmente in Unit-Selektion Sprachsynthese-Systeme zu erkennen. Diphone-Synthese Diphone-Synthese verwendet eine minimale Sprachdatenbank, die alle in einer Sprache auftretenden Diphone (sound-to-sound-Übergänge) enthält. Die Anzahl der Diphone hängt von der Phonotaktischkeit der Sprache ab: zum Beispiel Spanisch hat etwa 800 Diphone und Deutsch etwa 2500. Bei der Diphonsynthese ist in der Sprachdatenbank nur ein Beispiel jedes Diphons enthalten. Zur Laufzeit wird die Zielprosodie eines Satzes diesen Minimaleinheiten mittels digitaler Signalverarbeitungstechniken wie linearer Prädiktionscodierung, PSOLA oder MBROLA überlagert.oder neuere Techniken wie Pechmodifikation im Quellbereich mittels diskreter Cosinustransformation. Die Diphone-Synthese leidet an den sonic glitches der konlatenativen Synthese und der robotisch-schallenden Natur der formanten Synthese und hat nur wenige der Vorteile eines anderen Ansatzes als kleiner Größe. Dadurch sinkt der Einsatz in kommerziellen Anwendungen, obwohl er weiterhin in der Forschung eingesetzt wird, da es eine Reihe frei verfügbarer Software-Implementierungen gibt. Ein frühes Beispiel der Diphone-Synthese ist ein Lehrroboter, Leachim, der von Michael J. Freeman erfunden wurde. Leachim enthielt Informationen über die Klassencurricular und bestimmte biografische Informationen über die Schüler, die es zu lehren programmiert wurde. Es wurde in einem vierten Klassenzimmer in der Bronx, New York getestet. Domain-spezifische Synthese Domänen-spezifische Synthese setzt vorausgezeichnete Wörter und Phrasen zusammen, um vollständige Äußerungen zu erstellen. Es wird in Anwendungen verwendet, in denen die Vielfalt der Texte, die das System ausgeben wird, auf eine bestimmte Domäne beschränkt ist, wie z.B. Versandterminansagen oder Wetterberichte. Die Technologie ist sehr einfach zu implementieren und ist seit langem im kommerziellen Einsatz, in Geräten wie Sprechuhren und Taschenrechnern. Der Grad der Natürlichkeit dieser Systeme kann sehr hoch sein, weil die Vielfalt der Satztypen begrenzt ist, und sie passen eng an die Prosody und Innation der Originalaufnahmen. Da diese Systeme durch die Wörter und Phrasen in ihren Datenbanken begrenzt sind, sind sie nicht allgemein anwendbar und können nur die Kombinationen von Wörtern und Phrasen, mit denen sie vorprogrammiert wurden, synthetisieren. Die Mischung von Wörtern innerhalb natürlich gesprochener Sprache kann jedoch noch Probleme verursachen, es sei denn, die vielen Variationen werden berücksichtigt.So wird z.B. in nicht-rhotischen Dialekten der englischen Sprache die r in Wörtern wie klares locklcusə in der Regel nur dann ausgesprochen, wenn das folgende Wort ein Vokal als sein erster Buchstabe hat (z.B. "clear out" wird als ε ε ε ε Γοίοίοίοης angegeben). Ebenso auf Französisch werden viele Endkonsonanten nicht mehr schweigen, wenn gefolgt von einem Wort, das mit einem Vokal beginnt, eine Wirkung genannt Liaison. Diese Änderung kann nicht durch ein einfaches Wort-Konkatenationssystem wiedergegeben werden, das zusätzliche Komplexität erfordern würde, kontextempfindlich zu sein. Formante Synthese Formante Synthese verwendet keine menschlichen Sprachproben zur Laufzeit. Stattdessen wird die synthetisierte Sprachausgabe durch additive Synthese und ein akustisches Modell (physikalische Modellierungssynthese) erzeugt. Parameter wie Grundfrequenz, Voicing und Geräuschpegel werden im Laufe der Zeit variiert, um eine Wellenform der künstlichen Sprache zu erzeugen. Dieses Verfahren wird manchmal als regelbasierte Synthese bezeichnet; jedoch haben viele konlatenative Systeme auch regelbasierte Komponenten. Viele Systeme auf Basis von Formant-Synthese-Technologie erzeugen künstliche, robotisch-schallende Rede, die für menschliche Rede niemals irrt. Die maximale Natürlichkeit ist jedoch nicht immer das Ziel eines Sprachsynthesesystems, und formante Synthesesysteme haben Vorteile gegenüber konlatenativen Systemen. Formant-synthesize Sprache kann auch bei sehr hohen Geschwindigkeiten zuverlässig verständlich sein, um die akustischen Glieder zu vermeiden, die häufig plague concatenative Systeme. Hochgeschwindigkeits-Synthetisierungsrede wird von der visuell beeinträchtigt, um Computer mit einem Bildschirmleser schnell zu navigieren. Formante Synthesizer sind in der Regel kleinere Programme als kontaminierende Systeme, da sie keine Datenbank von Sprachproben aufweisen. Sie können daher in eingebetteten Systemen eingesetzt werden, wo Speicher- und Mikroprozessorleistung besonders begrenzt sind. Da formantbasierte Systeme die vollständige Kontrolle aller Aspekte der Ausgaberede haben, können eine Vielzahl von Prosodien und Intonationen ausgegeben werden, die nicht nur Fragen und Aussagen, sondern eine Vielzahl von Emotionen und Tönen der Stimme vermitteln. Beispiele für nicht-real-time aber hochgenaue Intonationssteuerung in Formant-Synthese sind die Arbeiten in den späten 1970er Jahren für die Texas Instruments Spielzeug Speak & Spell, und in den frühen 1980er Jahren Sega Arcade-Maschinen und in vielenAtari, Inc. Arcade-Spiele mit den TMS5220 LPC Chips. Die Schaffung einer richtigen Intonation für diese Projekte war schmerzhaft, und die Ergebnisse müssen noch durch Echtzeit-Text-zu-Sprach-Schnittstellen angepasst werden. Articulatory-Synthese Articulatory-Synthese bezieht sich auf rechnerische Techniken zur Synthesisierung von Sprache basierend auf Modellen des menschlichen Gesangstrakts und der dort auftretenden Artikulationsprozesse. Der erste artikuläre Synthesizer, der regelmäßig für Laborexperimente verwendet wurde, wurde Mitte der 1970er Jahre von Philip Rubin, Tom Baer und Paul Mermelstein entwickelt. Dieser Synthesizer, bekannt als ASY, basiert auf Vokaltraktmodellen, die in den 1960er und 1970er Jahren von Paul Mermelstein, Cecil Coker und Kollegen entwickelt wurden. Bisher wurden artikuläre Synthesemodelle nicht in kommerzielle Sprachsynthesesysteme integriert. Eine bemerkenswerte Ausnahme ist das von Trillium Sound Research entwickelte und vermarktete NeXT-basierte System, ein Spin-off-Unternehmen der University of Calgary, wo ein Großteil der ursprünglichen Forschung durchgeführt wurde. Nach dem Absturz der verschiedenen Inkarnationen von NeXT (gestartet von Steve Jobs in den späten 1980er-Jahren und 1997 mit Apple Computer zusammengeführt) wurde die Trillium-Software unter der GNU General Public License veröffentlicht, wobei die Arbeit als Gnuspeech weitergeführt wurde. Das 1994 erstmals vermarktete System bietet eine vollständige gelenksbasierte Text-Peech-Umsetzung mit einem Wellenleiter oder übertragungslinienanaloga der menschlichen Mund- und Nasentrakte, die von Carrés "distinctive region model" gesteuert werden. Neuere Synthesizer, entwickelt von Jorge C. Lucero und Kollegen, enthalten Modelle der Vokalfaltenbiomechanik, der glottalen Aerodynamik und der akustischen Wellenausbreitung in den Bronqui-, Traquea-, Nasen- und Mundhöhlen und bilden somit vollständige Systeme der physikalischen Sprachsimulation. HMM-basierte Synthese HMM-basierte Synthese ist eine Synthesemethode basierend auf versteckten Markov-Modellen, auch als statistische Parametric Synthesis bezeichnet. In diesem System werden das Frequenzspektrum (vocal tract,) Grundfrequenz (voice source,) und Dauer (prosody) der Rede gleichzeitig von HMMs modelliert. Speech-Wellenformen werden von HMMs selbst nach dem maximalen Wahrscheinlichkeitskriterium erzeugt. Sinewave-Synthese Sinewave-Synthese ist eine Technik zur Synthesisierung von Sprache, indem die Formanten (Hauptbänder der Energie) durch reine Tonpfeifen ersetzt werden.Tiefe lernbasierte Synthese Formulation Bei einem Eingabetext oder einer Sequenz von Spracheinheit Y {\displaystyle Y} kann die Zielrede X {\displaystyle X} durch X = arg ≠ max P (X  of Y, θ ) {\displaystyle X=\arg \max P(X.Y,\theta )} abgeleitet werden, wo θ\displaystyle \theta } ist. Typischerweise wird der Eingabetext zunächst einem akustischen Merkmalsgenerator zugeführt, dann werden die akustischen Merkmale an den neuronalen Vokoder geleitet. Für den akustischen Merkmalsgenerator ist die Loss-Funktion typischerweise L1 oder L2 Verlust. Diese Verlustfunktionen setzen eine Einschränkung, dass die Ausgangs-Akustik-Funktionsverteilungen Gaussian oder Laplacian sein müssen. In der Praxis, da die menschliche Sprachband reicht von etwa 300 bis 4000 Die Verlustfunktion wird so konzipiert, dass sie mehr Strafe in diesem Bereich hat: l o s s = α loss human + ( 1 − α ) loss other {\displaystyle loss=\alpha text{loss}_{\text{human}+(1-\alpha )text{loss}_{\text{other where loss human {\displaystyle text{loss}}}_{\text{human is Das akustische Merkmal ist typischerweise Spektrogramm oder Spektrogramm in Mel-Skala. Diese Merkmale erfassen die zeitliche Frequenzbeziehung des Sprachsignals und reichen somit aus, mit diesen akustischen Merkmalen intelligente Ausgänge zu erzeugen. Die bei der Spracherkennungsaufgabe verwendete Mel-Frequenz-Cepstrum-Funktion eignet sich nicht zur Sprachsynthese, da sie zu viel Information reduziert. Kurzgeschichte Im September 2016 schlug DeepMind WaveNet vor, ein tiefes generatives Modell von rohen Audio-Wellenformen. Dies zeigt der Gemeinschaft, dass tiefe lernbasierte Modelle die Fähigkeit haben, Rohwellenformen zu modellieren und bei der Erzeugung von Sprache aus akustischen Merkmalen wie Spektrogrammen oder Spektrogrammen im Mel-Skala oder sogar aus einigen vorverarbeiteten sprachlichen Merkmalen gut durchzufÃ1⁄4hren. Anfang 2017 schlug Mila (Forschungsinstitut) char2wav vor, ein Modell zur Herstellung von Rohwellenform in einer End-to-End-Methode. Auch Google und Facebook schlugen Tacotron bzw. VoiceLoop vor, akustische Eigenschaften direkt aus dem Eingabetext zu erzeugen. Im späteren Jahr schlug Google Tacotron2 vor, der den WaveNet Vocoder mit der überarbeiteten Tacotron-Architektur kombinierte, um die End-to-End-Sprachsynthese durchzuführen. Tacotron2 kann eine hochwertige Rede erzeugen, die der menschlichen Stimme nähert. Seitdem wurden End-to-End-Methoden zum heißesten Forschungsthema, weil viele Forscher auf der ganzen Welt die Macht des End-to-End-Sprach-Synthesizers bemerken. Vorteile und Nachteile Die Vorteile von End-to-End-Verfahren sind wie folgt: Nur ein einziges Modell, um Textanalyse, akustische Modellierung und Audiosynthese durchzuführen, d.h. die Sprache direkt von Zeichen synchronisieren Weniger Feature-Engineering Einfach ermöglicht eine reiche Konditionierung auf verschiedenen Attributen, z.B. Lautsprecher oder Sprache Anpassung an neue Daten ist einfacher robuster als mehrstufige Modelle, weil kein Bauteil Fehler kann Compound Leistungsstarke Modellkapazität, um die versteckten internen Strukturen von Daten zu erfassen Erfassbar, um verständliche und natürliche Sprache zu erzeugen Keine Notwendigkeit, eine große Datenbank zu halten, d.h. kleine Fußabdruck Auto-regressive-basierte Modelle leiden unter langsamen Inferenz-Problem Ausgangsrede sind nicht robust, wenn Daten nicht ausreichend Mangel an Regelbarkeit im Vergleich zu traditionellen konlatenativen und statistischen parametrischen Ansätzen Tend, um die flache Prosodie durch Mittelung über Trainingsdaten Tend zur Ausgabe von geglätteten akustischen Merkmalen zu lernen, weil der l1- oder l2-Verlust verwendet wird Challenges - langsame Inferenzproblem Um das langsame Inferenzproblem zu lösen, haben Microsoft-Forschung und Baidu-Forschung beide vorgeschlagen, nicht-autoregressive Modelle zu verwenden, um den Inferenzprozess schneller zu machen.Das von Microsoft vorgeschlagene FastSpeech-Modell verwendet Transformer-Architektur mit einem Dauermodell, um das Ziel zu erreichen. Außerdem macht das Dauermodell, das sich aus traditionellen Methoden leiht, die Sprachproduktion robuster.- Robustheitsproblem Forscher fanden heraus, dass das Robustheitsproblem stark mit den Fehlern der Textausrichtung zusammenhängt, und dies treibt viele Forscher dazu an, den Aufmerksamkeitsmechanismus zu überarbeiten, der die starke lokale Beziehung und monotone Spracheigenschaften nutzt.- Problem der Kontrollfähigkeit Um das Kontrollierbarkeitsproblem zu lösen, werden viele Arbeiten über Variationsauto-Encoder vorgeschlagen.- Flaches Prosody-Problem GST-Tacotron kann das flache Prosody-Problem leicht lindern, es hängt jedoch immer noch von den Trainingsdaten ab.- Beruhigtes akustisches Ausgabeproblem Um realistischere akustische Eigenschaften zu erzeugen, kann die GAN Lernstrategie angewendet werden. In der Praxis kann der neurale Vokoder aber auch dann gut verallgemeinern, wenn die Eingabemerkmale glatter sind als reale Daten. Semi-supervised learning Derzeit werden selbstbetreutes Lernen durch eine bessere Nutzung unmarkierter Daten viel Aufmerksamkeit gewinnen. Die Forschung zeigt, dass mit Hilfe von selbsterweiterten Verlusten die Notwendigkeit gepaarter Daten abnimmt. Zero-Shot Lautsprecher-Adaption Zero-Shot Lautsprecher-Adaption ist vielversprechend, weil ein einziges Modell Sprache mit verschiedenen Lautsprecher-Stilen und -Charakteristik erzeugen kann. Im Juni 2018 hat Google vorgeschlagen, das vortrainierte Lautsprecher-Verifikationsmodell als Lautsprecher-Encoder zu verwenden, um die Lautsprechereinbettung zu extrahieren. Der Lautsprecher-Encoder wird dann Teil des neuralen Text-zu-Sprach-Modells und kann den Stil und die Charakteristik der Ausgabesprache bestimmen. Dies zeigt die Community, dass nur ein einziges Modell verwendet wird, um Sprache von mehreren Stil zu erzeugen ist möglich. Neural Vokoder Neural Vokoder spielt eine wichtige Rolle bei der tiefen lernbasierten Sprachsynthese, um qualitativ hochwertige Sprache aus akustischen Features zu erzeugen. Das im Jahr 2016 vorgeschlagene WaveNet-Modell erreicht eine große Leistung bei der Sprachqualität. Wavenet faktorisierte die gemeinsame Wahrscheinlichkeit einer Wellenform x = EPMATHMARKEREP {\displaystyle \mathbf} =x_{1},...,x_{T als Produkt von bedingten Wahrscheinlichkeiten wie folgt p θ ( x ) = ∏ t = 1 T p ( x t | x 1 ,... x t - 1 ) (\displaystyle p_{\theta }(\mathbf {x}=)\prod t=1}^{T}p(x_{t}|x_{1}...,x_{t-1) Wo θ {\displaystyle \theta } der Modellparameter mit vielen erweiterten Faltungsschichten ist. Daher wird jede Audio-Probe x t {\displaystyle x_{t} auf die Samples zu allen vorherigen Zeitschritten konditioniert. Doch die autoregressive Natur von WaveNet macht den Inferenzprozess dramatisch langsam. Zur Lösung des langsamen Inferenzproblems, das aus der autoregressiven Charakteristik des WaveNet-Modells kommt, wird Parallel WaveNet vorgeschlagen. Parallel WaveNet ist ein inverses autoregressives Flow-basiertes Modell, das durch Wissensdestillation mit einem vortrainierten Lehrer WaveNet-Modell trainiert wird. Da das inverse autoregressive Flow-basierte Modell bei der Inferenz nicht autoregressiv ist, ist die Inferenzgeschwindigkeit schneller als Echtzeit. Inzwischen hat Nvidia ein strömungsbasiertes WaveGlow-Modell vorgeschlagen, das auch eine Sprache mit schneller als Echtzeitgeschwindigkeit erzeugen kann. Trotz der hohen Inferenzgeschwindigkeit hat Parallel WaveNet jedoch die Einschränkung der Notwendigkeit eines vortrainierten WaveNet-Modells und WaveGlow dauert viele Wochen, um mit begrenzten Rechengeräten zusammenzuarbeiten. Diese Frage wird von Parallel WaveGAN gelöst, die durch Multi-Resolution-Spektralverlust und GANs-Lernstrategie eine Rede produziert. Herausforderungen der Textnormalisierung Der Prozess der Normalisierung von Text ist selten unkompliziert.Texte sind voll von Heteronymen, Zahlen und Abkürzungen, die alle eine Erweiterung in eine phonetische Darstellung erfordern. Es gibt viele Schreibweisen auf Englisch, die auf Kontexten unterschiedlich ausgeprägt sind. Zum Beispiel "Mein neuestes Projekt ist zu lernen, wie ich meine Stimme besser projizieren kann" enthält zwei Aussprachen von Projekt". Die meisten text-to-speech (TTS)-Systeme erzeugen keine semantischen Darstellungen ihrer Eingabetexte, da solche Prozesse unzuverlässig, schlecht verstanden und rechnerisch unwirksam sind. Infolgedessen werden verschiedene heuristische Techniken verwendet, um den richtigen Weg zu erraten, Homographen zu disambiguieren, wie die Prüfung benachbarter Wörter und die Verwendung von Statistiken über Häufigkeit des Auftretens. In jüngster Zeit haben TTS-Systeme begonnen, HMMs zu verwenden, um "Teile der Rede" zu erzeugen, um Homographen zu disambiguieren. Diese Technik ist für viele Fälle sehr erfolgreich, wie z.B. ob die Lese als rot markiert werden sollte, was die Vergangenheit anbetrifft, oder als Reed, das die Gegenwart anzeigt. Typische Fehlerquoten bei der Verwendung von HMMs sind in der Regel unter fünf Prozent. Diese Techniken funktionieren auch für die meisten europäischen Sprachen gut, obwohl der Zugang zu den erforderlichen Ausbildungseinrichtungen in diesen Sprachen häufig schwierig ist. Die Entscheidung, wie man Zahlen umwandelt, ist ein weiteres Problem, das TTS-Systeme ansprechen müssen. Es ist eine einfache Programmier-Herausforderung, um eine Zahl in Wörter (zumindest auf Englisch), wie 1325 wird "eintausend dreihundert fünfundzwanzig. "Aber in vielen verschiedenen Kontexten treten Zahlen auf; 1325 kann auch als "ein drei zwei fünf", "drittzehn fünfundzwanzig" oder "drittzehn hundert und zwanzig fünf" gelesen werden. Ein TTS-System kann oft die Erweiterung einer Anzahl basierend auf umgebenden Wörtern, Zahlen und Pünktlichkeit, und manchmal bietet das System eine Möglichkeit, den Kontext anzugeben, wenn es mehrdeutig ist. Römische Ziffern können je nach Kontext auch unterschiedlich gelesen werden. Zum Beispiel liest "Henry VIII" als "Henry the Eighth", während "Chapter VIII" als "Chapter Eight" liest. Ebenso können Abkürzungen mehrdeutig sein. Zum Beispiel muss die Abkürzung für Zoll von dem Wort in differenziert werden, und die Adresse "12 St. John St." verwendet die gleiche Abkürzung für Saint und Street". TTS-Systeme mit intelligenten vorderen Enden können erraten über mehrdeutige Abkürzungen, während andere das gleiche Ergebnis in allen Fällen liefern, was zu nicht-sensischen (und manchmal komischen) Ausgängen, wie "Ulysses S. Grant" als "Ulysses South Grant" gemacht wird. Text-zu-Telefon Herausforderungen Sprachsynthese-Systeme verwenden zwei grundlegende Ansätze, um die Aussprache eines Wortes basierend auf seiner Schreibweise zu bestimmen, ein Prozess, der oft als Text-zu-Telefon oder Graphem-zu-Telefon-Umwandlung bezeichnet wird (phoneme ist der Begriff, der von Linguisten verwendet wird, um unverwechselbare Geräusche in einer Sprache zu beschreiben). Der einfachste Ansatz zur Text-zu-Phonem-Wandlung ist der wörterbuchbasierte Ansatz, bei dem ein großes Wörterbuch, das alle Wörter einer Sprache und ihre korrekten Aussprachen enthält, vom Programm gespeichert wird. Die Bestimmung der korrekten Aussprache jedes Wortes ist eine Angelegenheit, jedes Wort im Wörterbuch zu betrachten und die Rechtschreibung durch die im Wörterbuch angegebene Aussprache zu ersetzen. Der andere Ansatz ist regelbasierte, in der Ausspracheregeln auf Wörter angewendet werden, um ihre Aussprachen auf Grundlage ihrer Rechtschreibungen zu bestimmen. Dies ist ähnlich wie die "Ausklingen", oder synthetische Phonics, Ansatz zum Lesen. Jeder Ansatz hat Vorteile und Nachteile. Der wörterbuchbasierte Ansatz ist schnell und genau, aber es scheitert völlig, wenn es ein Wort gegeben wird, das nicht in seinem Wörterbuch ist. Da die Wörterbuchgröße wächst, macht auch die Speicherplatzanforderungen des Synthesesystems. Andererseits arbeitet der regelbasierte Ansatz an jeder Eingabe, aber die Komplexität der Regeln wächst wesentlich, da das System unregelmäßige Rechtschreibungen oder Aussprachen berücksichtigt. (Bedenken Sie, dass das Wort in englischer Sprache sehr verbreitet ist, doch ist das einzige Wort, in dem der Buchstabe f ausgesprochen wird [v].) Dadurch verwenden fast alle Sprachsynthesesysteme eine Kombination dieser Ansätze. Sprachen mit einer telefonischen Orthographie haben ein sehr regelmäßiges Schreibsystem, und die Vorhersage der Aussprache von Wörtern auf der Grundlage ihrer Schreibweisen ist recht erfolgreich. Sprachsynthese-Systeme für solche Sprachen verwenden oft die regelbasierte Methode ausgiebig, auf Wörterbücher nur für diese wenigen Wörter, wie ausländische Namen und Anleihen, deren Aussprachen sind nicht offensichtlich von ihren Rechtschreibungen. Andererseits sind Sprachsynthesesysteme für Sprachen wie Englisch, die extrem unregelmäßige Schreibsysteme haben, eher auf Wörterbücher angewiesen, und regelbasierte Methoden nur für ungewöhnliche Wörter zu verwenden, oder Wörter, die nicht in ihren Wörterbüchern sind.Herausforderungen der Evaluierung Die konsequente Auswertung von Sprachsynthesesystemen kann wegen mangelnder allgemein vereinbarter objektiver Bewertungskriterien schwierig sein. Verschiedene Organisationen verwenden häufig verschiedene Sprachdaten. Die Qualität der Sprachsynthesesysteme hängt auch von der Qualität der Herstellungstechnik (die eine analoge oder digitale Aufzeichnung beinhalten kann) und von den Einrichtungen, die zur Wiedergabe der Sprache verwendet werden. Die Bewertung von Sprachsynthesesystemen wurde daher oft durch Unterschiede zwischen Produktionstechniken und Replay-Einrichtungen beeinträchtigt. Seit 2005 haben einige Forscher jedoch begonnen, Sprachsynthesesysteme mit einem gemeinsamen Sprachdatensatz zu bewerten. Vorgesetzte und emotionale Inhalte Eine Studie in der Zeitschrift Speech Communication von Amy Drahota und Kollegen an der University of Portsmouth, UK, berichtete, dass Hörer auf Sprachaufnahmen feststellen konnten, auf besser als Zufallsstufen, ob der Sprecher lächelte oder nicht. Es wurde vorgeschlagen, dass die Identifizierung der Vocal-Features, dass Signal emotionalen Inhalt verwendet werden kann, um synthetisierte Rede Klang natürlicher zu machen. Eines der damit verbundenen Probleme ist die Änderung der Pitchkontur des Satzes, je nachdem, ob es sich um einen bejahenden, verhördlichen oder ausrufenden Satz handelt. Eines der Techniken zur Pechmodifikation verwendet diskrete Cosinus-Transformation in der Source-Domäne (lineare Prädiktion Rest). Solche Pitch-Synchron-Pitch-Modifikationstechniken benötigen eine Priori-Pitch-Markierung der Synthese-Sprachdatenbank unter Verwendung von Techniken wie der Epoch-Extraktion unter Verwendung eines dynamischen Plosion-Index, der auf den integrierten linearen Prädiktionsrest der gesprochenen Sprachbereiche aufgebracht wird. Dedizierte Hardware Icophone General Instrument SP0256-AL2 National Semiconductor DT1050 Digitalker (Mozer – Forrest Mozer)Texas Instrumente LPC Speech Chips Hardware und Software Systeme Beliebte Systeme bieten Sprachsynthese als integrierte Fähigkeit. Matt Die Mattel Intellivision Spielkonsole bot 1982 das Intellivoice Voice Synthesis Modul an. Es umfasste den SP0256 Narrator Sprachsynthesizer-Chip auf einer abnehmbaren Kartusche. Der Narrator hatte 2kB von Read-Only Memory (ROM,) und dies wurde verwendet, um eine Datenbank mit generischen Wörtern zu speichern, die kombiniert werden könnten, um Phrasen in Intellivision-Spiele zu machen. Da der Orator-Chip auch Sprachdaten aus dem externen Speicher annehmen könnte, könnten weitere Wörter oder Phrasen, die benötigt werden, innerhalb der Kartusche selbst gespeichert werden. Die Daten bestanden aus Strings von Analogfilterkoeffizienten, um das Verhalten des synthetischen Gesangstraktmodells des Chips zu verändern, anstatt einfache digitalisierte Proben. SAM Auch im Jahr 1982 veröffentlicht, Software Automatic Mouth war das erste kommerzielle All-Software-Sprachsynthese-Programm. Es wurde später als Basis für Macintalk verwendet. Das Programm war für nicht-Macintosh Apple-Computer (einschließlich der Apple II und der Lisa) verschiedene Atari-Modelle und den Commodore 64 verfügbar. Die Apple-Version bevorzugte zusätzliche Hardware, die DACs enthielt, obwohl sie stattdessen die Ein-Bit-Audioausgabe des Computers (mit der Hinzufügung von viel Verzerrung) verwenden konnte, wenn die Karte nicht vorhanden war. Der Atari nutzte den eingebetteten POKEY Audiochip. Sprachwiedergabe auf der Atari normalerweise deaktiviert unterbrechen Anfragen und schließen den ANTIC-Chip während der Vokalausgabe ab. Die hörbare Ausgabe ist extrem verzerrt Sprache, wenn der Bildschirm ist. Der Commodore 64 nutzte den 64er eingebetteten SID-Audiochip. Atari Arguly, das erste Sprachsystem, das in ein Betriebssystem integriert war, waren die 1400XL/1450XL Personalcomputer, die von Atari, Inc. mit dem Votrax SC01 Chip im Jahre 1983 entworfen wurden. Die 1400XL/1450XL Computer nutzten eine Finite State Machine, um World English Spelling Text-to-speech-Synthese zu ermöglichen. Leider wurden die 1400XL/1450XL Personalcomputer nie in Menge versendet. Die Atari ST Computer wurden mit stspeech.tos auf Diskette verkauft. Apfel Das erste Sprachsystem in ein Betriebssystem integriert, das in Menge versendet wurde, war Apple Computers MacInTalk. Die Software wurde von Entwicklern der 3. Partei Joseph Katz und Mark Barton (später, SoftVoice, Inc.) lizenziert und wurde während der Einführung des Macintosh-Computers 1984 vorgestellt. Dieser Januar Demo benötigt 512 Kilobytes RAM-Speicher. Infolgedessen konnte es nicht in den 128 Kilobytes RAM der erste Mac tatsächlich mit versendet laufen. So wurde die Demo mit einem Prototyp 512k Mac erreicht, obwohl die Teilnehmer nicht davon erzählt wurden und die Synthese-Demo erstellt erhebliche Aufregung für den Macintosh. In den frühen 1990er-Jahren erweiterte Apple seine Fähigkeiten und bietet System breite Text-zu-Sprache Unterstützung.Mit der Einführung von schnelleren PowerPC-basierten Computern umfassten sie eine höhere Qualität Sprachsampling. Apple führte auch Spracherkennung in seine Systeme ein, die einen Fluid-Befehlssatz zur Verfügung gestellt. In letzter Zeit hat Apple Sample-basierte Stimmen hinzugefügt. Das Sprachsystem von Apple Macintosh hat sich als Neugierde zu einem voll unterstützten Programm, PlainTalk, für Menschen mit Sehproblemen entwickelt. Stimme Over war zum ersten Mal 2005 in Mac OS X Tiger (10.4) vorgestellt. Während 10.4 (Tiger) und erste Versionen von 10.5 (Leopard) gab es nur eine Standard-Sprachversand mit Mac OS X.Starting mit 10.6 (Snow Leopard,) kann der Benutzer aus einer breiten Palette von mehreren Stimmen wählen. Stimme Über Stimmen verfügen über die Einnahme von realistisch klingenden Atemzügen zwischen Sätzen sowie eine verbesserte Klarheit bei hohen Leseraten über PlainTalk. Mac OS X enthält auch sagen, eine befehlszeilenbasierte Anwendung, die Text in hörbare Sprache umwandelt. Die AppleScript Standard-Additionen enthalten ein Sagenverb, das es einem Skript ermöglicht, eine der installierten Stimmen zu verwenden und die Tonhöhe, Sprechrate und Modulation des gesprochenen Textes zu steuern. Amazon Gebraucht in Alexa und als Software als Service in AWS (ab 2017.) Amiga Sicherheit Das zweite Betriebssystem, das fortgeschrittene Sprachsynthesefähigkeiten bietet, war AmigaOS, das 1985 eingeführt wurde. Die Sprachsynthese wurde von Commodore International von SoftVoice, Inc. lizenziert, die auch das ursprüngliche MacinTalk Text-zu-Sprach-System entwickelt. Es verfügte über ein komplettes System der Sprach-Emulation für Amerikanisches Englisch, mit männlichen und weiblichen Stimmen und Stress-Indikatoren-Marker, die durch das Audio-Chipset von Amiga ermöglicht werden. Das Synthesesystem wurde in eine Übersetzer-Bibliothek unterteilt, die unbeschränkten englischen Text in einen Standardsatz von phonetischen Codes und ein Erzählgerät umwandelte, das ein formales Modell der Spracherzeugung implementierte. AmigaOS verfügte auch über einen hochrangigen "Speak Handler", der Befehlszeilenbenutzer erlaubte, Textausgabe an Sprache umzuleiten. Speech-Synthese wurde gelegentlich in Drittprogrammen verwendet, insbesondere Wortverarbeiter und Bildungssoftware. Die Synthesesoftware blieb weitgehend unverändert von der ersten AmigaOS-Release und Commodore schließlich entfernte Sprachsyntheseunterstützung von AmigaOS 2.1 weiter. Trotz der englischen Einschränkung wurde eine inoffizielle Version mit mehrsprachiger Sprachsynthese entwickelt. Dies nutzte eine erweiterte Version der Übersetzer-Bibliothek, die eine Reihe von Sprachen übersetzen konnte, angesichts einer Reihe von Regeln für jede Sprache. Microsoft Windows Modern Windows Desktop-Systeme können SAPI 4 und SAPI 5 Komponenten verwenden, um Sprachsynthese und Spracherkennung zu unterstützen. SAPI 4.0 war als optionales Add-on für Windows 95 und Windows 98 erhältlich. Windows 2000 fügte Narrator, ein Text-zu-Sprache Dienstprogramm für Menschen, die Sehbehinderung haben. Drittanbieter-Programme wie JAWS für Windows, Fenster-Eyes, Nicht-visuelle Desktop-Zugriff, Supernova und System-Zugriff können verschiedene Text-zu-Sprach-Aufgaben wie Lesen Text laut von einer bestimmten Website, E-Mail-Konto, Textdokument, die Windows-Clipboard, die Tastatur-Eingabe des Benutzers usw. ausführen. Nicht alle Programme können Sprachsynthese direkt verwenden. Einige Programme können Plug-ins, Erweiterungen oder Add-ons verwenden, um Text laut zu lesen. Drittanbieter-Programme sind verfügbar, die Text aus der System-Clipboard lesen können. Microsoft Speech Server ist ein serverbasiertes Paket zur Sprachsynthese und -erkennung. Es ist für die Netzwerknutzung mit Webanwendungen und Callcentern konzipiert. Texas Instruments TI-99/4AIn in den frühen 1980er Jahren, TI war als Pionier in der Sprachsynthese bekannt, und ein sehr beliebtes Plug-in-Sprach-Synthesizer-Modul für die TI-99/4 und 4A. Sprachsynthesizer wurden mit dem Kauf einer Reihe von Kartuschen kostenlos angeboten und wurden von vielen TI-geschriebenen Videospielen verwendet (bemerkenswerte Titel, die bei dieser Promotion mit Sprache angeboten wurden, waren Alpiner und Parsec). Der Synthesizer verwendet eine Variante der linearen Prädiktionscodierung und hat einen kleinen eingebauten Vokabular. Die ursprüngliche Absicht bestand darin, kleine Kartuschen freizugeben, die direkt in die Synthesizereinheit eingesteckt wurden, was das eingebaute Vokabular des Geräts erhöhen würde. Der Erfolg der Software Text-zu-Sprache in der Terminal Emulator II Kartusche hat diesen Plan jedoch aufgehoben. Votrax Von 1971 bis 1996 produzierte Votrax eine Reihe von kommerziellen Sprachsynthesizerkomponenten. Ein Votrax-Synthesizer wurde in der ersten Generation Kurzweil Reading Machine für Blind enthalten. Text-zu-Sprachsysteme Text-zu-Speech (TTS) bezieht sich auf die Fähigkeit von Computern, Text laut zu lesen.Ein TTS Engine konvertiert geschriebenen Text in eine phonemische Darstellung, wandelt dann die phonemische Darstellung in Wellenformen um, die als Klang ausgegeben werden können. TTS-Motoren mit verschiedenen Sprachen, Dialekten und spezialisierten Vokabularen sind über Drittverleger verfügbar. Android Version 1.6 von Android zusätzlich Unterstützung für Sprachsynthese (TTS.) InternetCurrently, es gibt eine Reihe von Anwendungen, Plugins und Gadgets, die Nachrichten direkt von einem E-Mail-Client und Webseiten von einem Web-Browser oder Google Toolbar lesen können. Einige spezialisierte Software kann RSS-Feeds erzählen. Online-RSS-Erzähler vereinfachen einerseits die Informationsbereitstellung, indem Nutzer ihre Lieblingsnachrichtenquellen hören und in Podcasts umwandeln können. Andererseits sind on-line RSS-Reader auf fast jedem PC, der mit dem Internet verbunden ist, verfügbar. Benutzer können generierte Audiodateien auf tragbare Geräte herunterladen, z.B. mit Hilfe von Podcast-Empfänger, und sie beim Gehen, Joggen oder Kommutieren zu hören. Ein wachsendes Feld im Internet basierten TTS ist webbasierte Assistenztechnologie, z.B. Browsealoud von einem britischen Unternehmen und Readspeaker. Es kann TTS-Funktionalität jedem (aus Gründen der Zugänglichkeit, Komfort, Unterhaltung oder Informationen) mit Zugang zu einem Web-Browser liefern. Das gemeinnützige Projekt Pediaphon wurde 2006 gegründet, um der Wikipedia eine ähnliche webbasierte TTS-Schnittstelle zu bieten. Andere Arbeiten werden im Rahmen der W3C durch die W3C Audio Incubator Group unter Beteiligung von The BBC und Google Inc. durchgeführt. Open Source Einige Open-Source-Software-Systeme sind verfügbar, wie zum Beispiel: Festival Speech Synthesis System, das diphonebasierte Synthese verwendet, sowie modernere und besser schallende Techniken. eSpeak, die eine breite Palette von Sprachen unterstützt. gnuspeech, die artikuläre Synthese von der Free Software Foundation verwendet. Nach dem kommerziellen Ausfall der hardwarebasierten Intellivoice, Gaming-Entwickler sparsam verwendet Softwaresynthese in späteren Spielen. Frühere Systeme von Atari, wie die Atari 5200 (Baseball) und die Atari 2600 (Quadrun und Open Sesam), hatten auch Spiele unter Nutzung der Softwaresynthese. Einige E-Book-Reader, wie die Amazon Kindle, Samsung E6,PocketBook eReader Pro, enTourage eDGe, und das Bebook Neo. Das BBC Micro integriert den Texas Instruments TMS5220 Sprachsynthese-Chip, Einige Modelle von Texas Instruments Heimcomputern produziert 1979 und 1981 (Texas Instruments TI-99/4 und TI-99/4A) waren in der Lage, Text-to-phoneme Synthese oder rezitieren komplette Wörter und Phrasen (Text-to-dictionary,) mit einem sehr beliebten Speech Synthesizer Peripherie. TI nutzte einen proprietären Codec, um vollständige gesprochene Phrasen in Anwendungen einzubetten, in erster Linie Videospiele. IBMs OS/2 Warp 4 beinhaltete VoiceType, ein Vorläufer von IBM ViaVoice. GPS Navigationseinheiten von Garmin, Magellan, TomTom und anderen verwenden Sprachsynthese für die Automobilnavigation. Yamaha produzierte 1999 einen Musiksynthesizer, den Yamaha FS1R, der eine formale Synthesefähigkeit beinhaltete. Sequenzen von bis zu 512 einzelnen Vokalen und konsonanten Formanten könnten gespeichert und replayiert werden, so dass kurze Vokalphrasen synthetisiert werden können. Digitale Sound-Alikes Mit der Einführung von Adobe Voco Audio Editing und Generierung von Software-Prototypen, die als Teil der Adobe Creative Suite und der ähnlich aktivierten DeepMind WaveNet verwendet werden, wird eine tiefe neurale Netzwerk-basierte Audiosynthese-Software von Google-Sprachsynthese auf völlig unentscheidbar aus der Stimme eines echten Menschen. Adobe Voco nimmt ca. 20 Minuten von der gewünschten Zielsprache und danach kann es schallähnliche Stimme mit sogar Phonemen erzeugen, die nicht im Trainingsmaterial vorhanden waren. Die Software stellt ethische Bedenken, da sie es erlaubt, andere Volksstimmen zu stehlen und sie zu manipulieren, um alles zu sagen, was gewünscht wird. Auf der Konferenz zu Neural Information Processing Systems (NeurIPS) präsentierten Forscher von Google auf der Konferenz 2018 die Arbeit 'Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis', die das Lernen von der Lautsprecherverifikation überträgt, um Text-zu-Speech-Synthese zu erreichen, die fast wie jeder aus einer Sprachprobe von nur 5 Sekunden klingen kann (hören). Auch Forscher von Baidu Research präsentierten auf der NeurIPS-Konferenz 2018 ein Sprach-Klonsystem mit ähnlichen Zielen, obwohl das Ergebnis ziemlich unbesorgt ist.(hören)Mit 2019 fanden die digitalen Sound-Alikes ihren Weg in die Hände von Kriminellen, wie Symantec Forscher wissen, dass 3 Fälle, in denen digitale Sound-Alikes-Technologie für Verbrechen verwendet wurde.Dies erhöht den Stress auf die Desinformationssituation in Verbindung mit den Tatsachen, dass die menschliche Bildsynthese seit den frühen 2000er Jahren über den Punkt der Unfähigkeit des Menschen hinaus verbessert hat, einen echten Menschen mit einer realen Kamera aus einer Simulation eines Menschen mit einer Simulation einer Kamera zu erzählen.2D-Videofälschung Techniken wurden im Jahr 2016 vorgestellt, die eine nahezu Echtzeitfälschung von Gesichtsausdrücken in bestehenden 2D-Video ermöglichen. In SIGGRAPH 2017 wurde ein Audio-gesteuerter digitaler Look-alike des oberen Rumpfes von Barack Obama von Forschern der University of Washington präsentiert.(Ansicht)Es wurde nur von einem Sprachtrack als Quelldaten für die Animation nach der Trainingsphase zum Erwerb von Lippensync und breitere Gesichtsinformationen aus Trainingsmaterial, bestehend aus 2D-Videos mit Audio, durchgeführt. Im März 2020 wurde eine Freeware-Webapplikation veröffentlicht, die hochwertige Stimmen aus einem Sortiment von fiktionalen Zeichen aus einer Vielzahl von Medienquellen namens 15.ai generiert. Ursprüngliche Charaktere waren GLaDOS von Portal, Twilight Sparkle und Fluttershy von der Show My Little Pony: Friendship Is Magic, und der zehnte Doktor von DoctorWho. Nachfolgende Updates enthalten Wheatley von Portal 2, der Soldat von Team Fortress 2, und die verbleibende Hauptrolle von My Little Pony: Friendship Is Magic. Speech Synthese Markup Sprachen Für die Wiedergabe von Text als Sprache in einem XML-konformen Format wurden mehrere Markupsprachen festgelegt. Das jüngste ist die Rede Synthesis Markup Language (SSML), die 2004 eine W3C Empfehlung wurde. Ältere Sprachsynthese-Markup-Sprachen umfassen Java Speech Markup Language (JSML) und SABLE. Obwohl jede von ihnen als Standard vorgeschlagen wurde, wurden keine von ihnen weitgehend angenommen. Sprachsynthese-Markup-Sprachen unterscheiden sich von Dialog-Markup-Sprachen. VoiceXML umfasst beispielsweise Tags zur Spracherkennung, zum Dialogmanagement und zur Touchtonwahl, zusätzlich zu Text-zu-Sprach-Markup. Anwendungen Sprachsynthese ist seit langem ein lebenswichtiges Hilfstechnologie-Tool und seine Anwendung in diesem Bereich ist signifikant und weit verbreitet. Es ermöglicht die Beseitigung von Umweltbarrieren für Menschen mit einer Vielzahl von Behinderungen. Die längste Anwendung ist in der Verwendung von Bildschirmlesern für Menschen mit Sehbehinderung, aber Text-zu-Sprach-Systeme werden heute häufig von Menschen mit Dyslexie und anderen Leseschwierigkeiten sowie von vor-literate Kinder verwendet. Sie werden häufig auch eingesetzt, um denjenigen mit starker Sprachbeeinträchtigung in der Regel durch eine dedizierte Sprachausgabe-Kommunikationshilfe zu helfen. Eine erwähnte Anwendung der Sprachsynthese war die Kurzweil Reading Machine für den Blind, die Text-zu-Telefontik-Software auf der Grundlage der Arbeit von Haskins Laboratories und ein Black-Box-Synthesizer gebaut von VotraxSpeech Synthesetechniken werden auch in Unterhaltungsproduktionen wie Spiele und Animationen verwendet. Im Jahr 2007 kündigte Animo Limited die Entwicklung eines Software-Anwendungspakets auf Basis seiner Sprachsynthese-Software FineSpeech an, das sich ausdrücklich auf Kunden in der Unterhaltungsindustrie richtete, in der Lage ist, Erzählungen und Dialoglinien nach Benutzerspezifikationen zu erzeugen. Die Anwendung erreichte die Reife im Jahr 2008, als NEC Biglobe einen Web-Service bekannt gab, der es Benutzern erlaubt, Phrasen aus den Stimmen von Zeichen aus der japanischen Anime-Serie Code Geass:Lelouch der Rebellion R2 zu erstellen. In den letzten Jahren sind Text-zu-Sprache für Behinderungen und beeinträchtigte Kommunikationshilfen weit verbreitet. Text-zu-Sprache findet auch neue Anwendungen; beispielsweise ermöglicht die Sprachsynthese in Verbindung mit Spracherkennung die Interaktion mit mobilen Geräten über natürliche Sprachverarbeitungsschnittstellen. Text-zu-Sprache wird auch in der zweiten Spracherfassung verwendet. Voki ist zum Beispiel ein von Oddcast erstelltes Bildungstool, das es Benutzern ermöglicht, ihren eigenen sprechenden Avatar zu erstellen, indem verschiedene Akzente gesetzt werden. Sie können per E-Mail, auf Websites eingebettet oder auf sozialen Medien geteilt werden. Darüber hinaus ist die Sprachsynthese eine wertvolle rechnerische Hilfe für die Analyse und Bewertung von Sprachstörungen. Ein Synthesizer der Sprachqualität, entwickelt von Jorge C. Lucero et al.at der Universität von Brasília, simuliert die Physik der Phonation und umfasst Modelle von Vokalfrequenz Jitter und Tremor, Luftstromgeräusche und laryngeale Asymmetrien. Der Synthesizer wurde verwendet, um die Timbre von dysphonischen Lautsprechern mit kontrollierten Niveaus von Rauhigkeit, Atmung und Dehnung zu imitieren. Siehe auch Referenzen Externe Links Medien im Zusammenhang mit Sprachsynthese bei Wikimedia Commons Sprachsynthese bei Curlie Simulated Singen mit dem Singroboter Pavarobotti oder eine Beschreibung von der BBC auf, wie der Roboter das Singen synthetisierte.Fat Fetishism ist eine sexuelle Attraktion gerichtet auf übergewichtige oder fettleibige Menschen aufgrund ihres Gewichts und ihrer Größe. Eine Vielzahl von Fett Fetischismus ist Futter(er)ismus oder gewinnen, wo sexuelle Befriedigung nicht aus dem Fett selbst gewonnen wird, sondern aus dem Prozess des Gewinnens, oder anderen helfen, Körperfett zu gewinnen. Der Fat Fetishism beinhaltet auch Füllung und Polsterung, während der Fokus des Arousals auf die Empfindungen und Eigenschaften eines realen oder simulierten Gewinns liegt. Als Subkultur Die fette Fetischismusgemeinschaft hat sich mit Körper Positivität und fetten feministischen Bewegungen überlappt. Die National Association to Advance Fat Acceptance (NAAFA) hat als Advocacy-Organisation für Fette gearbeitet, aber wurde teilweise gebildet, um männliche Fett Fetischisten und andere Fett Bewunderer (FAs) finden Fett Frauen bis heute und haben Sex mit. Fett Fetischismus als Gemeinschaft ist überwiegend heterosexuell, konzentriert sich auf fette Frauen und dünnere Männer. Fat Fetishism umfasst sowohl Real-Life- und Internet-Communities. Fat Fetishism Praktiken und Subkulturen umfassen Internet-Pornos; Gewinnen und Füttern, was ist essen, um absichtlich Gewicht zu gewinnen; Hogging, das ist, wenn Männer suchen Fett Frauen sexuelle Ausbeutung; und Squashing, die sexuelle Anziehungskraft auf die Idee, von einer Fetten Person oder Menschen zerquetscht werden. Laut The Routledge Companion to Beauty Politics, "die geschlechts-, raced- und classed-Power-Dynamik vieler dieser Power-Dynamiken oft spiegeln, verstärken und sogar übertrieben vorhandene Rasse, Geschlecht, Klasse und sexuelle Ungleichheiten. " Soziologe Abigail C. Saguy hat vorgeschlagen, dass sie durch die Ablehnung des Frauengewichts die kulturelle Bedeutung des Frauengewichts gegenüber ihrem physischen Aussehen stärken und damit auch die Ungleichheit der Geschlechter stärken. Feederism Gainers und Feedees sind Menschen, die die Fantasie oder die Realität des Gewichts selbst genießen. Ermutige und Feeder genießen die Fantasie, jemandem zu helfen, Gewicht zu gewinnen. Gainer und Ermutiger sind häufige Etiketten unter Homosexuellen, während sowohl gerade Männer und Frauen als auch lesbische Frauen oft als Feeder und Feedees identifizieren. Einige bevorzugen den Begriff Feederismus über den Feederismus, da er eine gleichberechtigtere Beziehung zwischen Feeder und Feede vorschlägt. Während Gewinn und Fütterung oft als Fetisch angesehen werden, berichten viele innerhalb der Gewinn- und Futtermittel-Communities, sie mehr als Lebensstil, Identität oder sexuelle Orientierung zu betrachten. Feederismus wird von Medien als Tabu oder Nischeninteresse dargestellt. Negative Mediendarstellungen umfassen Feed, das ist ein Beispiel für nicht-konsensuellen Feederismus. Die Forschung hat gezeigt, dass die überwältigende Mehrheit der Feederismus-Beziehungen vollständig konsensuell und Unbeweglichkeit meist als Fantasie für Teilnehmer gehalten wird. Die Homosexuelle Gainer-Gemeinschaft wuchs aus der Girth & Mirth-Bewegung in den 70er Jahren. Bis 1988 gab es gewinnspezifische Newsletter und 1992 fand die erste Gewinnveranstaltung unter dem Namen EncourageCon in New Hope, Pennsylvania statt. Im Jahr 1996, GainRWeb gestartet, die erste Website gewidmet Homosexuell Männer in Gewichtszunahme. Siehe auch Bär (große Kultur)Große schöne Frau Große Handsome Man Chub (gay culture)Leblouh Referenzen Quellen Giovanelli, Dina und Natalie Peluso. 2006. "Feederismus: eine neue sexuelle Freude und Subkultur." Pp 309–314 in The Handbook of New Sexuality Studies. Herausgegeben von Steven Seidman. Oxford, UK: Routledge. Kathleen LeBesco.2004. Revolting Bodies?: Der Kampf, um die Fettidentität zu definieren. Univ of Massachusetts Press.ISBN 1-55849-429-4 Don Kulick und Anne Meneley. 2005. Fett: Die Anthropologie einer Obsession.ISBN 1-58542-386-6 Charles, K und Palkowski, M. 2015. Feederism:Eating, Weight Gain and Sexual Pleasure,Palgrave ISBN 978-1-137-47045-4 Weitere Lesung Pardes, Arielle (19 März 2015). " Die Frauen, die von beobachten Männer Gewicht gewinnen - VICE".www.vice.com. Textor, Alex Robertson (1999)."Organisierung, Spezialisierung und Wünsche in der Big Men's Movement: Vorläufige Forschung in der Studie der Subkultur-Formation".International Journal of Sexuality and Gender Studies.4 (3:) 217–239. doi:1023/A:1023223013536