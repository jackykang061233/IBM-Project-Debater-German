Verstärktes Lernen (RL) ist ein Bereich des maschinellen Lernens, der sich mit der Frage befasst, wie intelligente Agenten Maßnahmen in einem Umfeld ergreifen sollten, um den Begriff der kumulativen Belohnung zu maximieren. Verstärktes Lernen ist eines von drei Grundmodellen für maschinelles Lernen, neben dem beaufsichtigten Lernen und dem unkontrollierten Lernen. Verstärktes Lernen unterscheidet sich von überwachtem Lernen, wenn nicht gekennzeichnete Input-/Output- Paare vorgelegt werden müssen und keine suboptimalen Maßnahmen erforderlich sind, um explizit zu korrigieren. Stattdessen liegt der Schwerpunkt auf der Suche nach einem Gleichgewicht zwischen Exploration (nicht neugechartertes Gebiet) und Ausbeutung (der aktuellen Kenntnisse). Die Umwelt wird in der Regel in Form eines Markov-Beschlussverfahrens (MDP) angegeben, da viele verstärkte Lernalgorithmen für diesen Kontext dynamische Programmierungstechniken verwenden. Hauptunterschied zwischen den klassischen dynamischen Programmierungsmethoden und den verstärkten Lernalgorithmen besteht darin, dass Letztere nicht Kenntnis von einem genauen mathematischen Modell des MDP übernehmen und große MDPs anstreben, bei denen genaue Methoden unbrauchbar sind. Einleitung aufgrund ihrer Allgemeinität wird in vielen Disziplinen, wie Spieltheorie, Kontrolltheorie, Betriebsforschung, Informationstheorie, Simulationsoptimierung, Multi-agen-Systeme, schwindende Intelligenz und Statistik untersucht. In der Forschungs- und Kontrollliteratur wird das verstärkte Lernen mit einer dynamischen Programmierung oder einer neuro-dynamischen Programmierung bezeichnet. In der Theorie der optimalen Kontrolle, die vor allem die Existenz und Charakterisierung optimaler Lösungen und Algorithmen für ihre genaue Berechnung und weniger das Lernen oder die Annäherung betrifft, wurden auch die Probleme des Interesses an einem verstärkten Lernen untersucht. In der Wirtschafts- und Spieltheorie kann ein verstärktes Lernen genutzt werden, um zu erklären, wie das Gleichgewicht in Grenzen gesetzt werden kann. Grundlegende Verstärkung wird als Markov-Entscheidungsprozess (MDP): eine Reihe von Umwelt- und Agentenstaaten, S; eine Reihe von Maßnahmen, A, des Agenten; P a ( s , s ́) = Pr ( s + 1 = s′ s  t s = s s s s s s s s, a t = a ) Memestyle P_{a}(s)=\Prs_{t+1}=t \s_\s_\s_\s_\s_\t=s_styles=a. R a ( s , s ́ s ́ s ) HANAdisplaystyle R_{a}(s,s) ist die unmittelbare Belohnung nach dem Übergang von s s s s} zu s ́ ́ s ́ ́ s} mit Aktion a HANAdisplaystyle a}.Die Zweck des verstärkten Lernens ist es, eine optimale, oder fastoptimale Politik zu lernen, die die "reward-Funktion" oder andere Benutzer-Verstärkte Signale aus der unmittelbaren Belohnung der unmittelbaren zieht. Dies ähnelt ähnlichen Prozessen, die in der Tiermedizin auftreten. biologische Gehirne sind beispielsweise schwer um Signale wie Schmerzen und Hunger als negative Verstärkung zu interpretieren und die Nahrungsaufnahme als positive Verstärkung zu interpretieren. In einigen Fällen können Tiere lernen, Verhaltensweise zu verfolgen, die diese Belohnungen optimieren. Dies deutet darauf hin, dass Tiere in der Lage sind, das Lernen zu stärken. Ein grundlegender Verstärkter Erreger AI arbeitet mit seiner Umwelt in bestimmten Zeitschritten zusammen. Jedes Mal erhält der Agenten den aktuellen Staat s s s s s_{t} und belohnt r {\displaystyle r_{t}. Sie wählt dann eine Aktion aus dem Paket der verfügbaren Maßnahmen aus, die anschließend an die Umwelt weitergeleitet werden. Die Umwelt bewegt sich in einen neuen Staat, der mit dem Übergang assoziiert ist ( s t , t s t + 1 Memedisplaystyle s_{t+1}, ein t s t , s t + 1 ) s_{t+1}, a_{t},s_{t+1) ist bestimmt. Ziel eines verstärkten Lernens ist es, eine Politik zu erlernen:   : A × S → [ 0 ] {\displaystyle \pi :A\times S\rightarrow[0,1] ,  of ( a , s ) = Pr ( t = a ∣ s t = s ) s ) \pi,s)= Pr(a_{t}=a\mid s_{t}=s), die die erwartete kumulative Belohnung maximiert. Formalisierung des Problems als MDP übernimmt der Agenten direkt den aktuellen Umweltzustand; in diesem Fall wird das Problem die volle Observierbarkeit erklärt. Liegt der Agenten nur Zugang zu einem Unterteil von Staaten oder wenn die beobachteten Staaten durch Lärm verschmutzt werden, so wird der Erreger darauf hingewiesen, dass er eine teilweise Observierbarkeit hat, und das Problem muss formell als ein subservierbarer Markov-Entscheidungsprozess definiert werden. In beiden Fällen können die für den Agenten zur Verfügung stehenden Maßnahmen eingeschränkt werden. Zum Beispiel könnte der Zustand eines Kontos auf einen positiven Einfluss beschränkt werden; wenn der aktuelle Wert des Staates 3 beträgt und die Übergangsversuche zur Reduzierung des Werts um 4 nicht zulässig sind. Liegt die Leistung des Agenten im Vergleich zu der Leistung eines Agenten, der optimal gehandelt hat, führt der Leistungsunterschied zu dem Begriff des Bedauerns. Um in der Nähe optimal tätig zu werden, muss der Agenten die langfristigen Folgen seiner Maßnahmen (d. h. die Maximierung des künftigen Einkommens), wenngleich die mit dieser Maßnahme verbundene sofortige Belohnung negativ sein könnte. In diesem Zusammenhang ist das verstärkte Lernen besonders gut geeignet, um Probleme zu lösen, die einen langfristigen und kurzfristigen lohnenden Gewinn beinhalten. Es wurde erfolgreich auf verschiedene Probleme angewendet, darunter Roboterkontrolle, Aufzugsplanung, Telekommunikation, Backgammon, Checkers und Go (AlphaGo). Zwei Elemente machen das Lernen leistungsfähiger: die Verwendung von Proben zur Optimierung der Leistung und die Nutzung der Funktionsangleichung mit großen Umgebungen. Dank dieser beiden Schlüsselkomponenten kann das verstärkte Lernen in großen Umgebungen in folgenden Situationen genutzt werden: ein Umweltmodell ist bekannt, aber eine Analyselösung ist nicht verfügbar; nur ein Simulationsmodell der Umwelt wird (das Thema Simulationsoptimierung) gegeben. Nur die Möglichkeit, Informationen über die Umwelt zu sammeln, besteht darin, mit ihm zu interagieren. Die ersten beiden Probleme könnten als Planungsprobleme betrachtet werden (da einige Form des Modells verfügbar ist), während der letzte als echtes Lernproblem angesehen werden könnte. In verstärktem Lernen werden jedoch sowohl Planungsprobleme als auch Probleme beim maschinenlesbaren Lernen umgewandelt. Exploration Mit dem multiarmierten Bandit-Problem und dem finnischen Staatsraum MDPs in Burnetas und Katehakis (1997) wurde die Exploration gegenüber. Verstärktes Lernen erfordert Explorationsmechanismen; zufällige Auswahl von Maßnahmen ohne Bezug auf eine geschätzte Wahrscheinlichkeitsverteilung zeigt schlechte Leistung. Der Fall von (kleinen) Finken Markov-Entscheidungsprozessen ist relativ gut verstanden. Aufgrund fehlender Algorithmen, die gut mit der Anzahl der Staaten (oder dem Ausmaß der Probleme mit unbegrenzten Staatsräumen) sind einfache Explorationsmethoden jedoch die praktischste. Eine solche Methode ist   Memedisplaystyle \varepsilon } -greedy, wo 0  1  1  1  1 7.8displaystyle 0<\varepsilon 11} ein Parameter ist, der den Umfang der Exploration gegenüber der Ausbeutung kontrolliert. Mit der Wahrscheinlichkeit 1 −   WELLdisplaystyle 1-\varepsilon } wird die Nutzung ausgewählt, und der Agenten wählt die Maßnahme aus, die er für die beste langfristige Wirkung hält (die Maßnahmen werden einheitlich an Zufallsstichproben aufgeteilt). Alternativ wird mit der Wahrscheinlichkeit   Memedisplaystyle \varepsilon } die Exploration ausgewählt, und die Maßnahme wird einheitlich bei Zufall gewählt.   WELLdisplaystyle \varepsilon } ist in der Regel ein fester Parameter, kann jedoch entsprechend einem Zeitplan (die den Agenten nach und nach weniger erkunden) oder anpassungsfähig sein. Algorithms zur Kontrolle des Lernens Selbst wenn die Explorationsfrage missachtet wird und selbst wenn der Staat observierbar war (nachstehend wiedergegeben), bleibt das Problem, frühere Erfahrungen zu nutzen, um herauszufinden, welche Maßnahmen zu höheren kumulativen Belohnungen führen. Kritik an optimaler Politik s s  S  S  S ed s  1 0 , 1 {\ {\ \pi :A\times S\rightarrow[0,1] π ( a , s ) = Pr ( t = a  t  t s s = s s ) {\ s \pi,s)=\ Pr(a_{t}=a\mid s_{t}=s In der Strategiekarte wird die Wahrscheinlichkeit gegeben, einen Geschäftsbereich s s s s s s} durchzuführen. Es gibt auch deterministische Maßnahmen. State-Wert-Funktion Wertfunktion V ) ( ) s Memestyle V_Getpi (s) ist definiert als die erwartete Rückkehr mit Staat s s s s s} , d. h. s 0 = s s s s_{0}, und nach Politik {\ Memestyle \pi } .Hence, ungefähr die Wertfunktionsschätzung "gut" ist es in einem bestimmten Staat. V ) ( s ) = E ) [ R ] = E  [ [ ∑ t = 0 ∞ t t  t  t s 0 = s ] , {\displaystyle V_ cupi (} {E} [R]=\operatorname {E} links[\sum _t=0}^{\infty ammagamma ^t}r_{t}\mids_{0}=s\right,] in dem die zufällige variable R HANAstyle R} die Rendite verdichtet und als Summe künftiger Rabatte definiert wird: R =  = t = 0 ∞  t t r , KINGstyle R=\sum _t=0}^{\infty ammagamma ^t}r_{t, wo r {\displaystyle r_{t} die Belohnung für Schritt t {\displaystyle t},  [ [ 0 , 1 ) KINGstyle \gamma \in [0,1)} ist der Rabatt. Gamma ist weniger als 1, so dass Ereignisse in der nahen Zukunft weniger gewichtet werden als Ereignisse in unmittelbarer Zukunft. Der Algorithmus muss eine Politik mit einer maximal erwarteten Rückkehr finden. Aus der Theorie der MDP ist bekannt, dass die Suche ohne Verlust der Allgemeinheit auf die sogenannten stationären Maßnahmen beschränkt werden kann. Eine Politik ist stationärer Art, wenn die von ihr zurückgezahlte Aktion nur vom letzten besuchten Staat abhängt (aus der Geschichte des Beobachtungsbeauftragten). Die Suche kann weiter auf deterministische stationäre Maßnahmen beschränkt werden. Eine deterministische stationäre Politik wählt Maßnahmen aus, die auf dem aktuellen Zustand basieren. Da jede solche Politik mit einer Kartierung von Staaten bis hin zu den Aktionen identifiziert werden kann, können diese Strategien mit solchen Kartierungen ohne Verlust der Allgemeinheit identifiziert werden. Brute Kraft Zwei Schritte: Für jede mögliche Politik wird die Stichprobe zurückgeführt, während sie die Politik mit der größten erwarteten Rückkehr wählt. Ein Problem ist, dass die Zahl der Maßnahmen groß oder sogar unendlich sein kann. Eine andere ist, dass die Varianz der Rendite groß sein kann, was viele Proben benötigt, um die Rückkehr jeder Politik genau zu bewerten. Diese Probleme können gelöst werden, wenn wir eine gewisse Struktur annehmen und Proben aus einer Politik ermöglichen, die Schätzungen anderer zu beeinflussen. Die beiden wichtigsten Ansätze zur Erreichung dieses Ziels sind die Bewertung der Wertfunktion und die direkte politische Suche. Wertfunktion Standardfunktionskonzepte versuchen, eine Politik zu finden, die die Rückkehr maximiert, indem eine Reihe von Schätzungen der erwarteten Rendite für einige Politiken (normalerweise entweder die aktuelle [über die Politik] oder die optimale [off-policy] beibehalten werden. Diese Methoden stützen sich auf die Theorie der Markov-Entscheidungsprozesse, in denen die optimale Wirkung in einem stärkeren Sinne als der obengenannte definiert ist: Eine Politik wird als optimal bezeichnet, wenn sie die beste Rendite aus einem anfänglichen Staat erreicht (d. h. die ersten Vertriebenen spielen in dieser Definition keine Rolle). Wieder einmal kann eine optimale Politik immer unter stationären Politiken gefunden werden.  formal definiert den Wert einer Politik   ] \pi } von V ) ( ) = E [ R ∣ s ,   ] , {\  V  Vpi (})=E[R\mid s,\pi ], wo R Memestyle R} für die Rückkehr im Anschluss an . \pi } vom Staat . V Vpi .pi (}  V V  V  V  V  V  V  V . V  V . . .  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  V  KINGstyle V**}(s)=\max Meme_pi }V).pi (s) Eine Politik, die diese optimalen Werte in jedem Staat erreicht, ist optimal. Klar ist, dass eine Politik, die in diesem starken Sinne optimal ist, auch in dem Sinne optimal ist, dass sie die erwartete Rückkehr   \rho {^\pi }}pi pi, da  =  [ = E [ V  S ( S ) ]  steuerliche \rho pipi spi = }E[Vpipi (S)] , wo S {\displaystyle S} eine zufällige Stichprobe aus der Verteilung μ {\displaystyle \mu } der ersten Staaten (so μ ( s ) = Pr ( s 0 = s) s ) Memestyle \mu s)=\Pr(s_{0} Obwohl staatliche Werte ausreichend sind, um optimale Parameter zu definieren, ist es sinnvoll, Handlungswerte festzulegen. Angesichts eines Staates s s s s}, einer Maßnahme, die ein Memedisplaystyle a} und eine Politik   KINGstyle \pi }, des Handlungswertswertes des Paares ( s , a ) faserstilstyle (s,a)} unter {\ KINGstyle \pi } definiert durch Q   ( s , a) = E E [ R ∣ s , a , π ] , Memestyle Q^{\pi (}, a)=\operatorname {E} [R\mid s,a,\pi ],\,}, wo R displaystyle R} nun für die zufällige Rückkehr im Zusammenhang mit der ersten Maßnahme ein Memestyle a} in State s s s s s} und nach π KINGstyle \pi } steht. Die Theorie der MDP besagt, dass wir, wenn     WELLdisplaystyle \pi *** eine optimale Politik ist, durch die Wahl der Aktion von Q . . ( s , {\ ) Memestyle Q^{\pi (* (*pi,\cdot ) mit dem höchsten Wert auf jedem Staat, s s s s s s } handeln.Die Wirkung einer solchen optimalen Politik ( Q ∗ {\  cudisplaystyle Q*pi **pi ) wird als optimale Wirkung bezeichnet und wird in der Regel durch Q {\ RARstyle Q**} .In Zusammenfassung, das Wissen über die optimale Wirkungsfunktion allein reicht aus, um zu wissen, wie optimal gehandelt werden kann. Unter voller Kenntnis des MDP sind die beiden Grundansätze zur Berechnung der optimalen Wirkungsfunktion wertmäßig. beide Algorithmen berechnen eine Reihe von Funktionen Q k {\displaystyle Q_{k} ( k = 0 , 1 , 2 , ... KINGstyle k=0,1,2\ldots }), die sich auf Q ∗ KINGstyle Q**} . these Diese Funktionen beinhalten Rechenerwartungen im gesamten Staatsraum, was für alle, aber die kleinsten (finitären) MDP nicht sinnvoll ist. In der Verbesserung der Lernmethoden werden die Erwartungen durch die durchschnittliche Überschreitung von Proben und die Anwendung von Methoden zur Anpassung der Funktion an die Notwendigkeit, Wertfunktionen über große staatliche Aktionsräume zu vertreten, angeglichen. Monte Carlo-Methoden Monte Carlo-Methoden können in einem Algorithmus verwendet werden, der es zur Migrationspolitik macht. Politikgestaltung besteht aus zwei Schritten: Bewertung und Verbesserung der Politik. Monte Carlo wird im politischen Bewertungsschritt verwendet. In diesem Schritt besteht das Ziel darin, die Funktionswerte Q   ( s , a ) ) pipi Qpipi (s, a ) pipi Q^{\pi (s, a) (s,a) (oder eine gute Annäherung an sie) für alle staatlichen Aktionspaare ( s , a ) Memestyle (s,a)} .Assuming (für Einfachheit), dass das MDP eine ausreichende Erinnerung ist, um den Wert des Problems zu decken. In diesem Fall kann die Schätzung des Wertes eines bestimmten staatlichen Spielpaares ( , a ) Memedisplaystyle (s,a)} durch eine durchschnittliche Berechnung der aus (s , a ) faserstilstyle (s, a) s. In ausreichender Zeit kann dieses Verfahren somit eine genaue Schätzung Q Memedisplaystyle Q} der Aktionswertfunktion Q   pipi .pi . Dieses Verfahren stellt die Beschreibung des politischen Bewertungsschritts vor. In der Verbesserung der Politik wird die nächste Politik durch die Formulierung einer Gedy-Politik in Bezug auf Q {\displaystyle Q} : Angesichts eines staatlichen s s s s s} erreicht diese neue Politik eine Maßnahme, mit der die Q ( s , ) ) faserstyle Q(s,\cdot )} maximiert wird. In der Praxis lässt sich die Auswertung der Maximierungsmaßnahmen, wenn sie erforderlich sind, verschieben. Probleme mit diesem Verfahren sind: Das Verfahren kann zu viel Zeit verwenden, um eine suboptimale Politik zu bewerten. Es verwendet ineffiziente Proben, so dass ein langer Pfad die Schätzung nur des einzigen staatlichen Aktionspaares verbessert, das mit dem Fahrpfad begonnen hat. Wenn die Renditen entlang der Pfade hohe Schwankungen aufweisen, ist die Konvergenz langsam. Sie arbeitet nur in episodischen Problemen; Nur in kleinen, finnischen MDP funktioniert es. Geschwindigkeitsunterschiede Das erste Problem wird korrigiert, indem das Verfahren zur Änderung der Politik (in einigen oder allen Staaten) vor der Festlegung der Werte ermöglicht wird. Dies kann auch problematisch sein, da sie eine Konvergenz verhindern könnte. Die meisten aktuellen Algorithmen machen dies, was die Klasse der allgemeinen Iteration-Algorithmen erhöht. Viele gesetzgeberische Methoden gehören zu dieser Kategorie. Die zweite Frage kann korrigiert werden, indem Trajekte in die Lage versetzt werden, einen Beitrag zu allen staatlichen Aktionspaaren zu leisten. Dies kann auch in gewissem Maße mit dem dritten Problem helfen, obwohl eine bessere Lösung, wenn die Rückführungen hohe Schwankungen aufweisen, die zeitliche Differenz (TD)-Methoden, die auf der recursiven Bellman-Formel basieren, ist. Die Berechnung der TD-Methoden kann inkrement sein (wenn der Übergang geändert wird und der Übergang weggeworfen wird), oder die Charge (wenn die Übergangsregelungen aufgeteilt sind und die Schätzungen nach der Charge berechnet werden). Batch-Methoden, wie die minimale zeitliche Differenzmethode, können die Informationen in den Proben besser verwenden, während incrementale Methoden die einzige Wahl sind, wenn die Chargenmethoden aufgrund ihrer hohen Rechen- oder Speicherkomplexität unfähig sind. Manche Methoden versuchen, die beiden Ansätze zu kombinieren. Methoden, die auf zeitlichen Unterschieden basieren, überwinden auch das vierte Problem. Zur Lösung des fünften Problems werden die Methoden zur Angleichung der Funktionen verwendet. Lineare Funktionsangleichung beginnt mit einer Kartierung {\ WELLdisplaystyle \phi }, die jedem staatlichen Aktionspaar einen finite-dimensionalen Vektor zugewiesen. In diesem Fall werden die Aktionswerte eines staatlichen Aktionspaares ( , a ) Memestyle (s,a)} durch lineare Kombination der Komponenten von ing ( s , a ) {\displaystyle \phi (s,a) mit einigen Gewichten θ {\ \theta } : Q ( s , a) = ing i = 1 d  i i  i i ( s , a ) . 7.8displaystyle Q(s,a)=\sum _i=1}^{dthetheta {_iphiphi {_i}(s,a). Die Algorithmen passen dann die Gewichte an und passen stattdessen die Werte an, die mit den einzelnen staatlichen Spielpaaren verbunden sind. Methoden auf der Grundlage von Ideen aus nichtparametrischen Statistiken (die für den Aufbau ihrer eigenen Merkmale gesehen werden können) wurden untersucht. Wert iteration kann auch als Ausgangspunkt genutzt werden, wodurch der Q-Learning-Algorithmus und seine vielen Varianten erhöht werden. Das Problem bei der Verwendung von Aktionswerten ist, dass sie höchst genaue Schätzungen der konkurrierenden Aktionswerte benötigen, die schwer zu erhalten sind, wenn die Rendite laut ist, obwohl dieses Problem teilweise durch zeitliche Differenzierungsmethoden abgeschwächt wird. Mit der sogenannten kompatiblen Methode zur Angleichung der Funktion beeinträchtigt die allgemeine und Effizienz. Ein weiteres Problem, das spezifisch auf TD ist, stammt aus ihrer Abhängigkeit von der recursiven Bellman-Formel. Die meisten TD-Methoden haben einen sogenannten   Memedisplaystyle \lambda } Parameter ( 0 ≤ 1 ) Memedisplaystyle (0\leq \lambda \leq 1)}, der ständig zwischen Monte Carlo-Methoden interpoliert werden kann, die sich nicht auf die Bellman-Angleichungen und die grundlegenden TD-Methoden verlassen, die sich ausschließlich auf die Bellman-Sprachen stützen. Dies kann wirksam sein, um dieses Problem zu beleuchten. direkte politische Suche Eine alternative Methode besteht darin, direkt in den (ein Teil) Politikraum zu suchen, in dem das Problem ein Fall der krytischen Optimierung wird. Die beiden verfügbaren Ansätze sind stufen-basierte und stufenfreie Methoden. Gradient Methoden (Policy ziehungsmethoden) beginnen mit einer Kartierung von einem finite-dimensionalen (Parameter) Raum auf dem Gebiet der Politik: Angesichts des Parameters {\ Memestyle \theta } , π {\ {\ HANAstyle \pi ggio_theta }} . {\   {\ . \rho \theta } . Verteilen Sie die Leistungsfähigkeit durch . ( = ) = . {\ {\ {\ \rho (\theta )\=rho pipi liv_theta .,} unter milden Bedingungen wird diese Funktion als Funktion des Parameters . \theta } unterscheiden. Wenn die   WELLdisplaydisplaystyle \rho } bekannt war, könnte man den Aufwärtstrend nutzen. Da ein analytischer Ausdruck für die Steigung nicht verfügbar ist, ist nur eine laute Schätzung verfügbar. Eine solche Schätzung kann auf viele Weise erstellt werden, so dass Algorithmen wie die REINFORCE-Methode von Williams (die als wahrscheinliche Methode in der Simulationsanalyse bekannt ist) entstehen. Methoden zur politischen Suche wurden im Zusammenhang mit Robotik verwendet. Viele politische Suchmethoden können in der lokalen Optima (wie sie auf lokaler Suche basieren) stecken. Eine große Klasse von Methoden verhindert, dass man sich auf Bewegungsinformationen stützt. Dazu gehören simulierte Annobel, übergreifende Suche oder Methoden der Evolutionsberechnung. Viele stufenfreie Methoden können (in der Theorie und in der Grenze) zu einem globalen optimalen Ergebnis führen. Politische Suchmethoden können sich langsam an lauten Daten anpassen. Dies geschieht beispielsweise in episotischen Problemen, wenn die Trajekte lange sind und die Schwankungen der Renditen groß sind. Methoden, die sich auf zeitliche Unterschiede stützen, könnten in diesem Fall hilfreich sein. In den letzten Jahren wurden Schauspieler-critic-Methoden vorgeschlagen und gut auf verschiedenen Problemen durchgeführt. Modellbasierte Algorithmen Schließlich können alle oben genannten Methoden mit Algorithmen kombiniert werden, die zunächst ein Modell lernen. Beispielsweise lernt der Dyna-Algorithmus ein Modell aus der Erfahrung und verwendet, das neben den tatsächlichen Übergängen auch modellierte Übergangsregelungen für eine Wertfunktion vorsieht. Solche Methoden können manchmal auf die Verwendung von nicht-parametrischen Modellen ausgeweitet werden, etwa wenn die Übergangsregelungen einfach gespeichert und wieder auf den Lerngorithmus zurückgeführt werden. Es gibt andere Möglichkeiten, Modelle zu verwenden, als eine Wertfunktion zu aktualisieren. Beispielsweise wird das Modell verwendet, um das Verhalten direkt zu aktualisieren. Theorie sowohl die asymptotischen und finite-sample-Verhaltensweisen der meisten Algorithmen sind gut verstanden. Algorithms mit nachweislich guten Online-Leistungen (Bekämpfung des Explorationsproblems) sind bekannt. Effiziente Erforschung von MDP in Burnetas und Katehakis (1997). Finite-time-Leistungsbindungen sind auch für viele Algorithmen erschienen, aber diese Bindungen werden eher locker sein und müssen daher noch mehr Arbeit leisten, um die relativen Vorteile und Grenzen besser zu verstehen. Inkrementelle Algorithmen wurden asymptotische Konvergenzprobleme gelöst. Temporal-difference-basierte Algorithmen konvergieren sich unter einem breiteren Satz von Bedingungen als früher möglich (z.B. wenn sie mit willkürlicher, reibungsloser Funktionsangleichung verwendet werden). Forschungsthemen umfassen anpassungsfähige Methoden, die weniger (oder keine) Parameter unter einer Vielzahl von Bedingungen anwenden, die das Explorationsproblem in großen MDP-Kombinationen mit logikbasierten Rahmenbedingungen für das Erlernen von Lernprozessen in großem Maßstab und unter Teilinformationen (z.B. mittels prädikativer staatlicher Darstellung) modulares und hierarchisches verstärktes Lernen verbessern, um bestehende Wertfunktions- und politische Suchmethodiken, die mit großen (oder kontinuierlichen) Aktionsplänen gut arbeiten, um das lebensbegleitende Lernen effizient zu gestalten (z.B. auf der Grundlage von Montebug-Projekten). Intrinsische Motivation, die das Informations- und Neugier-Typverhalten von aufgabenabhängigen objektiven Verhaltensänderungen (normalerweise) unterscheidet, durch die Einführung einer Belohnungsfunktion, die auf der Maximierung neuartiger, kreativer Informationen basiert, die durch verstärktes Lernen genutzt werden, wurde aktiv in der Informatik Multiagenzien oder verteiltem verstärkten Lernen verfolgt. Anwendungen erweitern sich. Lernen der Akteure Verstärkte Lernalgorithmen wie TD-Lernen werden derzeit als Modell für dopaminbasiertes Lernen im Gehirn untersucht. In diesem Modell funktionieren die dopaminrgischen Projektionen von der substantia nigra bis zur Basalbandlia als Vorhersagefehler. Verstärktes Lernen wurde als Teil des Modells für das menschliche Lernen genutzt, insbesondere im Hinblick auf die Interaktion zwischen implizitem und explizitem Lernen bei der Erwerb von Fähigkeiten (die erste Veröffentlichung zu diesem Antrag war 1995-96). Kontrolle auf lokaler Ebene Algorithmic Trading und optimale Ausführung von Rechenressourcen Vergleich von verstärkten Lernalgorithmen Komplementative Stärkung des Lernens Associative Stärkung des Lernens kombiniert die Gesichtspunkte des krchastic Learning automata-Aufgaben und überwachte Lernmusterklassifikationsaufgaben. Insociative Stärkung des Lernprozesses wechselt das Lernsystem in einem geschlossenen Kreislauf mit seiner Umwelt. Vertiefung des Lernens Dieser Ansatz erweitert das verstärkte Lernen durch die Nutzung eines tiefen Neuralnetzes und ohne explizite Gestaltung des staatlichen Raums. Mehr Aufmerksamkeit auf das Erlernen von ATARI-Spiele von Google DeepMind hat auf die Vertiefung des Lernens oder das Ende des verstärkten Lernens gelegt. Verbesserung des Lernens Im umgekehrten Ausbau des Lernens (IRL) gibt es keine Belohnungsfunktion. Stattdessen wird die Belohnungsfunktion aufgrund eines beobachteten Verhaltens eines Experten vergeben. Es geht darum, das beobachtete Verhalten, das oft optimal oder optimal ist, zu verschleppen. Safe verstärktes Lernen Safe Verstärkungement Learning (SRL) kann als Lernprozess definiert werden, der die Erwartung der Rückkehr in Probleme maximiert, in denen es wichtig ist, für angemessene Systemleistung und/oder die Einhaltung von Sicherheitsengpässen bei Lern- und/oder Einführungsprozessen zu sorgen. Lesen Sie auch weiter Auer, Peter; Jaksch, Thomas; Ortner, Ronald (2010)."Near-optimal besorgniserregend für das verstärkte Lernen. Journal of Maschinen Learning Research.11: 1563–1600.Busoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010) Mehr lernen und dynamisch Programmplanung unter Einsatz der Funktion Ärztinen. Taylor & Francis CRC Presse.ISBNUNG DES RE1-4398-2108-4.François-Lavet, Vincent; Juris, Peter; Islam, Riashat; Bellemare, Marc G; Pineau, Joelle Luka."An Einführung zum vertieften Ausbau des Lernens und der Trends im Maschinenbau.11 (3–4): 219-354.arXiv:1811.12560code:2018 arXiv181112560F doi:10.1561/2200000071.S2CID 54434537.Powell, Oliver (2007) dynamische Programmierung: Lösung des Problems der Dimension. Nick-Interscience.ISBN gegen0-470-17155-4.Sutton, Richard S; Andrew G. KPMG. verstärktes Lernen: Einführung (2 ed). MIT Presse. ISBN: [0-262-03924-6.Sutton, Richard S. (1988) „Lernen Sie sich durch die Methode der zeitlichen Unterschiede vorhersagen“. Maschinenbau.3: 9–44. doi:10.1007/BF00115009.Szita, Istvan; Szepesvari, Csaba (2010) " modellbasierte Verstärkung des Lernens mit nahezu Tight Exploration Complexity Bounds (PDF).ICML 2010Omnipress.pp.1031–1038.Archived aus dem Original (PDF) für 2010-07-14. Externe Links: Stärkung des Lernsystems und der künstlichen Intelligenz (RLAI, Rich's Labor an der University of Québec)Autonomous Learning Laboratory (ALL, Andrew Barto's Labor an der University of Massachusetts Amherst) Hybrid Verstärkung des Lernprozesses zur Verbesserung des realen Lernens an der Universität Delft University of Technology University of Technology University of Technology University of Technology University of Technology University Andrew Ng Lecture zur Verstärkung des Lernens im Bereich des Lernens auf dem Blog-Code