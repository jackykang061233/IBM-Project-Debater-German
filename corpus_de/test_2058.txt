Eine Suchmaschine ist ein Software-System, das für die Durchführung von Web-Suche konzipiert ist. Sie suchen das World Wide Web systematisch nach bestimmten Informationen, die in einer textuellen Web-Suchabfrage angegeben sind. Die Suchergebnisse werden in der Regel in einer Zeile von Ergebnissen dargestellt, oft als Suchmaschinen-Ergebnisse Seiten (SERPs)Die Informationen können eine Mischung von Links zu Webseiten, Bildern, Videos, Infografiken, Artikeln, Forschungspapieren und anderen Arten von Dateien sein. Einige Suchmaschinen stellen auch Daten in Datenbanken oder offenen Verzeichnissen zur Verfügung. Im Gegensatz zu Web-Verzeichnern, die nur von menschlichen Redakteuren gehalten werden, halten Suchmaschinen auch Echtzeit-Informationen, indem ein Algorithmus auf einem Web-Crawler ausgeführt wird. Internetinhalte, die nicht von einer Web-Suchmaschine durchsucht werden können, werden im allgemeinen als Deep Web bezeichnet. Geschichte Pre-1990s Ein System zur Ortung veröffentlichter Informationen, das die immer zunehmende Schwierigkeit der Informationssuche in immer wachsenden zentralisierten Indizes der wissenschaftlichen Arbeit überwinden soll, wurde 1945 von Vannevar Bush beschrieben, der einen Artikel in The Atlantic Monthly mit dem Titel "As We May Think" schrieb, in dem er Bibliotheken der Forschung mit verbundenen Annotationen, die nicht anders sind als moderne Hyperlinks. Link-Analyse würde schließlich eine entscheidende Komponente von Suchmaschinen durch Algorithmen wie Hyper Search und PageRank. 1990er Jahre: Umfang der Suchmaschinen Die ersten Internet-Suchmaschinen predate das Debüt des Web im Dezember 1990: Die WHOIS-Nutzersuche stammt aus dem Jahr 1982 und die Multi-Netzwerk-Benutzersuche von Knowbot wurde erstmals 1989 umgesetzt. Die erste gut dokumentierte Suchmaschine, die Inhaltsdateien suchte, nämlich FTP-Dateien, war Archie, die am 10. September 1990 debütiert. Vor September 1993 wurde das World Wide Web vollständig von Hand indexiert. Es gab eine Liste von Webservern, die von Tim Berners-Lee bearbeitet und auf dem CERN Webserver gehostet wurden. Ein Snapshot der Liste im Jahr 1992 bleibt, aber da immer mehr Webserver online gingen, konnte die zentrale Liste nicht mehr aufrecht erhalten. Auf der NCSA-Website wurden neue Server unter dem Titel "Was ist neu! "Das erste Werkzeug für die Suche von Inhalten (im Gegensatz zu Benutzern) im Internet war Archie. Der Name steht für Archiv ohne die v., Es wurde von Alan Emtage Informatikstudent an der McGill University in Montreal, Quebec, Kanada erstellt. Das Programm heruntergeladen die Verzeichnislisten aller Dateien auf öffentlichen anonymen FTP-Seiten (File Transfer Protocol) und erstellt eine durchsuchbare Datenbank mit Dateinamen; Archie Search Engine hat die Inhalte dieser Seiten jedoch nicht indexiert, da die Menge der Daten so begrenzt war, dass es leicht manuell durchsucht werden konnte. Der Anstieg von Gopher (erstellt 1991 von Mark McCahill an der University of Minnesota) führte zu zwei neuen Suchprogrammen, Veronica und Jughead. Wie Archie suchten sie die Dateinamen und Titel, die in Gopher-Indexsystemen gespeichert wurden. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) gab eine Schlüsselwortsuche der meisten Gopher-Menütitel in den gesamten Gopher-Listen. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) war ein Tool, um Menüinformationen von bestimmten Gopher Servern zu erhalten. Während der Name der Suchmaschine "Archie Search Engine" kein Verweis auf die Archie Comic-Buchserie war, sind Veronica und Jughead Zeichen in der Serie und verweisen damit auf ihren Vorgänger. Im Sommer 1993 gab es keine Suchmaschine für das Web, obwohl zahlreiche spezialisierte Kataloge von Hand erhalten. Oscar Nierstrasz an der Universität Genf schrieb eine Reihe von Perl-Skripten, die diese Seiten periodisch spiegeln und in ein Standardformat umwandeln. Dies bildete die Grundlage für W3Catalog, die erste primitive Suchmaschine des Web, veröffentlicht am 2. September 1993. Im Juni 1993, Matthew Gray, dann am MIT, produzierte, was wahrscheinlich der erste Webroboter, der Perl-basierte World Wide Web Wanderer, und benutzte es, um einen Index namens Wandex zu erzeugen". Ziel des Wanderers war es, die Größe des World Wide Web zu messen, das es bis Ende 1995 tat. Die zweite Suchmaschine Aliweb erschien im November 1993. Aliweb nutzte keinen Web-Roboter, sondern hängt davon ab, von den Website-Administratoren der Existenz auf jeder Seite einer Indexdatei in einem bestimmten Format gemeldet zu werden. JumpStation (erstellt im Dezember 1993 von Jonathon Fletcher) verwendet einen Webroboter, um Webseiten zu finden und seinen Index zu erstellen und ein Webformular als Schnittstelle zu seinem Abfrageprogramm zu verwenden. Es war also das erste WWW-Ressourcen-Entdeckungstool, um die drei wesentlichen Merkmale einer Web-Suchmaschine (Crawling, Indizing und Suchen) wie unten beschrieben zu kombinieren.Aufgrund der begrenzten Ressourcen, die auf der Plattform zur Verfügung standen, beschränkte sich die Indexierung und damit die Suche auf die Titel und Überschriften, die in den Webseiten gefunden wurden. Einer der ersten "all text"-Crawler-basierten Suchmaschinen war WebCrawler, der 1994 herauskam. Im Gegensatz zu seinen Vorgängern erlaubte es den Nutzern, nach jedem Wort in jeder Webseite zu suchen, was seitdem der Standard für alle großen Suchmaschinen geworden ist. Es war auch die Suchmaschine, die von der Öffentlichkeit bekannt war. Auch im Jahr 1994 wurde Lycos (die an der Carnegie Mellon University begann) gestartet und zu einem großen kommerziellen Zweck. Die erste beliebte Suchmaschine im Web war Yahoo!Search. Das erste Produkt von Yahoo! gegründet von Jerry Yang und David Filo im Januar 1994, war ein Web-Verzeichnis namens Yahoo! Verzeichnis. 1995 wurde eine Suchfunktion hinzugefügt, so dass Benutzer Yahoo!Directory suchen! Es wurde eine der beliebtesten Möglichkeiten für Menschen zu finden Web-Seiten von Interesse, aber seine Suchfunktion im Web-Verzeichnis betrieben, anstatt seine Volltextkopien von Webseiten. Bald nach, eine Reihe von Suchmaschinen erschien und nach Popularität. Dazu gehören Magellan, Excite, Infoseek, Inktomi, Northern Light und AltaVista. Informationssuchende können das Verzeichnis auch durchsuchen, anstatt eine Keyword-basierte Suche zu machen. 1996, Robin Li entwickelte den RankDex-Website-Scoring-Algorithmus für Suchmaschinen-Ergebnisse-Seiten-Ranking und erhielt ein US-Patent für die Technologie. Es war die erste Suchmaschine, die Hyperlinks verwendet, um die Qualität der Websites, die es Indexing war, zu messen, vor dem sehr ähnlichen Algorithmus Patent von Google zwei Jahre später im Jahr 1998 eingereicht. Larry Page referenziert Li arbeitet in einigen seiner US-Patente für PageRank. Li verwendete später seine Rankdex-Technologie für die Baidu-Suchmaschine, die von Robin Li in China gegründet und im Jahr 2000 gestartet wurde. Im Jahr 1996 suchte Netscape eine einzige Suchmaschine einen exklusiven Deal als die vorgestellte Suchmaschine auf Netscapes Webbrowser. Es gab so viel Interesse, dass sich Netscape stattdessen mit fünf der großen Suchmaschinen befasst: für $5 Millionen pro Jahr würde jede Suchmaschine in Rotation auf der Netscape Suchmaschine Seite. Die fünf Motoren waren Yahoo! Magellan, Lycos, Infoseek und Excite. Google hat 1998 die Idee angenommen, Suchbegriffe zu verkaufen, von einem kleinen Suchmaschinen-Unternehmen namens goto.com. Diese Bewegung wirkte sich signifikant auf das SE-Geschäft aus, das vom Kampf auf eines der profitabelsten Unternehmen im Internet ging. Suchmaschinen wurden auch als einige der hellsten Sterne in der Internet-Investing-Frienzy bekannt, die in den späten 1990er Jahren aufgetreten. Mehrere Unternehmen traten spektakulär in den Markt und erhielten Rekordgewinne während ihrer ersten öffentlichen Angebote. Einige haben ihre öffentliche Suchmaschine genommen und sind Marketing-Nebenausgaben, wie Northern Light. Viele Suchmaschinen-Unternehmen wurden in der Dot-com-Blase gefangen, ein spekulationsgetriebener Marktboom, der 1990 gipfelte und im Jahr 2000 endete. 2000's-Present: Post-Dot-com-Blase Um 2000 stieg die Suchmaschine von Google auf Prominenz. Das Unternehmen erzielte bessere Ergebnisse für viele Suchanfragen mit einem Algorithmus namens PageRank, wie in der Zeitung Anatomy einer Suchmaschine von Sergey Brin und Larry Page, die späteren Gründer von Google, erklärt wurde. Dieser iterative Algorithmus rangiert Webseiten basierend auf der Anzahl und PageRank von anderen Websites und Seiten, die dort verlinken, auf der Prämisse, dass gute oder wünschenswerte Seiten mit mehr als anderen verknüpft sind. Larry Pages Patent für PageRank zitiert Robin Lis früheres RankDex-Patent als Einfluss. Google hielt auch eine minimalistische Schnittstelle zu seiner Suchmaschine. Viele seiner Konkurrenten haben hingegen eine Suchmaschine in einem Webportal eingebettet. Tatsächlich wurde die Google-Suchmaschine so populär, dass Spoof-Motoren wie Mystery Seeker entstanden. Bis 2000 lieferte Yahoo! Suchdienste basierend auf Inktomis Suchmaschine. Yahoo! erwarb 2002 Inktomi und Overture (die AlltheWeb und AltaVista gehörten) im Jahr 2003. Yahoo! wechselte bis 2004 auf die Suchmaschine von Google, als es seine eigene Suchmaschine auf Basis der kombinierten Technologien seiner Akquisitionen gestartet. Microsoft startete zunächst MSN Search im Herbst 1998 mit Suchergebnissen von Inktomi. Anfang 1999 begann die Website, Listen von Looksmart anzuzeigen, gemischt mit Ergebnissen von Inktomi. Für eine kurze Zeit im Jahr 1999, MSN Search verwendet Ergebnisse von AltaVista statt. Im Jahr 2004 begann Microsoft einen Übergang zu seiner eigenen Suchtechnologie, angetrieben von einem eigenen Web-Crawler (msnbot.)Microsofts rebranded Suchmaschine, Bing, wurde am 1. Juni 2009 gestartet. Am 29. Juli 2009, Yahoo!and Microsoft hat einen Deal abgeschlossen, in dem Yahoo!Search von Microsoft Bing-Technologie betrieben wird. Ab 2019 umfassen aktive Suchmaschinen-Crawler die von Google, Petal, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo und Yandex. Ansatz Eine Suchmaschine unterhält folgende Prozesse in naher Echtzeit: Web crawling Indexing SearchingWeb Suchmaschinen erhalten ihre Informationen durch Web-Crawling von Website zu Website. Die Spinne überprüft die Standarddateiname robots.txt, an sie adressiert. Die Roboter. txt-Datei enthält Richtlinien für Suchspins, die ihm sagen, welche Seiten zu kriechen und welche Seiten nicht zu kriechen. Nach der Überprüfung für Roboter. txt und entweder es zu finden oder nicht, die Spinne sendet bestimmte Informationen zurück, um in Abhängigkeit von vielen Faktoren, wie die Titel, Seiteninhalte, JavaScript, Cascading Style Sheets (CSS,) Überschriften oder seine Metadaten in HTML-Meta-Tags indexiert werden. Nach einer bestimmten Anzahl von Seiten, die gekrochen sind, indizierte Datenmenge oder auf der Website verbrachte Zeit, hört die Spinne auf zu kriechen und bewegt sich weiter. ["N]o Web-Crawler kann tatsächlich das gesamte erreichbare Web kriechen. Aufgrund unendlicher Webseiten, Spinnenfänger, Spam und anderer Exigenitäten des realen Internets wenden Raupen stattdessen eine Raupenpolitik an, um festzustellen, wann das Raupen einer Website als ausreichend angesehen werden sollte. Einige Webseiten sind erschöpfend gekrochen, während andere nur teilweise gekrochen werden". Indexing bedeutet die Zuordnung von Wörtern und anderen definierbaren Token auf Webseiten zu ihren Domainnamen und HTML-basierten Feldern. Die Verbände werden in einer öffentlichen Datenbank erstellt, die für Web-Suchabfragen zur Verfügung gestellt wird. Eine Abfrage eines Benutzers kann ein einziges Wort, mehrere Wörter oder ein Satz sein. Der Index hilft, Informationen über die Abfrage so schnell wie möglich zu finden. Einige der Techniken für Indexierung und Cache sind Handelsgeheimnisse, während Web-Crawling ist ein einfacher Prozess des Besuchs aller Seiten auf einer systematischen Basis. Zwischen den Besuchen durch die Spinne wird die geätzte Version der Seite (einige oder alle Inhalte, die benötigt werden, um sie zu machen) im Suchmaschinen-Arbeitsspeicher gespeichert schnell an einen Anfragenden gesendet. Wenn ein Besuch überfällig ist, kann die Suchmaschine nur als Web-Proxy handeln. In diesem Fall kann die Seite von den angegebenen Suchbegriffen abweichen. Die cached-Seite hält das Aussehen der Version, deren Wörter zuvor indiziert wurden, so kann eine cached-Version einer Seite nützlich für die Website sein, wenn die eigentliche Seite verloren ist, aber dieses Problem wird auch als eine milde Form von Linkrot betrachtet. Typischerweise ist es, wenn ein Benutzer eine Abfrage in eine Suchmaschine eingibt, ein paar Keywords. Der Index hat bereits die Namen der Seiten, die die Keywords enthalten, und diese werden sofort aus dem Index gewonnen. Die reale Verarbeitungslast erzeugt die Webseiten, die die Suchergebnisseliste sind: Jede Seite in der gesamten Liste muss nach Informationen in den Indexen gewichtet werden. Dann erfordert der Top-Suchergebnis-Artikel die Lookup, Rekonstruktion und Markup der Schnipsel, die den Kontext der Keywords angezeigt. Diese sind nur Teil der Verarbeitung jeder Suchergebnisse-Webseite erfordert, und weitere Seiten (neben oben) erfordern mehr dieser Postverarbeitung. Neben einfachen Keyword-Lookups bieten Suchmaschinen eigene GUI- oder befehlsgesteuerte Operatoren und Suchparameter an, um die Suchergebnisse zu verfeinern. Diese bieten die notwendigen Steuerungen für den Benutzer, der in den Feedback-Loop-Benutzern beschäftigt ist, durch Filterung und Gewichtung, während die Suchergebnisse, angesichts der ersten Seiten der ersten Suchergebnisse. Zum Beispiel hat die Google.com-Suchmaschine ab 2007 erlaubt, eine nach Datum zu filtern, indem sie auf "Suchwerkzeuge anzeigen" in der linken Spalte der ersten Suchergebnisse-Seite klicken und dann den gewünschten Datumsbereich auswählen. Es ist auch möglich, Gewicht nach Datum, weil jede Seite eine Änderungszeit hat. Die meisten Suchmaschinen unterstützen die Verwendung der booleischen Operatoren UND, OR und NICHT, um Benutzern zu helfen, die Suchanfrage zu verfeinern. Boolean-Operatoren sind für wörtliche Suchanfragen, die es dem Benutzer ermöglichen, die Suchbegriffe zu verfeinern und zu erweitern. Der Motor sucht die Wörter oder Phrasen genau wie eingegeben. Einige Suchmaschinen bieten eine erweiterte Funktion namens Nähensuche, die es Benutzern ermöglicht, den Abstand zwischen Keywords zu definieren. Es gibt auch konzeptbasierte Suche, bei der die Forschung die Verwendung statistischer Analysen auf Seiten mit den Wörtern oder Phrasen, die Sie suchen, beinhaltet. Die Nützlichkeit einer Suchmaschine hängt von der Relevanz des von ihm zurückgegebenen Ergebnisses ab. Während es Millionen von Webseiten geben kann, die ein bestimmtes Wort oder einen bestimmten Satz enthalten, können einige Seiten relevanter, populärer oder autoritärer sein als andere.Die meisten Suchmaschinen verwenden Methoden, um die Ergebnisse zu ordnen, um die besten Ergebnisse zuerst zu liefern. Wie eine Suchmaschine entscheidet, welche Seiten die besten Spiele sind und in welcher Reihenfolge die Ergebnisse angezeigt werden sollen, variiert weit von einem Motor zum anderen. Die Methoden ändern sich auch im Laufe der Zeit, da sich die Internetnutzung ändert und neue Techniken entstehen. Es gibt zwei Haupttypen von Suchmaschinen, die sich entwickelt haben: ein System von vordefinierten und hierarchisch geordneten Keywords, die der Mensch umfassend programmiert hat. Der andere ist ein System, das einen "invertierten Index" erzeugt, indem er Texte analysiert, die er lokalisiert. Diese erste Form setzt viel stärker auf den Computer selbst, um die Masse der Arbeit zu tun. Die meisten Web-Suchmaschinen sind kommerzielle Ventures, die durch Werbeeinnahmen unterstützt werden, und so ermöglichen einige von ihnen Werbekunden, ihre Listings höher in den Suchergebnissen für eine Gebühr platziert haben. Suchmaschinen, die kein Geld für ihre Suchergebnisse akzeptieren, machen Geld, indem Sie Search bezogene Anzeigen neben den regelmäßigen Suchergebnissen. Die Suchmaschinen verdienen jedes Mal, wenn jemand auf eine dieser Anzeigen klickt. Lokale Suche Lokale Suche ist der Prozess, der die Bemühungen der lokalen Unternehmen optimiert. Sie konzentrieren sich auf Veränderungen, um sicherzustellen, dass alle Suchvorgänge konsequent sind. Es ist wichtig, weil viele Leute bestimmen, wohin sie gehen wollen und was sie auf ihren Suchanfragen kaufen sollen. Marktanteil Ab Juni 2021, Google ist bei weitem die weltweit am häufigsten verwendete Suchmaschine, mit einem Marktanteil von 92.49,% und die weltweit anderen am häufigsten verwendeten Suchmaschinen waren: Russland und Ostasien In Russland, Yandex hat einen Marktanteil von 61,9,% im Vergleich zu Googles 28,3%. In China, Baidu ist die beliebteste Suchmaschine. Südkoreas Heimat-Suche-Portal, Naver, wird für 70% der Online-Suche im Land verwendet. Yahoo!Japan und Yahoo! Taiwan sind die beliebtesten Wege für Internet-Suche in Japan und Taiwan. China ist eines von wenigen Ländern, in denen Google nicht in den drei Top-Web-Suchmaschinen für Marktanteil ist. Google war zuvor eine Top-Suchmaschine in China, aber zog nach einer Meinungsverschiedenheit mit der Regierung über Zensur, und ein Cyberangriff. EuropeDie meisten Ländermärkte in der Europäischen Union werden von Google dominiert, mit Ausnahme der Tschechischen Republik, wo Seznam ein starker Wettbewerber ist. Die Suchmaschine Qwant basiert in Paris, Frankreich, wo sie die meisten seiner 50 Millionen monatlich registrierten Benutzer aus. Suchmaschine Vorspannung Obwohl Suchmaschinen auf Websites basierend auf einer Kombination aus Popularität und Relevanz programmiert sind, zeigen empirische Studien verschiedene politische, wirtschaftliche und soziale Vorurteile in den von ihnen bereitgestellten Informationen und die zugrunde liegenden Annahmen über die Technologie. Diese Bias können ein direktes Ergebnis wirtschaftlicher und kommerzieller Prozesse sein (z.B. Unternehmen, die mit einer Suchmaschine werben, können auch in ihren organischen Suchergebnissen populärer werden), und politische Prozesse (z.B. die Entfernung von Suchergebnissen, um lokale Gesetze einzuhalten). Zum Beispiel wird Google bestimmte neo-nazi-Websites in Frankreich und Deutschland nicht aufdecken, wo Holocaust-Verleugnung illegal ist. Biasen können auch ein Ergebnis sozialer Prozesse sein, da Suchmaschinen-Algorithmen häufig dazu ausgelegt sind, nicht-normative Ansichten zugunsten von populäreren Ergebnissen auszuschließen. Indexierungsalgorithmen von großen Suchmaschinen skew auf die Erfassung von US-basierten Websites, anstatt Websites aus nicht-US-Ländern. Google Bombing ist ein Beispiel für einen Versuch, Suchresultate nach politischen, sozialen oder kommerziellen Gründen zu manipulieren. Mehrere Wissenschaftler haben die kulturellen Veränderungen, die von Suchmaschinen ausgelöst wurden, und die Darstellung bestimmter kontroverser Themen in ihren Ergebnissen untersucht, wie Terrorismus in Irland, Klimawandel und Verschwörungstheorien. Individuelle Ergebnisse und Filterblasen Viele Suchmaschinen wie Google und Bing bieten kundenspezifische Ergebnisse basierend auf der Aktivitätsgeschichte des Nutzers. Dies führt zu einem Effekt, der als Filterblase bezeichnet wurde. Der Begriff beschreibt ein Phänomen, in dem Websites Algorithmen verwenden, um selektiv zu erraten, welche Informationen ein Benutzer sehen möchte, basierend auf Informationen über den Benutzer (wie Standort, vergangene Klick-Verhalten und Suchhistorie). Infolgedessen neigen Webseiten dazu, nur Informationen anzuzeigen, die mit dem früheren Standpunkt des Nutzers übereinstimmen. Dies bringt den Benutzer in einen Zustand der intellektuellen Isolation ohne gegensätzliche Informationen. Hauptbeispiele sind die personalisierten Suchergebnisse von Google und der personalisierte Nachrichtenstream von Facebook. Laut Eli Pariser, der den Begriff prägte, werden die Nutzer weniger mit widersprüchlichen Standpunkten konfrontiert und intellektuell in ihrer eigenen Informationsblase isoliert.Pariser verwandte ein Beispiel, in dem ein Nutzer Google nach BP suchte und über British Petroleum Investmentnachrichten erhielt, während ein anderer Sucher Informationen über die Deepwater Horizon Ölspritze bekam und dass die beiden Suchergebnisseiten "strikingly different" waren. Der Blaseneffekt kann negative Auswirkungen auf den öffentlichen Diskurs haben, so Pariser. Da dieses Problem erkannt wurde, sind konkurrierende Suchmaschinen entstanden, die versuchen, dieses Problem zu vermeiden, indem sie Nutzer, wie DuckDuckGo, nicht verfolgen oder bummeln. Andere Gelehrte teilen nicht die Ansicht von Pariser, die Beweise zu finden, um seine Dissertation nicht überzeugen. Religiöse Suchmaschinen Das globale Wachstum des Internets und der elektronischen Medien in der arabischen und muslimischen Welt in den letzten zehn Jahren hat islamische Anhänger im Nahen Osten und asiatischen Subkontinent ermutigt, ihre eigenen Suchmaschinen zu versuchen, ihre eigenen gefilterten Suchportale, die es Benutzern ermöglichen würden, sichere Suchanfragen durchzuführen. Mehr als übliche sichere Suchfilter, diese islamischen Web-Portale kategorisieren Websites in Halb- oder Hase zu sein, basierend auf Interpretation der "Law of Islam". ImHalalal kam im September 2011 online. Halalgoogling kam im Juli 2013 online. Diese verwenden Hasam-Filter auf den Sammlungen von Google und Bing (und andere). Während der Mangel an Investitionen und langsamen Tempo in Technologien in der muslimischen Welt hat den Fortschritt und den vermeintlichen Erfolg einer islamischen Suchmaschine behindert, als die wichtigsten Verbraucher islamische Anhänger, Projekte wie Muxlim, ein muslimischer Lebensstil Website, erhielt Millionen von Dollar von Investoren wie Rite Internet Ventures, und es auch gealtert. Andere religiös ausgerichtete Suchmaschinen sind Jewogle, die jüdische Version von Google, und SeekFind.org, die christlich ist. SeekFind filtert Standorte, die ihren Glauben angreifen oder abbauen. Eingabe von Suchmaschinen Web-Suchmaschinen-Einreichung ist ein Prozess, in dem ein Webmaster eine Website direkt an eine Suchmaschine eingibt. Während die Eingabe von Suchmaschinen manchmal als eine Möglichkeit zur Förderung einer Website präsentiert wird, ist es in der Regel nicht notwendig, weil die großen Suchmaschinen Web-Crawler verwenden, die schließlich die meisten Websites im Internet ohne Unterstützung finden. Sie können entweder eine Webseite zu einer Zeit einreichen, oder sie können die gesamte Website mit einer Sitemap einreichen, aber es ist normalerweise nur erforderlich, die Homepage einer Website einzureichen, da Suchmaschinen eine gut gestaltete Website krabbeln können. Es gibt zwei weitere Gründe, eine Website oder Webseite einer Suchmaschine einzureichen: eine völlig neue Website hinzuzufügen, ohne darauf zu warten, dass eine Suchmaschine zu entdecken, und eine Website-Datensatz aktualisiert nach einer erheblichen Neugestaltung. Einige Suchmaschinen-Einreichungssoftware übermittelt nicht nur Websites an mehrere Suchmaschinen, sondern fügt auch Links zu Webseiten von ihren eigenen Seiten hinzu. Dies könnte bei der Erhöhung des Rankings einer Website hilfreich erscheinen, da externe Links eines der wichtigsten Faktoren sind, die das Ranking einer Website bestimmen. John Mueller von Google hat jedoch erklärt, dass diese " kann zu einer enormen Anzahl von unnatürlichen Links für Ihre Website führen" mit einer negativen Auswirkung auf die Website-Ranking. Siehe auch Referenzen Weiter lesen Steve Lawrence; C. Lee Giles (1999). "Zulässigkeit von Informationen über das Internet".Nature.400 (6740:) 107–9.Bibcode:1999Natur.400..107L doi:10.1038/21987.PMID 10428673.S2CID 4347646.CS1 Maint: multiple namen: Autorenliste (link) Bing Liu (2007,) Web Data Mining: Exploring Hyperlinks, Contents and Usage Data.Springer,ISBN 3-540-37881-2 Bar-Ilan, J. (2004). Die Nutzung von Web-Suchmaschinen in der Informationsforschung. ARIST, 38, 231–288.Levene, Mark (2005). Eine Einführung in Suchmaschinen und Web-Navigation. Pearson.Hock, Randolph (2007). The Extreme Searcher's Handbook.ISBN 978-0-910965-76-7 Javed Mostafa (Februar 2005)."Seeking Better Web Searches".Scientific American.292 (2:) 66–73.Bibcode:2005SciAm.292b..66M doi:10.1038/scientificamerican0205-66.Ross, Nancy;2000, Diet. " Endanwender suchen im Internet: Eine Analyse von Termpaarthemen, die der Excite Suchmaschine vorgelegt werden." Journal of the American Society for Information Science.51 (10:) 949–958.doi:10.1002/1097-4571(2000)51:10<949:AID-ASI70>3.0.CO;2-5.Xie, M;. et al.(1998)."Qualitätsdimensionen von Internet-Suchmaschinen". Journal of Information Science.24 (5:) 365–372.doi:10.1177/016555159802400509.S2CID 34686531.Informationen Retrieval: Implementierung und Auswertung von Suchmaschinen. MIT Presse.2010 Externe Links Suchmaschinen bei Curlie