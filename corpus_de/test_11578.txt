Das tiefe Web, unsichtbares Web oder verstecktes Web sind Teile des World Wide Web, deren Inhalte nicht durch Standard-Websuchmaschinen indexiert werden. Im Gegensatz zu dem "surface Web", das für jedermann zugänglich ist, der das Internet nutzt. Informatik Michael K. Bergman wird im Jahr 2001 als Suchtzeit gutgeschrieben. Der Inhalt des tiefen Web ist hinter den Formen der Chancengleichheit verborgen und umfasst viele häufige Verwendungen wie Web-Post, Online-Banken, Privat- oder anderweitig eingeschränkten Zugang zu sozialen Medienseiten und -profilen, einige Web-Foren, die die Registrierung für Inhalte erfordern, und Dienstleistungen, die die Nutzer für bezahlen müssen, und die durch Paywalls geschützt sind, wie Video auf Abruf und einige Online-magazine und Zeitungen. Der Inhalt des tiefen Web kann durch eine direkte URL oder eine IP-Adresse zugänglich sein, kann jedoch ein Passwort oder einen anderen Sicherheitszugang erfordern, um frühere öffentliche Webseiten zu erhalten. Terminologie Im Jahr 2009 kam es zu einem ersten Konflikt zwischen den Begriffen "deep Web" und "dark Web", wenn eine tiefere Web-Suchesterminologie zusammen mit illegalen Aktivitäten im Freenet und Dunkelnet erörtert wurde. Diese kriminellen Aktivitäten umfassen den Handel mit persönlichen Passwörtern, gefälschten Ausweispapieren, Drogen, Feuerwaffen und Kinderpornografie. Seit ihrer Verwendung in der Berichterstattung über die Seidenstraße haben die Mediengeschäfte dazu geführt, dass sie "deep Web" gleich mit dem dunklen Web oder dunklennet verwenden, ein Vergleich, der einige als unzuverlässig erklärt und daher zu einer aktuellen Quelle von Verwirrung geworden ist. Presseberichte Kim Zetter und Andy Greenberg empfehlen die Verwendungsbedingungen in verschiedenen Moden. Obwohl das tiefe Web einen Bezug auf jede Website hat, die nicht über eine traditionelle Suchmaschine zugänglich werden kann, ist das dunkle Web ein Teil der vorsätzlich verborgenen tiefen Web-Seite und ist durch Standard-Browser und -methoden nicht zugänglich. Nichtindexierte Inhalte Bergman, in einem Papier über das im Journal of Electronic Publishing veröffentlichte tiefe Web, wies darauf hin, dass Justin Ellsworth 1994 den Begriff „unvisibles Web“ benutzte, um Websites zu nennen, die nicht mit einer Suchmaschine registriert wurden. Bergman nannte einen Artikel vom Januar 1996 von Frank Garcia: Es wäre eine Stelle, die vielleicht vernünftig konzipiert ist, aber sie haben es nicht mit jedem der Suchmaschinen registriert. Keiner kann sie finden! Sie sind versteckt. Ich rufe, dass das unsichtbare Web.Ansondere frühe Nutzung des Begriffs unvisibles Web von Bruce Mount und Matthew B. Koll of Personal Library Software, in einer Beschreibung des im Dezember 1996 gefundenen umfassenden Web-Tools. In der oben genannten Bergman-Studie von 2001 wurde die erste Nutzung des spezifischen Begriffs Deep Web, jetzt allgemein akzeptiert. Indexierungsmethoden Methoden, die verhindern, dass Web-Seiten von herkömmlichen Suchmaschinen indexiert werden, können als eine oder mehrere der folgenden Kategorien eingestuft werden: Kontextbezogenes Web: Seiten mit unterschiedlichen Inhalten für unterschiedliche Zugangsbedingungen (z.B. Bandbreiten von IP-Adressen oder frühere Navigationssequenzen). Dynamische Inhalte: dynamische Seiten, die in Reaktion auf eine eingereichte Anfrage zurückgeführt werden oder nur durch eine Form zugänglich sind, vor allem wenn Open-Domänen (wie Textfelder) verwendet werden; solche Felder sind schwer zu bedienen, ohne dass Domain-Know. begrenzten Zugangsinhalt: Websites, die den Zugang zu ihren Seiten auf technische Weise beschränken (z.B. unter Verwendung des Roboter-Ausgangs-Standards oder der GAPTCHAs oder der Richtlinie über Nicht-Shopping, die Suchmotoren davon abhalten und nachweisbare Kopien erstellen). Nicht-HTML/Text-Inhalte: Textinhalte, die in Multimedia-Dateien (Bild oder Video) oder bestimmten Dateiformaten enthalten sind, die nicht von Suchmaschinen bearbeitet werden. Privates Web: Websites, die Registrierung und Anmeldung erfordern (passwortgeschützte Ressourcen). Schriftliche Inhalte: Seiten, die nur über Links zugänglich sind, die von JavaScript hergestellt werden, sowie Inhalte, die dynamisch von Webservern über Flash- oder Reifenlösungen heruntergeladen werden. Software: Bestimmte Inhalte sind vorsätzlich aus dem regulären Internet zu verschleiern, das nur mit speziellen Software wie Tor, I2P oder anderen dunklennet-Software zugänglich ist. z.B. Tor erlaubt den Nutzern Zugriff auf Websites, die auf der Website von .onion Server anonym sind und ihre IP-Adresse verschleiern. Ungebundene Inhalte: Seiten, die nicht mit anderen Seiten verbunden sind, die verhindern können, dass Web-Glyzerprogramme Zugang zu Inhalten haben. Dieser Inhalt wird als Seiten ohne Backlink (auch bekannt als Link) bezeichnet. Suchmaschinen erkennen auch nicht immer alle Rückschlüsse von gesuchten Webseiten an. Webarchive: Webarchivale Dienste wie die Wayback-Maschine ermöglichen es den Nutzern, archivierte Versionen von Webseiten im Laufe der Zeit zu sehen, einschließlich Websites, die unzugänglich geworden sind, und werden nicht mit Suchmaschinen wie Google indexiert. Man kann ein Programm für die Wahrnehmung des tiefen Web nennen, da Webarchive, die nicht aus der Gegenwart stammen, nicht indexiert werden können, da frühere Versionen von Websites durch eine Suche unmöglich sind. Alle Websites werden zu einem Punkt aktualisiert, weshalb Webarchive als Deep Web-Inhalte gelten.robots.txt Dateien: A Roboter. HOR-Datei kann Suchmaschine-Botschafter empfehlen, Websites, die von Nutzern verwendet werden*: dann enttäuschend:/. Dies wird alle Suchmaschine Bots melden, um die gesamte Website nicht zu eliminieren und sie in die Suchmaschine einzufügen. Inhaltstypen Obwohl es nicht immer möglich ist, einen bestimmten Web-Server-Gehalt direkt zu entdecken, so dass es indexiert werden kann, kann eine Website mittelbar (an Computerknappheit) genutzt werden. Um Inhalte auf dem Web zu entdecken, verwenden Suchmaschinen Web-Glyzer, die Hyperlinks durch bekannte Protokoll- virtuelle Hafennummern verfolgen. Diese Technik ist ideal, um Inhalte auf der Oberfläche zu entdecken, ist aber oft unwirksam bei der Suche nach tiefen Webinhalten. Man versucht beispielsweise nicht, dynamische Seiten zu finden, die das Ergebnis von Datenanfragen aufgrund der unbestimmten Anzahl von Anfragen sind. Es wurde festgestellt, dass dies (teilweise) durch die Bereitstellung von Links zu Abfrageergebnissen überwunden werden kann, aber dies könnte die Popularität für ein Mitglied des tiefen Web unbeabsichtigt gefährden. DeepPeep, Intute, Deep Web Technologies, Scirus und Ahmia. Fi sind nur wenige Suchmaschinen, die das tiefe Web haben. Intute lief aus der Finanzierung und ist nun ein vorübergehendes statisches Archiv seit Juli 2011. Scirus Ruhestand in der Nähe Ende Januar 2013. Forscher haben untersucht, wie das tiefe Web in automatischer Weise gestrichen werden kann, einschließlich Inhalte, die nur von speziellen Software wie Tor.In 2001, Sriram Raghavan und Hector Garcia-Malla (Stanford Computer Science Department, Universität Stanford) genutzt werden können, die ein architektonisches Modell für ein verstecktes Web-Glyzer darstellen, das die Schlüsselbegriffe der Nutzer nutzt oder aus den Abfrageschnittstellen gesammelt hat, um ein Web-Formular zu Abfragen und den tiefen Web-Inhalten. Alexandros Ntoulas, Petros Zerfos und Junghoo Cho von UCLA haben einen versteckten Web-Glyzer geschaffen, der automatisch aussagekräftige Anfragen zur Ausgabe von Suchformularen generiert. Mehrere Form-Anfragen (z.B. DEQUEL) wurden vorgeschlagen, dass neben der Abfrage auch die Gewinnung strukturierter Daten aus den Ergebnisseiten möglich ist. Weitere Anstrengungen sind DeepPeep, ein von der National Science Foundation gefördertes Projekt der University of Tonga, das versteckte Web-Quellen (web-Formulare) in verschiedenen Bereichen auf der Grundlage neuartiger, konzentrierter Techniken sammelte. Kommerzielle Suchmaschinen haben begonnen, alternative Methoden zu erkunden, um das tiefe Web zu kandidieren. Das von Google im Jahr 2005 entwickelte und von OAI-PMH eingeführte und von Google eingeführte Protokoll ist ein Mechanismus, mit dem Suchmaschinen und andere interessierte Parteien auf bestimmten Web-Servern entdecken können. Beide Mechanismen ermöglichen es Web-Servern, die auf ihnen zugänglichen Websites bekannt zu machen und so eine automatische Entdeckung von Ressourcen zu ermöglichen, die nicht direkt mit der Oberfläche verbunden sind. Googles tiefes Web-Schlagsystem berechnet die Einreichungen für jedes einzelne Format und erweitert die zugehörigen computergestützten Seiten in den Suchmaschineindex Google. Die sichtbaren Ergebnisse stellen tausend Anfragen pro Sekunde für tiefe Webinhalte vor. In diesem System wird die Vorabrechnung von Beiträgen mit drei Algorithmen durchgeführt: Auswahl der Inputwerte für Textsuche, die Stichwörter akzeptieren, und Identifizierung von Inputs, die nur Werte einer bestimmten Art (z.B. Datum) akzeptieren, und Auswahl einer kleinen Anzahl von Inputkombinationen, die URL für die Aufnahme in den Web-Suche-Index generieren. 2008, um Nutzern von Tor versteckte Dienstleistungen in ihrem Zugang und der Suche nach einem versteckten .onion suffix zu erleichtern, entworfene Vladimir Swartz Tor2webs – eine Bewerbung, die den Zugang durch gemeinsame Web-Browser ermöglicht. Mit dieser Anwendung erscheinen die tiefen Webverbindungen als Zufallsssssssache, gefolgt von der .onion Top-Level-Domäne. Siehe auch das Memex-Programm Deep Link Gopher (Protocol) Links Weitere Lesung Externe Links Medien im Zusammenhang mit Deep Web auf dem Gebiet der Wikimedia Commons