Kanalkapazität in Elektrotechnik, Informatik und Informationstheorie ist die enge Obergrenze für den Satz, an dem Informationen zuverlässig über einen Kommunikationskanal übermittelt werden können. Nach den Bedingungen des lauteren Kanalsschlüssels ist die Kanalkapazität eines bestimmten Kanals die höchste Informationsrate (in Einheiten von Informationen pro Stück Zeit), die mit willkürlich geringen Fehlerwahrscheinlichkeit erreicht werden kann. Informationsgesellschaft, entwickelt von Claude E. Shannon im Jahr 1948, definiert den Begriff der Kanalkapazität und stellt ein mathematisches Modell vor, mit dem eine berechnen kann. Kernergebnis ist, dass die Kapazität des Kanals, wie oben definiert, durch die maximalen gegenseitigen Informationen zwischen der Eingabe und dem Output des Kanals, wo die Maximierung im Hinblick auf die Inputverteilung erfolgt, gewährleistet ist. Kern der Entwicklung moderner Draht- und Funkkommunikationssysteme ist der Begriff der Kanalkapazität, mit dem neuartigen Fehlerkorrekturkodierungsmechanismen, die zu einer sehr engen Leistung der durch die Kanalkapazität versprochenen Grenzen geführt haben. formale Definition Das grundlegende mathematische Modell für ein Kommunikationssystem ist: → Nachrichten W® f n → E n c o d s e q u e n c e X n Channel p ( y | x ) → R e c e i v e d s e q u e n c e Y n Decoder g n → E s t i m e s s s a g e W ^ {\displaystyle xrightarrow[8]Textbotschaft{] Kennzeichenff_{n}\\\hline End{array}}{\xrightmark[8] {Encoded \atop Sequenz} ]X^{n}}}{\begin{array|c·chline Text{Kanal(p(y)|hline end{array}}{\xrightarrow[8] {Received \atop Sequenz} Decoder_g_{n}\\\hline end{array}}{\xrightmark[8] {Estimated \atop Botschaft} {\}]hat {W}, wo: W Memestylestyle WK-Code ist die zu übertragende Botschaft; X displaystyle X} ist das Symbol für die Eingabe ( X n {\style X Xn} ist eine Sequenz von n displaystyle n} Symbol) in einem Alphabet X Memestyle {X} ; Y Memestyle Y} ist das kanalbildende Symbol ( Y n \displaystyle Y} ist eine Sequenz von n n n } n n n } Symbol) in einem Ystyle {Xstyle {X {X}  Y  W Letztlich ist es die Voraussetzung für die Verteilungsfunktion von Y Memedisplaystyle Y} ( y ) Memedisplaystyle p_{Y|X} (y|x) , die als eigenständiges Eigentum des Kommunikationskanals gilt. Kann die Auswahl der marginalen Verteilung p X ( x ) Memedisplaystyle p_{X}(x) vollständig die gemeinsame Verteilung P X , Y ( x , y ) displaystyle p_{X,Y}(x,y) aufgrund der Identität p X , Y ( x , y ) = p Y ( y ) X ( x ) } \ p_{X,} (Yx,yx)p_=p) {X, {X) Die Kanalkapazität ist definiert als C = sup p X ( x ) I ( X ; Y ) Memestyle \ C=\sup p_{X}(x)}I(X;Y,\, wo der supremum über alle möglichen Entscheidungen von p X ( x ) Memestyle p_{X}(x) . Additivität der Kanalkapazitätskanalkapazität ist über unabhängige Kanäle. Es bedeutet, dass die Verwendung von zwei unabhängigen Kanälen in kombinierter Weise die gleiche theoretische Kapazität bietet, wie sie selbst verwendet werden. Mehr formell, lassen Sie sich p_{1} und p 2 Memestyle p_{2} zwei eigenständige Kanäle bedienen, die wie oben beschrieben sind; p 1 {\displaystyle p_{1} mit einem Input- Alphabet X 1 {\displaystyle ggiostyle X{\_{1 und einem Output  Alphabet Y 1 {\displaystyle Memecal Y__{1 .Idem für p 2 KINGstyle p_2}. Wir definieren den Produktkanal p 1 × p 2 Memestyle p_{1}\times p_{2} als  ( ( x 1 , x 2 )  1 ( X 1 , X 2 ) , ( y 1 , y 2 ) ) ( Y 1 , Y 2 ) , ( p 1 }) ( y 1 } } } } ( x 1, x 2 ) Man stellt fest: Shannon Kapazität eines Diagramms, wenn G ein undirektiertes Diagramm ist, kann genutzt werden, um einen Kommunikationskanal zu definieren, in dem die Symbole die Graphen sind, und zwei Codeworte können miteinander verwechselt werden, wenn ihre Symbole in jeder Position gleich oder an der Grenze sind. Die rechnerische Komplexität, die die Shannon-Kapazität eines solchen Kanals zu finden, bleibt offen, aber es kann durch einen anderen wichtigen, unvariablen Diagramm, der Lovász-Nummer, überbrückt werden. Noisy-Kanal schlüsselt Die lauteren Sendercodes erklärt, dass für jede Fehlerwahrscheinlichkeit  > > 0 und für jede Übertragungsgeschwindigkeit R weniger als die Kanalkapazität C gibt es eine Kodierungs- und Entschlüsselungsregelung, die Daten zu Rate R, deren Fehler wahrscheinlich weniger als . ist, für eine hinreichend große Blocklänge übermittelt. Für jeden Satz, der über die Kanalkapazität hinaus liegt, liegt die Fehlerwahrscheinlichkeit beim Empfänger auf 0,5, da die Blocklänge in der Ungültigkeit geht. Beispiel Eine Anwendung des Kanalkapazitätskonzepts auf einen zusätzlichen weißen Gausssischen Lärm (AWGN)-Kanal mit B Hz Bandbreite und Signal-to-noise Verhältnis S/N ist der Shannon-Hartley-Kanal: C = B Log 2 ) ( 1 + S N ) Memedisplaystyle C=B\log_{2)left(1+ggiofrac S}{N)right) } C wird in Bits pro Sekunde gemessen, wenn der Logarithm in der Basis 2 oder in der zweiten Phase eingenommen wird, wenn der natürliche Logarithm verwendet wird, sofern B in Hertz liegt; die Signal- und Lärmbelässe S und N werden in einer linearen Stromeinheit (wie Watte oder Volts2) ausgedrückt. Seit S/N-Zahlen werden oft in dB angegeben, kann eine Umrechnung erforderlich sein. Beispielsweise entspricht ein Signal-to-noiseverhältnis von 30 dB einem linearen Leistungsverhältnis von 10 30 / 10 = 10 3 = 1000 Memestyle. 10/1030/10}=10^{3}=1000 . Kanalkapazität in der drahtlosen Kommunikation Dieser Abschnitt konzentriert sich auf das Szenario „One-antenna“, das sich auf Punkt bezieht. Kanalkapazität in Systemen mit mehreren Antennen, siehe Artikel über die MIMO. Bandlimit AWGN-Kanal Liegt der durchschnittliche Empfangsleistung bei P   WELLdisplaystyle {P} [W,] die Gesamtbreite ist W {\displaystyle W} in Hertz und die Lärmdichte ist N 0 {\displaystyle N_{0} [W/Hz,] die AWGN-Kanalkapazität ist C AWGN = W Log 2 ) ( 1 + P  N N 0 W ) 7.8displaystyle C_TONtext{AWGN==W\log _2(left(1+8) P}}{N_{0}W)right) [bits/s], wo P  N N 0 W  steuerlicher Stil Memefrac WELLbar P}}{N_{0} W ist das empfangene Signal-to-noiseverhältnis (SNR). Das Ergebnis ist bekannt als Shannon–Hartley theorem. Wenn die SNR groß ist (SNR  0 0 dB), die Kapazität C ≈ W Log 2 ⁡ P  0 N 0 W RARstyle C\ca W\log _2}{\frac 7.8bar P{N_{0}W ist Logarithmic in Power und etwa linear in Bandbreite. Dies ist die flächenbegrenzte Regelung. Wenn die SNR klein ist (SNR  0 0 dB), ist die Kapazität C  P P  0 N 0 ln  2 2 displaystyle C\tim Racfrac WELLbar P}}{N_{0}\ln   2} ist linear in Kraft, aber unempfindliche Bandbreite. Dies ist die kraftbegrenzte Regelung. Die Bandbreitenbegrenzte Regelung und die Stromversorgung werden in der Zahl dargestellt. Frequenzselektiver AWGN-Kanal Die Kapazität des Frequenz-Selektivkanals wird durch sogenannte Wasserfüllleistungszuteilung, C N c = n = 0 N c − 1 Log 2 ⁡ ( 1 + P n  | [ h  | n n [ 2 N 0 ] ) , C_{N_{c==\sum n=0}^{N_{c}-1}\log _2}\left(1+ggiofrac P_{n**} h__{n}|2}}{N_{0),right), wo P n  = = max { 1 λ − N 0) n | 2 ) , 0 } displaystyle P_{n**}=\max links((Porta 1}{\lambda) {\}frac N_{0}}{} h__{n}|2.right),0\right und | h  | n | 2 {\displaystyle HANA8.5bar h__{n}19522 ist der Gewinn von Unterkanal n {\displaystyle n} , wobei λ Memestyle \lambda } gewählt wurde, um den Leistungsdruck zu erfüllen. Langsamer Kanal In einem langsamen Kanal, in dem die Kohärenzzeit größer ist als die späte Anforderung, gibt es keine eindeutige Kapazität, da die maximal zuverlässige Kommunikation, die durch den Kanal unterstützt wird, Log 2 ⁡ ( 1 + [ h]  S  S ( 1 +  | 2 S N R ) Memestyle \log _2}(1+|h^{2}SNR) , hängt vom Zufallskanal ab. Liegt der Sender Daten an der Rate R encodes R} [bits/s/Hz], gibt es eine nicht-zero Wahrscheinlichkeit, dass die Decodefehlerwahrscheinlichkeit nicht willkürlich klein, p o t = P ( Log . ( 1 + [h ]  N ]  N )  N  N  2  2  2  2  2  2  2  2  1 1 1 1 1  which  which  which  which  which  which  which  which  which  which  which  which, was im System gesagt wird. Mit einer Wahrscheinlichkeit, dass der Kanal in tiefen Fade liegt, ist die Kapazität des langsamen Kanals in striktem Sinne Null. Jedoch ist es möglich, den größten Wert von R Memedisplaystyle R} zu bestimmen, so dass die Wahrscheinlichkeit, dass die Ausnutzungswahrscheinlichkeit p {out} geringer ist als   KINGstyle \epsilon }. Dieser Wert ist bekannt als die   WELLdisplaystyle \epsilon } - Outage-Kapazität. Schnellkanal In einem schnellen Kanal, in dem die verspätete Anforderung größer ist als die Kohärenzzeit und die Codeformlänge reicht viele Kohärenzzeiten, was einer über viele unabhängige Kanal-Fern durch Kodierung über eine große Zahl von Kohärenzzeitintervallen durchschnittlich übersteigen kann. So ist es möglich, eine zuverlässige Kommunikationsquote von E zu erreichen ( Log 2  1 ( 1 + | h) ) KINGstyle {E} (\log _2}(1+|h|2}SNR) [bits/s/Hz] und es ist sinnvoll, von diesem Wert als Kapazität des schnellen Kanals zu sprechen. Siehe auch Bandbreite (computing) Bandbreite (Signalverarbeitung)bit Rate Code Rate Fehler exponent Nyquist Rate Negentropy Redundancy, Datenkomprimierung, Empfänger Shannon–Hartley theorem Spectral Effizienz durchput Finite Block Länge Information Theorie Advanced Communication MIMO Cooperative Diversity Externe Links "Transmissionsrate eines Kanals"," Veröffentlichung von Mathematik, EMS Presse, 2001 [1994]AWGN-Kanalkapazität mit verschiedenen Engpässen auf dem Kanal (Interaktive Demonstration)=.