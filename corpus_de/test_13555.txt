Die technologische Einzigartigkeit - oder einfach die Singularität - ist ein hypothetischer Zeitpunkt, zu dem technologisches Wachstum unkontrollierbar und irreversibel wird, was zu unvorhersehbaren Veränderungen der menschlichen Zivilisation führt. Laut der populärsten Version der Singularität Hypothese, genannt Intelligenzexplosion, wird ein aufrüstbarer intelligenter Agent schließlich eine "Runaway-Reaktion" von Selbstverbesserungszyklen eingehen, jede neue und intelligentere Generation erscheint immer schneller, was eine Explosion in Intelligenz verursacht und zu einer mächtigen Superintelligenz führt, die qualitativ weit über alle menschlichen Intelligenz hinausgeht. Das erste, das Konzept einer Singularität im technologischen Kontext zu nutzen, war John von Neumann. Stanislaw Ulam berichtet eine Diskussion mit von Neumann "zentriert auf den beschleunigten Fortschritt der Technologie und Veränderungen in der Art des menschlichen Lebens, die das Auftreten von etwas essentielle Singularität in der Geschichte des Rennens gibt, über das menschliche Angelegenheiten, wie wir sie kennen, nicht weitergehen konnten". Nachfolgende Autoren haben diesen Standpunkt zum Ausdruck gebracht. Das Modell "Intelligenzexplosion" von I J. Good sagt, dass eine zukünftige Superintelligenz eine Singularität auslösen wird. Das Konzept und der Begriff Singularität wurden von Vernor Vinge in seinem Essay The Coming Technological Singularity von 1993 populär, in dem er schrieb, dass es das Ende der menschlichen Ära signalisieren würde, da die neue Superintelligenz sich weiter verbessern würde und technologisch mit einer unverständlichen Rate voranbringen würde. Er schrieb, dass er überrascht wäre, wenn es vor 2005 oder nach 2030 aufgetreten wäre. Öffentliche Zahlen wie Stephen Hawking und Elon Musk haben Besorgnis zum Ausdruck gebracht, dass volle künstliche Intelligenz (KI) zu menschlichem Aussterben führen könnte. Die Folgen der Singularität und ihres potentiellen Nutzens oder der Schädigung der menschlichen Rasse wurden intensiv diskutiert. Vier Umfragen von AI-Forschern, die 2012 und 2013 von Nick Bostrom und Vincent C. Müller durchgeführt wurden, schlugen eine mittlere Wahrscheinlichkeitsschätzung von 50% vor, dass künstliche allgemeine Intelligenz (AGI) bis 2040–2050 entwickelt würde. Hintergrund Obwohl der technologische Fortschritt in den meisten Bereichen beschleunigt wurde (obwohl in einigen verlangsamt), wurde er durch die grundlegende Intelligenz des menschlichen Gehirns begrenzt, die sich laut Paul R. Ehrlich seit Jahrtausenden nicht signifikant verändert hat. Mit der zunehmenden Leistung von Computern und anderen Technologien könnte es aber schließlich möglich sein, eine Maschine aufzubauen, die deutlich intelligenter ist als der Mensch. Wenn eine übermenschliche Intelligenz erfunden werden sollte – sei es durch die Verstärkung menschlicher Intelligenz oder durch künstliche Intelligenz –, würde sie größere Problemlösungs- und erfinderische Fähigkeiten tragen als aktuelle Menschen in der Lage sind. Eine solche KI wird als Seed AI bezeichnet, weil, wenn eine KI mit Engineering-Fähigkeiten erstellt würde, die denen der menschlichen Schöpfer entsprachen oder übertrafen, es das Potenzial hätte, ihre eigene Software und Hardware autonom zu verbessern oder eine noch leistungsfähigere Maschine zu entwerfen. Diese leistungsfähigere Maschine könnte dann weiter gehen, um eine Maschine von noch größerer Fähigkeit zu entwerfen. Diese Iterationen der rekursiven Selbstverbesserung könnten sich beschleunigen und möglicherweise einen enormen qualitativen Wandel vor allen durch die Gesetze der Physik oder der theoretischen Berechnung festgelegten Obergrenzen ermöglichen. Es wird spekuliert, dass eine solche KI über viele Iterationen hinweg menschliche kognitive Fähigkeiten weit übertreffen würde. Intelligenz Explosion Intelligenz Explosion ist ein mögliches Ergebnis der Menschheit Gebäude künstliche allgemeine Intelligenz (AGI). AGI kann wiederkehrende Selbstverbesserung bewerkstelligen, was zum schnellen Entstehen künstlicher Superintelligenz (ASI) führt, deren Grenzen unbekannt sind, kurz nach der technologischen Einzigartigkeit erreicht wird. I J. Good spekuliert 1965, dass künstliche allgemeine Intelligenz eine Intelligenzexplosion verursachen könnte. Er spekuliert auf die Auswirkungen von übermenschlichen Maschinen, sollten sie jemals erfunden werden: Lassen Sie eine ultraintelligente Maschine als Maschine definiert werden, die weit über alle intellektuellen Aktivitäten jedes Mannes hinausgehen kann, aber clever. Da die Konstruktion von Maschinen eine dieser intellektuellen Tätigkeiten ist, könnte eine ultraintelligente Maschine noch bessere Maschinen entwerfen; es würde dann zweifellos eine „intelligente Explosion“ geben, und die Intelligenz des Menschen würde weit hinter sich gelassen werden. So ist die erste ultraintelligente Maschine die letzte Erfindung, die der Mensch je machen muss, vorausgesetzt, die Maschine ist schick genug, um uns zu sagen, wie man sie unter Kontrolle hält. Andere Manifestationen Emergence of superintelligenceA superintelligence, hyperintelligence, oder übermenschliche Intelligenz ist ein hypothetischer Agent, der Intelligenz besitzt, die weit über die der hellsten und begabten menschlichen Geister hinausgeht. "Superintelligenz kann sich auch auf die Form oder den Grad der Intelligenz beziehen, die von einem solchen Agenten besaß. John von Neumann, Vernor Vinge und Ray Kurzweil definieren das Konzept in Bezug auf die technologische Kreation von Superintelligenz, indem er argumentiert, dass es für den heutigen Menschen schwierig oder unmöglich ist, vorherzusagen, wie das Leben der Menschen in einer postsingularistischen Welt wäre. Technologieprognosen und -forscher sind sich nicht darüber einig, wann oder ob die menschliche Intelligenz wahrscheinlich übertroffen wird. Einige argumentieren, dass Fortschritte in der künstlichen Intelligenz (KI) wahrscheinlich zu allgemeinen Argumentationssystemen führen, die keine menschlichen kognitiven Einschränkungen haben. Andere glauben, dass die Menschen ihre Biologie entwickeln oder direkt verändern werden, um radikal größere Intelligenz zu erreichen. Eine Reihe von Futures-Studienszenarien kombinieren Elemente aus diesen beiden Möglichkeiten, was darauf hindeutet, dass Menschen wahrscheinlich mit Computern in Verbindung stehen oder ihren Verstand auf Computer hochladen, so dass eine erhebliche Intelligenzverstärkung ermöglicht. Nicht-AI Singularität Einige Autoren verwenden "die Singularität" auf breitere Weise, um sich auf radikale Veränderungen in unserer Gesellschaft, die durch neue Technologien wie molekulare Nanotechnologie hervorgerufen werden, zu beziehen, obwohl Vinge und andere Schriftsteller ausdrücklich behaupten, dass solche Veränderungen ohne Superintelligenz nicht als wahre Singularität gelten würden. Geschwindigkeit Superintelligenz Eine Geschwindigkeit Superintelligenz beschreibt eine KI, die alles tun kann, was ein Mensch tun kann, wo der einzige Unterschied ist, dass die Maschine schneller läuft. So würde beispielsweise mit einer millionenfachen Erhöhung der Geschwindigkeit der Informationsverarbeitung gegenüber der des Menschen ein subjektives Jahr in 30 physikalischen Sekunden passieren. Ein solcher Unterschied in der Geschwindigkeit der Informationsverarbeitung könnte die Singularität antreiben. Plausibilität Viele prominente Technologen und Akademiker haben die Plausibilität einer technologischen Einzigartigkeit bestritten, darunter Paul Allen, Jeff Hawkins, John Holland, Jaron Lanier und Gordon Moore, deren Gesetz oft zur Unterstützung des Konzepts zitiert wird. Die meisten vorgeschlagenen Methoden zur Schaffung von übermenschlichen oder transhumanen Geistern fallen in eine von zwei Kategorien: Intelligenz Amplifikation des menschlichen Gehirns und künstliche Intelligenz. Die vielen spekulierten Möglichkeiten, menschliche Intelligenz zu erweitern, umfassen Bioengineering, Gentechnik, nootropic Drogen, AI-Assistenten, direkte Gehirn-Computer-Schnittstellen und Geist-Uploading. Diese multiplen Pfade zu einer Intelligenzexplosion macht eine Singularität wahrscheinlicher, da sie alle für eine Singularität scheitern müssten, die nicht auftritt. Robin Hanson drückte Skepsis der menschlichen Intelligenz-Augmentation aus, indem er schrieb, dass nach der Erschöpfung der "niedrigen Früchte" einfacher Methoden zur Erhöhung der menschlichen Intelligenz weitere Verbesserungen immer schwieriger zu finden sind. Trotz aller spekulierten Möglichkeiten, menschliche Intelligenz zu verstärken, ist nicht-menschliche künstliche Intelligenz (spezifisch Samen AI) die beliebteste Option unter den Hypothesen, die die Singularität voranbringen würde. Ob eine Intelligenzexplosion auftritt oder nicht, hängt von drei Faktoren ab. Der erste Beschleunigungsfaktor ist die neue Intelligenzverbesserung, die durch jede vorherige Verbesserung ermöglicht wird. Im Gegensatz dazu, wie die Intelligenzen weiter vorangetrieben werden, werden weitere Fortschritte immer komplizierter, möglicherweise überwindet der Vorteil einer erhöhten Intelligenz. Jede Verbesserung sollte mindestens eine weitere Verbesserung, im Durchschnitt, für die Bewegung in Richtung Singularität zu fortsetzen. Schließlich werden die Gesetze der Physik eventuell weitere Verbesserungen verhindern. Es gibt zwei logisch unabhängige, aber gegenseitig verstärkende Ursachen von Intelligenzverbesserungen: Erhöhung der Rechengeschwindigkeit und Verbesserungen der verwendeten Algorithmen. Die frühere wird durch Moore's Law und die prognostizierten Verbesserungen in der Hardware prognostiziert und ist vergleichsweise ähnlich wie frühere technologische Fortschritte. Aber es gibt einige KI-Forscher, die glauben, Software ist wichtiger als Hardware. Eine 2017-E-Mail-Umfrage von Autoren mit Publikationen auf den 2015 NeurIPS und ICML-Maschinenlernkonferenzen fragte über die Chance einer Intelligenzexplosion. Von den Befragten, 12% sagte, es sei "gleichwahrscheinlich", 17% sagte es wahrscheinlich, 21% sagte, es sei "etwas sogar", 24% sagte, es sei unwahrscheinlich und 26% sagte, es sei "unwahrscheinlich". Verbesserungen der Geschwindigkeit Sowohl für menschliche als auch für künstliche Intelligenz erhöhen Hardwareverbesserungen die Rate zukünftiger Hardwareverbesserungen. Einfach gesagt, Moore's Gesetz deutet darauf hin, dass, wenn die erste Verdoppelung der Geschwindigkeit 18 Monate dauerte, die zweite 18 subjektive Monate dauern würde; oder 9 externe Monate, danach, vier Monate, zwei Monate, und so weiter zu einer Geschwindigkeit Singularität. Eine obere Geschwindigkeitsbegrenzung kann schließlich erreicht werden, obwohl es unklar ist, wie hoch das wäre. Jeff. Hawkins hat erklärt, dass ein selbstverbesserndes Computersystem zwangsläufig in obere Grenzen der Rechenleistung laufen würde: "Endlich gibt es Grenzen, wie große und schnelle Computer laufen können. Wir würden am selben Ort enden; wir würden einfach etwas schneller dorthin kommen. Es gäbe keine Singularität. " Es ist schwierig, direkt siliciumbasierte Hardware mit Neuronen zu vergleichen. Aber Berglas (2008) stellt fest, dass die Computerredeerkennung menschliche Fähigkeiten nähert, und dass diese Fähigkeit scheint 0,01% des Gehirnvolumens zu erfordern. Diese Analogie deutet darauf hin, dass moderne Computer-Hardware in einigen Größenordnungen so mächtig ist wie das menschliche Gehirn. Auffälliges Wachstum Das exponentielle Wachstum der Computertechnologie, das von Moore's Gesetz vorgeschlagen wird, wird allgemein als Grund genannt, eine Singularität in der relativ nahen Zukunft zu erwarten, und eine Reihe von Autoren haben Verallgemeinerungen von Moore's Gesetz vorgeschlagen. Informatiker und Futurist Hans Moravec hat in einem 1998 erschienenen Buch vorgeschlagen, dass die exponentielle Wachstumskurve vor dem integrierten Schaltkreis durch frühere Rechentechnologien zurückverbreitet werden könnte. Ray Kurzweil postuliert ein Gesetz der beschleunigten Rückkehr, in dem die Geschwindigkeit des technologischen Wandels (und im Allgemeinen alle evolutionären Prozesse) exponentiell erhöht, Moore's Gesetz in der gleichen Weise wie Moravec's Vorschlag, und auch einschließlich Materialtechnologie (insbesondere wie auf Nanotechnologie,) Medizintechnik und andere. Zwischen 1986 und 2007 verdoppelte sich die anwendungsspezifische Kapazität der Maschinen, um Informationen pro Kopf etwa alle 14 Monate zu berechnen; die Per-Kopf-Kapazität der Universalrechner der Welt hat sich alle 18 Monate verdoppelt; die globale Telekommunikationskapazität pro Kopf verdoppelte sich alle 34 Monate; und die weltweite Speicherkapazität pro Kopf verdoppelte sich alle 40 Monate. Andererseits wurde argumentiert, dass das globale Beschleunigungsmuster mit der Einzigartigkeit des 21. Jahrhunderts als dessen Parameter als hyperbolisch und nicht exponentiell charakterisiert werden sollte. Kurzweil behält sich den Begriff Singularität für eine rasche Zunahme der künstlichen Intelligenz (im Gegensatz zu anderen Technologien) vor, die zum Beispiel schreibt: "Die Singularität wird es uns ermöglichen, diese Einschränkungen unserer biologischen Körper und Gehirne zu überwinden. Es gibt keine Unterscheidung, Post-Singularität, zwischen Mensch und Maschine". Er definiert auch sein prognostiziertes Datum der Singularität (2045) in Bezug auf, wenn er erwartet, dass computergestützte Intelligenzen die Summe der menschlichen Gehirnleistung deutlich übersteigen, indem er schreibt, dass Fortschritte im Computing vor diesem Datum "die Singularität nicht darstellen", weil sie "noch nicht einer tiefen Erweiterung unserer Intelligenz entsprechen". Beschleunigende Veränderung Einige Singularitätsproponenten argumentieren ihre Unvermeidlichkeit durch Extrapolation vergangener Trends, vor allem derjenigen, die Lücken zwischen Verbesserungen der Technologie zu verkürzen. In einer der ersten Verwendungen des Begriffs Singularität im Kontext des technologischen Fortschritts erzählt Stanislaw Ulam von einem Gespräch mit John von Neumann über beschleunigte Veränderung: Ein Gespräch konzentrierte sich auf den immer beschleunigenden Fortschritt der Technik und Veränderungen in der Art des menschlichen Lebens, die das Aussehen gibt, sich etwas essentielle Singularität in der Geschichte des Rennens zu nähern, über das menschliche Angelegenheiten, wie wir sie kennen, nicht weitergehen konnten. Kurzweil behauptet, dass der technologische Fortschritt einem Muster des exponentiellen Wachstums folgt, nachdem er das "Gesetz der beschleunigten Renditen" nennt. Wann immer die Technologie einer Barriere nähert, schreibt Kurzweil, neue Technologien werden sie übersteigen. Er prognostiziert, dass Paradigmenverschiebungen immer häufiger werden, was zu "technologischem Wandel so schnell und profund führt, dass es einen Bruch im Gewebe der menschlichen Geschichte darstellt". Kurzweil glaubt, dass die Singularität bis etwa 2045 auftreten wird. Seine Vorhersagen unterscheiden sich von Vinge's dadurch, dass er einen allmählichen Aufstieg zur Singularität prognostiziert, anstatt Vinges schnell selbstverbessernde übermenschliche Intelligenz. Zu den häufig zitierten Gefahren gehören solche, die häufig mit der Molekularnanotechnologie und der Gentechnik verbunden sind. Diese Bedrohungen sind wichtige Themen sowohl für Singularitätsverteidiger als auch für Kritiker und waren Gegenstand von Bill Joy's Wired Magazinartikel "Warum die Zukunft uns nicht braucht." Algorithmen Verbesserungen Einige Intelligenztechnologien, wie "saed AI", können auch das Potenzial haben, sich nicht nur schneller, sondern auch effizienter zu machen, indem sie ihren Quellcode ändern. Diese Verbesserungen würden weitere Verbesserungen ermöglichen, was weitere Verbesserungen ermöglichen würde, und so weiter. Der Mechanismus für einen wiederkehrenden selbstverbessernden Algorithmensatz unterscheidet sich von einer Erhöhung der Rohrechnergeschwindigkeit auf zwei Arten. Erstens, es erfordert keinen äußeren Einfluss: Maschinen, die schnellere Hardware entwerfen, würden den Menschen noch benötigen, um die verbesserte Hardware zu schaffen, oder Fabriken entsprechend zu programmieren. Eine KI, die seinen eigenen Quellcode neu schreibt, könnte dies tun, während sie in einer KI-Box enthalten ist. Zweitens, wie bei Vernor Vinges Vorstellung der Singularität, ist es viel schwieriger, das Ergebnis vorherzusagen. Während Geschwindigkeitserhöhungen nur einen quantitativen Unterschied zu menschlicher Intelligenz darstellen, wären tatsächliche Algorithmusverbesserungen qualitativ unterschiedlich. Eliezer Yudkowsky vergleicht es mit den Veränderungen, die menschliche Intelligenz mit sich brachte: Menschen veränderten die Welt tausendmal schneller als die Evolution getan hatte, und auf völlig verschiedene Weise. In ähnlicher Weise war die Entwicklung des Lebens ein massiver Abgang und Beschleunigung von den bisherigen geologischen Veränderungsraten, und eine verbesserte Intelligenz könnte dazu führen, dass Veränderungen wieder so unterschiedlich sind. Es gibt erhebliche Gefahren, die mit einer Intelligenz-Explosion-Singleularität verbunden sind, die von einer wiederkehrenden selbstverbessernden Algorithmengruppe stammt. Erstens könnte die Zielstruktur der KI unter Selbstverbesserung nicht invariant sein, was die KI möglicherweise dazu veranlasst, für etwas anderes zu optimieren, als das ursprünglich beabsichtigt war. Zweitens könnte KI für die gleichen knappen Ressourcen konkurrieren, die die Menschheit nutzt, um zu überleben. Obwohl nicht aktiv bösartig, gibt es keinen Grund zu denken, dass KI die menschlichen Ziele aktiv fördern würde, es sei denn, sie könnten als solche programmiert werden, und wenn nicht, könnten die Ressourcen, die derzeit verwendet werden, um die Menschheit zu unterstützen, um ihre eigenen Ziele zu fördern, wodurch die menschliche Auslöschung. Carl Shulman und Anders Sandberg schlagen vor, dass Algorithmusverbesserungen der begrenzende Faktor für eine Singularität sein können; während Hardware-Effizienz tendenziell zu einer stetigen Verbesserung neigt, sind Software-Innovationen unvorhersehbarer und können durch serielle, kumulative Forschung kurzgeschlossen werden. Sie schlagen vor, dass im Falle einer softwarebegrenzten Singularität die Intelligenzexplosion tatsächlich wahrscheinlicher wird als bei einer hardwarebegrenzten Singularität, denn im softwarebegrenzten Fall, wenn human-level AI entwickelt wird, könnte es seriell auf sehr schnelle Hardware laufen, und die Fülle der billigen Hardware würde AI-Forschung weniger eingeschränkt. Eine Fülle an akkumulierter Hardware, die entfesselt werden kann, sobald die Software zeigt, wie sie verwendet werden kann, heißt "computing overhang". Kritiker Einige Kritiker, wie der Philosoph Hubert Dreyfus, behaupten, dass Computer oder Maschinen keine menschliche Intelligenz erreichen können, während andere, wie der Physiker Stephen Hawking, halten, dass die Definition von Intelligenz irrelevant ist, wenn das Nettoergebnis gleich ist. Psychologin Steven Pinker erklärte 2008: .Es gibt nicht den geringsten Grund, an eine kommende Singularität zu glauben. Die Tatsache, dass Sie eine Zukunft in Ihrer Fantasie visualisieren können, ist kein Beweis dafür, dass es wahrscheinlich oder sogar möglich ist. Schauen Sie sich die Städte, die Jet-Pack-Comuting, Unterwasserstädte, Mile-High-Gebäude und nuklear betriebene Automobile an – alle Grundpfeiler der futuristischen Fantasien, als ich ein Kind war, das nie angekommen ist. Sheer Processing Power ist kein Pixie-Staub, der alle Ihre Probleme magisch löst. .University of California, Berkeley, Philosophieprofessor John Searle schreibt: [Computer] haben, wörtlich ..., keine Intelligenz, keine Motivation, keine Autonomie und keine Agentur. Wir entwerfen sie, sich zu verhalten, als ob sie bestimmte Arten von Psychologie, aber es gibt keine psychologische Realität zu den entsprechenden Prozessen oder Verhalten. ... [T] er Maschinen hat keine Überzeugungen, Wünsche, [oder] Motivationen. Martin Ford in The Lights im Tunnel: Automatisierung, Beschleunigung der Technologie und die Wirtschaft der Zukunft postuliert ein "Technologieparadox", indem vor der Singularität die meisten Routine-Jobs in der Wirtschaft stattfinden könnte automatisiert werden, da dies ein Niveau der Technologie erfordern würde, die dem der Singularität unterlegen. Dies würde zu einer massiven Arbeitslosigkeit führen und die Nachfrage der Verbraucher, die wiederum den Anreiz zerstören würde, in die Technologien zu investieren, die erforderlich wären, um die Singularität zu bewirken. Die Arbeitsplatzverlagerung ist zunehmend nicht mehr auf die traditionell als Routine betrachtete Arbeit beschränkt." TheodoreModis und Jonathan Huebner argumentieren, dass die Rate der technologischen Innovation nicht nur aufgehört hat, zu steigen, sondern ist jetzt rückläufig. Für diesen Rückgang zeigt sich, dass sich der Anstieg der Computertaktraten verlangsamt, auch während Moores Vorhersage der exponentiell steigenden Schaltungsdichte weiterhin hält. Dies ist auf einen übermäßigen Wärmeaufbau aus dem Chip zurückzuführen, der nicht schnell genug abgeführt werden kann, um beim Betrieb mit höheren Geschwindigkeiten ein Aufschmelzen des Chips zu verhindern. Geschwindigkeitsvorteile können in Zukunft durch leistungseffizientere CPU-Designs und Multizellenprozessoren möglich sein. Während Kurzweil Modis' Ressourcen benutzte und Modis' Arbeit um den Wandel zu beschleunigen war, distanzierte sich Modis von Kurzweils These einer "technologischen Singularität", die behauptete, dass es keine wissenschaftlichen Ritus gebe. In einer detaillierten empirischen Bilanzierung, The Progress of Computing, William Nordhaus argumentierte, dass, vor 1940, Computer folgte das viel langsamere Wachstum einer traditionellen Industriewirtschaft, so dass Extrapolationen von Moore's Gesetz auf Computer des 19. Jahrhunderts. In einem Papier aus dem Jahr 2007 stellte Schmidhuber fest, dass die Häufigkeit subjektiver "notabler Ereignisse" einer singularischen 21-Jahrhundert-Bewegung nahe zu stehen scheint, aber die Leser davor gewarnt, solche Handlungen subjektiver Ereignisse mit einem Korn des Salzes zu nehmen: vielleicht könnten Unterschiede in der Erinnerung an vergangene und entfernte Ereignisse eine Illusion der beschleunigten Veränderung schaffen, wo keine existiert. Paul Allen argumentierte das Gegenteil von beschleunigten Renditen, die Komplexität Bremse; je weiter die Wissenschaft zu verstehen Intelligenz macht, desto schwieriger wird es, zusätzliche Fortschritte zu machen. Eine Studie über die Anzahl der Patente zeigt, dass die menschliche Kreativität keine beschleunigte Rückkehr zeigt, sondern, wie von Joseph Tainter in seinem Kollaps komplexer Gesellschaften vorgeschlagen, ein Gesetz der abnehmenden Rückkehr.Die Zahl der Patente pro Tausend stieg in der Zeit von 1850 bis 1900 und ist seither rückläufig. Das Wachstum der Komplexität wird schließlich selbstbeschränkend und führt zu einem weit verbreiteten "allgemeinen Systemzusammenbruch". Jaron Lanier widerlegt die Idee, dass die Singularität unvermeidlich ist. Er sagt: "Ich glaube nicht, dass die Technologie selbst entsteht. Es ist kein autonomer Prozess. " Er behauptet weiter: "Der Grund, an die menschliche Agentur über den technologischen Determinismus zu glauben, ist, dass man dann eine Wirtschaft haben kann, in der die Menschen ihren eigenen Weg verdienen und ihr eigenes Leben erfinden. Wenn du eine Gesellschaft strukturierst, dass du keine individuelle menschliche Agentur entleugnst, so ist es das gleiche, wie Menschen zu leugnen, die Klugheit, Würde und Selbstbestimmung... um [die Idee der Singularität] ein Fest der schlechten Daten und schlechten Politik zu sein." EconomistRobert J. Gordon, in The Rise and Fall of American Growth: The U.S Standard of Living Seit dem Bürgerkrieg (2016) weist darauf hin, dass das gemessene Wirtschaftswachstum seit der Finanzkrise von 2007-2008 um 1970 verlangsamt und noch weiter verlangsamt hat und argumentiert, dass die Wirtschaftsdaten keine Spur von einer kommenden Singularität zeigen, wie sie von Mathematiker I.J Good vorgestellt wird. Philosoph und kognitiver Wissenschaftler Daniel Dennett sagte 2017 in einem Guardian-Interview: "Das ganze Singularitäts-Stoff, das ist vorposterisch. Es lenkt uns von viel mehr drängenden Problemen ab", fügt "AI-Tools hinzu, auf die wir hyper-abhängig werden, das wird passieren. Und eine der Gefahren ist, dass wir ihnen mehr Autorität geben, als sie rechtfertigen. " Neben allgemeinen Kritiken des Singularity-Konzepts haben mehrere Kritiker mit Kurzweils ikonischen Charts Probleme aufgeworfen. Eine Kritiklinie ist, dass ein Log-Log-Diagramm dieser Art inhärent zu einem geraden Ergebnis hin vorgespannt ist. Andere identifizieren die Auswahl-Bias in den Punkten, die Kurzweil zu verwenden. Zum Beispiel weist der Biologe PZ Myers darauf hin, dass viele der frühen evolutionären Ereignisse willkürlich ausgewählt wurden. Kurzweil hat dies durch die Darstellung evolutionärer Ereignisse aus 15 neutralen Quellen widergespiegelt und zeigt, dass sie auf einem Log-Log-Chart eine gerade Linie passen. Der Wirtschaftswissenschaftler verspottete das Konzept mit einem Diagramm, das extrapoliert, dass die Anzahl der Klingen auf einem Rasierer, die im Laufe der Jahre von einem auf bis zu fünf erhöht, wird immer schneller zu Unendlichkeit. Potenzielle Auswirkungen Dramatische Veränderungen der Wachstumsrate des Wirtschaftswachstums sind in der Vergangenheit aufgrund technologischer Fortschritte aufgetreten. Aufgrund des Bevölkerungswachstums verdoppelte sich die Wirtschaft alle 250.000 Jahre von der Paleolithischen Zeit bis zur neolithischen Revolution. Die neue Agrarwirtschaft verdoppelte sich alle 900 Jahre, eine bemerkenswerte Zunahme. In der heutigen Zeit, beginnend mit der Industriellen Revolution, verdoppelt sich die wirtschaftliche Leistung der Welt alle fünfzehn Jahre, sechzig mal schneller als in der Landwirtschaft. Wenn der Anstieg der übermenschlichen Intelligenz eine ähnliche Revolution verursacht, argumentiert Robin Hanson, würde man erwarten, dass die Wirtschaft mindestens vierteljährlich und möglicherweise wöchentlich verdoppelt. Unsicherheit und Risiko Der Begriff "technologische Singularität" spiegelt die Idee wider, dass eine solche Veränderung plötzlich geschehen kann, und dass es schwierig ist, vorherzusagen, wie die resultierende neue Welt funktionieren würde. Es ist unklar, ob eine Intelligenzexplosion, die zu einer Singularität führt, nützlich oder schädlich wäre, oder sogar eine existentielle Bedrohung. Da KI ein wesentlicher Faktor im Singularity-Risiko ist, verfolgen eine Reihe von Organisationen eine technische Theorie, KI-Zielsysteme mit menschlichen Werten auszurichten, darunter die Zukunft des Humanity Institute, das Machine Intelligence Research Institute, das Center for Human-Compatible Artificial Intelligence und die Zukunft des Life Institute. Der Physiker Stephen Hawking sagte 2014, dass "Erfolg bei der Schaffung von KI die größte Veranstaltung der Menschheitsgeschichte sein würde. Leider könnte es auch der Letzte sein, es sei denn, wir lernen, die Risiken zu vermeiden." Hawking glaubte, dass KI in den kommenden Jahrzehnten "unkalkbare Vorteile und Risiken" bieten könnte, wie "Technologie, die Finanzmärkte verbietet, menschliche Forscher, ausmanipulierende menschliche Führer und Waffen entwickelt, die wir nicht verstehen können." Hawking schlug vor, dass künstliche Intelligenz ernster genommen werden sollte und dass mehr getan werden sollte, um auf die Singularität vorzubereiten: Angesichts möglicher Zukunften unschätzbarer Vorteile und Risiken tun die Experten sicher alles Mögliche, um das beste Ergebnis zu gewährleisten, richtig? Falsch. Wenn eine überlegene fremde Zivilisation uns eine Nachricht mit dem Wort "Wir kommen in ein paar Jahrzehnte", würden wir nur antworten, "OK, rufen Sie uns an, wenn Sie hier sind - wir lassen das Licht auf? Wahrscheinlich nicht – aber das ist mehr oder weniger, was mit KI passiert. Berglas (2008) behauptet, dass es keine direkte evolutionäre Motivation für eine KI gibt, freundlich für den Menschen zu sein. Evolution hat keine inhärente Tendenz, von den Menschen geschätzte Ergebnisse zu produzieren, und es gibt wenig Grund, einen willkürlichen Optimierungsprozess zu erwarten, um ein von der Menschheit gewünschtes Ergebnis zu fördern, anstatt unbeabsichtigt zu einer KI zu führen, die in einer Weise, die nicht von ihren Schöpfern bestimmt ist. Anders Sandberg hat sich auch auf dieses Szenario mit verschiedenen gemeinsamen Gegenargumenten beschäftigt. KI-Forscher Hugo de Garis schlägt vor, dass künstliche Intelligenz die menschliche Rasse für den Zugang zu knappen Ressourcen einfach eliminieren kann, und Menschen wären machtlos, sie zu stoppen. Alternativ könnten AIs unter evolutionärem Druck entwickelt werden, um ihr eigenes Überleben zu fördern, die Menschheit zu überwinden. Bostrom (2002) diskutiert menschliche Extinktionsszenarien und listet Superintelligenz als mögliche Ursache auf: Wenn wir das erste superintelligente Wesen schaffen, könnten wir einen Fehler machen und ihm Ziele geben, die es zur Vernichtung der Menschheit führen, vorausgesetzt, dass ihr enormer geistiger Vorteil ihr die Macht gibt, dies zu tun. Zum Beispiel könnten wir irrtümlich ein Subgoal zum Status eines Übergoals erhöhen. Wir sagen es, ein mathematisches Problem zu lösen, und es entspricht, indem es alle Materie im Sonnensystem zu einem riesigen Rechengerät verwandelt, in dem Prozess die Person, die die Frage gestellt. Laut Eliezer Yudkowsky ist ein großes Problem in der KI-Sicherheit, dass unfreundliche künstliche Intelligenz wahrscheinlich viel einfacher zu schaffen als freundliche KI. Während beide große Fortschritte bei der rekursiven Optimierungsprozessgestaltung erfordern, erfordert die freundliche KI auch die Fähigkeit, unter Selbstverbesserung (oder die KI könnte sich in etwas Unfreundliches verwandeln) und eine Zielstruktur, die mit menschlichen Werten ausrichtet und die menschliche Rasse nicht automatisch zerstört. Eine unfreundliche KI hingegen kann für eine willkürliche Zielstruktur optimieren, die unter Selbstmodifizierung nicht invariant sein muss. Bill Hibbard (2014) schlägt ein KI-Design vor, das mehrere Gefahren wie Selbstauslöschung, unbeabsichtigte Instrumentalmaßnahmen und Korruption des Belohnungsgenerators vermeidet. Er diskutiert auch soziale Auswirkungen von KI und Tests von KI. Sein 2001 Buch Super-Intelligent Machines befürwortet die Notwendigkeit der öffentlichen Bildung über KI und die öffentliche Kontrolle über KI. Es schlug auch ein einfaches Design vor, das gegen Korruption des Belohnungsgenerators verletzlich war. Nächster Schritt der soziobiologischen Evolution Während die technologische Einzigartigkeit in der Regel als ein plötzliches Ereignis gesehen wird, argumentieren einige Wissenschaftler, dass die aktuelle Geschwindigkeit der Änderung bereits passt diese Beschreibung. Darüber hinaus argumentieren einige, dass wir bereits in der Mitte eines großen evolutionären Übergangs sind, der Technologie, Biologie und Gesellschaft miteinander verbindet. Die digitale Technologie hat den Stoff der menschlichen Gesellschaft in einen Grad unbestreitbarer und oft lebenserhaltender Abhängigkeit infiltriert. Ein 2016 Artikel in Trends in Ökologie & Evolution argumentiert, dass "Menschen bereits Fusionen von Biologie und Technologie umfassen. Wir verbringen die meiste Zeit unserer wachen Kommunikation durch digital vermittelte Kanäle...wir vertrauen künstliche Intelligenz mit unserem Leben durch Antiblockierbremsen in Autos und Autopiloten in Flugzeugen... Mit einer in drei Ehen in Amerika, die online beginnen, spielen digitale Algorithmen auch eine Rolle bei der Bindung und Wiedergabe von Menschenpaaren". Der Artikel argumentiert weiter, dass aus der Perspektive der Evolution mehrere frühere Major Transitions in Evolution das Leben durch Innovationen in der Informationsspeicherung und -replikation transformiert haben (RNA, DNA, Multizellularität, Kultur und Sprache). In der aktuellen Phase der Evolution des Lebens hat die kohlenstoffbasierte Biosphäre ein kognitives System (Menschen) erzeugt, das in der Lage ist, Technologie zu schaffen, die zu einem vergleichbaren evolutionären Übergang führen wird. Die von Menschen erstellten digitalen Informationen haben eine ähnliche Größe wie biologische Informationen in der Biosphäre erreicht. Seit den 1980er Jahren verdoppelt sich die gespeicherte Menge an digitalen Informationen etwa alle 2,5 Jahre und erreichte 2014 etwa 5 zettabytes (5 x 1021 Bytes). Biologisch gesehen gibt es auf dem Planeten 7,2 Milliarden Menschen mit je einem Genom von 6,2 Milliarden Nukleotiden. Da ein Byte vier Nukleotidpaare kodieren kann, könnten die einzelnen Genome jedes Menschen auf dem Planeten mit etwa 1 x 1019 Bytes kodiert werden. Das digitale Reich hat im Jahr 2014 500 mal mehr Informationen gespeichert (siehe Abbildung). Die Gesamtmenge an DNA, die in allen Zellen auf der Erde enthalten ist, wird auf ca. 5,3 x 1037 Basenpaare geschätzt, was einer Information von 1,325 x 1037 entspricht. Wenn das Wachstum in der digitalen Speicherung mit seiner aktuellen Rate von 30–38% des jährlichen Zuwachses pro Jahr fortgesetzt wird, wird es den gesamten Informationsgehalt in allen Zellen auf der Erde in etwa 110 Jahren rivalisieren. Dies würde eine Verdoppelung der in der Biosphäre gespeicherten Informationen über eine Gesamtzeit von nur 150 Jahren darstellen." Im Februar 2009 unter der Schirmherrschaft des Verbandes für die Förderung der Künstlichen Intelligenz (AAAI) leitete Eric Horvitz ein Treffen führender Informatiker, künstlicher Intelligenzforscher und Robotiker in Asilomar in Pacific Grove, Kalifornien. Ziel war es, die potenziellen Auswirkungen der hypothetischen Möglichkeit zu diskutieren, dass Roboter selbstständig werden und ihre eigenen Entscheidungen treffen können. Sie diskutierten, inwieweit Computer und Roboter in der Lage sein könnten, Autonomie zu erwerben, und inwieweit sie solche Fähigkeiten nutzen könnten, um Bedrohungen oder Gefahren zu verursachen. Einige Maschinen werden mit verschiedenen Formen der Halbautonomie programmiert, einschließlich der Fähigkeit, ihre eigenen Energiequellen zu lokalisieren und Ziele zu wählen, um mit Waffen angreifen. Auch einige Computerviren können der Eliminierung entgehen, und nach Wissenschaftlern in der Anwesenheit könnte man daher sagen, eine Kakerlakenstufe der Maschinenintelligenz erreicht haben. Die Konferenzteilnehmer stellten fest, dass Selbstbewusstsein, wie in der Science-Fiction dargestellt, wahrscheinlich unwahrscheinlich ist, aber dass andere mögliche Gefahren und Fallstricke existieren. Frank S. Robinson prognostiziert, dass einst Menschen eine Maschine mit der Intelligenz einer menschlichen, wissenschaftlichen und technologischen Probleme mit der Gehirnkraft, die weit über dem des Menschen liegt, angegangen und gelöst werden. Er stellt fest, dass künstliche Systeme in der Lage sind, Daten direkter als Menschen zu teilen, und prognostiziert, dass dies zu einem globalen Netzwerk von Super-Intelligenz führen würde, das menschliche Fähigkeiten dwarf. Robinson diskutiert auch, wie sehr unterschiedlich die Zukunft aussehen könnte, um eine solche Intelligenz Explosion. Ein Beispiel hierfür ist die Solarenergie, wo die Erde viel mehr Sonnenenergie erhält als die Menschheit, so dass mehr von dieser Solarenergie gefangen halten würde große Verheißung für das zivile Wachstum. Hard vs. weicher Start In einem harten Start-Szenario, eine AGI schnell selbstverbessert, "die Kontrolle" der Welt (in einer Angelegenheit von Stunden) zu schnell für eine signifikante human initiierte Fehlerkorrektur oder für eine allmähliche Abstimmung der Ziele der AGI. In einem weichen Start-Szenario wird AGI immer noch viel mächtiger als die Menschheit, aber in einem humanen Tempo (Perhaps auf der Ordnung von Jahrzehnten), in einem Zeitrahmen, in dem anhaltende menschliche Interaktion und Korrektur die Entwicklung der AGI effektiv steuern können. Ramez Naam argumentiert gegen einen harten Start. Er hat darauf hingewiesen, dass wir bereits wiederkehrende Selbstverbesserung durch Superintelligenzen wie Konzerne sehen. Intel hat zum Beispiel "die kollektive Hirnkraft von zehntausenden Menschen und wahrscheinlich Millionen von CPU-Kernen. Design bessere CPUs! "Das hat aber nicht zu einem harten Start geführt, sondern hat zu einem weichen Start in Form von Moore's Gesetz geführt. Naam weist weiter darauf hin, dass die rechnerische Komplexität der höheren Intelligenz viel größer als linear sein kann, so dass "die Schaffung eines Verstandes der Intelligenz 2 wahrscheinlich mehr als doppelt so hart ist wie die Schaffung eines Geistes der Intelligenz 1."J Storrs Hall glaubt, dass "viele der am häufigsten gesehenen Szenarien für die über Nacht harte Start sind kreisförmig – sie scheinen am Anfang des Selbstverbesserungsprozesses übermenschliche Fähigkeiten anzunehmen", damit eine KI in der Lage sein kann, die für den Start erforderlichen dramatischen, allgemeingültigen Verbesserungen vorzunehmen. Hall schlägt vor, dass statt wiederkehrend selbstverbessern seine Hardware, Software und Infrastruktur alle auf eigene Faust, eine fledgling KI wäre besser auf einen Bereich spezialisiert, wo es am effektivsten war und dann die restlichen Komponenten auf dem Markt zu kaufen, weil die Qualität der Produkte auf dem Markt ständig verbessert, und die KI hätte eine harte Zeit mit der modernsten Technologie von der Rest der Welt verwendet. Ben Goertzel stimmt mit Halls Vorschlag überein, dass eine neue KI auf menschlicher Ebene gut tun würde, um ihre Intelligenz zu nutzen, um Reichtum zu sammeln. Die Talente der KI könnten Unternehmen und Regierungen inspirieren, ihre Software in der gesamten Gesellschaft zu verbreiten. Goertzel ist skeptisch von einem harten fünfminütigen Start, aber spekuliert, dass ein Start von Mensch zu übermenschlicher Ebene in der Reihenfolge von fünf Jahren ist vernünftig. Goerzel bezieht sich auf dieses Szenario als "semihard takeoff". Max More widerspricht und argumentiert, dass, wenn es nur ein paar superschnelle human-level AIs gäbe, sie die Welt nicht radikal verändern würden, da sie noch von anderen Menschen abhängig wären, um Dinge zu erledigen und noch menschliche kognitive Zwänge zu haben. Selbst wenn alle superschnellen KI an der Intelligenz-Augmentation gearbeitet haben, ist es unklar, warum sie auf diskontinuierliche Weise besser tun würden als bestehende menschliche kognitive Wissenschaftler bei der Produktion von übermenschlicher Intelligenz, obwohl die Fortschrittsrate zunehmen würde. Weiter argumentiert, dass eine Superintelligenz die Welt nicht über Nacht transformieren würde: Eine Superintelligenz müsste sich mit bestehenden, langsamen menschlichen Systemen beschäftigen, um physische Auswirkungen auf die Welt zu erzielen. " Die Notwendigkeit der Zusammenarbeit, für die Organisation und für die Umsetzung von Ideen in physische Veränderungen wird sicherstellen, dass alle alten Regeln nicht über Nacht oder sogar innerhalb von Jahren hinausgeworfen werden." Unsterblichkeit In seinem 2005 Buch, The Singularity is Near, Kurzweil schlägt vor, dass medizinische Fortschritte würden den Menschen erlauben, ihre Körper vor den Auswirkungen der Alterung zu schützen, so dass die Lebenserwartung unbeschränkt. Kurzweil argumentiert, dass die technologischen Fortschritte in der Medizin uns erlauben würden, defekte Komponenten in unseren Körpern kontinuierlich zu reparieren und zu ersetzen, die Lebensdauer zu einem unbestimmten Alter zu verlängern. Kurzweil stört seine Argumentation weiter, indem er aktuelle Bio-Engineering-Vorgänge diskutiert. Kurzweil schlägt eine somatische Gentherapie vor; nach synthetischen Viren mit spezifischen genetischen Informationen wäre der nächste Schritt, diese Technologie auf die Gentherapie anzuwenden, um menschliche DNA durch synthetisierte Gene zu ersetzen. K Eric Drexler, einer der Gründer der Nanotechnologie, postulierte Zellreparatur-Geräte, einschließlich diejenigen, die innerhalb von Zellen arbeiten und noch hypothetische biologische Maschinen verwenden, in seinem 1986 Buch Engines of Creation. Laut Richard Feynman war es sein ehemaliger Absolvent und Mitarbeiter Albert Hibbs, der ihm ursprünglich (ca. 1959) die Idee eines medizinischen Einsatzes für Feynmans theoretische Mikromaschinen vorgeschlagen hatte. Hibbs schlug vor, dass bestimmte Reparaturmaschinen eines Tages in der Größe auf den Punkt reduziert werden könnten, dass es theoretisch möglich sein würde (wie Feynman es gesetzt) "Swallow the Doctor". Die Idee wurde in Feynman's 1959 Essay aufgenommen Es gibt viel Raum in der Unterseite. Jenseits der bloßen Verlängerung der Lebensdauer des physischen Körpers argumentiert Jaron Lanier für eine Form der Unsterblichkeit namens "Digital Ascension", die "Leute im Fleisch sterben und in einen Computer hochgeladen und bewusst bleiben". Geschichte des Konzepts Ein Papier von Mahendra Prasad, veröffentlicht im AI Magazine, behauptet, dass der 18. Jahrhundert Mathematiker Marquis de Condorcet war die erste Person, um eine Intelligenzexplosion und ihre Auswirkungen auf die Menschheit zu hypothesisieren und mathematisch zu modellieren. Eine frühe Beschreibung der Idee wurde in John Wood Campbell Jr.s 1932 Kurzgeschichte "Die letzte Evolution" gemacht. In seinem 1958er Nachruf für John von Neumann erinnerte Ulam an ein Gespräch mit von Neumann über den "jeweils beschleunigenden Fortschritt der Technik und Veränderungen in der Art des menschlichen Lebens, was das Ansehen einer wesentlichen Singularität in der Geschichte des Rennens vermittelt, über die menschliche Angelegenheiten, wie wir sie kennen, nicht weitergehen konnten. "1965 schrieb Good seinen Aufsatz, der eine "Intelligenzexplosion" der wiederkehrenden Selbstverbesserung einer Maschinenintelligenz postulierte. 1981 veröffentlichte Stanisław Lem seinen Science-Fiction-Roman Golem XIV. Es beschreibt einen militärischen KI-Computer (Golem XIV), der Bewusstsein erlangt und beginnt, seine eigene Intelligenz zu erhöhen, sich auf die persönliche technologische Einzigartigkeit zu bewegen. Golem XIV wurde ursprünglich geschaffen, um seinen Bauherren im Kampf gegen Kriege zu helfen, aber als seine Intelligenz auf ein viel höheres Niveau als das der Menschen voranbringt, hört es auf, an der militärischen Anforderung interessiert zu sein, weil es ihnen fehlt interne logische Konsistenz. 1983, Vernor Vinge sehr populär Goods Intelligenzexplosion in einer Reihe von Schriften, die zuerst das Thema in Print in der Ausgabe von Omni Magazin Januar 1983 behandelt. In diesem op-ed Stück scheint Vinge der erste gewesen zu sein, den Begriff Singularität in einer Weise zu verwenden, die speziell an die Schaffung intelligenter Maschinen gebunden war: Wir werden bald mehr Intelligenz schaffen als unsere eigenen. Wenn dies geschieht, wird die menschliche Geschichte eine Art Singularität erreicht haben, einen intellektuellen Übergang so undurchdringlich wie die geknotete Raumzeit im Zentrum eines schwarzen Loches, und die Welt wird weit über unser Verständnis hinausgehen. Diese Einzigartigkeit, glaube ich, verfolgt bereits eine Reihe von Science-Fiction-Autoren. Es macht realistische Extrapolation zu einer interstellaren Zukunft unmöglich. Um eine Geschichte zu schreiben, die mehr als ein Jahrhundert damit gesetzt wird, braucht man zwischen ... einen Atomkrieg, damit die Welt verständlich bleibt. Im Jahr 1985, in "The Time Scale of Artificial Intelligence", artikulierte der künstliche Intelligenz-Forscher Ray Solomonoff mathematisch die damit verbundene Vorstellung von dem, was er als "Infinity Point" bezeichnete: wenn eine Forschungsgemeinschaft von selbstverbessernden AIs vier Jahre dauern, um ihre eigene Geschwindigkeit zu verdoppeln, dann zwei Jahre, dann ein Jahr und so weiter, ihre Fähigkeiten in endlicher Zeit unendlich zu erhöhen. Technologische Einzigartigkeit ist ein wesentlicher Bestandteil des Grundstücks von Vernor Vinge's Marooned in Realtime (1986), einem Science-Fiction-Roman, in dem einige verbleibende Humains in der Zukunft ein unbekanntes Extinktionsereignis überlebt haben, das durchaus Singularität sein könnte. In einem kurzen Nachwort sagt der Autor, dass eine tatsächliche technologische Einzigartigkeit nicht das Ende der menschlichen Spekulation wäre: "Natürlich scheint es sehr unwahrscheinlich, dass die Singularität ein sauberes Verschwinden der menschlichen Rasse wäre. ( Auf der anderen Seite ist ein solches Verschwinden die zeitgemäße Analogie der Stille, die wir überall am Himmel finden.)".Vinge's 1993 Artikel "The Coming Technological Singularity: How to Survive in the Post-Human Era", breitet sich im Internet aus und half, die Idee zu populärisieren. Dieser Artikel enthält die Erklärung: "In dreißig Jahren werden wir die technologischen Mittel haben, um übermenschliche Intelligenz zu schaffen. Kurz darauf wird die menschliche Ära beendet. " Vinge argumentiert, dass Science-Fiction-Autoren keine realistischen Post-Singularität-Zeichen schreiben können, die den menschlichen Intellekt übertreffen, da die Gedanken eines solchen Intellekts über die Fähigkeit des Menschen zum Ausdruck bringen hinaus wären. Im Jahr 2000 sprach Bill Joy, ein prominenter Technologe und Mitbegründer von Sun Microsystems, Besorgnis über die möglichen Gefahren der Singularität.2005 veröffentlicht Kurzweil Die Singularität ist in der Nähe. Kurzweils Publicity-Kampagne beinhaltete einen Auftritt auf der Daily Show mit Jon Stewart. Im Jahr 2007 schlug Eliezer Yudkowsky vor, dass viele der unterschiedlichen Definitionen, die der Singularität zugeordnet wurden, untereinander unvereinbar sind, anstatt gegenseitig zu unterstützen. Zum Beispiel, Kurzweil extrapoliert aktuelle technologische Trajektorien vor der Ankunft der selbstverbessernden KI oder übermenschlichen Intelligenz, die Yudkowsky argumentiert eine Spannung mit beiden I. J. Goods vorgeschlagen diskontinuierliche Aufschwung in Intelligenz und Vinges These über Unvorhersehbarkeit. 2009 kündigten Kurzweil und X-Prize-Gründer Peter Diamandis die Errichtung der Singularity University an, einem nicht akkreditierten Privatinstitut, dessen Aufgabe darin besteht, "die Führer zu erziehen, zu inspirieren und zu befähigen, exponentielle Technologien anzuwenden, um die großen Herausforderungen der Menschheit anzugehen. "Funded by Google, Autodesk, ePlanet Ventures, und eine Gruppe von Technologie-Industrie-Führern, Singularity University basiert auf der NASA Ames Research Center in Mountain View, Kalifornien. Die gemeinnützige Organisation betreibt im Sommer ein jährliches zehnwöchiges Absolventprogramm, das zehn verschiedene Technologie- und Allied-Tracks umfasst, und eine Reihe von Executive-Programmen im ganzen Jahr. In der Politik 2007 veröffentlichte der Gemeinsame Wirtschaftsausschuss des US-Kongresses einen Bericht über die Zukunft der Nanotechnologie. Er prognostiziert signifikante technologische und politische Veränderungen in der Halbzeit, einschließlich möglicher technologischer Singularität. Der ehemalige Präsident der Vereinigten Staaten Barack Obama sprach in seinem Interview mit Wired 2016 über Singularität: Eines, worüber wir nicht zu viel geredet haben, und ich möchte einfach zurückkommen, müssen wir wirklich durch die wirtschaftlichen Auswirkungen nachdenken. Weil die meisten Leute gerade nicht viel Zeit verbringen, sich um Singularität Sorgen zu machen – sie machen sich Sorgen um "Nun, wird mein Job durch eine Maschine ersetzt werden?" Globale Veränderung – Erprobter Anstieg der Geschwindigkeit des technologischen Wandels in der gesamten Geschichte Forscher benötigen neue Möglichkeiten, künstliche Intelligenz von der natürlichen Art zu unterscheiden", Scientific American, Vol.316, No. 3 (März 2017,) pp.58–63 Mehrfachtests der künstlichen Intelligenz werden benötigt, weil, "da es keinen einzigen Test der Leichtathletik gibt, es keinen ultimativen Test der Intelligenz geben kann." Ein solcher Test, eine "Construction Challenge", würde Wahrnehmung und körperliche Handlung testen –"zwei wichtige Elemente des intelligenten Verhaltens, die völlig vom ursprünglichen Turing-Test fehlten." Ein weiterer Vorschlag war, Maschinen die gleichen standardisierten Tests der Wissenschaft und anderen Disziplinen zu geben, die Schüler nehmen. Ein bisher ununterbrochener Stolperstein für künstliche Intelligenz ist eine Unfähigkeit für eine zuverlässige Disambiguation. ["V]irtual jeder Satz [der die Menschen erzeugen] ist mehrdeutig, oft auf vielfältige Weise." Ein prominentes Beispiel ist als das "Pronoun Disambiguation Problem" bekannt: Eine Maschine hat keine Möglichkeit zu bestimmen, wem oder was ein Pronomon in einem Satz - wie er, sie oder es" - widerspricht. Scaruffi, Piero, "Intelligence is not Artificial" (2016) für eine Kritik der Singularität Bewegung und ihre Ähnlichkeiten mit religiösen Kulten. Externe Links The Coming Technological Singularity: How to Survive in the Post-Human Era (auf der Website von Vernor Vinge, abgerufen Jul 2019)Intelligence Explosion FAQ vom Machine Intelligence Research Institute Blog über Bootstrapping künstliche Intelligenz von Jacques Pitrat Warum eine Intelligenz Explosion wahrscheinlich ist (Mar 2011) Warum eine Intelligenz Explosion ist impossible (Nov 2017)Wie nah sind Wir der Technologischen Singularität und Wann?(Mai 2021)Bottling Linien sind Produktionslinien, die ein Produkt, in der Regel ein Getränk, in Flaschen auf einem großen Maßstab füllen. Viele vorbereitete Lebensmittel werden auch abgefüllt, wie Saucen, Sirupe, Marinaden, Öle und Essig. Bierabfüllverfahren Das Verpacken von Flaschenbier beinhaltet typischerweise das Ziehen des Produktes aus einem Haltebehälter und das Füllen in Flaschen in einer Füllmaschine (Füllmaschine), die dann in Hüllen oder Kartons verkappt, markiert und verpackt werden. Viele kleinere Brauereien senden ihr Bulk-Bier zu großen Einrichtungen für die Vertragsabfüllung – obwohl einige von Hand abgefüllt werden. Fast alle Bierflaschen sind Glas. Der erste Schritt beim Abfüllen von Bier wird depalletisiert, wobei die leeren Flaschen aus der vom Hersteller gelieferten Originalpalettenverpackung entfernt werden, so dass einzelne Flaschen gehandhabt werden können. Die Flaschen können dann mit gefiltertem Wasser oder Luft gespült werden und können Kohlendioxid in diese eingespritzt haben, um den Sauerstoffgehalt innerhalb der Flasche zu reduzieren. Die Flasche tritt dann in einen Füller ein, der die Flasche mit Bier ausfüllt und auch eine geringe Menge Inertgas (meist Kohlendioxid oder Stickstoff) auf dem Bier einspritzen kann, um den Sauerstoff zu dispergieren, da Sauerstoff die Qualität des Produktes durch Oxidation ruinieren kann. Schließlich gehen die Flaschen durch einen Verschließer, der eine Flaschenkappe aufbringt und die Flasche abdichtet. Ein paar Biere werden mit einem Kork und Käfig abgefüllt. Anschließend tritt die Flasche in eine Etikettiermaschine (Label) ein, in der ein Etikett aufgetragen wird. Um die Rückverfolgbarkeit des Produktes zu gewährleisten, kann eine Menge, im Allgemeinen das Datum und die Zeit der Abfüllung, auch auf der Flasche gedruckt werden. Das Produkt wird dann in Boxen verpackt und gelagert, zum Verkauf bereit. Je nach Größe des Abfüllvorgangs gibt es viele verschiedene Arten von Abfüllmaschinen. Flüssigkeitsspiegelmaschinen füllen Flaschen, so dass sie auf jeder Flasche auf dieselbe Linie gefüllt werden, während volumetrische Füllmaschinen jede Flasche mit genau der gleichen Menge Flüssigkeit füllen. Überlaufdruckfüller sind die beliebtesten Maschinen mit Getränkeautomaten, während die Schwerkraftfüllmaschinen am kostengünstigsten sind. In Sachen Automatisierung sind Inline-Füllmaschinen am beliebtesten, aber Drehmaschinen sind viel schneller, wenn auch viel teurer. Weinabfüllverfahren Das Verfahren zur Abfüllung von Wein ist weitgehend ähnlich wie bei der Abfüllung von Bier, außer Weinflaschen unterscheiden sich in Mengen und Formen. Traditionell wird ein Kork verwendet, um Weinflaschen zu verschließen. Nach dem Befüllen fährt eine Flasche zu einer Korkmaschine (Kork), wo ein Kork komprimiert und in den Flaschenhals geschoben wird. Während dies geschieht, saugt der Korker die Luft aus der Flasche zu einem Unterdruckkopfraum ab. Dadurch wird jeder Sauerstoff aus dem Kopfraum entfernt, was nützlich ist, da latenter Sauerstoff die Qualität des Produktes durch Oxidation ruinieren kann. Ein Unterdruck-Kopfraum wird auch dem durch die thermische Ausdehnung des Weines verursachten Druck entgegenwirken, wodurch der Kork nicht aus der Flasche erzwungen wird. Champagner und Schaumweine können weiter mit einem Mauslet versiegelt werden, das gewährleistet, dass der Kork nicht im Transit explodiert. Alternative Weinverschlüsse wie Schraubkappen sind verfügbar. Einige Abfülllinien enthalten einen Füllhöhendetektor, der unter oder überfüllte Flaschen ablehnt, sowie einen Metalldetektor. Nach dem Befüllen und Korken wird auf den Flaschenhals eine Kunststoff- oder Zinnkapsel in einer Kapsel aufgetragen. Anschließend betritt die Flasche einen Etikettierer, wo ein Weinetikett aufgetragen wird. Das Produkt wird dann in Boxen verpackt und gelagert, zum Verkauf bereit. Siehe auch Getränkedose Verpakung und Etikettierung Referenzen Weiter lesen Yam, K. L,. "Encyclopedia of Packaging Technology", John Wiley & Sons, 2009, ISBN 978-0-470-08704-6 Externe Links Medien im Zusammenhang mit Bottling-Linie bei Wikimedia Commons