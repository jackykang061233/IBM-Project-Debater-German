Dimensionsreduktion oder Dimensionsreduktion ist die Transformation von Daten aus einem hochdimensionalen Raum in einen niederdimensionalen Raum, so dass die niederdimensionale Darstellung einige aussagekräftige Eigenschaften der ursprünglichen Daten, ideal nahe ihrer intrinsischen Dimension, behält. Die Arbeit in hochdimensionalen Räumen kann aus vielen Gründen unerwünscht sein; Rohdaten sind aufgrund des Flusens der Dimensionalität oft spärlich und die Auswertung der Daten ist in der Regel rechnerisch unbrauchbar. Die Dimensionsreduktion ist in Bereichen üblich, die mit einer großen Anzahl von Beobachtungen und/oder einer großen Anzahl von Variablen wie Signalverarbeitung, Spracherkennung, Neuroinformatik und Bioinformatik umgehen. Methoden werden häufig in lineare und nichtlineare Ansätze unterteilt. Ansätze können auch in Feature-Auswahl und Feature-Extraktion unterteilt werden. Die Dimensionsreduktion kann zur Geräuschreduktion, Datenvisualisierung, Clusteranalyse oder als Zwischenschritt zur Erleichterung anderer Analysen verwendet werden. Feature-Auswahl-Ansätze versuchen, eine Teilmenge der Eingabevariablen zu finden (auch Features oder Attribute genannt). Die drei Strategien sind: die Filterstrategie (z.B. Informationsgewinn), die Wrapper-Strategie (z.B. Suche nach Genauigkeit) und die Embedded-Strategie (ausgewählte Funktionen addieren oder werden beim Aufbau des Modells auf Basis von Vorhersagefehlern entfernt). Datenanalysen wie Regression oder Klassifizierung können im reduzierten Raum genauer als im ursprünglichen Raum durchgeführt werden. Feature-Projektion Feature-Projektion (auch Feature-Extraktion genannt) transformiert die Daten aus dem hochdimensionalen Raum zu einem Raum von weniger Dimensionen. Die Datentransformation kann linear sein, wie bei der Hauptkomponentenanalyse (PCA), aber auch viele nichtlineare Dimensionalitätsreduktionstechniken existieren. Für mehrdimensionale Daten kann die Tensordarstellung in der Dimensionsreduktion durch multilineares Subraumlernen genutzt werden. Hauptkomponentenanalyse (PCA) Die Hauptlineartechnik zur Dimensionsreduktion, Hauptkomponentenanalyse, führt eine lineare Zuordnung der Daten zu einem tieferen Raum durch, so dass die Varianz der Daten in der niederdimensionalen Darstellung maximiert wird. In der Praxis wird die Kovarianz (und manchmal die Korrelation)-Matrix der Daten aufgebaut und die Eigenvektoren auf dieser Matrix berechnet. Die Eigenvektoren, die den größten Eigenwerten (die Hauptkomponenten) entsprechen, können nun dazu verwendet werden, einen großen Bruchteil der Varianz der ursprünglichen Daten zu rekonstruieren. Darüber hinaus können die ersten wenigen Eigenvektoren oft in Bezug auf das großflächige physikalische Verhalten des Systems interpretiert werden, da sie oft die überwiegende Mehrheit der Energie des Systems, insbesondere in niederdimensionalen Systemen, beisteuern. Dennoch muss dies im Einzelfall nachgewiesen werden, da nicht alle Systeme dieses Verhalten zeigen. Der ursprüngliche Raum (mit Dimension der Anzahl der Punkte) wurde reduziert (mit Datenverlust, aber hoffentlich die wichtigste Varianz halten) auf den Raum von einigen Eigenvektoren überspannt. Nicht-negative Matrix Factorization (NMF) NMF zersetzt eine nicht-negative Matrix auf das Produkt von zwei nicht-negativen, die ein vielversprechendes Werkzeug in Bereichen, in denen nur nicht-negative Signale existieren, wie Astronomie. NMF ist bekannt, da die multiplikative Update-Regel von Lee & Seung, die kontinuierlich entwickelt wurde: die Einbeziehung von Unsicherheiten, die Berücksichtigung fehlender Daten und parallele Berechnung, sequentielle Konstruktion, die zur Stabilität und Linearität von NMF führt, sowie andere Updates einschließlich der Verarbeitung fehlender Daten in der digitalen Bildverarbeitung. Bei einer stabilen Bauteilbasis während der Konstruktion und einem linearen Modellierungsprozess kann sequentielle NMF den Fluss bei der direkten Abbildung von umstellaren Strukturen in Astromony bewahren, als eine der Methoden der Erkennung von Exoplaneten, insbesondere zur direkten Abbildung von umstellaren Scheiben. Im Vergleich zu PCA entfernt NMF nicht das Mittel der Matrizen, was zu unphysischen nicht-negativen Flußmitteln führt; daher kann NMF mehr Informationen als PCA erhalten, wie Ren et al. Kernel PCA Die Hauptkomponentenanalyse kann mittels des Kerneltricks nichtlinear eingesetzt werden. Die resultierende Technik ist in der Lage, nichtlineare Mappings zu erstellen, die die Varianz in den Daten maximieren. Die resultierende Technik wird Kernel PCA genannt. Graphischer Kernel PCA Andere prominente nichtlineare Techniken umfassen vielfältige Lerntechniken wie Isomap, lokal lineare Einbettung (LLE,) Hessian LLE, Laplacian eigenmaps, und Methoden basierend auf tangent Raumanalyse. Diese Techniken konstruieren eine niederdimensionale Datendarstellung mit einer Kostenfunktion, die lokale Eigenschaften der Daten behält und als Definition eines graphischen Kernels für Kernel PCA angesehen werden kann. In jüngster Zeit wurden Techniken vorgeschlagen, die anstelle der Definition eines festen Kernels versuchen, den Kernel mit der Halbdefinit-Programmierung zu lernen. Das prominenteste Beispiel einer solchen Technik ist die maximale Varianz Entfaltung (MVU). Die zentrale Idee von MVU besteht darin, alle paarweisen Abstände zwischen nächstgelegenen Nachbarn (im inneren Produktraum) genau zu erhalten, während die Abstände zwischen Punkten maximiert werden, die nicht nächstgelegenen Nachbarn sind. Ein alternativer Ansatz zur Nachbarschaftskonservierung ist durch die Minimierung einer Kostenfunktion, die Unterschiede zwischen den Abständen in den Eingabe- und Ausgaberäumen misst. Wichtige Beispiele solcher Techniken sind: klassische multidimensionale Skalierung, die mit PCA identisch ist; Isomap, die geodätische Entfernungen im Datenraum verwendet; Diffusionskarten, die Diffusionsdistanzen im Datenraum verwenden; t-distributed stochastische Nachbarinbettung (t-SNE), die die Divergenz zwischen Verteilungen über Paare von Punkten minimiert; und curvilineare Komponentenanalyse. Eine andere Herangehensweise an die nichtlineare Dimensionsreduktion erfolgt durch den Einsatz von Autoencodern, einer speziellen Art von zukunftsweisenden neuronalen Netzwerken mit einer flaschenhalsverdeckten Schicht. Das Training von Tiefencodierern wird typischerweise mit einem gierigen schichtweisen Vortraining (z.B. mit einem Stapel von eingeschränkten Boltzmann-Maschinen) durchgeführt, dem eine auf Rückverbreitung basierende Feinabstimmungsstufe folgt. Lineare diskriminierende Analyse (LDA) Lineare diskriminierende Analyse (LDA) ist eine Verallgemeinerung von Fishers linearem Diskriminant, ein Verfahren, das in Statistiken, Mustererkennung und maschinellem Lernen verwendet wird, um eine lineare Kombination von Merkmalen zu finden, die zwei oder mehr Klassen von Objekten oder Ereignissen charakterisieren oder trennen. Generalisierte diskriminierende Analyse (GDA) GDA beschäftigt sich mit nichtlinearer diskriminierender Analyse mittels Kernelfunktionsoperator. Die zugrunde liegende Theorie liegt nahe an den Stützvektormaschinen (SVM), soweit das GDA-Verfahren eine Abbildung der Eingangsvektoren in einen hochdimensionalen Merkmalsraum liefert. Ähnlich wie bei LDA besteht das Ziel von GDA darin, eine Projektion für die Merkmale in einen geringeren Raum zu finden, indem das Verhältnis zwischen Klassenstreuung und innerhalb der Klassenstreuung maximiert wird. Autoencoder Autoencoder können verwendet werden, um nichtlineare Dimensionsreduktionsfunktionen und Codierungen zusammen mit einer inversen Funktion von der Codierung zur ursprünglichen Darstellung zu lernen. t-SNE T-distributed Stochastic Neighbor Embedding (t-SNE) ist eine nichtlineare Dimensionsreduktionstechnik, die zur Visualisierung von hochdimensionalen Datensätzen nützlich ist. Es wird nicht für den Einsatz in der Analyse wie Clustering oder Outlier-Erkennung empfohlen, da es nicht notwendigerweise Dichten oder Entfernungen gut bewahrt. UMAP Uniform multiple Annäherung und Projektion (UMAP) ist eine nichtlineare Dimensionsreduktionstechnik. Visuell ist es dem t-SNE ähnlich, aber es geht davon aus, dass die Daten gleichmäßig auf einem örtlich angeschlossenen Riemannschen Verteiler verteilt werden und dass die Riemannsche Metrik lokal konstant oder annähernd örtlich konstant ist. Reduzierung der Abmessungen Bei hochdimensionalen Datensätzen (d.h. mit einer Anzahl von Abmessungen über 10) wird die Dimensionsreduktion üblicherweise vor der Anwendung eines K-nächsten Nachbaralgorithmus (k-NN) durchgeführt, um die Auswirkungen des Fluchs der Dimensionalität zu vermeiden. Merkmalsextraktion und Dimensionsreduktion können in einem Schritt mit der Hauptkomponentenanalyse (PCA,) lineare diskriminante Analyse (LDA,) canonische Korrelationsanalyse (CCA,) oder nicht-negative Matrix Factorization (NMF)-Techniken als Vorverarbeitungsschritt kombiniert werden, gefolgt von der Clusterung von K-N auf Merkmalsvektoren im reduzierten Dimensionsraum. Bei maschinellem Lernen wird dieser Prozess auch als Tief-dimensionale Einbettung bezeichnet. Bei sehr hochdimensionalen Datensätzen (z.B. bei der Durchführung der Ähnlichkeitssuche auf Live-Videostreams, DNA-Daten oder hochdimensionale Zeitreihen) mit einer schnellen K-NN-Suche mit lokalitätssensitivem Hashing, zufälliger Projektion, Skizzen oder anderen hochdimensionalen Ähnlichkeits-Suchtechniken aus der VLDB-Konferenz-Toolbox könnte die einzige mögliche Option sein. Anwendungen Eine Dimensionsreduktionstechnik, die manchmal in der Neurowissenschaft verwendet wird, ist maximal informative Dimensionen, die eine niederdimensionale Darstellung eines Datensatzes so findet, dass möglichst viele Informationen über die ursprünglichen Daten erhalten bleiben. Siehe auch Hinweise Referenzen Externe Links JMLR Special Issue on Variable and Feature Selection ELastic MAPsLocally Linear Das Ministerium für Integration und Gleichstellung der Geschlechter (Schwedisch: Integrations- och jämställdhetsdepartementet) war ein Ministerium der Regierungsbüros Schwedens. Zu seinen Aufgabenbereichen gehören Verbraucherangelegenheiten, Demokratiefragen, Gleichstellung der Geschlechter, Menschenrechtsfragen, Integrationsfragen, Großstädte, Minderheitenfragen, Nichtregierungsorganisationen und Jugendpolitik. Der einzige Minister für das Ministerium war Nyamko Sabuni, der als Minister für Gleichstellung der Geschlechter und als Minister für Integration diente. Sie war darauf bedacht, öffentlich darauf hinzuweisen, dass sie eine Gleichberechtigung und keine Feministin ist, keine Gruppe auf der Grundlage von Geschlecht, Rasse, Alter, Religion oder anderen typischen Diskriminierungsgrund als Fokus und Winkel über anderen in ihrer Gleichstellungsarbeit. Auf Schwedisch wird die Rolle (Jämställdhetsminister) genauer in den sogenannten Gleichstellungsminister übersetzt. Die Büros des Ministeriums befanden sich am Fredsgatan 8 im Zentrum von Stockholm. Das Ministerium wurde aufgelöst nach der allgemeinen Wahl 2010 mit Geschlechtergleichstellung in das Ministerium für Bildung und Forschung und Integration in das Ministerium für Beschäftigung. Geschichte Das Ministerium wurde am 1. Januar 2007 gegründet, nach einer Entscheidung der neuen Regierung, die am 6. Oktober 2006 stattfand. Zuvor wurden die Zuständigkeiten des Justizministeriums und des Außenministeriums bearbeitet. Behörden Das Ministerium für Integration und Gleichstellung der Geschlechter war für die folgenden Regierungsbehörden zuständig: Organisiert unter dem Justizministerium: Der Vorstand der Aufsicht der Immobilienmakler (Fastighetsmäklarnämnden, FMN)Das Marktgericht (Marknadsdomstolen)Die schwedische Verbraucheragentur (Konsumentverket, KO)Der Nationale Vorstand für Verbraucherbeschwerden (Allmänna reklamationsnämnden, ARN)Der Reisegarantierat (Resegarantinämnden) Organisiert unter dem Ministerium für Bildung: Der schwedische Nationalrat für Jugend (Ungdomsstyrelsen)Defunct Agenturen: The Equal Opportunities Bürgerbeauftragter (Jämställdhetsombudsmannen, JämO)Der Bürgerbeauftragte gegen Diskriminierung aufgrund sexueller Orientierung (Ombudsmannen mot diskriminering på grund av sexuell läggning, HomO)Der Bürgerbeauftragte gegen ethnische Diskriminierung (Ombudsmannen mot etnisk diskriminering, DO)The