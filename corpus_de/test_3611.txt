Ein mehrschichtiger Perceptron (MLP) ist eine Klasse von zukunftsweisenden künstlichen neuronalen Netzwerken (ANN). Der Begriff MLP wird mehrdeutig verwendet, manchmal lose, um jeglichen Feedforward ANN zu bedeuten, manchmal streng auf Netzwerke aus mehreren Schichten von Perceptrons (mit Schwellwertaktivierung); siehe § Terminologie. Multilayer-Perceptrons werden manchmal kolloquial als Vanille-Neural-Netzwerke bezeichnet, insbesondere wenn sie eine einzige versteckte Schicht haben. Ein MLP besteht aus mindestens drei Knotenschichten: einer Eingangsschicht, einer versteckten Schicht und einer Ausgangsschicht. Außer den Eingangsknoten ist jeder Knoten ein Neuron, das eine nichtlineare Aktivierungsfunktion verwendet. MLP nutzt eine beaufsichtigte Lerntechnik namens Backpropagation für Training. Seine multiplen Schichten und die nichtlineare Aktivierung unterscheiden MLP von einem linearen Perceptron. Es kann Daten unterscheiden, die nicht linear trennbar sind. Theorie Aktivierungsfunktion Hat ein Mehrschicht-Perceptron in allen Neuronen eine lineare Aktivierungsfunktion, d.h. eine lineare Funktion, die die gewichteten Eingänge auf den Ausgang jedes Neurons abbildet, so zeigt lineare Algebra, dass eine beliebige Anzahl von Schichten auf ein Zweischicht-Eingangsmodell reduziert werden kann. In MLPs verwenden einige Neuronen eine nichtlineare Aktivierungsfunktion, die entwickelt wurde, um die Häufigkeit von Aktionspotentialen oder Brennen von biologischen Neuronen zu modellieren. Die beiden historisch gebräuchlichen Aktivierungsfunktionen sind beide Sigmoide, und werden durch y (v i) = tanh ċ ( v i) und y (v i) = ( 1 + e - v i) - 1 \{displaystyle y(v_{i})=\tanh(v_{i}) ~~{textrm und}~y(v_{i})=(1+e^{-v_{i}})^{-1 .Inrezente Entwicklungen des Tiefenlernens der Gleichrichter-Lineareinheit (ReLU) werden häufiger als eine der möglichen Möglichkeiten genutzt, die mit den Sigmoiden verbundenen numerischen Probleme zu überwinden. Das erste ist eine hyperbolische Tangente, die von -1 bis 1 reicht, während das andere die logistische Funktion ist, die in Form, aber von 0 bis 1 reicht. Hier ist y i \{displaystyle y_{i} der Ausgang des i \{displaystyle i} th node (neuron) und v i \{displaystyle v_{i} die gewichtete Summe der Eingangsanschlüsse. Es wurden alternative Aktivierungsfunktionen vorgeschlagen, einschließlich der Gleichrichter- und Softplus-Funktionen. Weitere spezialisierte Aktivierungsfunktionen umfassen radiale Basisfunktionen (in radialen Basisnetzen, einer anderen Klasse von überwachten neuronalen Netzmodellen). Ebenen Die MLP besteht aus drei oder mehr Schichten (einer Eingabe und einer Ausgangsschicht mit einer oder mehreren verdeckten Schichten) nichtlinear aktivierender Knoten. Da MLPs vollständig verbunden sind, verbindet jeder Knoten in einer Schicht mit einem bestimmten Gewicht w i j \{displaystyle w_{ij} mit jedem Knoten in der folgenden Schicht. Learning Learning Learning tritt im Perceptron durch Änderung von Verbindungsgewichten nach jeder Datenverarbeitung auf, basierend auf der Fehlermenge in der Ausgabe im Vergleich zum erwarteten Ergebnis. Dies ist ein Beispiel für beaufsichtigtes Lernen und wird durch Backpropagation, eine Verallgemeinerung des am wenigsten mittleren Quadrat-Algorithmus im linearen Perceptron durchgeführt. Wir können den Fehlergrad in einem Ausgabeknoten j \{displaystyle j} in der n \{displaystyle n} th data point (Training Beispiel) durch e j (n) = d j (n) - y j (n ) \{displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n cept Die Knotengewichte können dann anhand von Korrekturen eingestellt werden, die den Fehler in der gesamten Ausgabe minimieren, gegeben durch E (n) = 1 2 Σ j e j 2 (n) \{displaystyle \{mathcal E}(n)={\frac 1}{2}\sum j}e_{j}{2}(n) .Usinggradient descent, die Änderung jedes Gewichtes ist Δ w i ( n ) = - η ∂ E ( n ) ∂ v j ( n) y i ( n ) \{displaystyle \Delta w_{ji}(n)=-\eta \{frac \{partial \{mathcal E}(n)}{partial v_{j}( Das zu berechnende Derivat hängt vom induzierten lokalen Feld v j \{displaystyle v_{j} ab, das sich selbst ändert. Es ist einfach zu beweisen, dass dieses Derivat für einen Ausgangsknoten auf - ∂ E ( n ) ∂ v j ( n ) = e j ( n ) φ ' ( v j ( n ) ) \{displaystyle \-{frac \{partial \{\{mathcal E}(n)}{partial v_{j}(n)}=e_{j}(n) Die Analyse ist schwieriger für die Gewichtsänderung an einen versteckten Knoten, aber es kann gezeigt werden, dass das betreffende Derivat - ∂ E ( n ) ∂ v j ( n ) = φ ' ( v j ( n ) ) ≠ k . Dies hängt von der Gewichtsänderung der k \{displaystyle k} der Knoten ab, die die Ausgangsschicht darstellen. Um die versteckten Schichtgewichte zu ändern, ändern sich die Ausgangsschichtgewichte entsprechend der Ableitung der Aktivierungsfunktion, so dass dieser Algorithmus eine Rückverbreitung der Aktivierungsfunktion darstellt. Terminologie Der Begriff "Multilayer-Perceptron" bezieht sich nicht auf einen einzigen Perceptron, der mehrere Schichten aufweist. Vielmehr enthält es viele Perceptrons, die in Schichten organisiert sind. Eine Alternative ist "multilayer perceptron network". Darüber hinaus sind MLP-Perceptrons keine Perceptrons im strengsten möglichen Sinne. Wahre Perceptrons sind formal ein Spezialfall von künstlichen Neuronen, die eine Schwellwertaktivierungsfunktion wie die Heaviside-Stufenfunktion verwenden. MLP-Perceptrons können beliebige Aktivierungsfunktionen verwenden. Ein wahrer Perceptron führt eine binäre Klassifikation durch, ein MLP Neuron ist frei, entweder Klassifikation oder Regression durchzuführen, je nach seiner Aktivierungsfunktion. Der Begriff "Multilayer-Perceptron" wurde später ohne Bezug auf die Natur der Nodes/Schichten angewendet, die aus willkürlich definierten künstlichen Neuronen und nicht speziell Perceptronen zusammengesetzt sein können. Diese Interpretation vermeidet die Lockerung der Definition von Perceptron zu einem künstlichen Neuron im Allgemeinen. Anwendungen MLPs sind nützlich in der Forschung für ihre Fähigkeit, Probleme stochastisch zu lösen, was oft ungefähre Lösungen für extrem komplexe Probleme wie Fitness Approximation erlaubt. MLPs sind Universal-Funktions-Atmatoren, wie von Cybenko's Theorem gezeigt, so können sie verwendet werden, um mathematische Modelle durch Regressionsanalyse zu erstellen. Da die Klassifizierung ein besonderer Fall der Regression ist, wenn die Antwortvariable kategorisch ist, machen MLPs gute Klassifikatoralgorithmen. MLPs waren in den 1980er Jahren eine beliebte maschinelle Lernlösung, die Anwendungen in verschiedenen Bereichen wie Spracherkennung, Bilderkennung und maschinelle Übersetzungssoftware fand, aber danach standen starke Konkurrenz von viel einfacheren (und damit verbundenen) Unterstützungsvektormaschinen. Das Interesse an Backpropagation-Netzwerken kehrte aufgrund der Erfolge des Deep Learning zurück. Referenzen Externe Links Eine sanfte Einführung in die Backpropagation - Ein intuitives Tutorial von Shashi Sathyanarayana Dies ist eine aktualisierte PDF-Version eines Blog-Artikels, der zuvor hier verlinkt wurde. Dieser Artikel enthält Pseudocode ("Training Wheels for Training Neural Networks") zur Implementierung des Algorithmus. Weka: Open Source Data Mining Software mit Mehrschicht-Perceptron-Implementierung. Neuroph Studio Dokumentation, implementiert diesen Algorithmus und ein paar andere. Cocaine (Configurable Omnipotent Custom Applications Integrated Network Engine) ist ein Open-Source-PaaS-System für die Erstellung von benutzerdefinierten Cloud-Hosting-Apps, die Bluemix, Google App Engine oder Heroku ähnlich sind. Auf diese Weise wurden bereits mehrere Dienste implementiert, darunter ein Dienst zum Erkennen einer Benutzerregion oder Sprache, ein Dienst zum Zugriff auf MongoDB-Speicher und ein URL-Fetcher. Geschichte Andrey Sibiryov, der ursprüngliche Entwickler von Cocaine, erhielt die Idee von Heroku, einer anderen Cloud-Plattform als Service. Zu dieser Zeit Heroku, nur unterstützte Anwendungen in Ruby entwickelt. Mit Heroku konnte der Entwickler eine Ruby-App erstellen und diese in die Cloud schieben, während Heroku die Infrastruktur und Lastausgleichsprobleme behandelte. Sibiryov war jedoch nicht zufrieden mit der Dokumentation für Heroku, so entschied er sich, seine eigene PaaS-Lösung zu schaffen. Zunächst war Cocaine ein persönliches Projekt für Sibiryov. Dies änderte sich jedoch, als Yandex einen internen Bedarf an einer skalierbaren Plattform entdeckte, die Millionen von Anfragen pro Sekunde (RPS) bewältigen könnte. Kokain diente diesen Zwecken. Jetzt wird Cocaine in der Yandex-Infrastruktur verwendet. Architektur Die Cloud besteht aus einer oder mehreren unabhängigen Maschinen, die den Cocaine Server (Cocained) installiert haben. Die Nutzer wissen nichts über den Standort der Dienste, auf die sie zugreifen – nur die Adresse des Lastsaldors und der App-Name stehen ihnen zur Verfügung. Benutzeranfragen werden an den Load Balancer gesendet, der sie in die Cloud übergibt. In der Cloud wird für jede Anforderung die optimale Maschine ausgewählt und anschließend die Anforderung ausgeführt. Die Infrastrukturdetails und die Umgebungseinstellungen der App sind ebenfalls dem Entwickler verborgen. Der Entwickler muss nur den Code in die Cloud schicken und ein besonderes Manifest für die Ausführung des Codes schreiben. Es ist nicht notwendig, etwas anderes einzurichten, wie Datenbanken, Key-value-Stores oder HTTP-Clients. Es gibt Dienste, die dies tun, die aus der Sicht des Programmierers native Module für die Programmiersprache sind, wird der Code eingeschrieben. Der Storage-Service ermöglicht beispielsweise den Zugriff auf den Elliptics-Speicher, und der Uatraits-Service verwendet Daten über den Benutzer Agenten eines Clients und dessen HTTP-Header, um die Eigenschaften des Geräts zu bestimmen, das die Anfrage gesendet hat. Anwendung der Technologie In der Cloud können Apps in verschiedenen Programmiersprachen (C, C,+ Go, JavaScript, Python, Ruby und andere) und Frameworks (Node.js) geschrieben werden. Cocaine an einen Webserver gebunden bietet eine automatisch skalierbare Bereitstellungsumgebung für Web-Apps. Die Cloud verarbeitet dank der gestreamten Datenaustausch-Schnittstelle große Informationsmengen. Cocaine implementiert fehlertolerante und leistungsstarke Busübertragung von Nachrichten und Ereignissen. Mit dem Load Balancer kann der Benutzer Systeme erstellen, die auf Benutzeraktionen reagieren. Apps in der Cloud laufen unabhängig voneinander. Dies ermöglicht die Unterstützung mehrerer Versionen der gleichen App und "Soft Migration", die für die Übertragung von Benutzern auf aktualisierte Versionen von Produkten eingerichtet wurden. Apps können in der Cloud durch einen isolierten Umgebungslauf getestet werden. So werden Programmfehler das physische System oder andere Apps nicht beeinflussen. Siehe auch Amazon Web Services Jelastic Engine Yard Heroku Force.com Skytap VMware Rackspace Cloud GoGrid Windows Azure OpenShift Oracle Cloud Referenzen Externe Links Cocaine PaaS (offizielle Website, archiviert,) archiviert vom Original auf 2015-02-14. "Search Engine Giant Yandex startet Cocaine", TechCrunch, 2013-10-16."Russisch Google Yandex Free Cloud Service Cocaine", The Guardian, 2013-10-17. "Free Cocaine Giveaway von Russian Search Engine Yandex", Das Register, 2013-10-17.