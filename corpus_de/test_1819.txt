Eine Metaanalyse ist eine statistische Analyse, die die Ergebnisse mehrerer wissenschaftlicher Studien kombiniert. Meta-Analyse kann durchgeführt werden, wenn es mehrere wissenschaftliche Studien zur gleichen Frage gibt, mit jeder einzelnen Studie Berichte Messungen, die erwartet werden, dass ein gewisses Maß an Fehler. Ziel ist es dann, Ansätze aus der Statistik zu verwenden, um eine gebündelte Schätzung nahe der unbekannten gemeinsamen Wahrheit zu ableiten, basierend auf der Art, wie dieser Fehler wahrgenommen wird. Nicht nur kann die Meta-Analyse eine Schätzung der unbekannten gemeinsamen Wahrheit liefern, sie hat auch die Fähigkeit, Ergebnisse aus verschiedenen Studien zu kontrastieren und Muster unter Studienergebnissen zu identifizieren, Quellen der Meinungsverschiedenheit unter diesen Ergebnissen oder andere interessante Beziehungen, die mit mehreren Studien zu beleuchten kommen. Bei der Durchführung einer Meta-Analyse muss jedoch ein Ermittler Entscheidungen treffen, die die Ergebnisse beeinflussen können, einschließlich der Entscheidung, wie man nach Studien sucht, die Auswahl von Studien basierend auf einer Reihe von objektiven Kriterien, die Behandlung unvollständiger Daten, die Analyse der Daten, und die Abrechnung oder Auswahl nicht zur Veröffentlichung von Vorurteilen. Urteilsgespräche, die bei der Durchführung einer Metaanalyse durchgeführt werden, können die Ergebnisse beeinflussen. Zum Beispiel untersuchten Wanous und Kollegen vier Paare von Meta-Analysen zu den vier Themen von (a) Job-Performance und Zufriedenheit Beziehung, (b) realistische Job-Vorschauen, (c) korreliert von Rollenkonflikt und Mehrdeutigkeit, und (d) die Job-Effizienz und Abwesenheitsbeziehung, und illustrierten, wie verschiedene Urteilsgespräche von den Forschern verschiedene Ergebnisse produzierten. Metaanalysen sind oft, aber nicht immer wichtige Bestandteile eines systematischen Überprüfungsverfahrens. So kann beispielsweise eine Metaanalyse an mehreren klinischen Studien einer medizinischen Behandlung durchgeführt werden, um ein besseres Verständnis dafür zu erhalten, wie gut die Behandlung funktioniert. Hier ist es zweckmäßig, die von der Cochrane-Kollaboration verwendete Terminologie zu verfolgen und Meta-Analysen zu verwenden, um statistische Methoden der Kombination von Beweisen zu verweisen, andere Aspekte der "Forschungssynthese" oder "Erkennungssynthese" zu hinterlassen, wie z.B. die Kombination von Informationen aus qualitativen Studien, für den allgemeineren Kontext systematischer Bewertungen. Eine Metaanalyse ist eine Sekundärquelle. Geschichte Die historischen Wurzeln der Metaanalyse lassen sich auf die Astronomie-Studien des 17. Jahrhunderts zurückverfolgen, während ein 1904 vom Statistiker Karl Pearson im British Medical Journal veröffentlichtes Papier, das Daten aus mehreren Studien der Typhus-Inokulation zusammenfasst, als das erste Mal angesehen wird, dass ein metaanalytischer Ansatz verwendet wurde, um die Ergebnisse mehrerer klinischer Studien zu aggregieren. Die erste Meta-Analyse aller konzeptuell identischen Experimente zu einer bestimmten Forschungsfrage, die von unabhängigen Forschern durchgeführt wurde, wurde als die 1940 Buch-Länge-Publikation Extrasensory Perception After Sixty Years, Autorin von Duke University Psychologen J. G. Pratt, J. B. Rhein, und Mitarbeiter identifiziert. Dies umfasste eine Überprüfung von 145 Berichten über ESP-Experimente, die von 1882 bis 1939 veröffentlicht wurden, und beinhaltete eine Schätzung des Einflusses nicht veröffentlichter Papiere auf den Gesamteffekt (das Datei-Zeichner-Problem). Der Begriff Metaanalyse wurde 1976 vom Statistiker Gene V prägen. Glas, der sagte, "Mein großes Interesse ist derzeit in dem, was wir kommen, um die Meta-Analyse der Forschung zu nennen. Der Begriff ist etwas groß, aber es ist präzise und apt .Meta-Analyse bezieht sich auf die Analyse von Analysen". Obwohl dies dazu führte, dass er weithin als moderner Gründer der Methode anerkannt wurde, prädiziert die Methodik hinter dem, was er als Metaanalyse bezeichnete, seine Arbeit seit mehreren Jahrzehnten. Die statistische Theorie zur Meta-Analyse wurde durch die Arbeit von Nambury S. Raju, Larry V. Hedges, Harris Cooper, Ingram Olkin, John E. Hunter, Jacob Cohen, Thomas Chalmers, Robert Rosenthal, Frank L. Schmidt, John E. Hunter und Douglas G. Bonett stark vorangebracht. 1992 wurde die Metaanalyse erstmals auf ökologische Fragen von Jessica Gurevitch angewandt, die die Metaanalyse zur Untersuchung des Wettbewerbs in Feldexperimenten nutzten. Schritte in einer Meta-Analyse Eine Meta-Analyse wird in der Regel durch eine systematische Überprüfung vorangegangen, da dies die Identifizierung und kritische Beurteilung aller relevanten Beweise ermöglicht (durch Begrenzung des Risikos von Bias in zusammenfassenden Schätzungen). Die allgemeinen Schritte sind dann wie folgt: Formulierung der Forschungsfrage, z.B. mit dem PICO-Modell (Population, Intervention, Vergleich, Ergebnis). Literatursuche Auswahl der Studien "(Inkorporationskriterien) Basierend auf Qualitätskriterien, z.B. die Anforderung der randomisation und blinding in einer klinischen Studie Auswahl spezifischer Studien zu einem gut spezifizierten Thema, z.B. die Behandlung von Brustkrebs. Entscheiden Sie, ob nicht veröffentlichte Studien enthalten sind, um Veröffentlichungsvoreinschätzungen zu vermeiden (Datei Schublade Problem)Entscheiden Sie, welche abhängigen Variablen oder zusammenfassenden Maßnahmen erlaubt sind. Zum Beispiel bei der Betrachtung einer Metaanalyse veröffentlichter (Aggregat-)Daten: Unterschiede (Differenzdaten) Mittel (kontinuierliche Daten) δ = δ = μ t - μ c σ, \{displaystyle \delta \={frac \{mu _{t} \mu_c}\sigma }}, in dem μ t \{displaystyle \mu} Auswahl eines Meta-Analysemodells, z.B. fester Effekt oder zufällige Effekte Meta-Analyse. Prüfen Sie Quellen der zwischenstuden Heterogenität, z.B. unter Gruppenanalyse oder Metaregression. Das Cochrane Handbuch enthält eine formale Anleitung zur Durchführung und Berichterstattung von Metaanalysen. Für die Berichterstattungsrichtlinien siehe die bevorzugte Berichterstattungsartikel für systematische Bewertungen und Meta-Analysen (PRISMA). Methoden und Annahmen Ansätze Im Allgemeinen können zwei Arten von Beweisen bei der Durchführung einer Metaanalyse unterschieden werden: einzelne Teilnehmerdaten (IPD) und aggregierte Daten (AD). Die Aggregatdaten können direkt oder indirekt sein. AD ist häufiger verfügbar (z.B. aus der Literatur) und stellt typischerweise zusammenfassende Schätzungen wie Quotenverhältnisse oder relative Risiken dar. Dies kann direkt über konzeptuell ähnliche Studien mit mehreren Ansätzen synthetisiert werden (siehe unten). Andererseits messen indirekte Aggregatdaten die Wirkung zweier Behandlungen, die jeweils in einer Metaanalyse gegen eine ähnliche Kontrollgruppe verglichen wurden. Zum Beispiel, wenn die Behandlung A und die Behandlung B direkt mit dem Placebo in separaten Meta-Analysen verglichen wurden, können wir diese beiden zusammengefassten Ergebnisse verwenden, um eine Schätzung der Auswirkungen von A vs B in einem indirekten Vergleich als Effekt A vs Placebo minus Effekt B vs Placebo zu erhalten. IPD-Beweise stellen Rohdaten dar, die von den Studienzentren erhoben werden. Diese Unterscheidung hat das Bedürfnis nach verschiedenen metaanalytischen Methoden, wenn die Nachweissynthese gewünscht wird, erhöht und zur Entwicklung von einstufigen und zweistufigen Methoden geführt. In einstufigen Methoden werden die IPD aus allen Studien gleichzeitig modelliert, während die Clustering von Teilnehmern in Studien berücksichtigt werden. Zweistufige Methoden berechnen zunächst Zusammenfassungsstatistiken für AD aus jeder Studie und berechnen dann die Gesamtstatistik als gewichtetes Mittel der Studienstatistik. Durch die Reduktion von IPD auf AD können auch zweistufige Methoden angewendet werden, wenn IPD zur Verfügung steht; dies macht sie zu einer ansprechenden Wahl bei der Durchführung einer Metaanalyse. Obwohl konventionell angenommen wird, dass ein- und zweistufige Methoden ähnliche Ergebnisse liefern, haben jüngste Studien gezeigt, dass sie gelegentlich zu unterschiedlichen Schlussfolgerungen führen können. Statistische Modelle für aggregierte Daten Modelle, die nur Studieneffekte enthalten Das Fixeffektmodell liefert einen gewichteten Durchschnitt einer Reihe von Studienschätzungen. Die Abweichung der Schätzungen wird häufig als Studiengewicht verwendet, so dass größere Studien dazu neigen, mehr als kleinere Studien zum gewichteten Durchschnitt beizutragen. Wenn also Studien innerhalb einer Metaanalyse von einer sehr großen Studie dominiert werden, werden die Ergebnisse kleinerer Studien praktisch ignoriert. Vor allem geht das Fixeffektmodell davon aus, dass alle eingeschlossenen Studien die gleiche Population untersuchen, dieselbe Variable und Ergebnisdefinitionen verwenden, etc. Diese Annahme ist in der Regel unrealistisch, da Forschung oft anfällig für mehrere Quellen der Heterogenität ist; z.B. Behandlungseffekte können je nach Ort, Dosierung, Studienbedingungen,... Random-Effekte Modell Ein gemeinsames Modell zur Synthese heterogener Forschung ist das Zufallseffektmodell der Metaanalyse. Dies ist einfach der gewichtete Durchschnitt der Effektgrößen einer Gruppe von Studien. Das Gewicht, das bei diesem Verfahren der gewichteten Mittelung mit einer zufälligen Wirkungsmetaanalyse angewendet wird, wird in zwei Schritten erreicht: Schritt 1: Inverse Varianzgewichtung Schritt 2: Ungewichtung dieser inversen Varianzgewichtung durch Anwendung einer zufälligen Effektvarianzkomponente (REVC), die einfach aus dem Ausmaß der Variabilität der Effektgrößen der zugrunde liegenden Studien abgeleitet ist. Dies bedeutet, dass umso größer diese Variabilität in Effektgrößen (anders als Heterogenität bekannt), je größer die Ungewichtung ist und dies einen Punkt erreichen kann, wenn das zufällige Effekt-Metaanalyseergebnis einfach die ungewichtete mittlere Effektgröße über die Studien hinweg wird. Im anderen Extremfall, wenn alle Effektgrößen ähnlich sind (oder die Variabilität nicht den Probenahmefehler überschreitet), wird kein REVC angewendet und die zufälligen Effekte der Meta-Analyse werden nur auf eine fixierte Effekt-Metaanalyse (nur inverse Varianzgewichtung) zurückgegriffen. Das Ausmaß dieser Umkehr hängt allein von zwei Faktoren ab: Heterogenität der Präzision Heterogenität der Effektgröße Da keiner dieser Faktoren automatisch eine fehlerhafte größere Studie oder zuverlässigere kleinere Studien anzeigt, wird die Umverteilung der Gewichte unter diesem Modell keine Beziehung zu dem, was diese Studien tatsächlich bieten könnten, haben. In der Tat wurde gezeigt, dass die Umverteilung von Gewichten einfach in einer Richtung von größeren bis kleineren Studien ist, da die Heterogenität zunimmt, bis schließlich alle Studien gleiches Gewicht haben und keine Umverteilung mehr möglich ist. Ein weiteres Problem mit dem Zufallseffektmodell besteht darin, dass die am häufigsten verwendeten Konfidenzintervalle in der Regel ihre Deckungswahrscheinlichkeit nicht über dem angegebenen Nominalniveau behalten und damit den statistischen Fehler erheblich unterschätzen und in ihren Schlussfolgerungen möglicherweise überbewerten. Es wurden mehrere Korrekturen vorgeschlagen, aber die Aussprache geht weiter. Ein weiteres Anliegen ist, dass der durchschnittliche Behandlungseffekt manchmal sogar noch weniger konservativ gegenüber dem feststehenden Effektmodell sein kann und daher in der Praxis irreführend ist. Eine Interpretationsreparatur, die vorgeschlagen wurde, ist, ein Prädiktionsintervall um die zufällige Wirkungsschätzung zu erstellen, um den Bereich der möglichen Effekte in der Praxis darzustellen. Eine Annahme hinter der Berechnung eines solchen Prädiktionsintervalls besteht jedoch darin, dass Versuche als mehr oder weniger homogene Körper betrachtet werden und die Patientenpopulationen und Vergleicherbehandlungen als austauschbar angesehen werden sollten und dies in der Praxis in der Regel unhaltbar ist. Die am weitesten verbreitete Methode zur Schätzung zwischen Studienvarianz (REVC) ist der DerSimonian-Laird (DL) Ansatz. Mehrere fortgeschrittene iterative (und rechnerisch teure) Techniken zur Berechnung der zwischen Studienvarianz existieren (wie maximale Wahrscheinlichkeit, Profilwahrscheinlichkeit und begrenzte maximale Wahrscheinlichkeitsmethoden) und zufällige Effekte Modelle mit diesen Methoden können in Stata mit dem metaan Befehl ausgeführt werden. Der Metaan-Befehl muss vom klassischen Metan (Einzel-a)-Befehl in Stata unterschieden werden, der den DL-Schätzer verwendet. Diese fortschrittlichen Methoden wurden auch in einem kostenlosen und einfachen Microsoft Excel Add-on, MetaEasy implementiert. Ein Vergleich zwischen diesen fortgeschrittenen Methoden und der DL-Methode zur Berechnung der Studienvarianz ergab jedoch, dass es kaum zu gewinnen gibt und DL in den meisten Szenarien durchaus ausreichend ist. Die meisten Metaanalysen umfassen jedoch zwischen 2 und 4 Studien und eine solche Probe ist häufiger als nicht unzureichend, um die Heterogenität genau zu schätzen. So scheint es, dass in kleinen Metaanalysen eine falsche Null zwischen der Studienvarianzschätzung erhalten wird, was zu einer falschen Homogenitätsannahme führt. Insgesamt zeigt sich, dass die Heterogenität in Metaanalysen und Sensitivitätsanalysen, in denen hohe Heterogenitätsstufen angenommen werden, konsequent unterschätzt wird. Diese oben genannten Zufallseffekte und Softwarepakete beziehen sich auf Metaanalysen und Forscher, die individuelle Patientendaten (IPD)-Metaanalysen durchführen möchten, müssen Ansätze zur Modellierung von Mischeffekten berücksichtigen. IVhet-Modell Doi & Barendregt in Zusammenarbeit mit Khan, Thalib und Williams (von der University of Queensland, University of Southern Queensland und Kuwait University) haben eine inverse Varianz quasi Wahrscheinlichkeit basierte Alternative (IVhet) zu den zufälligen Effekten (RE) Modell, für die Details online verfügbar sind. Dies wurde in die MetaXL-Version 2.0 integriert, ein kostenloses Microsoft-Excel-Add-in für die Meta-Analyse von Epigear International Pty Ltd, und am 5. April 2014 verfügbar gemacht. Die Autoren weisen darauf hin, dass ein deutlicher Vorteil dieses Modells darin besteht, dass es die beiden Hauptprobleme des Zufallseffektmodells löst. Der erste Vorteil des IVhet-Modells besteht darin, dass die Deckung auf dem nominalen (in der Regel 95%) Niveau für das Konfidenzintervall bleibt, im Gegensatz zum zufälligen Effektmodell, das mit zunehmender Heterogenität in die Deckung fällt. Der zweite Vorteil ist, dass das IVhet-Modell die inversen Varianzgewichte einzelner Studien beibehält, im Gegensatz zum RE-Modell, das kleine Studien mit zunehmender Heterogenität mehr Gewicht (und damit größere Studien weniger) gibt. Wenn Heterogenität groß wird, werden die einzelnen Studiengewichte unter dem RE-Modell gleich und damit kehrt das RE-Modell einen arithmetischen Mittelwert anstelle eines gewichteten Mittels zurück. Dieser Nebeneffekt des RE-Modells tritt nicht mit dem IVhet-Modell auf, das sich somit in zwei Perspektiven von der RE-Modellschätzung unterscheidet: Gebündelte Schätzungen werden größere Versuche bevorzugen (im Gegensatz zu größeren Versuchen im RE-Modell) und ein Vertrauensintervall haben, das innerhalb der nominalen Deckung unter Unsicherheit bleibt (Heterogenität). Doi & Barendregt schlagen vor, dass, während das RE-Modell eine alternative Methode zur Bündelung der Studiendaten bietet, ihre Simulationsergebnisse zeigen, dass mit einem genaueren Wahrscheinlichkeitsmodell mit unhaltbaren Annahmen, wie beim RE-Modell, nicht unbedingt bessere Ergebnisse liefert. Die letztgenannte Studie berichtet auch, dass das IVhet-Modell die Probleme im Zusammenhang mit der Unterschätzung des statistischen Fehlers, der schlechten Erfassung des Vertrauensintervalls und der verstärkten MSE mit dem Zufallseffektmodell löst und die Autoren folgern, dass Forscher die Verwendung des zufälligen Effektmodells in der Metaanalyse aufgeben sollten. Während ihre Daten zwingend sind, sind die Verdrängungen (in Bezug auf die Größe der spuriös positiven Ergebnisse innerhalb der Cochrane-Datenbank) riesig und erfordern daher eine sorgfältige unabhängige Bestätigung. Die Verfügbarkeit einer kostenlosen Software (MetaXL), die das IVhet-Modell (und alle anderen Modelle zum Vergleich) betreibt, erleichtert dies für die Forschungsgemeinschaft. Direkte Beweise: Modelle mit zusätzlichen Informationen Qualität Effekte Modell Doi und Thalib ursprünglich eingeführt die Qualitätseffekte Modell. Sie führten einen neuen Ansatz zur Anpassung an die Inter-Study-Variabilität ein, indem sie den Beitrag der Varianz durch eine relevante Komponente (Qualität) zusätzlich zu dem Beitrag der Varianz durch Zufallsfehler, der in jedem Fixeffekt Meta-Analyse-Modell verwendet wird, um Gewichte für jede Studie zu erzeugen. Die Stärke der Qualitätseffekte Meta-Analyse ist, dass es verfügbaren methodischen Nachweisen über subjektive zufällige Effekte verwendet werden kann und damit hilft, die schädliche Lücke zu schließen, die zwischen Methodik und Statistiken in der klinischen Forschung eröffnet hat. Dazu wird eine synthetische Biasvarianz basierend auf Qualitätsinformationen berechnet, um inverse Varianzgewichte einzustellen und das qualitätsgerechte Gewicht der Ith-Studie eingeführt. Diese eingestellten Gewichte werden dann in der Metaanalyse verwendet. Mit anderen Worten, wenn die Studie i von guter Qualität ist und andere Studien von schlechter Qualität sind, wird ein Anteil ihrer qualitätsgerechten Gewichte mathematisch umverteilt, um zu studieren i geben es mehr Gewicht auf die Gesamteffektgröße. Da sich die Studien in Bezug auf Qualität zunehmend ähneln, wird die Umverteilung immer weniger und hört auf, wenn alle Studien von gleicher Qualität sind (bei gleicher Qualität wird das Qualitätseffektmodell dem IVhet-Modell zugrunde gelegt – siehe vorangegangener Abschnitt). Eine kürzliche Bewertung des Qualitätseffektmodells (mit einigen Updates) zeigt, dass trotz der Subjektivität der Qualitätsbewertung die Leistung (MSE und wahre Varianz unter Simulation) der mit dem Zufallseffektmodell erzielbaren überlegen ist. Dieses Modell ersetzt somit die unhaltbaren Interpretationen, die in der Literatur eingebunden sind und eine Software zur weiteren Erkundung dieses Verfahrens zur Verfügung steht. Indirekte Nachweise: Netzwerk-Metaanalysemethoden Indirekte Vergleichs-Metaanalysemethoden (auch Netzwerk-Metaanalysen genannt, insbesondere wenn mehrere Behandlungen gleichzeitig beurteilt werden) verwenden in der Regel zwei Hauptmethoden. Zunächst ist das Bucher-Verfahren, das ein einziger oder wiederholter Vergleich einer geschlossenen Schleife von Drei-Behandlungen ist, so dass eine dieser beiden Untersuchungen gemeinsam ist und den Knoten bildet, an dem die Schleife beginnt und endet. Um mehrere Behandlungen zu vergleichen, werden daher mehrere Zwei-Zwei-Vergleiche (3-Behandlungsschleifen) benötigt. Diese Methode erfordert, dass Versuche mit mehr als zwei Armen nur zwei Arme ausgewählt haben, da unabhängige paarweise Vergleiche erforderlich sind. Die alternative Methode verwendet eine komplexe statistische Modellierung, um die mehreren Armversuche und Vergleiche gleichzeitig zwischen allen konkurrierenden Behandlungen einzubeziehen. Diese wurden mit Bayesischen Methoden, gemischten linearen Modellen und Meta-Regressionsansätzen durchgeführt. Bayesischer Rahmen Die Angabe eines Bayesischen Netzwerk-Metaanalysemodells beinhaltet das Schreiben eines gerichteten azyklischen Graphen (DAG) Modells für allgemeine Markov-Kette Monte Carlo (MCMC) Software wie WinBUGS. Darüber hinaus müssen für eine Anzahl der Parameter Vorverteilungen vorgegeben und die Daten in einem bestimmten Format geliefert werden. Gemeinsam bilden die DAG Vor- und Daten ein Bayesisches Hierarchisches Modell. Um die Dinge weiter zu komplizieren, müssen wegen der Art der MCMC-Schätzung überdisperse Ausgangswerte für eine Reihe unabhängiger Ketten gewählt werden, so dass Konvergenz beurteilt werden kann. Derzeit gibt es keine Software, die solche Modelle automatisch generiert, obwohl es einige Werkzeuge gibt, die dabei helfen. Die Komplexität des Bayesischen Ansatzes hat eine begrenzte Nutzung dieser Methode. Methodologie für die Automatisierung dieser Methode wurde vorgeschlagen, erfordert jedoch, dass die Ergebnisse auf Armebene verfügbar sind, und dies ist in der Regel nicht verfügbar. Große Ansprüche werden manchmal für die inhärente Fähigkeit des Bayesischen Rahmens gemacht, die Netzwerk-Metaanalyse und seine größere Flexibilität zu bewältigen. Diese Wahl der Umsetzung des Rahmens für Inferenz, Bayesischer oder häufiger, kann jedoch weniger wichtig sein als andere Wahlen zur Modellierung von Effekten (siehe Diskussion über Modelle oben). Frequentist multivariate Framework Auf der anderen Seite beinhalten die häufigen multivariaten Methoden Approximationen und Annahmen, die bei der Anwendung der Methoden nicht explizit angegeben oder überprüft werden (siehe oben die Diskussion über Metaanalysemodelle).Beispielsweise ermöglicht das mvmeta-Paket für Stata die Netzwerk-Metaanalyse in einem häufigen Rahmen. Wenn im Netz jedoch kein gemeinsamer Komparator vorhanden ist, muss dies dadurch gehandhabt werden, dass der Datensatz mit fiktiven Armen mit hoher Varianz erweitert wird, was nicht sehr objektiv ist und eine Entscheidung erfordert, was eine ausreichend hohe Varianz darstellt. Die andere Frage ist die Verwendung des Zufallseffektmodells sowohl in diesem häufigistischen Rahmen als auch im Bayesischen Rahmen. Senn riet Analysten, vorsichtig über die Interpretation der 'random Effects' Analyse zu sein, da nur ein zufälliger Effekt erlaubt ist, aber man könnte viele vorsehen. Senn geht weiter zu sagen, dass es ziemlich naiv ist, auch wenn nur zwei Behandlungen verglichen werden, davon ausgehen, dass zufällige Wirkungsanalysen alle Unsicherheiten über die Art und Weise, wie Auswirkungen von Versuch zu Versuch variieren können. Neue Modelle der Meta-Analyse wie die oben diskutiert würde sicherlich helfen, diese Situation zu lindern und wurden im nächsten Rahmen umgesetzt. Allgemeines paarweise Modellierungsgerüst Ein Ansatz, der seit Ende der 1990er Jahre versucht wurde, ist die Umsetzung der multiplen Drei-Behandlungs-Low-Loop-Analyse. Dies ist nicht populär, weil der Prozess schnell überwältigend wird, wenn die Netzkomplexität zunimmt. Die Entwicklung in diesem Bereich wurde dann zugunsten der Bayesischen und multivariaten häufigistischen Methoden, die als Alternativen entstanden, aufgegeben. In letzter Zeit wurde für komplexe Netzwerke von einigen Forschern die Automatisierung der Drei-Behandlungs-Loop-Methode entwickelt, um dies der Hauptforschungsgemeinschaft zur Verfügung zu stellen. Dieser Vorschlag beschränkt jede Prüfung auf zwei Eingriffe, führt aber auch einen Workaround für mehrere Armversuche ein: ein anderer fester Steuerknoten kann in verschiedenen Strecken ausgewählt werden. Es verwendet auch robuste Meta-Analyseverfahren, so dass viele der oben hervorgehobenen Probleme vermieden werden. Weitere Forschungen zu diesem Rahmen sind erforderlich, um festzustellen, ob dies den Bayesischen oder multivariaten häufigistischen Rahmen überlegen ist. Forscher, die dies ausprobieren wollen, haben Zugang zu diesem Rahmen durch eine freie Software. Maßgeschneiderte Metaanalyse Eine weitere Form von zusätzlichen Informationen kommt aus der vorgesehenen Einstellung. Ist die Zieleinstellung zur Anwendung der Meta-Analyseergebnisse bekannt, so kann es möglich sein, Daten aus der Einstellung zu verwenden, um die Ergebnisse so zu maßgeben, dass eine "tailored meta-analysis" entsteht. Dies wurde in Testgenauigkeits-Metaanalysen verwendet, bei denen empirische Kenntnisse der Test-positiven Rate und der Prävalenz verwendet wurden, um eine Region in Receiver Operating Characteristic (ROC) Raum als "anwendbare Region" bekannt abzuleiten. Anschließend werden Studien für die Zieleinstellung anhand eines Vergleichs mit dieser Region ausgewählt und aggregiert, um eine auf die Zieleinstellung zugeschnittene Zusammenfassungsschätzung zu erstellen. Die Aggregation IPD und AD Meta-Analyse kann auch angewendet werden, um IPD und AD zu kombinieren. Dies ist praktisch, wenn die Forscher, die die Analyse durchführen, ihre eigenen Rohdaten beim Sammeln von Aggregaten oder Zusammenfassungsdaten aus der Literatur haben. Das generalisierte Integrationsmodell (GIM) ist eine Verallgemeinerung der Metaanalyse. Es erlaubt, dass das auf den einzelnen Teilnehmerdaten (IPD) angebrachte Modell von den zur Berechnung der Aggregatdaten (AD) verwendeten unterscheidet. GIM kann als Modellkalibrierungsverfahren zur Integration von Informationen mit mehr Flexibilität angesehen werden. Validierung von Metaanalyseergebnissen Die Metaanalyseschätzung stellt einen gewichteten Durchschnitt über Studien dar, und wenn es Heterogenität gibt, kann dies dazu führen, dass die Gesamtschätzung nicht repräsentativ für einzelne Studien ist. Qualitative Bewertung der primären Studien mit etablierten Werkzeugen kann potenzielle Vorurteile aufdecken, aber nicht quantifiziert die aggregierte Wirkung dieser Vorurteile auf die Gesamtschätzung. Obwohl das Metaanalyseergebnis mit einer unabhängigen prospektiven Primärstudie verglichen werden konnte, ist eine solche externe Validierung oft unpraktisch. Dies hat zur Entwicklung von Methoden geführt, die eine Form der ausscheidenden Kreuzvalidierung ausnutzen, manchmal auch als intern-externe Kreuzvalidierung (IOCV) bezeichnet. Hier entfällt jede der k- eingeschlossenen Studien wiederum und verglichen mit der zusammenfassenden Schätzung aus der Aggregation der verbleibenden k- 1 Studien. Eine allgemeine Validierungsstatistik, die auf IOCV basiert, wurde entwickelt, um die statistische Gültigkeit der Metaanalyseergebnisse zu messen. Für die Prüfgenauigkeit und -vorhersage, insbesondere bei multivariaten Effekten, wurden auch andere Ansätze vorgeschlagen, die versuchen, den Prädiktionsfehler abzuschätzen. Herausforderungen Eine Metaanalyse mehrerer kleiner Studien prognostiziert nicht immer die Ergebnisse einer einzigen großen Studie. Einige haben argumentiert, dass eine Schwäche der Methode ist, dass Quellen von Bias nicht durch die Methode gesteuert werden: eine gute Meta-Analyse kann für schlechtes Design oder Bias in den ursprünglichen Studien nicht korrigieren. Dies würde bedeuten, dass nur methodologisch fundierte Studien in eine Meta-Analyse aufgenommen werden sollten, eine Praxis namens "beste Beweissynthese". Andere Meta-Analysten würden schwächere Studien umfassen und eine Variable auf Studienebene hinzufügen, die die methodologische Qualität der Studien widerspiegelt, um den Effekt der Studienqualität auf die Effektgröße zu untersuchen. Andere haben jedoch argumentiert, dass ein besserer Ansatz darin besteht, Informationen über die Varianz in der Studienprobe zu erhalten, so weit wie möglich ein Netz zu gießen, und dass methodologische Auswahlkriterien unerwünschte Subjektivität einführen und den Zweck des Ansatzes besiegen. Publication Bias: the file drawer problem Ein weiteres potenzielles Pitfall ist die Abhängigkeit von der verfügbaren Stelle der veröffentlichten Studien, die möglicherweise übertriebene Ergebnisse aufgrund der Veröffentlichung Bias erstellen, da Studien, die negative Ergebnisse zeigen oder unbedeutende Ergebnisse weniger wahrscheinlich veröffentlicht werden. Zum Beispiel sind Pharmaunternehmen bekannt, negative Studien zu verbergen, und Forscher haben möglicherweise nicht veröffentlichte Studien übersehen, wie Dissertationsstudien oder Konferenz Abstracts, die nicht Veröffentlichung erreichten. Dies ist nicht leicht gelöst, da man nicht wissen kann, wie viele Studien unbewiesen sind. Dieses Problem der Dateischublade (bedeutend mit negativen oder nicht signifikanten Ergebnissen, die in einem Kabinett entfernt werden), kann zu einer voreingenommenen Verteilung von Effektgrößen führen, wodurch ein ernster Basisratenrückgang entsteht, bei dem die Bedeutung der veröffentlichten Studien überschätzt wird, da andere Studien entweder nicht zur Veröffentlichung vorgelegt oder abgelehnt wurden. Dies sollte bei der Interpretation der Ergebnisse einer Metaanalyse ernsthaft berücksichtigt werden. Die Verteilung der Effektgrößen kann mit einem Trichter-Plot visualisiert werden, der (in seiner häufigsten Version) ein Streubild des Standardfehlers gegenüber der Effektgröße ist. Es nutzt die Tatsache, dass die kleineren Studien (d.h. größere Standardfehler) mehr Streuung der Wirkungsstärke (sein weniger präzise) haben, während die größeren Studien weniger Streuung haben und die Spitze des Trichters bilden. Wenn viele negative Studien nicht veröffentlicht wurden, ergeben die übrigen positiven Studien einen Trichtergrund, in dem die Basis auf eine Seite gesäumt wird (Asymmetrie des Trichtergrundes). Im Gegensatz dazu, wenn es keine Veröffentlichungsvorspannung gibt, hat die Wirkung der kleineren Studien keinen Grund, auf eine Seite gesäumt zu werden, und so ergibt sich ein symmetrischer Trichterplan. Dies bedeutet auch, dass, wenn keine Veröffentlichungsvorspannung vorliegt, kein Zusammenhang zwischen Standardfehler und Effektgröße besteht. Ein negativer oder positiver Zusammenhang zwischen Standardfehler und Effektgröße würde bedeuten, dass kleinere Studien, die Effekte nur in einer Richtung festgestellt haben, wahrscheinlicher veröffentlicht und/oder zur Veröffentlichung vorgelegt werden. Neben dem visuellen Trichterdiagramm wurden auch statistische Methoden zur Erkennung von Veröffentlichungsvorspannung vorgeschlagen. Diese sind kontrovers, weil sie typischerweise eine geringe Leistung zur Detektion von Bias haben, aber auch unter Umständen falsche Positive machen können. So können z.B. kleine Studieneffekte (vorgespannte kleinere Studien), bei denen methodologische Unterschiede zwischen kleineren und größeren Studien bestehen, Asymmetrie in Effektgrößen verursachen, die Veröffentlichungsvoreinschätzungen ähneln. Kleine Studieneffekte können jedoch ebenso problematisch für die Interpretation von Metaanalysen sein, und das Imperativ ist auf metaanalytischen Autoren, um mögliche Quellen von Vorurteilen zu untersuchen. Eine Tandem-Methode zur Analyse der Veröffentlichungsvorspannung wurde vorgeschlagen, falsche positive Fehlerprobleme zu reduzieren. Diese Tandem-Methode besteht aus drei Stufen. Erstens berechnet man Orwins ausfallsicheres N, um zu überprüfen, wie viele Studien hinzugefügt werden sollten, um die Teststatistik auf eine triviale Größe zu reduzieren. Wenn diese Anzahl von Studien größer ist als die Anzahl der Studien, die in der Meta-Analyse verwendet werden, ist es ein Zeichen, dass es keine Veröffentlichung Voreingenommenheit gibt, da man in diesem Fall eine Menge Studien benötigt, um die Effektgröße zu reduzieren. Zweitens kann man den Regressionstest eines Eggers durchführen, der prüft, ob das Trichterdiagramm symmetrisch ist. Wie bereits erwähnt: ein symmetrischer Trichtergrund ist ein Zeichen, dass es keine Veröffentlichungsvorspannung gibt, da die Effektgröße und die Probengröße nicht abhängig sind. Drittens kann man das Trimm-and-Füll-Verfahren durchführen, das Daten eingibt, wenn das Trichterdiagramm asymmetrisch ist. Das Problem der Veröffentlichungsvorspannung ist nicht trivial, da vorgeschlagen wird, dass 25 % der Meta-Analysen in den psychologischen Wissenschaften unter Veröffentlichungsvorspannung leiden können. Die geringe Leistung bestehender Tests und Probleme mit dem visuellen Erscheinungsbild des Trichtergrundstücks bleibt jedoch ein Problem, und Schätzungen der Veröffentlichungsvorspannung können niedriger sein als das, was wirklich existiert. Die meisten Diskussionen über Veröffentlichungsvoreinschätzungen konzentrieren sich auf die Journal-Praktiken, die die Veröffentlichung von statistisch signifikanten Ergebnissen begünstigen. Allerdings können fragwürdige Forschungspraktiken, wie das Nacharbeiten statistischer Modelle, bis die Bedeutung erreicht wird, auch statistisch bedeutsame Erkenntnisse zur Unterstützung der Hypothesen der Forscher bevorzugen. Probleme im Zusammenhang mit Studien, die nicht-statistisch signifikante Effekte melden Studien berichten oft nicht die Auswirkungen, wenn sie keine statistische Bedeutung erreichen. Beispielsweise können sie einfach sagen, dass die Gruppen keine statistisch signifikanten Unterschiede zeigten, ohne andere Informationen (z.B. Statistik oder p-Wert) zu melden. Ausschluss dieser Studien würde zu einer Situation führen, die ähnlich der Veröffentlichungsvoreinschätzung ist, aber ihre Einbindung (Ergebnis von Null-Effekten) würde auch die Metaanalyse beeinträchtigen. MetaNSUE, eine von Joaquim Radua erstellte Methode, hat gezeigt, dass Forscher unvoreingenommen diese Studien aufnehmen können. Seine Schritte sind wie folgt: Maximale Wahrscheinlichkeitsschätzung der metaanalytischen Wirkung und der Heterogenität zwischen den Studien. Mehrere Imputation der NSUEs addiert Rauschen zur Schätzung der Wirkung. Separate Meta-Analysen für jeden eingegebenen Datensatz. Bündelung der Ergebnisse dieser Metaanalysen. Probleme im Zusammenhang mit dem statistischen Ansatz Andere Schwächen sind, dass es nicht festgestellt wurde, wenn die statistisch genauste Methode zur Kombination von Ergebnissen die festen, IVhet-, zufälligen oder Qualitätseffektmodelle ist, obwohl die Kritik an dem Zufallseffektmodell aufgrund der Wahrnehmung, dass die neuen zufälligen Effekte (in der Metaanalyse verwendet) im Wesentlichen formale Vorrichtungen zur Erleichterung der Glättung oder Schrumpfung sind und Vorhersagen unmöglich oder unwahrscheinlich sein können. Das Hauptproblem bei der Methode der zufälligen Effekte besteht darin, dass es den klassischen statistischen Gedanken verwendet, einen "kompromise Schätzer" zu erzeugen, der die Gewichte in der Nähe des natürlich gewichteten Schätzwertes macht, wenn Heterogenität über Studien groß ist, aber in der Nähe des inversen Varianz gewichteten Schätzwertes, wenn die zwischen Studie Heterogenität klein ist. Was jedoch ignoriert wurde, ist die Unterscheidung zwischen dem Modell, das wir wählen, um einen bestimmten Datensatz zu analysieren, und dem Mechanismus, mit dem die Daten aufgenommen wurden. Eine zufällige Wirkung kann in einer dieser Rollen vorhanden sein, aber die beiden Rollen sind ganz unterschiedlich. Es gibt keinen Grund zu denken, dass das Analysemodell und der Datenerzeugungsmechanismus (Modell) in Form ähnlich sind, aber viele Unterfelder von Statistiken haben die Gewohnheit entwickelt, für Theorie und Simulationen davon auszugehen, dass der Datenerzeugungsmechanismus (Modell) identisch ist mit dem Analysemodell, das wir wählen (oder möchten, dass andere wählen). Als hypothetische Mechanismen zur Herstellung der Daten ist das zufällige Effektmodell für die Meta-Analyse albern und es ist besser, dieses Modell als oberflächliche Beschreibung und etwas zu betrachten, das wir als analytisches Werkzeug wählen – aber diese Wahl für die Meta-Analyse kann nicht funktionieren, weil die Studieneffekte ein festes Merkmal der jeweiligen Meta-Analyse sind und die Wahrscheinlichkeitsverteilung nur ein beschreibendes Werkzeug ist. Probleme, die sich aus einer programmgesteuerten Bias ergeben Die schwerste Störung der Meta-Analyse tritt oft auf, wenn die Person oder die Person, die die Meta-Analyse durchführt, eine wirtschaftliche, soziale oder politische Agenda wie den Übergang oder die Niederlage der Rechtsvorschriften haben. Menschen mit diesen Arten von Tagesordnungen können wahrscheinlicher sein, die Meta-Analyse durch persönliche Vorurteile zu missbrauchen. Zum Beispiel, Forscher, die für die Agenda des Autors günstig sind, werden wahrscheinlich ihre Studien kirsch-picked, während diejenigen nicht günstig werden ignoriert oder als "nicht glaubwürdig" gekennzeichnet. Darüber hinaus können die begünstigten Autoren selbst voreingenommen oder bezahlt werden, um Ergebnisse zu erzielen, die ihre allgemeinen politischen, sozialen oder wirtschaftlichen Ziele in einer Weise unterstützen, wie die Auswahl von kleinen günstigen Datensätzen und nicht die Einbeziehung größerer ungünstiger Datensätze. Der Einfluss solcher Bias auf die Ergebnisse einer Meta-Analyse ist möglich, da die Methodik der Meta-Analyse sehr schlecht ist. Eine 2011 durchgeführte Studie, um mögliche Interessenkonflikte an zugrunde liegenden Forschungsstudien für medizinische Metaanalysen offenzulegen, hat 29 Metaanalysen überprüft und festgestellt, dass Interessenkonflikte in den Studien, die den Metaanalysen zugrunde liegen, selten offengelegt wurden. Die 29 Meta-Analysen enthielten 11 von allgemeinen Medizinzeitschriften, 15 von Spezialmedizinzeitschriften und drei von der Cochrane Datenbank von Systematic Reviews. Die 29 Meta-Analysen überprüften insgesamt 509 randomisierte kontrollierte Versuche (RCT). Davon berichteten 318 RCT Finanzierungsquellen, mit 219 (69%) die Finanzierung aus der Industrie (d.h. ein oder mehrere Autoren, die finanzielle Bindungen an die Pharmaindustrie haben). Von den 509 RCTs berichteten 132 Autorenkonflikte mit Zinsvergaben, wobei 91 Studien (69%) einen oder mehrere Autoren mit finanziellen Bindungen an die Industrie enthüllten. Die Informationen wurden jedoch in den Meta-Analysen selten reflektiert. Nur zwei (7%) berichteten über RCT-Finanzierungsquellen und keine gemeldeten RCT-Autor-Industriebindungen. Die Autoren schlossen "ohne Anerkennung von COI durch Industriefinanzierung oder Autor Industrie finanzielle Bindungen von RCTs in Meta-Analysen enthalten, Leserverständnis und Beurteilung der Beweise aus der Meta-Analyse kann beeinträchtigt werden. " So hat beispielsweise 1998 ein US-Bundesrichter festgestellt, dass die United States Environmental Protection Agency den Meta-Analyseprozess missbraucht hatte, um eine Studie zu erstellen, in der Krebsrisiken für Nichtraucher aus Umwelttabakrauch (ETS) genannt werden, mit der Absicht, die politischen Entscheidungsträger zu rauchfreiem Arbeitsplatzgesetz zu beeinflussen. Der Richter fand, dass: EPAs Studienauswahl stört. Erstens gibt es Beweise in der Aufzeichnung, die die Anschuldigung unterstützen, dass EPA "die Kirsche ausgewählt" ihre Daten. Ohne Kriterien für die Bündelung von Studien in eine Meta-Analyse kann das Gericht nicht bestimmen, ob der Ausschluss von Studien, die die a priori-Hypothese von EPA beeinträchtigen könnten, zufällig oder absichtlich war. Zweitens widerspricht das EPA fast die Hälfte der verfügbaren Studien direkt mit dem zweckdienlichen Ziel von EPA zur Analyse der epidemiologischen Studien und Konflikte mit den Risikobewertungsleitlinien von EPA. Siehe ETS-Risikobewertung bei 4-29 "(Diese Daten sollten auch im Interesse der Wäge aller verfügbaren Beweise untersucht werden, wie dies in den Leitlinien für die Karzinogen-Risikobewertung von EPA (US EPA, 1986a) empfohlen wird (Empphasis Added)). Drittens steht die selektive Verwendung von Daten im Widerspruch zum Radon Research Act. Das EPA-Programm soll "Daten und Informationen zu allen Aspekten der Raumluftqualität gewinnen" (Radon Research Act § 403(a)(1)) (Erfassung hinzugefügt). Infolge des Missbrauchs räumte das Gericht die Kapitel 1-6 und die Anhänge zu den "Respiratorischen Gesundheitseffekten des Passivrauchens: Lung Cancer und andere Störungen" von EPA ein. Schwache Inklusionsstandards führen zu irreführenden Schlussfolgerungen Meta-Analysen in der Bildung sind oft nicht restriktiv genug in Bezug auf die methodische Qualität der Studien, die sie beinhalten. Zum Beispiel führen Studien, die kleine Proben oder forschungsorientierte Maßnahmen enthalten, zu aufgeblasenen Effektgrößenschätzungen. Anwendungen in der modernen Wissenschaft Die moderne statistische Meta-Analyse kombiniert mehr als nur die Effektgrößen einer Reihe von Studien mit einem gewichteten Durchschnitt. Es kann testen, ob die Ergebnisse der Studien mehr Variation zeigen als die Variation, die aufgrund der Probenahme von verschiedenen Anzahl von Forschungsteilnehmern erwartet wird. Darüber hinaus können Studienmerkmale wie eingesetztes Messinstrument, gemusterte Populationen oder Aspekte des Studiendesigns kodiert und verwendet werden, um die Varianz des Schätzers zu reduzieren (siehe statistische Modelle oben). So können einige methodologische Schwächen in Studien statistisch korrigiert werden. Weitere Verwendungen von metaanalytischen Methoden umfassen die Entwicklung und Validierung von klinischen Prädiktionsmodellen, bei denen die Metaanalyse verwendet werden kann, um einzelne Teilnehmerdaten aus verschiedenen Forschungszentren zu kombinieren und die Allgemeinheit des Modells zu bewerten oder sogar bestehende Prädiktionsmodelle zu aggregieren. Meta-Analyse kann mit Einzel-Subjekt-Design sowie Gruppenforschungsdesigns durchgeführt werden. Dies ist wichtig, weil viel Forschung mit Einzelprojekten durchgeführt wurde. Für die am besten geeignete metaanalytische Technik für die Einzelforschung besteht ein beträchtlicher Streit. Die Metaanalyse führt zu einer Verschiebung der Betonung von Einzelstudien bis hin zu mehreren Studien. Sie unterstreicht die praktische Bedeutung der Effektgröße anstelle der statistischen Bedeutung einzelner Studien. Diese Verschiebung des Denkens wurde als "meta-analytisches Denken" bezeichnet. Die Ergebnisse einer Metaanalyse werden oft in einem Waldgrundstück gezeigt. Ergebnisse aus Studien werden mit unterschiedlichen Ansätzen kombiniert. Ein Ansatz, der häufig in der Metaanalyse in der Gesundheitsforschung eingesetzt wird, wird als "inverse Varianzmethode" bezeichnet. Die durchschnittliche Effektgröße in allen Studien wird als gewichtetes Mittel berechnet, wobei die Gewichte gleich der inversen Varianz der Wirkungsschätzung jeder Studie sind. Größere Studien und Studien mit weniger zufälligen Variationen erhalten ein größeres Gewicht als kleinere Studien. Weitere gemeinsame Ansätze sind die Mantel-Haenszel-Methode und die Peto-Methode. Die auf Seed basierende d Mapping (früher unterschriebene Differential Mapping, SDM) ist eine statistische Technik zur Meta-Analyse von Studien über Unterschiede in der Gehirnaktivität oder Struktur, die Neuroimaging-Techniken wie fMRI, VBM oder PET verwendet. Verschiedene Hochdurchsatztechniken wie Mikroarrays wurden verwendet, um Genexpression zu verstehen. MikroRNA Expressionsprofile wurden verwendet, um differentiell exprimierte Mikro zu identifizieren RNAs insbesondere Zell- oder Gewebetyp oder Krankheitsbedingungen oder zur Überprüfung der Wirkung einer Behandlung. Eine Metaanalyse solcher Expressionsprofile wurde durchgeführt, um neue Schlussfolgerungen abzuleiten und die bekannten Erkenntnisse zu validieren. Siehe auch Schätzungsstatistiken Metascience Newcastle–Ottawa Skala Bias Reporting Bias Review Journal Sekundärforschung Studie Heterogenität Systematische Überprüfung Galbraith Plot Datenaggregation ReferenzenWeitere Informationen Externe Links Cochrane Handbook für systematische Bewertungen von Interventionen Meta-Analyse bei 25 (Gene V Glass) Vorhergehende Reporting Items für systematische Bewertungen und Meta-Analysen (PRISMA) Statement, "eine Hinweise auf Meta-Basis “metansue” R-Paket und grafische Schnittstelle Best Evidence Encyclopedia