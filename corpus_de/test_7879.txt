State–action–reward–state–action (SARSA) ist ein Algorithmus zum Erlernen einer Markov-Entscheidungsprozesspolitik, die im Verstärkungslernbereich des maschinellen Lernens verwendet wird. Es wurde von Rummery und Niranjan in einer technischen Note mit dem Namen "Modified Connectionist Q-Learning" (MCQ-L) vorgeschlagen. Der von Rich Sutton vorgeschlagene alternative Name SARSA wurde nur als Fußnote erwähnt. Dieser Name spiegelt einfach die Tatsache wider, dass die Hauptfunktion zur Aktualisierung des Q-Wertes von dem aktuellen Zustand des Agenten S1, der Aktion, die der Agent wählt A1, der Belohnung R der Agent für die Wahl dieser Aktion erhält, der Zustand S2, den der Agent nach der Einnahme dieser Aktion eintritt, und schließlich die nächste Aktion A2 der Agent in seinem neuen Zustand wählt. Das Akronym für den Quintupel (st, at, rt, st+1, at+1) ist SARSA. Einige Autoren verwenden eine etwas andere Konvention und schreiben die Quittung (st, at, rt+1, st+1, at+1), je nachdem, welcher Zeitschritt die Belohnung formal zugewiesen wird. Der Rest des Artikels verwendet die ehemalige Konvention. ) Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha ,[r_{t}+\gamma \Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t} Ein SARSA-Agent interagiert mit der Umwelt und aktualisiert die Politik auf der Grundlage von ergriffenen Aktionen, daher ist dies als on-policy learning algorithm bekannt. Der Q-Wert für eine Zustandsreaktion wird durch einen Fehler aktualisiert, der durch die Lernrate alpha angepasst wird. Q-Werte stellen die mögliche Belohnung dar, die im nächsten Schritt zur Aktion a in states erhalten wurde, plus die ermäßigte zukünftige Belohnung, die aus der nächsten Zustandsbeobachtung erhalten wurde. Das Q-Learning von Watkin aktualisiert eine Schätzung der optimalen Zustands-Action-Wert-Funktion Q ≠ {\displaystyle Q^{}* basierend auf der maximalen Belohnung verfügbarer Aktionen. Während SARSA die Q-Werte lernt, die mit der Übernahme der Politik verbunden sind, erlernt Watkins Q-Learning die Q-Werte, die mit der Übernahme der optimalen Politik verbunden sind, während er einer Explorations-/Exploitationspolitik folgt. Einige Optimierungen des Q-Learning von Watkin können auf SARSA angewendet werden. Hyperparameter Lernrate (alpha)Die Lernrate bestimmt, inwieweit neu erworbene Informationen alte Informationen überwiegen. Ein Faktor von 0 wird den Agenten nichts lernen, während ein Faktor von 1 würde den Agenten nur die neuesten Informationen betrachten. Diskontfaktor (gamma)Der Diskontfaktor bestimmt die Bedeutung zukünftiger Belohnungen. Ein Faktor von 0 macht den Agenten opportunistisch, indem er nur aktuelle Belohnungen betrachtet, während ein Faktor, der sich auf 1 nähert, es für eine langfristige hohe Belohnung strebt. Wenn der Rabattfaktor 1 erfüllt oder überschreitet, können die Q {\displaystyle Q}-Werte divergieren. Ausgangsbedingungen Q(s0, a0)) Da SARSA ein iterativer Algorithmus ist, nimmt er implizit eine Anfangsbedingung an, bevor das erste Update erfolgt. Ein niedriger (unbestimmter) Anfangswert, auch als "optimistische Ausgangsbedingungen" bezeichnet, kann die Exploration fördern: Egal, welche Aktion stattfindet, die Aktualisierungsregel bewirkt, dass sie höhere Werte als die andere Alternative hat und somit ihre Wahlwahrscheinlichkeit erhöht. Im Jahr 2013 wurde vorgeschlagen, dass die erste Belohnung r verwendet werden konnte, um die ursprünglichen Bedingungen zurückzusetzen. Nach dieser Idee wird zum ersten Mal eine Aktion ergriffen, um den Wert von Q einzustellen. Dies ermöglicht ein sofortiges Lernen bei festen deterministischen Belohnungen. Dieser Rücksetz-of-Initial-Bedingungen (RIC)-Ansatz scheint in wiederholten binären Wahlversuchen mit menschlichem Verhalten konsistent zu sein. = Referenzen = Eine Zigarrenbox ist ein Box-Container für Zigarrenverpackungen. Traditionell wurden Zigarrenkästen aus Holz, Karton oder Papier hergestellt. Spanische Zeder wurde als die beste Art von Holz für Zigarren-Boxen wegen seiner schönen Getreide, feine Textur, und angenehmen Geruch und Fähigkeit, Bugs zu halten beschrieben. Eucalyptus und gelbe Pappel waren beliebte Ersatzstoffe, die manchmal angefärbt und gesäumt wurden, um sie zu ähneln. Andere typische Hölzer für Zigarrenkästen sind Mahagoni, Elm und weiße Eiche; weniger beliebt sind Basswood, Circassian Walnuss und Rosenholz. Es gibt mehrere Arten von Zigarrenkisten, unterscheiden sich sowohl im Aufbau als auch im Zweck. Folgende sind häufige Boxen: Kabinett-Auswahl, Schiebe- oder Klappdeckel, typischerweise Lagerung von 25 oder 50 Zigarren 8-9-8, rundseitige Box mit drei Schichten, Zählung 8, 9 und 8 Zigarren. Flache Oberseite oder 13-Topper, zwei Schichten mit 12 auf der Unterseite und 13 auf der Oberseite Boxen von kastengepressten Zigarren, zwei Schichten mit derselben Anzahl von Zigarren gespeichert. Zigarrenkästen, Etiketten und Bands gelten als Kunstobjekt, mit auf sie spezialisierten Unternehmen und Büchern auf ihrem Design, Bedeutung und Bedeutung gedruckt. Dadurch können Zigarrenkästen und deren entsprechende Etiketten als Sammlerstücke angesehen werden. Es gibt eine wachsende Bewegung von Gitarren und Ukuleles, die aus Zigarrenkisten und anderen Materialien hergestellt werden, die normalerweise nicht für Musikinstrumente verwendet werden. Medienreferenzen Externe Links Das kanadische Museum der Zivilisation - Zigarrencontainer, die Unsere Vergangenheit 1883-1935 speichern