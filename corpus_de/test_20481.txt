Clusteranalyse oder Clustering ist die Aufgabe, eine Reihe von Gegenständen so zu vereinen, dass Gegenstände in derselben Gruppe (ein Cluster) einander gegenüber denen anderer Gruppen (Cluster) ähnlich sind. Es ist eine Hauptaufgabe der Sondierungs-Datenanalyse und einer gemeinsamen Analyse der statistischen Daten, die in vielen Bereichen verwendet wird, einschließlich Mustererkennung, Bildanalyse, Bioinformatik, Datenkompression, Computerbilder und Maschinenbau. Clusteranalyse selbst ist kein spezieller Algorithmus, aber die allgemeine Aufgabe zu lösen. Es kann durch verschiedene Algorithmen erreicht werden, die in ihrem Verständnis deutlich voneinander abweichen, was ein Cluster darstellt und wie man sie effizient finden kann. Volksnotationen von Clustern umfassen Gruppen mit kleinen Entfernungen zwischen Cluster-Mitgliedern, dichten Gebieten des Datenraums, Intervallen oder bestimmten statistischen Datenverteilungen. Clustering kann daher als Multi-Ziel-Verringerungsproblem definiert werden. Der geeignete Clustering-Algorithmus und Parametereinstellungen (einschließlich Parameter wie die Fernfunktion zur Nutzung, eine Dichteschwelle oder die Anzahl der erwarteten Cluster) hängen von den einzelnen Daten ab und dienen der Nutzung der Ergebnisse. Clusteranalyse als solche ist keine automatische Aufgabe, sondern ein iterativer Prozess der Wissenssuche oder interaktive Multi-Zieloptimierung, der Prozess und Scheitern umfasst. Es ist oft notwendig, die Vorverarbeitungs- und Modellparameter zu ändern, bis das Ergebnis die gewünschten Eigenschaften erreicht. Neben dem Begriff „Clustering“ gibt es eine Reihe von Begriffen mit ähnlichen Bedeutungen, einschließlich automatischer Klassifikation, numerischer Steueronomy, Botryologie (von griechischer βότρυς Trauben), Typologie und Gemeinschaftserkennung. Vielfältige Unterschiede sind häufig bei der Nutzung der Ergebnisse: Während der Data Mining die entstehenden Gruppen von Interesse sind, ist die automatische Einstufung der entstehenden Diskriminierungsmacht von Interesse. Clusteranalyse wurde in der anthropologie von Fahrer und Kroeber im Jahr 1957 geboren und in die Psychologie von Joseph Zubin im Jahr 1938 und Robert Tryon im Jahr 1939 aufgenommen und von Cattell Anfang 1964 für die Trait theoretische Einstufung in der Persönlichkeitstheorie verwendet. Definition Der Begriff eines Clusters kann nicht genau definiert werden, was einer der Gründe ist, warum es so viele Cluster-Algorithmen gibt. Es gibt einen gemeinsamen Nenner: eine Gruppe von Datenobjekten. Verschiedene Forscher beschäftigen jedoch unterschiedliche Clustermodelle, und für jedes dieser Clustermodelle können immer noch verschiedene Algorithmen erhalten. Der Begriff eines Clusters, wie von verschiedenen Algorithmen festgestellt, variiert in seinen Eigenschaften erheblich. Verständnis dieser "Clustermodelle" ist entscheidend, um die Unterschiede zwischen den verschiedenen Algorithmen zu verstehen. Typische Clustermodelle umfassen: Konnektivitätsmodelle: z.B. hierarchische Clusterbildung baut Modelle auf, die auf der Fernverbindung basieren. Centroid Modelle: Zum Beispiel stellt der k-means-Algorithmus jedes Cluster mit einem einzigen mittleren Vektor dar. Vertriebsmodelle: Cluster werden unter Verwendung statistischer Daten, wie multivariate normale Verteilungen, die durch den erwarteten Abbau-maximisierungsgorithmus verwendet werden. Dichtemodelle: DBSCAN und OPIK definieren Cluster als vernetzte Regionen im Datenraum. Subspace Modelle: In Biclustering (auch als Kocluster oder zwei-mode-Clustering bekannt) werden Cluster mit beiden Gruppenmitgliedern und relevanten Eigenschaften entwickelt. Gruppenmodelle: Manche Algorithmen bieten kein raffiniertes Modell für ihre Ergebnisse und bieten die Zusammenstellung von Informationen an. Graphikbasierte Modelle: eine Klique, die ist, ein Teil von Knoten in einem Diagramm, das bedeutet, dass alle zwei Knoten in der Untergruppe mit einem Rand verbunden sind, als modellische Form von Clustern angesehen werden können. Lockerung der vollständigen Konnektivitätsanforderungen (ein Bruchteil der Kanten kann fehlen) sind als quasi-cliques bekannt, wie im HCS-Cluster-Algorithmus. Signifizierte Graphikmodelle: Jeder Weg in einem unterzeichneten Diagramm hat ein Zeichen aus dem Produkt der Zeichen auf den Kanten. Nach Annahmen der Gleichgewichtstheorie können die Kanten das Zeichen ändern und zu einem bifurzierten Diagramm führen. Die schwächere "Clusterierbarkeit axiom" (keinen Zyklus hat genau einen negativen Rand) führt mit mehr als zwei Clustern oder Unterblättern mit nur positiven Kanten. Neural Modelle: das am besten bekannte unüberwindete Neuralnetz ist die Selbstorganisationskarte, und diese Modelle können in der Regel ähnlich wie eine oder mehrere der oben genannten Modelle sein und auch Teilraummodelle umfassen, wenn Neuralnetze eine Form der Hauptkomponentenanalyse oder der unabhängigen Komponenteanalyse umsetzen. Ein Cluster ist im Wesentlichen eine Reihe solcher Cluster, die in der Regel alle Objekte in den Daten enthalten. Darüber hinaus kann sie die Beziehung zwischen den Clustern auf jeden Fall festlegen, z.B. eine Rangfolge von Clustern, die ineinander eingebettet sind. Clusterings können ungefähr als: Hard-Clustering: Jedes Objekt gehört zu einem Cluster oder nicht Soft-Clustering (auch: fuzzy-Clustering): Jedes Objekt gehört zu jedem Cluster zu einem gewissen Grad (z.B. eine Wahrscheinlichkeit des Zugehörigkeits zum Cluster). Es gibt auch feinere Unterscheidungen, beispielsweise: Strenge Aufteilung der Clustering: Jedes Objekt gehört zu genau einem Cluster, das die Trennung von Clustern mit Auslierern betrifft: Objekte können auch zu keiner Cluster gehören und als Auslierer gelten Überschneidung von Clustern (auch: alternative Clusterierung, Multi-view-Clustering): Gegenstände können zu mehr als einem Cluster gehören; in der Regel mit harten Clustern Hierarchisches Clustering: Objekte, die zu einem Kindercluster gehören, gehören ebenfalls zum Stammcluster-Unterraum: Während sich die Clusterbildung innerhalb eines einzigartigen Teilraums überlappt, werden Cluster nicht überschneiden Clustering-Algorithmen können wie oben aufgeführt auf ihrem Clustermodell zusammengefasst werden. Nachstehender Übersicht werden nur die wichtigsten Beispiele für Cluster-Algorithmen aufgeführt, da es möglicherweise über 100 veröffentlichte Cluster-Algorithmen gibt. Nicht alle bieten Modelle für ihre Cluster an und können daher nicht leicht kategorisiert werden. In der Liste der Statistikalgorithmen ist ein Überblick über Algorithmen zu finden. Es gibt keine objektive Korrektur des Cluster-Algorithmus, aber wie es bemerkt wurde, „Clustering befindet sich im Auge des Inhabers. Der am besten geeignete Cluster-Algorithmus für ein bestimmtes Problem muss oft versuchsweise gewählt werden, es sei denn, es gibt einen mathematischen Grund, ein Cluster-Modell über ein anderes zu bevorzugen. Ein Algorithmus, der für eine Art von Modell bestimmt ist, wird in der Regel nicht auf einen Datenkatalog verzichten, der eine radikal unterschiedliche Art von Modell enthält. K-means können beispielsweise nicht konvex Cluster finden. Konnektivitätsbasiertes Clustering (hierarchisches Clustering)Connectivitätsbasiertes Clustering, auch als hierarchisches Cluster bekannt, basiert auf der Kernidee von Gegenständen, die eher mit nahe gelegenen Gegenständen als auf Objekte verbunden sind. Diese Algorithmen verbinden Objekte, um Cluster auf der Grundlage ihrer Entfernung zu bilden. Ein Cluster kann weitgehend durch die maximale Entfernung beschrieben werden, die erforderlich ist, um Teile des Clusters zu verbinden. In unterschiedlichen Entfernungen werden unterschiedliche Cluster gebildet, die mit einer Dendrogram vertreten werden können, die erklärt, wo der gemeinsame Name "hierarchische Clusterbildung" ausfällt: Diese Algorithmen bieten keine einheitliche Aufteilung der Daten, sondern eine umfassende Hierarchie von Clustern, die in bestimmten Entfernungen miteinander verschmelzen. In einem derdrogram markiert die y-Achse die Entfernung, in der die Cluster zusammengelegt werden, während die Objekte entlang der x-Achse angebracht sind, so dass die Cluster keine Mischung finden. Konnektivitätsbasiertes Clustering ist eine ganze Familie von Methoden, die sich durch die Berechnung der Entfernungen unterscheiden. Neben der üblichen Auswahl an Fernfunktionen muss der Nutzer auch über das Linkage-Kriterium entscheiden (da ein Cluster aus mehreren Gegenständen besteht, gibt es mehrere Kandidaten, um die Entfernung zu berechnen). Volksentscheide sind bekannt als Single-linkage-Clustering (mindestens Gegenstand Entfernungen), vollständige Verbindungscluster (höchsten Ende der Objektabstände), UPGMA oder WPGMA („Ungewichtete oder gewichtete Gruppenmethode mit Arithmetic Mean“, bekannt als durchschnittliche Verbindungscluster). Darüber hinaus kann es sich bei hierarchischen Clustern um einenlomerativen (Anfang mit Einzelelementen und Aggregation in Cluster) oder umständlich handeln (Anfang mit den vollständigen Daten und Aufteilung in Teilbereiche). Diese Methoden werden keine einzigartige Aufteilung der Daten enthalten, aber eine Hierarchie, von der der Nutzer noch geeignete Cluster auswählen muss. Sie sind nicht sehr robust gegenüber Auslierern, die entweder als zusätzliche Cluster auftreten oder sogar andere Cluster zur Zusammenlegung führen (bekannt als „Ketten Phänomen“, insbesondere mit Singlelinkage-Clustering). Im allgemeinen ist die Komplexität der O (n 3 ) Memedisplaystyle HANA mathematischcal O O(n3)3) für ein lomeratives Clustering und O ( 2 n − 1 ) {\displaystyle KINGcal O O(2^{n-1) für die Entwicklung von Clustern, die sie zu langsam für große Datensets machen. Für einige Sonderfälle sind optimale effiziente Methoden (wie die Komplexität O (n 2 ) JPYstyle HANA mathematischcal O O(n2)2) ) bekannt: SLINK für Single-linkage und CLINK für die vollständige Vernetzung. In der Data Mining Community werden diese Methoden als theoretische Grundlage der Clusteranalyse anerkannt, aber häufig als obsolet. Sie haben jedoch für viele spätere Methoden, wie z.B. dichtebasierte Clusterbildung, Inspiration gegeben. Linkage Clusterbildung Beispiele Centroid-basierte Clusterbildung In Centroid-basierte Clusterbildung werden Cluster durch einen zentralen Vektor vertreten, der möglicherweise nicht unbedingt Mitglied der Datenerhebung sein kann. Wenn die Anzahl der Cluster auf k fixiert ist, geben k-means Clustering eine formale Definition als Optimierungsproblem: finden die k-Clusterzentren und geben die Objekte an das nächste Clusterzentrum, so dass die Quadratkilometer des Clusters minimiert werden. Das Optimierungsproblem selbst ist bekannt, NP-hard zu sein, so dass der gemeinsame Ansatz nur für ungefähre Lösungen sucht. Eine besonders bekannte ungefähre Methode ist Lloyd's Algorithmus, oft nur als "k-means-Algorithmus" bezeichnet (obwohl ein anderer Algorithmus diesen Namen eingeführt hat). Es findet jedoch nur eine lokale Optimierung und wird häufig mit unterschiedlichen Randomisierungen betrieben. Variationen von k-means umfassen häufig solche Optimierungen wie die Wahl des bestens von mehreren Flügen, aber auch die Einschränkung der Centroide an Mitglieder der Datensets (k-medoids), die Auswahl der Medien (K-medians Clustering), die Auswahl der ersten weniger randomisierten Zentren (k-means)+ oder die Ermöglichung eines fy-Clusterauftrags (fuzzy c-means). Die meisten k-means-Typ-Algorithmen erfordern die Anzahl von Clustern – k – im Voraus festzulegen, was als eines der größten Rückschläge dieser Algorithmen gilt. Darüber hinaus bevorzugen die Algorithmen Cluster von ungefähr ähnlicher Größe, da sie immer ein Objekt an die nächste Centroide vergeben werden. Dies führt oft zu falschen Grenzen von Clustern (was nicht überraschend ist, da der Algorithmus Clusterzentren optimiert, nicht Clustergrenzen). K-means hat eine Reihe interessanter theoretischer Eigenschaften. Erstens wird der Datenraum in eine als Voronoi-Tabelle bekannte Struktur aufgeteilt. Zweitens ist es konzeptuell nahe der nächsten Klassifikation, und als solche ist im Maschinenlernen beliebt. Drittens kann es als eine Variante von modellbasierten Clustern angesehen werden, und Lloyd'sAlgorithmus als eine Variante des erhoffungs-maximisierungsgorithmus für dieses Modell erörtert werden. k-means Clustering Beispiele Centroid-basierte Clusterbildungsprobleme wie k-means und k-medoide sind Sonderfälle des unkapazitätierten, metrischen Standortproblems, ein canonisches Problem in den Forschungs- und Rechengruppen. In einem Problem der Grundanlage (von denen es zahlreiche Varianten gibt, die eine detailliertere Gestaltung ermöglichen), besteht die Aufgabe darin, die besten Lagerstandorte zu finden, um eine Reihe von Verbrauchern optimal zu bedienen. Eines kann Lager als Cluster Centroids und "Verbraucherstandorte" als die zu bündelnden Daten betrachten. Dies ermöglicht es, die gut entwickelten Algorithmen-Lösungen aus der Anlagestandortliteratur auf das derzeit als Zenroid-basierte Problem anzuwenden. Verteilungsbasierte Cluster Das Clustering-Modell, das eng mit Statistiken verknüpft ist, basiert auf Vertriebsmodellen. Cluster können dann leicht als Gegenstände definiert werden, die am ehesten zum gleichen Vertrieb gehören. Ein bequemes Eigentum dieses Ansatzes ist, dass dies die Art und Weise ähnelt, wie künstliche Datensets erzeugt werden: durch Probenahme von Zufallsgegenständen aus dem Vertrieb. Obwohl die theoretische Grundlage dieser Methoden ausgezeichnet ist, leiden sie an einem zentralen Problem, das als Nachrüstung bekannt ist, es sei denn, die Probleme werden auf die Modellkomplexität gesetzt. In der Regel wird ein komplexeres Modell in der Lage sein, die Daten besser zu erklären, was die Auswahl des geeigneten Modells in der Regel schwierig macht. Eine prominente Methode ist bekannt als Gaussssian-Mixmodelle (Nutzung des erwarteten Abbau-maximisierungsgorithmus). In diesem Zusammenhang werden die Daten in der Regel mit einer festen (um eine Überrüstung zu vermeiden) Anzahl von Gausssischen Vertriebenen, die paraphiert werden und deren Parameter sich optimieren lassen, um die Daten besser anzupassen. Dies wird sich auf ein lokales optimales Maß annähern, so dass mehrere Runs unterschiedliche Ergebnisse erzielen können. Um ein hartes Cluster zu erhalten, werden Gegenstände oft der Gaussian-Vertrieb zugewiesen, die sie am wahrscheinlichsten gehören; für weiche Cluster ist dies nicht erforderlich. Verteilungsbasierte Clusterbildung stellt komplexe Modelle für Cluster her, die Korrelation und Abhängigkeit zwischen den Eigenschaften erfassen können. Jedoch stellen diese Algorithmen eine zusätzliche Belastung für den Nutzer dar: für viele echte Datensätze kann es kein kurz definiertes mathematisches Modell geben (z.B. wenn Gaussian-Vertriebe eine ziemlich starke Annahme der Daten sind). Gausssian-Mixmodell, das Beispiele für dichtesbasierte Clusterbildung Konzentrationen auf Dichtebasis werden als Gebiete mit höherer Dichte definiert als die restlichen Daten. Objekte in dünnen Gebieten – die für getrennte Cluster erforderlich sind – gelten in der Regel als Lärm und Grenzpunkte. DBSCAN ist die beliebteste, dichtebasierte Clusterbildungsmethode. Im Gegensatz zu vielen neueren Methoden verfügt sie über ein klar definiertes Clustermodell namens Bevölkerungsdichte. In ähnlicher Weise ist es auf Anschlusspunkten innerhalb bestimmter Entfernungsschwellen basiert. In der ursprünglichen Variante, die als Mindestzahl anderer Objekte innerhalb dieses Radius definiert ist, verbindet sie jedoch nur die Punkte, die ein Dichtekriterium erfüllen. Ein Cluster besteht aus allen dichtegebundenen Gegenständen (die im Gegensatz zu vielen anderen Methoden ein Cluster willkürlicher Form bilden können) sowie allen Gegenständen, die innerhalb dieser Gegenstände liegen. Ein weiteres interessantes Eigentum von DBSCAN ist, dass seine Komplexität recht gering ist – es erfordert eine lineare Anzahl von Anfragen auf der Datenbank – und dass sie im Wesentlichen dieselben Ergebnisse entdecken wird (es ist deterministisch für Kern- und Lärmpunkte, aber nicht für Grenzübergangsstellen), weshalb es nicht notwendig ist, es mehrfach zu betreiben. OPIK ist eine allgemeine Ausrichtung der DBSCAN, die die Notwendigkeit beseitigt, einen angemessenen Wert für den verschiedenen Parameter   Memestyle \varepsilon } zu wählen und ein hierarchisches Ergebnis im Zusammenhang mit dem Verbundcluster herbeizuführen. DeLi-Clu, dichte-Link-Clustering kombiniert Ideen von Single-linkage-Clustering und OPIKen, beseitigt den Parameter {\ Memedisplaystyle \varepsilon } vollständig und bietet Leistungsverbesserungen gegenüber OPIK durch Verwendung eines R-tree-Index. Leitmotiv der DBSCAN und OPIK ist, dass sie eine Art von Dichteverlust erwarten, um Clustergrenzen zu erkennen. Datensets mit z.B. überschneidenden Gausssian-Vertrieben – ein gemeinsamer Einsatzfall bei künstlichen Daten – werden die von diesen Algorithmen hergestellten Clustergrenzen oft willkürlich aussehen, da die Clusterdichte kontinuierlich sinkt. In einer Reihe von Daten, die aus Mischungen von Gausssians bestehen, werden diese Algorithmen fast immer durch Methoden wie EM-Clustering, die diese Art von Daten genau modellieren können, ausgeschrieben. Mean-Verschiebung ist ein Cluster-Ansatz, bei dem jeder Gegenstand auf der Basis der Kerndichte-Schätzung in die am meisten betroffenen Gebiete verlagert wird. Letztendlich werden Objekte an lokale Maxima der Dichte angeglichen. ähnlich wie k-means Clustering können diese „Anreize“ als Vertreter für die Datensets dienen, aber die mittlere Umlagerung kann willkürliche Cluster wie DBSCAN erkennen. Aufgrund des teuren iterativen Verfahrens und der Bevölkerungsdichte ist der mittlere Umbruch in der Regel langsamer als DBSCAN oder k-Means. Neben diesem Umstand wird die Anwendbarkeit des Mittelumsatzes auf multidimensionale Daten durch das unmoothe Verhalten der Kerndichte-Schätzung behindert, was zu einer Überfragmentierung von Clustern führt. dichtesbasierte Clusterbildung Beispiele Grid-basiertes Clustering Die netzgestützte Technik wird für eine mehrdimensionale Datenmenge verwendet. In dieser Technik schaffen wir eine Netzstruktur, und der Vergleich erfolgt auf Netzen (auch bekannt als Zellen). Die netzbasierte Technik ist schnell und hat eine geringe Rechenfunktion. Es gibt zwei Arten von netzbasierten Clustern: STING und CLIQUE. Maßnahmen im Zusammenhang mit dem netzbasierten Cluster-Algorithmus sind: Kluftdatenraum in eine finite Anzahl von Zellen. Zufallsauswahl für eine Zelle „c“, wo c nicht vorab geschleppt werden sollte. kalkuliert die Dichte von „c“, wenn die Dichte von „c“ größer ist als die Schwellendichte Mark Cell „c“ als neues Cluster, in dem die Dichte aller Nachbarn von „c“ berechnet wird, wenn die Dichte einer benachbarten Zelle größer ist als die Schwellendichte, dann die Zelle im Cluster anfügen und wiederholen die Schritte 4.2 und 4.3 bis es keine Nachbarn mit einer Bevölkerungsdichte gibt. Wiederholte Schritte 2,3 und 4 bis alle Zellen werden geschleppt. Jüngste Entwicklungen In den letzten Jahren wurden erhebliche Anstrengungen unternommen, um die Leistung bestehender Algorithmen zu verbessern. Unter ihnen sind CLARANS und BIRCH. Angesichts der jüngsten Notwendigkeit, größere und größere Datensets (auch bekannt als große Daten) zu verarbeiten, ist die Bereitschaft, die semantische Bedeutung der generierten Leistungscluster zu vermarkten. Dies führte zur Entwicklung von Vorclustern wie canopy Clustering, die enorme Datensets effizient verarbeiten können, aber die daraus resultierenden Cluster sind lediglich ein grobes Vorteil der Daten, um dann die Aufteilung mit bestehenden langsameren Methoden wie k-means Clustering zu analysieren. Für hochdimensionale Daten versagen viele der bestehenden Methoden nicht aufgrund des Problems der Dimension, das besondere Fernfunktionen in hochdimensionalen Räumen erschwert. Dies führte zu neuen Cluster-Algorithmen für hochdimensionale Daten, die sich auf Teilspace-Clustering konzentrieren (wo nur einige Eigenschaften verwendet werden, und Clustermodelle umfassen die einschlägigen Eigenschaften des Clusters) und Korrelationsclustern, die auch für willkürliche (korlinke) Teilspace-Cluster, die durch eine Korrelation ihrer Eigenschaften modelliert werden können. Beispiele für solche Cluster-Algorithmen sind CLIQUE und SUBCLU. Ideen aus dichtebasierten Clusterbildungsmethoden (insbesondere die DBSCAN/OPIKK-Familie von Algorithmen) wurden an Teilspace-Clustering (HiSC, hierarchische Teilspace Clustering und DiSH) und Korrelationscluster (HiCO, hierarchische Korrelationscluster, 4C mit "Korrelation Konnektivität" und ERiC angepasst, um hierarche dichtebasierte Korrelationscluster zu erkunden). Mehrere unterschiedliche Clusterbildungssysteme auf der Grundlage gegenseitiger Informationen wurden vorgeschlagen. One ist die Art und Weise der Information von Marina Meilă; eine andere bietet hierarchische Clusterbildung.Einsatz von genetischen Algorithmen kann ein breites Spektrum unterschiedlicher Funktionalitäten optimiert werden, einschließlich gegenseitiger Informationen. Überzeugung propagation, eine jüngste Entwicklung in der Informatik und der Statistikphysik, hat zur Entstehung neuer Arten von Cluster-Algorithmen geführt. Bewertung und Bewertung (oder Validierung) von Clusterergebnissen sind so schwierig wie das Cluster selbst. Volksbezogene Ansätze umfassen interne Bewertung, bei der das Cluster zusammen mit einem einzigen Qualitäts-Score zusammengefasst wird, externe Bewertung, bei der die Clusterbildung im Vergleich zu einer bestehenden "Grundregel"-Klassifikation, einer manuellen Bewertung durch einen menschlichen Experten und einer indirekten Bewertung, indem der Nutzen des Clusters in seiner geplanten Anwendung bewertet wird. Interne Bewertungsmaßnahmen leiden unter dem Problem, dass sie Funktionen darstellen, die selbst als Clusterziel angesehen werden können. Zum Beispiel könnte man die Daten, die durch den Vierer-Koeffizient gesetzt wurden, bündeln, es sei denn, es gibt keinen bekannten effizienten Algorithmen. Indem man eine solche interne Bewertungsmaßnahme nutzt, vergleicht man die Ähnlichkeit der Optimierungsprobleme und nicht unbedingt, wie nützlich die Clusterbildung ist. Externe Bewertung hat ähnliche Probleme: wenn wir solche "Grunde Wahrheit"-Labels haben, dann brauchen wir nicht Cluster, und in praktischen Anwendungen, die wir normalerweise nicht haben. Auf der anderen Seite spiegeln die Etiketten nur eine mögliche Aufteilung der Daten fest, was nicht bedeutet, dass es keine andere, vielleicht sogar bessere Clusterbildung gibt. Keine dieser Ansätze kann daher letztlich die tatsächliche Qualität eines Clusters beurteilen, aber dies erfordert eine menschliche Bewertung, die sehr subjektiven Charakter hat. Jedoch können solche Statistiken bei der Ermittlung von schlechten Clustern durchaus informativ sein, aber eine sollte keine subjektive menschliche Bewertung abschaffen. Interne Bewertung Wenn ein Clusterergebnis auf der Grundlage der Daten bewertet wird, die selbst gebündelt wurden, wird dies als interne Bewertung bezeichnet. Diese Methoden verleihen dem Algorithmus, der Cluster mit hoher Ähnlichkeit innerhalb eines Clusters und geringer Ähnlichkeit zwischen Clustern produziert, in der Regel den besten Wert. Ein Rückschritt bei der Anwendung interner Kriterien in der Cluster-Bewertung ist, dass hohe Ergebnisse auf einer internen Maßnahme nicht zwangsläufig zu wirksamen Informationsanfragen führen. Darüber hinaus ist diese Bewertung auf Algorithmen ausgerichtet, die dasselbe Clustermodell verwenden. K-Meeres, die natürliche Optimierung von Objektabflüssen, und ein entfernungsbasiertes internes Kriterium wird die entstehende Clusterbildung wahrscheinlich übersteigen. Die internen Bewertungsmaßnahmen sind daher am besten geeignet, einen Einblick in Situationen zu erhalten, in denen ein Algorithmus besser als ein anderer funktioniert, aber dies bedeutet nicht, dass ein Algorithmus mehr gültige Ergebnisse als ein anderer produziert. Validierung, gemessen an einem solchen Index, hängt davon ab, dass diese Art von Struktur in der Datensammlung existiert. Ein Algorithmus, der für eine Art von Modellen entworfen wurde, hat keine Chance, wenn die Daten festgesetzt werden, eine radikal unterschiedliche Reihe von Modellen enthält oder wenn die Bewertung ein radikal anderes Kriterium darstellt. z.B. k-means Clustering kann nur Konvex-Cluster finden, und viele Evaluierungsindexe nehmen Konvex-Cluster an. Keine Daten mit nicht konvexen Clustern, weder die Verwendung von k-means noch ein Bewertungskriterium, das Konvexity übernimmt, sind tragfähig. Mehr als ein Dutzend interner Bewertungsmaßnahmen gibt es in der Regel auf der Unterweisung, dass Gegenstände im gleichen Cluster höher sein sollten als in verschiedenen Clustern. Zum Beispiel können die folgenden Methoden verwendet werden, um die Qualität von Clustern-Algorithmen nach internen Kriterien zu bewerten: Davies-Bouldin Index Der Index Davies-Bouldin kann anhand folgender Formel berechnet werden: D B = 1 n  i i = 1 n max j  i i ( i i +  j j d ( c i, c j ) ) 7.8displaystyle DB=1,0frac 1}{n _sum _i=1}^{n}\max {_j\neq i}\left(Getfrac ggiosigma) {_i}+\sigma j{d(c_{i},c_{j}) ofright, wo n die Zahl der Cluster ist, c i {\displaystyle c_{i} ist die Zenroid von Cluster i KINGstyle i} ,  i i KINGstyle \sigma {_i} ist die durchschnittliche Entfernung aller Elemente in Cluster i {\displaystyle i} auf Centroid c i {\displaystyle c_{i} und d ( c i , c ) Memestyle d(c_{i},c_{j) ist die Entfernung zwischen Centroiden c {\displaystyle c_{i} und c displaystyle c_{j} .Since-Algorithmen, die Cluster mit niedrigen intra-Cluster-Entfernungen (high intra-Cluster vergleichbareity) und hohen Inter-Clusterentfernungen (niedrige Inter-Cluster- Ähnlichkeiten) produzieren, werden einen niedrigen Index von Davies–Bouldin aufweisen, dem Cluster-Algorithmus, der eine Sammlung von Clustern mit dem kleinsten Davies-Bouldin Index produziert, wird als der beste Algorithmus auf der Grundlage dieses Kriteriums angesehen. Dunn Index Ziel des Dunn Index ist die Ermittlung der dichten und gut getrennten Cluster. Es ist definiert als Verhältnis zwischen der minimalen Inter-Clusterentfernung bis zur maximalen intra-Cluster- Entfernung. Für jede Clusteraufteilung kann der Dunn Index anhand folgender Formel berechnet werden: D = min 1 ≤ i n d ( i , j ) max 1 ≤ k n d ′ ( k ) , Memedisplaystyle D= } 7.8frac {_1\leq i n}d(i,j)}{\max_1\leq kleq n n}d^{\ (k)  betweenk)  between (d)  betweenk) und i-cluster, d)  j- und  i-Cluster(i) Die Inter-Clusterentfernung d(i,j) zwischen zwei Clustern kann eine Reihe von Fernmaßnahmen wie die Entfernung zwischen den Centroiden der Cluster sein. Gleichermaßen kann die intra-Clusterentfernung d '(k) auf unterschiedliche Weise gemessen werden, wie z.B. die maximale Entfernung zwischen einzelnen Elementen in Cluster k. Seit dem internen Kriterium werden Cluster mit hoher intra-Cluster- Ähnlichkeit und geringer Inter-Cluster-Dimension angestrebt, sind Algorithmen, die Cluster mit hohem Dunn-Index produzieren, wünschenswerter. Turkoeffizient In diesem Cluster unterscheidet sich der Axkoeffizient mit der durchschnittlichen Entfernung zu Elementen in anderen Clustern. Objekte mit einem hohen Ax-Wert werden als gut gefächert angesehen, Gegenstände mit geringem Wert können sich herausstellen. Dieser Index funktioniert gut mit k-means Clustering und wird auch verwendet, um die optimale Anzahl von Clustern zu bestimmen. Externe Bewertung In der externen Bewertung werden die Clusterergebnisse anhand von Daten bewertet, die nicht für Clustering verwendet wurden, wie bekannte Klassenetiketten und externe Benchmarks. Solche Benchmarks bestehen aus einer Reihe von vorrangigen Gegenständen, und diese Sätze werden oft von (experten) Menschen erstellt. So können die Benchmarks als Goldstandard für die Bewertung betrachtet werden. Diese Arten von Bewertungsmethoden messen, wie nah die Clustering an den vorher festgelegten Benchmark-Klassen ist. Kürzlich wurde jedoch diskutiert, ob dies für echte Daten angemessen ist, oder nur auf synthetischen Daten mit einer tatsächlichen Bodenhaftung, da Klassen interne Strukturen enthalten können, dürfen die vorhandenen Eigenschaften keine Trennung von Clustern oder Klassen enthalten. Darüber hinaus kann die Vervielfältigung des bekannten Wissens nicht unbedingt das angestrebte Ergebnis sein. In einem speziellen Szenario mit eingeschränkter Clusterbildung, in dem Meta-Informationen (wie Klassenetiketten) bereits im Clusterverfahren verwendet werden, ist das Abhalten von Informationen für Evaluierungszwecke nicht trivial. Eine Reihe von Maßnahmen werden von Varianten zur Bewertung von Klassifikationsaufgaben angepasst. Zur Zählung der Zahl der Zeiten wurde eine Klasse korrekt einem einzigen Datenpunkt (bekannt als echte positive) zugewiesen, so dass die Kombinationsparameter beurteilen, ob jedes Paar von Datenpunkten, das wirklich im gleichen Cluster liegt, in demselben Cluster vorhergesagt wird. Wie bei der internen Bewertung gibt es mehrere externe Bewertungsmaßnahmen, zum Beispiel: Purity: Purity ist eine Maßnahme, in der Cluster eine einzelne Klasse enthalten. Ihre Berechnung kann wie folgt aussehen: Jedes Cluster zählt die Anzahl der Datenpunkte aus der am häufigsten genannten Klasse. Jetzt nehmen wir die Summe aller Cluster und teilen sich durch die Gesamtzahl der Datenpunkte. Formell, da einige Bündel von Clustern M HANAdisplaystyle M} und einige Klassen D Memedisplaystyle D} , beide Trennlinien N HANAdisplaystyle N} Datenpunkte, Reinheit kann definiert werden als: 1 N  max m  M M max d  | D. 1 N sum {_m\in M}\max {_d\in D||m\cap · Diese Maßnahme ist nicht mit vielen Clustern bestraft, und mehr Cluster werden es einfacher machen, eine hohe Reinheit zu erzeugen. Ein Reinheitswert von 1 ist immer möglich, indem jeder Datenpunkt in seinem eigenen Cluster platziert wird. Reinheit funktioniert auch nicht gut für unausgewogene Daten, wo sogar schlecht funktionierende Cluster-Algorithmen einen hohen Reinheitswert bieten. Liegt beispielsweise eine Größe von 1000 Datenset aus zwei Klassen, einem mit 999 Punkten und dem anderen, der 1 Punkt enthält, so wird jede mögliche Trennmenge mindestens 99,9% betragen. Randindex In dem Randindex wird untersucht, wie ähnliche Cluster (die durch den Cluster-Algorithmus ausgelöst werden) die Benchmark-Klassifikationen sind. Sie kann anhand der folgenden Formel berechnet werden: R I = T P + T N T P + F P + F N + T N HANAdisplaystyle RI=TONfrac TP+TN}{TP+FN+TN, wo T P WELLdisplaystyle TP} die Zahl der echten positiven Effekte ist, T N {\displaystyle TN} ist die Zahl der tatsächlichen negativen, F P TONdisplaystyle FP} ist die Zahl der falschen positiven Effekte, und F N Memestyle FN} ist die Zahl der falschen Negative. Die hier gezählten Fälle sind die Anzahl der korrekten Doppelaufträge. Dies ist die Anzahl der Punkte, die in der vorhergesagten Teilung und in der Bodenhaftung zusammengelegt werden, F P Memestyle FP} ist die Anzahl der Punkte, die in der vorhergesagten Teilung zusammengefasst sind, aber nicht in der Boden Wahrheitsverteilung usw. Kommt der Datensatz in der Größe N, dann T P + T N + F P + F N = (N 2 ) HANAdisplaystyle TP+TN+FN= N}{2 .Oneissue with the Rand Index ist, dass falsche positive und falsche negative Auswirkungen gleichermaßen gewichtet werden. Dies kann ein unerwünschtes Merkmal für einige Cluster-Anwendungen sein. In der F-Maßnahme geht es um dieses Problem, wie auch um den anpassbaren Randindex. F-Maßnahme Die F-Maßnahme kann verwendet werden, um den Beitrag von falschen negativen Nachteilen durch Gewichtung durch einen Parameter β ≥ 0 Memestyle \beta \geq auszugleichen 0}, Präzision und Rückruf (beide externen Evaluierungsmaßnahmen selbst) werden wie folgt definiert: P = T P + F P HANAdisplaystyle P= LANDfrac TP+TP+FP R = T P T P + F N LANDdisplaystyle R= Finanzfrac TP+TP+FN, wo P WELLdisplaystyle P} die Präzisionsrate und R KINGstyle R} ist der Rückrufsatz. Wir können die F-Maßnahme anhand folgender Formel berechnen: F β = ( β 2 + 1 )  P P  R R β 2 ⋅ P + R KINGstyle F_Getbeta {(\beta {^2}+1)\cdot P\cdot R}{\beta {^2}\cdot P+R} Wann β = 0 Kaffeestyle \beta =0} , F 0 = P Memestyle F_{0}=P .In anderen Worten hat Rückruf keine Auswirkungen auf die F-Maßnahme, wenn β = 0 {\displaystyle \beta =0} und zunehmend β Memestyle \beta } einen zunehmenden Gewicht auf die letzte F-Maßnahme zugewiesen. T N KINGstyle TN} wird nicht berücksichtigt und kann von 0 nach oben variieren. Jaccard Index Jaccard Index wird verwendet, um die Ähnlichkeit zwischen zwei Datensätzen zu quantifizieren. Der Jaccard-Index hat einen Wert zwischen 0 und 1. Ein Index von 1 bedeutet, dass die beiden Datensätze identisch sind, und ein Index von 0 zeigt, dass die Datensätze keine gemeinsamen Elemente aufweisen. Jaccard Index wird in der folgenden Formel definiert: J (A , B ) = | A  B B | A  B B | = T P + F P + F P + F N RARstyle J(A,B)= {A\cap B|}{A\cup BUST== TP+TP+FP+FN Dies ist einfach die Zahl der einzigartigen Elemente, die für beide Set gemeinsam sind, aufgeteilt durch die Gesamtzahl der einzigartigen Elemente in beiden Sätzen. T N KINGstyle TN} wird nicht berücksichtigt und kann von 0 nach oben variieren. Dice Index Die Dice symmetrische Maßnahme verdoppelt das Gewicht auf T P {\displaystyle TP}, während die T N {\displaystyle TN} : D S C = 2 T P + F P + F P + F P + F N HANAdisplaystyle DSC= ggiofrac 2TP22TP+FP+FN Fowkes – Mombsindex In den Foppkes-Mombs-Index wird die Ähnlichkeit zwischen den Clustern berechnet, die durch den Cluster-Algorithmus und die Benchmark- Klassifikationen zurückgeführt wurden. Je höher der Wert der Foppkes-Ms-Index ist, desto größer sind die Cluster und die Benchmark-Klassifikationen. Es kann anhand der folgenden Formel berechnet werden: F M = T P + F P  T T P + F N {\displaystyle M= LAND TP+TP+FP}}\cdot Memefrac TP}{TP+FN, wo T P displaystyle TP} die Zahl der tatsächlichen positiven Effekte ist, F P SSOdisplaystyle FP} ist die Zahl der falschen positiven Effekte, und F N VILLEdisplaystyle FN} ist die Zahl der falschen Nachteile. Der F M Memedisplaystyle FM} Index ist das geometrische Mittel der Genauigkeit und erinnert an P WELLdisplaystyle P} und R {\displaystyle R} und ist damit auch bekannt als G-Maßnahme, während die F-Maßnahme ihre harmonische Wirkung darstellt.Präzision und Rückruf sind darüber hinaus auch bekannt als die Indizes B I {\displaystyle B BI} und B I Memedisplaystyle BIIII} .Chance normalisierte Repräsentations-, Präzisions- und G-Maßnahmen entsprechen der Information, Markedness und Matthews Correlation und beziehen sich stark auf Kappa. Die gegenseitigen Informationen sind eine Informationsmaßnahme, wie viel Informationen zwischen einer Clusterbildung und einer Boden-Flagge-Klassifikation ausgetauscht werden, die eine nichtlineare Ähnlichkeit zwischen zwei Clustern erkennen können. Normalisierte gegenseitige Informationen sind eine Familie von Berichtigungs-for-Tance-Varianten, die für unterschiedliche Cluster-Nummern eine geringere Verzerrung aufweisen. KonfusionsmatrixA Verwirrungsmatrix kann verwendet werden, um die Ergebnisse eines Klassifikations-(oder Clustering-)Algorithmus schnell zu erkennen. Es zeigt, wie unterschiedlich ein Cluster aus dem Gold Standard-Cluster ist. Cluster-Trend Messung der Cluster-Trends ist es, zu messen, was in den zu bündelnden Daten vorhanden ist, und kann als erster Test durchgeführt werden, bevor man Clustern sucht. Ein Weg, um die Daten gegen Zufallsdaten zu vergleichen. Durchschnittlich sollten Zufallsdaten keine Cluster haben. Kinderstatistik Es gibt mehrere Formulierungen der Hopkins-Statistik. Ein typisches ist wie folgt. Letztlich X KINGstyle X} ist das Set von n {\displaydisplaystyle n} Datenpunkte in d Memestyle d}. Prüfung einer Zufallsstichprobe (ohne Ersatz) von m   n {\displaystyle m\ll n} Datenpunkte mit Mitgliedern x i displaystyle x_{i} . Auch produziert ein Set Y WELLdisplaystyle Y} von m KINGstyle m}, gleichmäßig verteilte Datenpunkte. Jetzt werden zwei Fernmaßnahmen festgelegt, u i WELLdisplaystyle u_{i}, um die Entfernung von y i  Y Y HANAstyle y_{i}\in Y} von seinem nächsten Nachbarn in X und w i {\displaystyle w_{i}, um die Entfernung von x i  X X {\displaystyle x_{i}\in X} von seinem nächsten Nachbarn in X zu sein. Wir definieren dann die Statistik des Gehirns als H =  i i = 1 m u i d  i i = 1 m u i d +  i i = 1 m w i d , H= LAND i=1}{m_u_{i}^{d}}}{\sum i=1}{m_u_{i}^{d++\sum i=1}^{m_w_{i}^{d,\ Mit dieser Definition sollten einheitliche zufällige Daten in der Regel Werte in der Nähe von 0,5 aufweisen, und Clusterdaten sollten in der Regel Werte in der Nähe von 1.Wie immer aufweisen, Daten, die nur einen einzigen Gaussian enthalten, werden in der Nähe von 1, da diese statistischen Maßnahmen von einer einheitlichen Verteilung abweichen, nicht multimodalität, so dass diese Statistiken in der Anwendung weitgehend unfrei sind (wie reale Daten nie einheitlich sind). Anwendungen Biologie, Rechenbiologie und Bioinformatik Pflanzen- und Tierökologie-Clusteranalyse werden verwendet, um räumliche und zeitliche Vergleiche von Gemeinschaften (Bestände) von Organismen in heterogenen Umgebungen zu beschreiben. Es wird auch in Pflanzen systematischen verwendet, um künstliche Phylogenien oder Gruppen von Organismen (Einzelpersonen) auf der Art, Art oder Höhe zu erzeugen, die eine Reihe von Eigenschaften aufweisen. Transkriptomik Clustering wird verwendet, um Gruppen von Genen mit verwandten Ausdrucksmustern (auch als „gemischte Gene“ bekannt) wie im HCS-Cluster-Algorithmus zu bauen. Häufig enthalten solche Gruppen funktionell verwandte Proteine wie Enzyme für einen bestimmten Weg oder Gene, die gemeinsam reglementiert sind. Hochdurchsatzversuche, die mit bestimmten Sequenzetiketten (EST) oder DNA-Mikroarrays verwendet werden, können ein wirksames Instrument für die Genomnotierung sein – ein allgemeiner Aspekt der Genomik. Folgenabschätzung Sequence Clustering wird verwendet, um homologe Sequenzen in Genefamilien zu vereinen. Dies ist ein sehr wichtiges Konzept in der Bioinformatik und der Evolutionsbiologie im Allgemeinen. Lesen Sie die Entwicklung durch Genalzer. High- Byput-Genotyping-Plattformen Clustering-Algorithmen werden verwendet, um Genotypen automatisch zuzuweisen. menschliches genetisches Cluster Die Ähnlichkeit der genetischen Daten wird bei der Bündelung der Bevölkerungsstrukturen verwendet. Medizin On PET-Scans können Clusteranalysen verwendet werden, um zwischen verschiedenen Arten von Geweben in einem dreidimensionalen Bild für viele verschiedene Zwecke zu unterscheiden. Analyse der antimikrobiellen Aktivität Clusteranalysen können verwendet werden, um die Muster der Antibiotikaresistenz zu analysieren, antimikrobielle Verbindungen nach ihrem Handlungsmechanismus einzustufen, Antibiotika nach ihrer antibakteriellen Aktivität einzustufen. IMRT-Segment Clustering kann genutzt werden, um eine Grippekarte in unterschiedliche Regionen für die Umwandlung in tragfähige Bereiche in die Strahlentherapie von MLC aufzuteilen. Unternehmens- und Marketing-Marktforschung Cluster-Analysen werden in der Marktforschung weit verbreitet, wenn sie mit multivarialen Daten aus Umfragen und Testgruppen zusammenarbeiten. Marktforscher nutzen Clusteranalyse, um die allgemeine Bevölkerung der Verbraucher in Marktsegmente zu teilen und die Beziehungen zwischen verschiedenen Verbraucher-/potenten Kundengruppen besser zu verstehen und in Marktsegmentierung, Produktposition, neue Produktentwicklung und Auswahl von Testmärkten zu verwenden. Bündelung von Einkaufsartikeln Clustering kann genutzt werden, um alle auf dem Web verfügbaren Einkaufsgüter in eine Reihe von einzigartigen Produkten zu vereinen. Beispielsweise können alle Gegenstände auf eBay in einzigartige Produkte zusammengefasst werden (eBay hat nicht das Konzept einer SKU). World Wide Web Social Network Analyse In der Studie über soziale Netzwerke kann Clusterbildung genutzt werden, um Gemeinschaften in großen Gruppen von Menschen zu erkennen. Suchergebnisgruppe In intelligenter Zusammenlegung der Dateien und Websites kann Clustering verwendet werden, um ein relevanteres Bündel von Suchergebnissen im Vergleich zu normalen Suchmaschinen wie Google zu erstellen. Derzeit gibt es eine Reihe von Web-basierten Cluster-Werkzeugen wie Clusty. In Fällen, in denen ein Suchtermin auf eine Vielzahl unterschiedlicher Dinge verweisen könnte, kann es auch dazu dienen, ein umfassenderes Ergebnis zu finden. Jede gesonderte Nutzung des Begriffs entspricht einer einzigartigen Reihe von Ergebnissen, die einen Ranking-Algorithmus ermöglichen, umfassende Ergebnisse zu erzielen, indem sie das Top-ergebnis jedes Clusters zieht. Kartografie Flickrs Karte von Fotos und anderen Karten-Websites verwenden Clustering, um die Anzahl der Marker auf einer Karte zu reduzieren. Dies macht es sowohl schneller als auch senkt die Menge des visuellen Druckers. Entwicklung der Computer-Software Clustering ist in der Softwareentwicklung sinnvoll, da es dazu beiträgt, die Alteigenschaften im Code durch eine Reformierung der Funktionalität zu verringern, die sich ausbreitet. Es handelt sich um eine Form der Umstrukturierung und ist daher eine Möglichkeit der direkten präventiven Wartung. Bildsegmentierung Clustering kann genutzt werden, um ein digitales Bild in unterschiedlichen Regionen für die Grenzerkennung oder die Anerkennung zu spalten. Entwicklung von Algorithmen Clustering kann genutzt werden, um unterschiedliche Nischen innerhalb der Bevölkerung eines evolutionären Algorithmus zu ermitteln, damit die Fortpflanzungsmöglichkeiten besser unter den sich entwickelnden Arten oder Unterarten verteilt werden können. Empfohlene Systeme Empfohlene Systeme sind so konzipiert, dass neue Produkte auf der Grundlage des Geschmacks eines Benutzers empfohlen werden. manchmal verwenden sie Cluster-Algorithmen, um die Präferenzen der Nutzer auf der Grundlage der Präferenzen anderer Nutzer im Nutzercluster vorherzusagen. Markov Kette Monte Carlo Methoden Clustering wird häufig genutzt, um Extrema in der Zielverteilung zu finden. Ungewöhnliche Erkennung Anomalien/Outliers sind in der Regel explizit oder implizit – definiert in Bezug auf die Clusterstruktur in Daten. natürliche Sprache Verarbeitung Clustering kann genutzt werden, um lexical ambiguity zu lösen. Analyse von Social Science Crime Analysis Cluster-Analysen können genutzt werden, um Bereiche zu ermitteln, in denen es größere Kriminalitätshäufigkeiten gibt. Indem sie diese unterschiedlichen Bereiche oder "Hotspots" ermitteln, in denen ein ähnliches Verbrechen über einen Zeitraum stattgefunden hat, ist es möglich, die Strafverfolgungsmittel effizienter zu verwalten. Bildungsdaten Bergbau Clusteranalyse wird beispielsweise verwendet, um Gruppen von Schulen oder Studenten mit ähnlichen Eigenschaften zu identifizieren. Typologien aus Umfragedaten, Projekte wie die vom Pew Research Center durchgeführten Projekte verwenden eine Clusteranalyse, um Typologien von Meinungen, Gewohnheiten und Demografien zu erkennen, die in Politik und Marketing nützlich sein könnten. Andere Feldroboter Clustering-Algorithmen werden für die Sensibilisierung der Robotik verwendet, um Objekte zu verfolgen und Ausbrüche in Sensordaten zu erkennen. Mathematik Um strukturelle Ähnlichkeiten zu finden, z.B. wurden 3000 chemische Verbindungen im Raum von 90 Spitzenindikatoren zusammengefasst. Klimatologie Wettersysteme oder bevorzugter Druck auf den Meeresspiegel zu finden. Finanzclusteranalysen wurden für Cluster-Bestände in Sektoren verwendet. Erdölgeologie Clusteranalyse wird verwendet, um fehlende Kerndaten oder fehlende Logkurven zu reaktivieren, um die Speichereigenschaften zu bewerten. Geochemie Konzentration chemischer Eigenschaften an verschiedenen Stichprobenstandorten. Siehe auch spezielle Arten der Clusteranalyse automatische Cluster-Algorithmen Ausgewogene Clusterbildung Clustering hochdimensionale Datenkonzeptuale Clustering Consensus Clustering Constrained Clustering Community Erkennungsdaten Streaming HCS Clustering Sequence Clustering Spectral Clustering-Clustering-Techniken, die in der Clusteranalyse künstlicher Neuralnetzen (ANN)Nearest benachbarter Nachbarschaftskomponenten-Analysen zur Analyse der konglomee A Datenprojektion und zur Vorverarbeitungskomponente Analyse der Hauptkomponenten Multidimensionale Analyse von Cluster- gewichteter Modellierung der Dimension Abgrenzung der Anzahl der Cluster in einer parallelen Koordinierung der Strukturierten Datenanalyse Links