The Future of Humanity Institute (FHI) ist ein interdisziplinäres Forschungszentrum an der Universität Oxford, in dem Fragen der Menschheit und ihre Perspektiven untersucht werden. 2005 wurde sie als Teil der Philosophie und der Oxford Martin School gegründet. Ihr Direktor ist Philosoph Nick Bostrom und seine Forschungsmitarbeiter und -kollegen umfassen Futurist Anders Sandberg, Ingenieur K. Eric Drexler, Wirtschaftswissenschaftler Robin Hanson und Giving Was We Can Gründer Toby Ord.Sharing ein Büro und eng mit dem Zentrum für Wirksamkeit Altruismus zusammenarbeiten, hat das vom Institut angekündigte Ziel, die Forschung zu konzentrieren, wo es langfristig den größten positiven Unterschied für die Menschheit machen kann. Sie setzt sich in eine Mischung aus akademischen und Sensibilisierungsaktivitäten ein, um fundierte Diskussionen und öffentliches Engagement in Regierung, Unternehmen, Hochschulen und anderen Organisationen zu fördern. Nick Bostrom hat das Institut im November 2005 als Teil der Oxford Martin School gegründet, dann die James Martin 21st Century School. FHI war zwischen 2008 und 2010 Gastgeber der Global Catastrophic Risks Conference, schrieb 22 akademische Fachzeitschriften und veröffentlichte 34 Kapitel in akademischen Mengen. FHI-Forscher wurden über 5000 Mal in den Medien erwähnt und haben dem Weltwirtschaftsforum, dem privaten und gemeinnützigen Sektor (wie der Macartian Foundation und der Weltgesundheitsorganisation) sowie den staatlichen Stellen in Schweden, Singapur, Belgien, dem Vereinigten Königreich und den Vereinigten Staaten politische Ratschläge erteilt. Bostrom und Bioethist Julian Savulescu haben im März 2009 auch das Buch Human Promotion veröffentlicht. Kürzlich konzentrierte sich die FHI auf die Gefahren moderner künstlicher Intelligenz (AI). 2014 veröffentlichte ihre Forscher mehrere Bücher zum AI-Risiko, darunter die Superintelligence von Stuart Armstrong und die Superintelligence von Bostrom: Paths, Dangers, Strategien. Im Jahr 2018 empfahl das Open Philanthropy-Projekt einen Zuschuss von bis zu 4,4 Mio. £ an die FHI über drei Jahre, wobei ein großer Teil davon an die erfolgreiche Einstellung gebunden ist. vorhandenes Risiko Das größte Thema der FHI hat Zeit zu erkunden, ist ein globales katastrophales Risiko, und insbesondere das vorhandene Risiko. In einem Papier von 2002 hat Bostrom ein "ein vorhandenes Risiko" definiert als "wo ein negatives Ergebnis entweder ein anihilatees, intelligentes Leben oder eine dauerhafte und drastische Eindämmung seines Potenzials wäre. Dies umfasst Szenarien, in denen die Menschheit nicht unmittelbar geschädigt ist, aber es versäumt, den Raum zu kollidieren und die verfügbaren Ressourcen des unservierbaren Universums in menschlichen wertvollen Projekten zu nutzen, wie in Bostroms Papier 2003 diskutiert, "A Astro-Abfall: Die Kosten der verzögerten technologischen Entwicklung". Bostrom und Mailand irirkovićs Buch „Global Catastrophic Risks“ 2008 sammeln Beiträge zu einer Vielzahl solcher Risiken, sowohl natürlichen als auch anthropogen. Mögliche katastrophale Risiken aus der Natur sind Supervolcanismus, Folgen und energische Ereignisse wie z.B. Gamma-ray-Rollen, kosmische Strahlung, Solarflanzen und Supernovae. Diese Gefahren sind relativ klein und relativ gut verstanden, obwohl Pandemien Ausnahmen als Folge mehr Gemeinsamer sein können und sich mit technologischen Trends auseinandersetzen können. synthetische Pandemien über gepanzerte biologische Wirkstoffe werden von FHI stärker berücksichtigt. Technologische Ergebnisse des Instituts interessieren sich insbesondere für anthropogene Klimaänderungen, nukleare Kriegsführung und nuklearer Terrorismus, molekulare Nanotechnologie und künstliche allgemeine Intelligenz. In Erwartung der größten Risiken, die sich aus künftigen Technologien ergeben, und insbesondere aus fortgeschrittenen künstlichen Erkenntnissen kommt FHI mit anderen bestehenden Risikominderungsorganisationen überein, wie dem Zentrum für die Untersuchung bestehender Risiken und dem Forschungsinstitut für Maschinen und Geräte. FHI-Forscher haben auch die Auswirkungen des technologischen Fortschritts auf soziale und institutionelle Risiken untersucht, wie Totalitatismus, automatisierte Arbeitslosigkeit und Informationsrisiken. FHI Senior Research Fellow Toby 2020 Ord veröffentlichte sein Buch Precipice: Bestehendes Risiko und die Zukunft der Menschheit, in der er argumentiert, dass die Wahrung der Zukunft der Menschheit zu den wichtigsten moralischen Fragen unserer Zeit gehört. Anthropische Begründung der FHI widmet sich viel ihrer Aufmerksamkeit auf exotische Bedrohungen, die von anderen Organisationen wenig erforscht wurden, sowie auf methodische Überlegungen, die eine angemessene Risikominderung und Prognose in Kenntnis setzen. Insbesondere hat das Institut anthropische Gründe in seiner Forschung hervorgehoben, als erschlossenes Gebiet mit allgemeinen epistemologischen Auswirkungen. Anthropische Argumente FHI haben untersucht, darunter das doomsday Argument, das behauptet, dass die Menschheit wahrscheinlich bald zum Aussterben kommen wird, weil es unwahrscheinlich ist, dass ein Punkt in der menschlichen Geschichte, der sehr früh ist. Stattdessen werden die heutigen Menschen in der Nähe der Mitte der Verteilung von Menschen sein, die jemals leben. Bostrom hat auch das Simulationsgedanke bekannt gegeben. Ein wiederkehrendes Thema in der FHI-Forschung ist das Fermi Paradox, das überraschende Fehlen konservierbarer fremder Zivilisationen. Robin Hanson hat argumentiert, dass es ein "Great Filter" geben muss, um die Raumkolonie zu verhindern, um dem Paradox Rechnung zu tragen. Dieser Filter kann in der Vergangenheit liegen, wenn die Intelligenz viel seltener ist als die derzeitige Biologie vorhersagen würde; oder es kann in Zukunft liegen, wenn die vorhandenen Risiken noch größer sind als heute anerkannt. menschliche Verbesserung und Rationalisierung eng mit der Arbeit der FHI in Bezug auf Risikobewertung, Astrofall und die Gefahren künftiger Technologien verknüpft sind ihre Arbeit auf das Versprechen und die Risiken der menschlichen Verbesserung. Die betreffenden Änderungen können biologisch, digital oder soziologische sein, und es wird ein Schwerpunkt auf die radikalsten hypothetischen Veränderungen gelegt, statt auf die gleichartigen kurzfristigen Innovationen. FHIs Bioethikforschung konzentriert sich auf die möglichen Folgen der Gentherapie, der lebenslangen Erweiterung, der Gehirnimplantate und der Gehirn-Computer-Schnittstellen und der Aufnahme von Köpfen. FHI konzentrierte sich auf Methoden zur Bewertung und Verbesserung der menschlichen Intelligenz und Rationalität, um die Geschwindigkeit und Richtung des technologischen und sozialen Fortschritts zu gestalten. FHIs Arbeit an menschlicher Reizbarkeit, wie er in kognitiven Hemmern und Verdrängungen verbreitet ist, umfasst eine laufende Zusammenarbeit mit Amlin, um das systemische Risiko zu untersuchen, das sich aus Verzerrungen im Modell ergibt. Ausgewählte Veröffentlichungen Toby Ord: Das Präcipice: vorhandenes Risiko und die Zukunft der Menschheit, 2020.ISBN 1526600218 Nick Bostrom: Superintelligence: Paths, Dangers, Strategien, 2014ISBN 0-415-93858-9 Nick Bostrom und Mailand Cirkovic: Global Catastrophic Risks, 2011ISBN gegen0-1957050-9 Nick Bostrom und Julian Savulescu: Human Promotion, 2011ISBN 0-19-929972-2 Nick Bostrom: Anthropic Bias: Wahlwirkungen in Wissenschaft und Philosophie, 2010ISBN 015-938-958-958-930 Nick Bostrom und Anders Sandberg: Fahrplan 2008 Siehe auch Verweise Externe Links FHI offizielle Webseite