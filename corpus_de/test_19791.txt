Computer-Vision ist ein interdisziplinäres wissenschaftliches Feld, das sich darum kümmert, wie Computer aus digitalen Bildern oder Videos ein hohes Verständnis gewinnen können. Aus der Sicht der Technik versucht sie, Aufgaben zu verstehen und zu automatisieren, die das menschliche visuelle System tun kann. Computer-Visionsaufgaben umfassen Methoden zum Erfassen, Verarbeiten, Analysieren und Verstehen digitaler Bilder und zur Extraktion hochdimensionaler Daten aus der realen Welt, um numerische oder symbolische Informationen, z.B. in Form von Entscheidungen, zu erzeugen. Verständnis in diesem Zusammenhang bedeutet die Transformation von visuellen Bildern (die Eingabe der Netzhaut) in Beschreibungen der Welt, die Sinn für Gedankenprozesse machen und geeignete Handlungen hervorrufen können. Dieses Bildverstehen kann als das Verzicht auf symbolische Informationen aus Bilddaten mit Modellen betrachtet werden, die mit Hilfe von Geometrie, Physik, Statistik und Lerntheorie aufgebaut sind. Die wissenschaftliche Disziplin der Computervision beschäftigt sich mit der Theorie hinter künstlichen Systemen, die Informationen aus Bildern entnehmen. Die Bilddaten können viele Formen annehmen, wie Videosequenzen, Ansichten von mehreren Kameras, mehrdimensionale Daten von einem 3D-Scanner oder einem medizinischen Scangerät. Die technologische Disziplin der Computervision versucht, ihre Theorien und Modelle auf den Bau von Computer-Vision-Systemen anzuwenden. Unterdomänen der Computer-Vision umfassen Szenenrekonstruktion, Objekterkennung, Ereigniserkennung, Video-Tracking, Objekterkennung, 3D-Pose-Schätzung, Lernen, Indexierung, Bewegungsschätzung, visuelle Servoing, 3D-Szene-Modellierung und Bildwiedergabe. Definition Computer Vision ist ein interdisziplinäres Feld, das sich darum kümmert, wie Computer aus digitalen Bildern oder Videos auf hoher Ebene verstanden werden können. Aus der Sicht der Technik versucht sie, Aufgaben zu automatisieren, die das menschliche visuelle System tun kann. "Computer Vision beschäftigt sich mit der automatischen Extraktion, Analyse und dem Verständnis von nützlichen Informationen aus einem einzigen Bild oder einer Folge von Bildern. Es umfasst die Entwicklung einer theoretischen und algorithmischen Basis, um ein automatisches visuelles Verständnis zu erreichen. " Als wissenschaftliche Disziplin beschäftigt sich die Computervision mit der Theorie hinter künstlichen Systemen, die Informationen aus Bildern entnehmen. Die Bilddaten können viele Formen annehmen, wie Videosequenzen, Ansichten von mehreren Kameras oder mehrdimensionale Daten von einem medizinischen Scanner. Als technologische Disziplin will die Computervision ihre Theorien und Modelle für den Bau von Computer-Vision-Systemen anwenden. Geschichte In den späten 1960er Jahren begann die Computer-Vision an Universitäten, die künstliche Intelligenz Pionier. Es sollte das menschliche visuelle System als Sprungbrett für Roboter mit intelligentem Verhalten imitieren. 1966 wurde angenommen, dass dies durch ein Sommerprojekt erreicht werden konnte, indem eine Kamera an einem Computer befestigt und "beschreiben, was sie sah". Was die Computersicht aus dem vorherrschenden Bereich der digitalen Bildverarbeitung damals unterschieden war ein Wunsch, dreidimensionale Struktur aus Bildern zu extrahieren, mit dem Ziel, das vollständige Szenenverständnis zu erreichen. Studien in den 1970er Jahren bildeten die frühen Grundlagen für viele der heutigen Computer-Visionsalgorithmen, einschließlich der Entnahme von Kanten aus Bildern, der Markierung von Linien, der nicht-polyedralen und polyedralen Modellierung, der Darstellung von Objekten als Verbindung von kleineren Strukturen, der optischen Strömung und der Bewegungsschätzung. In den nächsten zehn Jahren wurden Untersuchungen auf der Grundlage strengererer mathematischer Analysen und quantitativer Aspekte der Computersicht durchgeführt. Dazu gehören das Konzept des Skalenraums, die Inferenz der Form aus verschiedenen Cues wie Schattierung, Textur und Fokus und Konturmodelle, die als Schlangen bekannt sind. Forscher erkannten auch, dass viele dieser mathematischen Konzepte innerhalb desselben Optimierungsrahmens behandelt werden könnten, wie Regularisierung und Markov Zufallsfelder. In den 1990er Jahren wurden einige der bisherigen Forschungsthemen aktiver als die anderen. Die Forschung in projektiven 3-D-Rekonstruktionen führte zu einem besseren Verständnis der Kamerakalibrierung. Mit dem Aufkommen von Optimierungsverfahren zur Kamerakalibrierung wurde realisiert, dass viele der Ideen bereits in der Bündelanpassungstheorie aus dem Bereich der Photogrammetrie untersucht wurden. Dies führte zu Methoden für sparsame 3-D-Rekonstruktionen von Szenen aus mehreren Bildern. Fortschritte wurden auf dem dichten Stereo-Korrespondenz-Problem und weiteren Multi-View-Stereo-Techniken gemacht. Gleichzeitig wurden Variationen des Graphenschnitts zur Lösung der Bildsegmentierung verwendet.Dieses Jahrzehnt markierte auch das erste Mal, dass statistische Lerntechniken in der Praxis genutzt wurden, um Gesichter in Bildern zu erkennen (siehe Eigenface). Gegen Ende der 1990er Jahre kam eine signifikante Veränderung mit der verstärkten Interaktion zwischen den Bereichen Computergrafik und Computervision zustande. Dies beinhaltete bildbasierte Rendering, Bildmorphing, Interpolation, Panoramabildheftung und Frühlichtfeld-Rendering. Neuere Arbeiten haben die Resistenz von funktionsbasierten Methoden gesehen, die in Verbindung mit maschinellen Lerntechniken und komplexen Optimierungsrahmen verwendet werden. Die Weiterentwicklung der Deep Learning-Techniken hat das Leben auf den Bereich der Computer-Vision gebracht. Die Genauigkeit von Deep Learning Algorithmen auf mehreren Benchmark Computer Vision-Datensätzen für Aufgaben von Klassifikation, Segmentierung und optischem Fluss hat bisherige Methoden übertroffen. Weitere Felder Solid-state Physik Solid-state Physik ist ein weiteres Feld, das eng mit der Computer Vision verbunden ist. Die meisten Computer-Vision-Systeme verlassen sich auf Bildsensoren, die elektromagnetische Strahlung erfassen, die typischerweise als sichtbares oder Infrarotlicht ausgebildet ist. Die Sensoren werden mittels Quantenphysik entwickelt. Das Verfahren, mit dem Licht mit Oberflächen interagiert, wird anhand der Physik erläutert. Physik erklärt das Verhalten der Optik, die ein Kernteil der meisten Abbildungssysteme sind. Hervorragende Bildsensoren benötigen sogar Quantenmechanik, um ein vollständiges Verständnis des Bildbildungsprozesses zu gewährleisten. Auch können verschiedene Messprobleme in der Physik mit Computervision, beispielsweise Bewegung in Flüssigkeiten, angesprochen werden. Neurobiologie Ein drittes Feld, das eine wichtige Rolle spielt, ist die Neurobiologie, insbesondere die Studie des biologischen Sehsystems. Im letzten Jahrhundert gab es eine umfangreiche Studie über Augen, Neuronen und die Gehirnstrukturen, die der Verarbeitung von visuellen Reize bei Menschen und verschiedenen Tieren gewidmet sind. Dies hat zu einer groben, aber komplizierten Beschreibung der Funktionsweise realer Visionssysteme geführt, um bestimmte visionsbezogene Aufgaben zu lösen. Diese Ergebnisse haben zu einem Teilfeld innerhalb der Computer-Vision geführt, wo künstliche Systeme entworfen sind, um die Verarbeitung und das Verhalten von biologischen Systemen auf unterschiedlichen Ebenen der Komplexität zu mimieren. Auch einige der lernbasierten Methoden, die innerhalb der Computer-Vision entwickelt wurden (z.B. neuronales Netz und Deep Learning basierte Bild- und Merkmalsanalyse und -klassifikation) haben ihren Hintergrund in der Biologie. Einige Stränge der Computer-Vision-Forschung sind eng mit der Studie der biologischen Vision verbunden – in der Tat, so viele Stränge der KI-Forschung eng mit der Forschung in das menschliche Bewusstsein verknüpft sind, und die Verwendung gespeicherter Kenntnisse zur Interpretation, Integration und Nutzung visueller Informationen. Der Bereich der biologischen Vision untersucht und modelt die physiologischen Prozesse hinter der visuellen Wahrnehmung bei Menschen und anderen Tieren. Computervision hingegen untersucht und beschreibt die Prozesse, die in Software und Hardware hinter künstlichen Visionssystemen umgesetzt werden. Interdisziplinärer Austausch zwischen biologischer und computergestützter Vision hat sich für beide Bereiche bewährt. Signalverarbeitung Ein anderes Feld im Zusammenhang mit der Computervision ist die Signalverarbeitung. Viele Verfahren zur Verarbeitung von einvariablen Signalen, typischerweise zeitlichen Signalen, können auf natürliche Weise zur Verarbeitung von zweivariablen Signalen oder mehrvariablen Signalen in der Computersicht erweitert werden. Wegen der spezifischen Art der Bilder gibt es jedoch viele Methoden, die innerhalb der Computervision entwickelt wurden, die bei der Verarbeitung von einvariablen Signalen kein Gegenstück haben. Zusammen mit der Vieldimensionalität des Signals definiert dies ein Teilfeld in der Signalverarbeitung als Teil der Computervision. Robotik Navigation Die Roboternavigation beschäftigt sich manchmal mit der autonomen Wegplanung oder Beratung für Robotersysteme, um durch eine Umgebung zu navigieren. Ein detailliertes Verständnis dieser Umgebungen ist erforderlich, um durch sie zu navigieren. Informationen über die Umwelt könnten von einem Computer-Vision-System bereitgestellt werden, das als Vision-Sensor fungiert und hochrangige Informationen über die Umwelt und den Roboter liefert. Sonstige Felder Neben den oben erwähnten Ansichten zur Computersicht können auch viele der damit verbundenen Forschungsthemen aus rein mathematischer Sicht untersucht werden. Beispielsweise basieren viele Methoden in der Computer-Vision auf Statistiken, Optimierung oder Geometrie. Schließlich ist ein wesentlicher Teil des Feldes dem Implementierungsaspekt der Computervision gewidmet; wie bestehende Methoden in verschiedenen Kombinationen von Software und Hardware realisiert werden können, oder wie diese Methoden modifiziert werden können, um Verarbeitungsgeschwindigkeit zu gewinnen, ohne zu viel Leistung zu verlieren.Computer Vision wird auch in Mode-E-Commerce, Inventarmanagement, Patentsuche, Möbel und der Schönheitsindustrie verwendet. Störungen Die am engsten mit der Computervision verbundenen Felder sind Bildverarbeitung, Bildanalyse und Bildverarbeitung. Es gibt eine signifikante Überschneidung im Bereich der Techniken und Anwendungen, die diese decken. Dies bedeutet, dass die in diesen Feldern verwendeten und entwickelten Grundtechniken ähnlich sind, was interpretiert werden kann, da es nur ein Feld mit unterschiedlichen Namen gibt. Andererseits scheint es für Forschergruppen, wissenschaftliche Zeitschriften, Konferenzen und Unternehmen erforderlich zu sein, sich als eigens zu einem dieser Bereiche gehörend zu präsentieren oder zu vermarkten und somit verschiedene Charakterisierungen, die jedes der Felder von den anderen unterscheiden, vorzustellen. Computergrafiken erzeugen Bilddaten von 3D-Modellen, Computer Vision produziert oft 3D-Modelle aus Bilddaten. Es gibt auch einen Trend zu einer Kombination der beiden Disziplinen, wie sie z.B. in einer erweiterten Realität erforscht werden. Die folgenden Charakterisierungen erscheinen relevant, sollten jedoch nicht als allgemein akzeptiert gelten: Die Bildverarbeitung und Bildanalyse neigen dazu, sich auf 2D-Bilder zu fokussieren, wie man ein Bild in ein anderes umwandelt, z.B. durch pixelweise Operationen wie Kontrastverbesserung, lokale Operationen wie Kantenextraktion oder Rauschentfernung oder geometrische Transformationen, wie etwa das Drehen des Bildes. Diese Charakterisierung bedeutet, dass Bildverarbeitung/Analyse weder Annahmen erfordern noch Interpretationen über den Bildinhalt erzeugen. Computer Vision umfasst 3D-Analyse von 2D-Bildern. Dies analysiert die auf ein oder mehrere Bilder projizierte 3D-Szene, z.B. wie man Struktur oder andere Informationen über die 3D-Szene aus einem oder mehreren Bildern rekonstruiert. Die Computersicht beruht oft auf mehr oder weniger komplexen Annahmen über die in einem Bild dargestellte Szene. Machine Vision ist der Prozess der Anwendung einer Reihe von Technologien & Methoden, um bildgebende automatische Inspektion, Prozesssteuerung und Roboterführung in industriellen Anwendungen bereitzustellen. Machine Vision konzentriert sich tendenziell auf Anwendungen, vor allem in der Fertigung, z.B. visionsbasierte Roboter und Systeme für visionsbasierte Inspektion, Messung oder Picking (wie Bin Picking). Dies bedeutet, dass Bildsensortechnologien und Steuerungstheorie oft mit der Verarbeitung von Bilddaten zur Steuerung eines Roboters integriert sind und dass die Echtzeitverarbeitung durch effiziente Implementierungen in Hardware und Software betont wird. Es bedeutet auch, dass die äußeren Bedingungen wie Beleuchtung in der Bildverarbeitung gesteuert werden können und werden, als sie im allgemeinen Computervision sind, die den Einsatz verschiedener Algorithmen ermöglichen können. Es gibt auch ein Feld namens Bildgebung, das sich in erster Linie auf den Prozess der Bilderzeugung konzentriert, aber manchmal auch auf die Verarbeitung und Analyse von Bildern. Zum Beispiel umfasst die medizinische Bildgebung wesentliche Arbeiten zur Analyse von Bilddaten in medizinischen Anwendungen. Schließlich ist die Mustererkennung ein Feld, das verschiedene Methoden verwendet, um Informationen aus Signalen im Allgemeinen zu extrahieren, hauptsächlich auf der Grundlage statistischer Ansätze und künstlicher neuronaler Netzwerke. Ein wesentlicher Teil dieses Feldes ist die Anwendung dieser Methoden auf Bilddaten. Photogrammetrie überschneidet sich auch mit Computer-Vision, z.B. Stereophotogrammetrie gegen Computer-Stereo-Vision. Anwendungen Anwendungen Die Anwendungen reichen von Aufgaben wie industrielle Bildverarbeitungssysteme, die z.B. Flaschen auf einer Produktionslinie inspizieren, über künstliche Intelligenz und Computer oder Roboter, die die Welt um sie herum verstehen können. Die Computersicht- und Bildverarbeitungsfelder haben eine erhebliche Überschneidung. Computer Vision umfasst die Kerntechnologie der automatisierten Bildanalyse, die in vielen Bereichen verwendet wird. Machine Vision bezieht sich in der Regel auf einen Prozess, der automatisierte Bildanalyse mit anderen Methoden und Technologien kombiniert, um eine automatisierte Inspektion und Roboterführung in industriellen Anwendungen bereitzustellen. In vielen Computer-Vision-Anwendungen werden die Computer vorprogrammiert, um eine bestimmte Aufgabe zu lösen, aber Methoden auf Lernbasis werden zunehmend häufiger. Beispiele für Anwendungen der Computer-Vision sind Systeme für: Automatische Inspektion, z.B. in Fertigungsanwendungen; Bestehende Menschen in Identifikationsaufgaben, z.B. ein Spezies-Identifizierungssystem; Steuerungsprozesse, z.B. ein Industrieroboter; Erkennung von Ereignissen, z.B. zur visuellen Überwachung oder Personenzählung, z.B. in der Restaurantindustrie; Interaction, z.B. als Eingabe an ein Gerät zur computer-humanen Interaktion; Modellierung von Objekten.Medizin Eines der prominentesten Anwendungsfelder ist die medizinische Computersicht oder die medizinische Bildverarbeitung, gekennzeichnet durch die Extraktion von Informationen von Bilddaten zur Diagnose eines Patienten. Ein Beispiel hierfür ist die Detektion von Tumoren, Arteriosklerose oder anderen malignen Veränderungen; Messungen von Organabmessungen, Blutfluss usw. sind ein weiteres Beispiel. Es unterstützt auch die medizinische Forschung durch die Bereitstellung neuer Informationen: z.B. über die Struktur des Gehirns oder über die Qualität der medizinischen Behandlungen. Anwendungen der Computer-Vision im medizinischen Bereich umfassen auch die Verbesserung von Bildern, die von Menschen interpretiert werden - zum Beispiel ultrasonic Bilder oder Röntgenbilder -, um den Einfluss von Rauschen zu reduzieren. Bildverarbeitung Ein zweiter Anwendungsbereich in der Computersicht ist in der Industrie, manchmal als Bildverarbeitung bezeichnet, wo Informationen zur Unterstützung eines Produktionsprozesses extrahiert werden. Ein Beispiel ist die Qualitätskontrolle, bei der Details oder Endprodukte automatisch überprüft werden, um Mängel zu finden. Ein weiteres Beispiel ist die Messung der Position und Orientierung der von einem Roboterarm aufzunehmenden Details. Machine Vision wird auch in landwirtschaftlichen Prozess stark verwendet, um unerwünschte Lebensmittel aus Schüttgut zu entfernen, ein Verfahren genannt optische Sortierung. Militärische militärische Anwendungen sind wahrscheinlich einer der größten Bereiche für Computer Vision. Die offensichtlichen Beispiele sind die Erkennung von feindlichen Soldaten oder Fahrzeugen und Raketenführung. Mehr fortschrittliche Systeme für die Flugkörperführung senden den Flugkörper an einen Bereich anstatt an ein bestimmtes Ziel, und die Zielauswahl erfolgt, wenn der Flugkörper den Bereich auf der Grundlage von lokal erfassten Bilddaten erreicht. Moderne militärische Konzepte, wie "Battlefield-Bewusstsein", bedeuten, dass verschiedene Sensoren, einschließlich Bildsensoren, eine reiche Menge an Informationen über eine Kampfszene liefern, die zur Unterstützung strategischer Entscheidungen genutzt werden kann. In diesem Fall wird die automatische Verarbeitung der Daten verwendet, um die Komplexität zu reduzieren und Informationen von mehreren Sensoren zu verschmelzen, um die Zuverlässigkeit zu erhöhen. Autonome Fahrzeuge Einer der neueren Einsatzbereiche sind autonome Fahrzeuge, darunter Tauchfahrzeuge, Landfahrzeuge (kleine Roboter mit Rädern, Autos oder LKW), Luftfahrzeuge und unbemannte Luftfahrzeuge (UAV). Die Autonomie reicht von vollautonomen (unbemannten) Fahrzeugen bis zu Fahrzeugen, bei denen computer-visionsbasierte Systeme einen Fahrer oder einen Piloten in verschiedenen Situationen unterstützen. Vollautonome Fahrzeuge nutzen typischerweise Computer-Vision für die Navigation, z.B. um zu wissen, wo sie ist, oder um eine Karte ihrer Umgebung (SLAM) und Hindernisse zu erkennen. Es kann auch zur Erkennung bestimmter aufgabenspezifischer Ereignisse verwendet werden, z.B. eines UAV, der nach Waldbränden sucht. Beispiele für Stützsysteme sind Hinderniswarnsysteme in Autos und Systemen zur autonomen Landung von Flugzeugen. Mehrere Autohersteller haben Systeme für das autonome Fahren von Autos demonstriert, aber diese Technologie hat noch nicht ein Niveau erreicht, wo sie auf den Markt gebracht werden kann. Es gibt zahlreiche Beispiele für militärische autonome Fahrzeuge, die von fortgeschrittenen Raketen bis zu UAVs für Rekonstruktionsmissionen oder Raketenführungen reichen. Die Raumexploration wird bereits mit autonomen Fahrzeugen unter Verwendung von Computer-Vision gemacht, z.B. der Kuriosität der NASA und dem Yutu-2-Rover der CNSA. Tactile Feedback Materialien wie Gummi und Silizium werden verwendet, um Sensoren zu schaffen, die Anwendungen wie die Erkennung von Mikro- und Kalibrierroboterhand ermöglichen. Gummi kann verwendet werden, um eine Form zu schaffen, die über einen Finger platziert werden kann, innerhalb dieser Form wäre mehrere Dehnungsmessstreifen. Die Fingerform und Sensoren könnten dann auf einem kleinen Gummiblatt mit einer Reihe von Gummistiften aufgesetzt werden. Ein Benutzer kann dann die Fingerform tragen und eine Oberfläche verfolgen. Ein Computer kann dann die Daten aus den Dehnungsmessstreifen lesen und messen, wenn ein oder mehrere der Stifte nach oben gedrückt wird. Wird ein Stift nach oben geschoben, so kann der Rechner dies als Unvollkommenheit in der Oberfläche erkennen. Diese Art von Technik ist nützlich, um genaue Daten der Unvollkommenheiten auf einer sehr großen Oberfläche zu erhalten. Eine weitere Variante dieses Fingerformsensors sind Sensoren, die eine in Silizium suspendierte Kamera enthalten. Das Silizium bildet eine Kuppel um die Außenseite der Kamera und eingebettet in das Silizium sind Punktmarker, die gleich beabstandet sind. Diese Kameras können dann auf Geräte wie Roboterhand platziert werden, um dem Computer hochgenaue taktile Daten zu ermöglichen. Weitere Anwendungsbereiche umfassen: Unterstützung der visuellen Effekte Erstellung für Kino und Rundfunk, z.B. Kamera-Tracking (matchmoving.)Überwachung. Jede der oben beschriebenen Anwendungsbereiche verwendet eine Reihe von Computer-Visionsaufgaben; mehr oder weniger definierte Messprobleme oder Bearbeitungsprobleme, die mit einer Vielzahl von Methoden gelöst werden können. Einige Beispiele für typische Computer-Vision-Aufgaben werden unten vorgestellt. Computer-Visionsaufgaben umfassen Methoden zum Erfassen, Verarbeiten, Analysieren und Verstehen digitaler Bilder und zur Extraktion hochdimensionaler Daten aus der realen Welt, um numerische oder symbolische Informationen, z.B. in Form von Entscheidungen, zu erzeugen. Das Verständnis in diesem Zusammenhang bedeutet die Transformation von visuellen Bildern (die Eingabe der Netzhaut) in Beschreibungen der Welt, die mit anderen Gedankenprozessen verbinden und entsprechende Handlungen hervorrufen können. Dieses Bildverstehen kann als das Verzicht auf symbolische Informationen aus Bilddaten mit Modellen betrachtet werden, die mit Hilfe von Geometrie, Physik, Statistik und Lerntheorie aufgebaut sind. Anerkennung Das klassische Problem in der Computervision, der Bildverarbeitung und der Bildverarbeitung ist die Feststellung, ob die Bilddaten bestimmte Objekte, Merkmale oder Aktivitäten enthalten. Verschiedene Sorten des Erkennungsproblems sind in der Literatur beschrieben. Objekterkennung (auch Objektklassifikation genannt) – ein oder mehrere vordefinierte oder gelernte Objekte oder Objektklassen können erkannt werden, in der Regel zusammen mit ihren 2D-Positionen im Bild oder 3D-Pose in der Szene. Blippar, Googlebrillen und mögen Das bietet eigenständige Programme, die diese Funktionalität illustrieren. Identifikation – eine individuelle Instanz eines Objekts wird erkannt. Beispiele sind die Identifizierung des Gesichts oder Fingerabdrucks einer bestimmten Person, die Identifizierung von handschriftlichen Ziffern oder die Identifizierung eines bestimmten Fahrzeugs. Erkennung – die Bilddaten werden für einen bestimmten Zustand gescannt. Beispiele sind die Detektion möglicher abnormaler Zellen oder Gewebe in medizinischen Bildern oder die Erkennung eines Fahrzeugs in einem automatischen Straßenmautsystem. Die auf relativ einfachen und schnellen Berechnungen basierende Erkennung wird manchmal dazu verwendet, kleinere Bereiche interessanter Bilddaten zu finden, die durch rechnerisch anspruchsvollere Techniken weiter analysiert werden können, um eine korrekte Interpretation zu erzielen. Derzeit basieren die besten Algorithmen für solche Aufgaben auf konvolutionalen neuronalen Netzwerken. Eine Darstellung ihrer Fähigkeiten zeigt die ImageNet Large Scale Visual Recognition Challenge; dies ist ein Benchmark in der Objektklassifizierung und -detektion, mit Millionen von Bildern und 1000 Objektklassen im Wettbewerb. Die Leistung von konvolutionalen neuronalen Netzwerken auf den ImageNet-Tests liegt nun in der Nähe des Menschen. Die besten Algorithmen kämpfen noch mit Objekten, die klein oder dünn sind, wie eine kleine Ameise auf einem Stamm einer Blume oder einer Person, die eine Quill in ihrer Hand hält. Sie haben auch Probleme mit Bildern, die mit Filtern verzerrt wurden (ein zunehmend häufigeres Phänomen mit modernen Digitalkameras). Im Gegensatz dazu beunruhigen diese Bilder selten Menschen. Menschen neigen jedoch dazu, Probleme mit anderen Problemen zu haben. Zum Beispiel sind sie nicht gut, Objekte in feinkörnige Klassen einzuordnen, wie die jeweilige Hunderasse oder Vogelart, während Falten-Neuralnetze dies mit Leichtigkeit behandeln. Es existieren mehrere spezialisierte Aufgaben, die auf der Erkennung basieren, wie z.B.: Content-basiertes Abrufen von Bildern – Auffinden aller Bilder in einem größeren Satz von Bildern, die einen bestimmten Inhalt haben. Der Inhalt kann auf unterschiedliche Weise angegeben werden, z.B. in Bezug auf Ähnlichkeit relativ zu einem Zielbild (gib mir alle Bilder ähnlich Bild X) unter Verwendung von umgekehrten Bildsuchetechniken oder in Bezug auf hochrangige Suchkriterien, die als Texteingabe angegeben werden (gib mir alle Bilder, die viele Häuser enthalten, werden im Winter genommen und haben keine Autos in ihnen). Möglichkeit Schätzung – Schätzung der Position oder Orientierung eines bestimmten Objekts relativ zur Kamera. Ein Beispiel für diese Technik wäre die Unterstützung eines Roboterarms bei der Entnahme von Gegenständen aus einem Förderband in einer Montageliniensituation oder bei der Entnahme von Teilen aus einem Behälter. Optische Zeichenerkennung (OCR) – Identifikation von Zeichen in Bildern gedruckter oder handschriftlicher Texte, in der Regel im Hinblick auf die Kodierung des Textes in einem Format, das für die Bearbeitung oder Indexierung (z.B. ASCII) besser geeignet ist. 2D-Codelesung – Lesen von 2D-Codes wie Datenmatrix und QR-Codes. Gesichtserkennung Shape Recognition Technology (SRT) bei Menschen Gegensysteme, die Menschen (Kopf- und Schultermuster) von Objekten unterscheiden Bewegungsanalyse Mehrere Aufgaben betreffen die Bewegungsschätzung, bei der eine Bildsequenz verarbeitet wird, um eine Schätzung der Geschwindigkeit entweder an jedem Bildpunkt oder in der 3D-Szene oder sogar der Kamera, die die Bilder produziert, zu erzeugen.Beispiele solcher Aufgaben sind: Egomotion – Bestimmung der 3D-Hartbewegung (Rotation und Übersetzung) der Kamera aus einer von der Kamera erzeugten Bildfolge. Tracking – nach den Bewegungen eines (gewöhnlich) kleineren Satzes von Interessepunkten oder Objekten (z.B. Fahrzeuge, Objekte, Menschen oder andere Organismen) in der Bildsequenz. Dies hat enorme Industrieanwendungen, da die meisten Hochlaufmaschinen auf diese Weise überwacht werden können. Optische Strömung – für jeden Punkt im Bild zu bestimmen, wie sich dieser Punkt relativ zur Bildebene bewegt, d.h. seine scheinbare Bewegung. Diese Bewegung ergibt sich sowohl, wie sich der entsprechende 3D-Punkt in der Szene bewegt und wie sich die Kamera relativ zur Szene bewegt. Szenenrekonstruktion Bei einem oder (typisch) mehr Bildern einer Szene oder einem Video zielt die Szenerekonstruktion darauf ab, ein 3D-Modell der Szene zu berechnen. Im einfachsten Fall kann das Modell ein Satz von 3D-Punkten sein. Mehr anspruchsvolle Methoden produzieren ein komplettes 3D-Oberflächenmodell. Das Aufkommen von 3D-Bildgebung, die keine Bewegung oder Scan erfordert, und damit verbundene Verarbeitungsalgorithmen ermöglicht schnelle Fortschritte in diesem Bereich. Mit Grid-basierten 3D-Sensoren können 3D-Bilder aus mehreren Winkeln erfasst werden. Algorithmen sind jetzt verfügbar, um mehrere 3D-Bilder zusammen in Punktwolken und 3D-Modelle zu nähen. Bildwiedergabe Ziel der Bildwiedergabe ist die Entfernung von Rauschen (Sensorgeräusche, Bewegungsunschärfe usw.) aus Bildern. Der einfachste Weg zur Geräuschentfernung sind verschiedene Arten von Filtern wie Tiefpassfilter oder Medianfilter. Mehr anspruchsvolle Methoden nehmen ein Modell an, wie die lokalen Bildstrukturen aussehen, um sie von Lärm zu unterscheiden. Durch die erste Analyse der Bilddaten in Bezug auf die lokalen Bildstrukturen, wie Zeilen oder Kanten, und anschließende Steuerung der Filterung anhand lokaler Informationen aus dem Analyseschritt wird üblicherweise ein besseres Geräuschabtragsniveau gegenüber den einfacheren Ansätzen erreicht. Ein Beispiel in diesem Bereich ist die Bemalung. Systemmethoden Die Organisation eines Computer-Vision-Systems ist hoch anwendungsabhängig. Einige Systeme sind eigenständige Anwendungen, die ein bestimmtes Mess- oder Detektionsproblem lösen, während andere ein Teilsystem eines größeren Designs darstellen, das beispielsweise auch Teilsysteme zur Steuerung von mechanischen Aktuatoren, Planung, Informationsdatenbanken, Mensch-Maschine-Schnittstellen usw. enthält. Die konkrete Implementierung eines Computer-Vision-Systems hängt auch davon ab, ob seine Funktionalität vorgegeben ist oder ob ein Teil davon während des Betriebs erlernt oder verändert werden kann. Viele Funktionen sind einzigartig für die Anwendung. Es gibt jedoch typische Funktionen, die in vielen Computer-Vision-Systemen gefunden werden. Bildaufnahme – Ein digitales Bild wird von einem oder mehreren Bildsensoren erzeugt, die neben verschiedenen Arten von lichtempfindlichen Kameras Reichweitensensoren, Tomographiegeräte, Radar-, Ultraschallkameras usw. umfassen. Je nach Sensortyp sind die resultierenden Bilddaten ein gewöhnliches 2D-Bild, ein 3D-Volume oder eine Bildsequenz. Die Pixelwerte entsprechen typischerweise der Lichtintensität in einem oder mehreren Spektralbändern (graue Bilder oder Farbbilder), können aber auch auf verschiedene physikalische Maßnahmen, wie Tiefe, Absorption oder Reflexion von Schall- oder elektromagnetischen Wellen oder Kernmagnetresonanz bezogen sein. Vorverarbeitung – Bevor ein Computer-Vision-Verfahren auf Bilddaten angewendet werden kann, um bestimmte Informationen zu extrahieren, ist es in der Regel erforderlich, die Daten zu verarbeiten, um sicherzustellen, dass es bestimmte Annahmen erfüllt, die durch die Methode. Beispiele sind: Re-Sampling, um sicherzustellen, dass das Bildkoordinatensystem korrekt ist. Lärmreduzierung, um sicherzustellen, dass Sensorgeräusche keine falschen Informationen einführen. Im Gegensatz zur Verbesserung, um sicherzustellen, dass relevante Informationen erkannt werden können. Skalieren Sie die Raumdarstellung, um die Bildstrukturen auf lokal geeigneten Skalen zu verbessern. Feature-Extraktion – Bildmerkmale auf verschiedenen Ebenen der Komplexität werden aus den Bilddaten extrahiert. Typische Beispiele solcher Merkmale sind: Linien, Kanten und Stege. Lokalisierte Zinspunkte wie Ecken, Blobs oder Punkte. Mehr komplexe Merkmale können mit Textur, Form oder Bewegung zusammenhängen. Erkennung/Segmentierung – In der Verarbeitung wird irgendwann entschieden, welche Bildpunkte oder Bereiche des Bildes für die Weiterverarbeitung relevant sind. Beispiele sind: Auswahl eines bestimmten Satzes von Zinspunkten. Segmentierung eines oder mehrerer Bildbereiche, die ein bestimmtes Interessesobjekt enthalten. Segmentierung von Bild in geschachtelte Szenenarchitektur mit Vordergrund, Objektgruppen, Einzelobjekten oder fremden Objektteilen (auch als Ortstaxon-Szenenhierarchie bezeichnet), während die visuelle Salience oft als räumliche und zeitliche Aufmerksamkeit realisiert wird.Segmentierung oder Co-Segmentierung von einem oder mehreren Videos in eine Reihe von per-frame- Vordergrundmasken, unter Beibehaltung ihrer zeitlichen semantischen Kontinuität. Hochrangige Verarbeitung– Bei diesem Schritt ist der Eingang typischerweise ein kleiner Datensatz, beispielsweise ein Satz von Punkten oder ein Bildbereich, der angenommen wird, ein bestimmtes Objekt zu enthalten. Die verbleibende Verarbeitung beschäftigt sich beispielsweise mit der Überprüfung, dass die Daten modell- und anwendungsspezifische Annahmen erfüllen. Schätzung anwendungsspezifischer Parameter, wie Objektpose oder Objektgröße. Bilderkennung – Klassifizierung eines erkannten Objekts in verschiedene Kategorien. Bildregistrierung – Vergleichen und Kombinieren von zwei verschiedenen Ansichten desselben Objekts. Entscheidung Die endgültige Entscheidung, die für die Anwendung erforderlich ist, zum Beispiel: Pass/Fall bei automatischen Inspektionsanwendungen. Match/No-match in Erkennungsanwendungen. Flagge für weitere menschliche Überprüfung in medizinischen, militärischen, Sicherheits- und Anerkennungsanwendungen. Bildverarbeitungssysteme Die Bildverarbeitungssysteme (IUS) umfassen drei Abstraktionsebenen wie folgt: niedriges Niveau umfasst Bildprimitiven wie Kanten, Texturelemente oder Regionen; Zwischenebene umfasst Grenzen, Oberflächen und Volumina; und hohes Niveau umfasst Objekte, Szenen oder Ereignisse. Viele dieser Anforderungen sind ausschließlich Themen für die weitere Forschung. Die Repräsentationsanforderungen bei der Gestaltung von IUS für diese Ebenen sind: Darstellung von prototypischen Konzepten, Konzeptorganisation, Raumwissen, zeitlichem Wissen, Skalierung und Beschreibung durch Vergleich und Differenzierung. Während die Inferenz sich auf den Prozess der Ableitung neuer, nicht explizit dargestellter Tatsachen aus derzeit bekannten Tatsachen bezieht, bezieht sich die Kontrolle auf den Prozess, der die vielen Inferenz-, Such- und Matching-Techniken in einem bestimmten Stadium der Verarbeitung anwenden sollte. Inferenz- und Kontrollanforderungen für IUS sind: Such- und Hypothesenaktivierung, Matching- und Hypothesentests, Erzeugung und Nutzung von Erwartungen, Änderung und Fokus der Aufmerksamkeit, Sicherheit und Stärke des Glaubens, Inferenz und Zielzufriedenheit. Hardware Es gibt viele Arten von Computer-Vision-Systemen; aber alle von ihnen enthalten diese grundlegenden Elemente: eine Energiequelle, mindestens ein Bild-Erfassungsgerät (Kamera, ccd, etc.), einen Prozessor, Steuer- und Kommunikationskabel oder irgendeine Art von drahtlosem Verbindungsmechanismus. Darüber hinaus enthält ein praktisches Vision-System Software sowie ein Display, um das System zu überwachen. Vision-Systeme für Innenräume, wie die meisten Industrien, enthalten ein Beleuchtungssystem und können in einer kontrollierten Umgebung platziert werden. Darüber hinaus umfasst ein abgeschlossenes System viele Accessoires wie Kamerastützen, Kabel und Steckverbinder. Die meisten Computer-Vision-Systeme verwenden sichtbar-Licht-Kameras passiv eine Szene mit Frameraten von maximal 60 Frames pro Sekunde (meist viel langsamer). Einige Computer-Vision-Systeme verwenden Bild-Akquisition-Hardware mit aktiver Beleuchtung oder etwas anderes als sichtbares Licht oder beide, wie strukturierte-light 3D-Scanner, thermographische Kameras, hyperspektrale Bildkameras, Radar-Imaging, Lidar-Scanner, Magnetresonanz-Bilder, Side-Scan-Sonar, synthetische Apertur-Sonar, etc. Solche Hardware erfasst Bilder, die dann oft mit denselben Computer-Visionsalgorithmen verarbeitet werden, die zur Bearbeitung von sichtbaren Lichtbildern verwendet werden. Während herkömmliche Rundfunk- und Verbrauchervideosysteme mit einer Rate von 30 Frames pro Sekunde arbeiten, haben Fortschritte in der digitalen Signalverarbeitung und Consumer-Grafiken-Hardware schnelle Bildaufnahme, Verarbeitung und Anzeige für Echtzeit-Systeme in der Größenordnung von Hundert bis Tausende von Frames pro Sekunde ermöglicht. Für Anwendungen in der Robotik sind schnelle Echtzeit-Videosysteme von entscheidender Bedeutung und können die für bestimmte Algorithmen erforderliche Verarbeitung oft vereinfachen. In Kombination mit einem Hochgeschwindigkeitsprojektor ermöglicht eine schnelle Bilderfassung die Realisierung von 3D-Messungen und Feature-Tracking. Egozentrische Vision-Systeme bestehen aus einer tragbaren Kamera, die automatisch Bilder aus einer ersten Person Perspektive macht. Seit 2016 entwickeln sich Vision Processing Units als neue Prozessorklasse, um CPUs und Grafik Processing Units (GPUs) in dieser Rolle zu ergänzen. Siehe auch Computational Abbildung Computational Photography Machine vision glossary Space Mapping Teknomo–Fernandez Algorithmus Visuelle Wahrnehmung Vision WissenschaftEgozentrische Vision Visuelle Agnosia Computer-Audiolisten Liste der Computer-Vision Themen Liste der aufstrebenden Technologien Outline der künstlichen Intelligenz Outline der Computer-VisionReferenzenWeiterlesen David Marr (1982). Vision.W H. Freeman and Company.ISBN 978-0-7167-1284-8.Azriel Rosenfeld; Avinash Kak (1982). Digitale Bildbearbeitung. Academic Press.ISBN 978-0-12-597301-4.Barghout, Lauren; Lawrence W. Lee (2003). Perzeptuelles Informationsverarbeitungssystem. US-Patentanmeldung 10/618,543.ISBN 978-0-262-08159-7.Berthold K.P Horn (1986). Robot Vision.MIT Presse.ISBN 978-0-262-08159-7. Michael C. Fairhurst (1988). Computer Vision für Robotersysteme. Prentice Hall.ISBN 978-0-13-166919-2.Olivier Faugeras (1993). Dreidimensional Computer Vision, Geometric Viewpoint.MIT Presse.ISBN 978-0-262-06158-2.Tony Lindeberg (1994). Scale-Space-Theorie im Computer Vision.Springer.ISBN 978-0-7923-9418-1.James L. Crowley und Henrik I. Christensen Eds.)(1995).Vision as Process.Springer-Verlag.ISBN 978-3-540-58143-7.CS1 maint: extra text: Autorenliste (link) Gösta H. Granlund; Hans Knutsson (1995). Signalverarbeitung für Computer Vision. Kluwer Academic Publisher.ISBN 978-0-7923-9530-0.Reinhard Klette; Karsten Schluens; Andreas Koschan (1998). Computer Vision – Dreidimensionale Daten aus Bildern. Springer, Singapore.ISBN 978-981-3083-71-4.Emanuele Trucco; Alessandro Verri (1998). Einführungstechniken für 3-D Computer Vision. Prentice Hall.ISBN 978-0-13-261108-4.Bernd Jähne (2002). Digital Image Processing.Springer.ISBN 978-3-540-67754-3.Richard Hartley und Andrew Zisserman (2003). Multiple View Geometrie in Computer Vision. Cambridge University Press.ISBN 978-0-521-54051-3.Gérard Medioni; Sing Bing Kang (2004). Emerging Topics in Computer Vision. Prentice Hall. ISBN 978-0-13-101366-7.R Fisher; K Dawson-Howe; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Wörterbuch der Computer Vision und Bildbearbeitung. John Wiley.ISBN 978-0-470-01526-1.Nikos Paragios und Yunmei Chen und Olivier Faugeras (2005). Handbuch der mathematischen Modelle in Computer Vision.Springer.ISBN 978-0-387-26371-7.Wilhelm Burger; Mark J. Burge (2007). Digitale Bildbearbeitung: Ein agorithmischer Ansatz mit Java.Springer.ISBN 978-1-84628-379-6.Pedram Azad; Tilo Gockel; Rüdiger Dillmann (2008). Computer Vision – Prinzipien und Praxis. Elektor International Medien BV.ISBN 978-0-905705-71-2.Richard Szeliski (2010). Computer Vision: Algorithmen und Anwendungen.Springer-Verlag.ISBN 978-1848829343.J R. Parker (2011). Algorithmen für Bildbearbeitung und Computer Vision (2. ed.). Wiley.ISBN 978-0470643853. Richard J. Radke (2013). Computer Vision for Visual Effects.Cambridge University Press.ISBN 978-0-521-76687-6.Nixon, Mark; Aguado, Alberto (2019). Feature-Extraktion und Bildbearbeitung für Computer Vision (4. ed.). Akademische Presse. ISBN 978-0128149768. Externe Links USC Iris Computer Vision Konferenz Liste Computer Vision Papiere im Web Eine vollständige Liste von Papieren der wichtigsten Computer Vision Konferenzen. Computer Vision Online Nachrichten, Quellcode, Datensätze und Jobangebote im Zusammenhang mit Computer Vision. Keith Price's Annotated Computer Vision Bibliography CVonline Bob Fisher's Compendium of Computer Vision. British Machine Vision Association Unterstützung der Computervisionsforschung in Großbritannien über die BMVC- und MIUA-Konferenzen, Annals des BMVA (offene Zeitschrift), BMVA Summer School und eintägige Treffen Computer Vision Container, Joe Hoeller GitHub: Weit verbreiteter Open-Source-Container für GPU beschleunigte Computer Vision Anwendungen. Verwendet von Forschern, Universitäten, Privatunternehmen sowie den USA Gov't.