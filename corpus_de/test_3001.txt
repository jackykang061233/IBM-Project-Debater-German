Die Geschichte der Computer-Hardware deckt die Entwicklungen von frühen einfachen Geräten bis zur Hilfeberechnung zu modernen Tagesrechnern ab. Vor dem 20. Jahrhundert wurden die meisten Berechnungen von Menschen durchgeführt. Frühe mechanische Werkzeuge, um Menschen mit digitalen Berechnungen zu helfen, wie der Abacus, wurden als Rechenmaschinen oder Rechner (und andere proprietäre Namen) bezeichnet. Der Maschinenbediener wurde als Computer bezeichnet. Bei den ersten Berechnungshilfen handelte es sich um rein mechanische Vorrichtungen, die die Bedienungsperson aufforderten, die Anfangswerte eines elementaren arithmetischen Betriebs einzurichten, dann die Vorrichtung zu manipulieren, um das Ergebnis zu erhalten. Später stellten Computer Zahlen in kontinuierlicher Form dar (z.B. Abstand entlang einer Skala, Rotation einer Welle oder einer Spannung). Zahlen könnten auch in Form von Ziffern dargestellt werden, die durch einen Mechanismus automatisch manipuliert werden. Obwohl dieser Ansatz in der Regel komplexere Mechanismen erforderte, erhöht er die Genauigkeit der Ergebnisse. Die Entwicklung der Transistortechnologie und dann der integrierte Schaltungschip führten zu einer Reihe von Durchbrüchen, beginnend mit Transistorrechnern und dann integrierten Schaltungsrechnern, wodurch digitale Rechner weitgehend analoge Rechner ersetzen. Metalloxid-Halbleiter (MOS) großflächige Integration (LSI) ermöglichte dann den Halbleiterspeicher und den Mikroprozessor, was zu einem weiteren Schlüsseldurchbruch führte, den miniaturisierten Personalcomputer (PC,) in den 1970er Jahren. Die Kosten für Computer wurden nach und nach so niedrig, dass Personal Computer bis in die 1990er Jahre, und dann mobile Computer (Smartphones und Tablets) in den 2000er Jahren, wurde ubiquitous. Frühgeräte Alte und mittelalterliche Geräte wurden verwendet, um die Berechnung für Tausende von Jahren zu unterstützen, meist mit einer Korrespondenz mit Fingern. Das früheste Zählgerät war wahrscheinlich eine Form von Tally Stick. Der Lebombo-Knochen aus den Bergen zwischen Swasiland und Südafrika kann das älteste bekannte mathematische Artefakt sein. Es stammt aus 35.000 BCE und besteht aus 29 verschiedenen Kerben, die bewusst in die Fibula des Babons geschnitten wurden. Später Rekord-Hilfe im gesamten fruchtbaren Crescent enthalten calculi (Clay Kugeln, Kegel, etc.), die Anzahl der Elemente, wahrscheinlich Vieh oder Getreide, in hohlen ungebrannten Tonbehältern versiegelt repräsentiert. Die Verwendung von Zählstäben ist ein Beispiel. Der Abacus wurde früh für arithmetische Aufgaben verwendet. Was wir jetzt den römischen Abacus nennen, wurde in Babylonien bereits um 2700-2300 v. Chr. verwendet. Seitdem wurden viele andere Formen der Berechnung von Brettern oder Tischen erfunden. In einem mittelalterlichen europäischen Zählhaus würde ein kariertes Tuch auf einen Tisch gelegt, und Marker bewegten sich nach bestimmten Regeln, als Hilfe zur Berechnung von Geldsummen. Mehrere analoge Computer wurden in alten und mittelalterlichen Zeiten gebaut, um astronomische Berechnungen durchzuführen. Dazu gehören der Astrolabe- und Antikythera-Mechanismus aus der hellenistischen Welt (ca. 150–100 BC). In römischem Ägypten, Held von Alexandria (ca. 10–70 n. Chr.) machte mechanische Geräte einschließlich automata und einen programmierbaren Wagen. Andere frühmechanische Geräte, die zur Durchführung einer oder einer anderen Art von Berechnungen verwendet werden, umfassen die Planisphäre und andere mechanische Recheneinrichtungen, die von Abu Rayhan al-Biruni (c. AD 1000) erfunden wurden; das Equatorium und universale Breitengrad-unabhängige Astrolabe von Abū Ishāq Ibrāhīm al-Zarqālī-Turm (c. Die Schlossuhr, eine von Ismail al-Jazari im Jahre 1206 erfundene hydromotorisierte mechanische astronomische Uhr, war der erste programmierbare analoge Computer. Ramon Llull erfand den Lullian Circle: eine eigentümliche Maschine zur Berechnung von Antworten auf philosophische Fragen (in diesem Fall, mit dem Christentum zu tun) über logische Combinatorik. Diese Idee wurde von Leibniz Jahrhunderten später aufgenommen und ist damit eines der Gründungselemente in der Informatik und Informationswissenschaft. Renaissance-Rechnungswerkzeuge Scottish Mathematiker und Physiker John Napier entdeckte, dass die Multiplikation und Aufteilung der Zahlen durch die Addition bzw. Subtraktion der Logarithmen dieser Zahlen durchgeführt werden könnte. Bei der Herstellung der ersten logarithmischen Tabellen musste Napier viele mühsame Multiplikationen ausführen. Es war an dieser Stelle, dass er seine Napier' Knochen entworfen, ein akussartiges Gerät, das stark vereinfachte Berechnungen, die Multiplikation und Division. Da reale Zahlen als Distanzen oder Intervalle auf einer Linie dargestellt werden können, wurde in den 1620er Jahren, kurz nach Napiers Arbeit, die Schieberegel erfunden, um Multiplikations- und Divisionsvorgänge deutlich schneller durchzuführen als bisher möglich. Edmund Gunter baute ein Rechengerät mit einer einzigen logarithmischen Skala an der University of Oxford. Sein Gerät vereinfachte arithmetische Berechnungen, einschließlich Multiplikation und Division. William Oughtred hat dies 1630 mit seiner kreisförmigen Dia-Regel erheblich verbessert. Er folgte dem mit der modernen Diaregel im Jahre 1632, im Wesentlichen eine Kombination von zwei Gunter Regeln, zusammen mit den Händen gehalten. Dia-Regeln wurden von Generationen von Ingenieuren und anderen mathematisch engagierten professionellen Arbeitern verwendet, bis die Erfindung des Taschenrechners. Mechanische Rechner Wilhelm Schickard, ein deutsches Polymath, entwarfen 1623 eine Rechenmaschine, die eine mechanisierte Form von Napiers Stangen mit der weltweit ersten mechanischen Addiermaschine kombinierte, die in die Basis eingebaut wurde. Da es sich um ein einziges Zahngetriebe handelte, gab es Umstände, unter denen sein Tragwerk verklemmt würde. Ein Feuer zerstörte mindestens eine der Maschinen im Jahre 1624 und es wird angenommen, dass Schickard zu entmutigt war, einen anderen zu bauen. Im Jahre 1642, während noch ein Teenager, Blaise Pascal begann einige Pionierarbeit an Rechenmaschinen und nach drei Jahren Anstrengung und 50 Prototypen erfand er einen mechanischen Rechner. In den folgenden zehn Jahren baute er zwanzig dieser Maschinen (genannt Pascal's Taschenrechner oder Pascaline). Neun Pascalines haben überlebt, von denen die meisten in europäischen Museen ausgestellt sind. Es besteht eine anhaltende Debatte darüber, ob Schickard oder Pascal als "Erfinder des mechanischen Rechners" betrachtet werden sollten und das Spektrum der zu berücksichtigenden Fragen anderswo diskutiert wird. Gottfried Wilhelm von Leibniz erfand den Stufenrechner und seinen berühmten Stufentrommelmechanismus um 1672. Er versuchte, eine Maschine zu schaffen, die nicht nur zur Addition und Subtraktion verwendet werden konnte, sondern einen beweglichen Wagen nutzen würde, um eine lange Multiplikation und Division zu ermöglichen. Leibniz sagte einmal "Es ist unwürdig, dass ausgezeichnete Männer Stunden verlieren, wie Sklaven in der Arbeit der Berechnung, die sicher an alle anderen, wenn Maschinen verwendet werden konnte. " Leibniz hat jedoch keinen voll erfolgreichen Transportmechanismus eingebaut. Leibniz beschreibt auch das binäre Ziffernsystem, ein zentraler Bestandteil aller modernen Computer. Bis zu den 1940er Jahren basierten jedoch auf dem Dezimalsystem viele spätere Entwürfe (einschließlich Charles Babbages Maschinen von 1822 und sogar ENIAC von 1945). Um 1820 schuf Charles Xavier Thomas de Colmar, was im Rest des Jahrhunderts zum ersten erfolgreichen, massenproduzierten mechanischen Rechner, dem Thomas Arithmometer, werden würde. Es könnte verwendet werden, um zu addieren und zu subtrahieren, und mit einem beweglichen Wagen könnte der Bediener auch multiplizieren und durch einen Prozess der langen Multiplikation und langen Teilung teilen. Es nutzte eine gestufte Trommel ähnlich in der Konzeption wie die von Leibniz erfunden. Mechanische Rechner blieben bis in die 1970er Jahre im Einsatz. Datenverarbeitung der Karten 1804 entwickelte der französische Weber Joseph Marie Jacquard eine Webmaschine, in der das gewebte Muster von einem Papierband aus gestanzten Karten gesteuert wurde. Das Papierband konnte geändert werden, ohne die mechanische Konstruktion der Webmaschine zu ändern. Dies war eine wichtige Errungenschaft in der Programmierbarkeit. Seine Maschine war eine Verbesserung gegenüber ähnlichen Webwebstühlen. Gestanzte Karten wurden wie in der von Basile Bouchon vorgeschlagenen Maschine von Stanzbändern vorangetrieben. Diese Bands würden die Informationsaufnahme für automatische Klaviere und neuere numerische Steuerungswerkzeuge inspirieren. In den späten 1880er Jahren erfand der Amerikaner Herman Hollerith Datenspeicherung auf Stanzkarten, die dann von einer Maschine gelesen werden konnten. Um diese gestanzten Karten zu verarbeiten, erfand er den Tabulator und die Keypunch-Maschine. Seine Maschinen verwendeten elektromechanische Relais und Zähler. Holleriths Methode wurde in der 1890 United States Census verwendet. Diese Volkszählung wurde zwei Jahre schneller bearbeitet als die vorherige Volkszählung. Holleriths Firma wurde schließlich der Kern von IBM. Bis 1920 konnten elektromechanische Tabulierungsmaschinen addieren, subtrahieren und insgesamt drucken. Maschinenfunktionen wurden durch Einfügen von Dutzenden von Drahtspringern in abnehmbare Bedienfelder geleitet. Als die Vereinigten Staaten 1935 Social Security einführten, wurden IBM-Panzerkartensysteme verwendet, um Aufzeichnungen von 26 Millionen Arbeitnehmern zu verarbeiten. Punched Karten wurden ubiquitous in der Industrie und Regierung für Buchhaltung und Verwaltung. Leslie Comries Artikel zu Stanzkarten-Methoden und W. J. Eckerts Veröffentlichung von Stanzkartenmethoden in Scientific Computation im Jahr 1940, beschrieben Stanzkarten-Techniken ausreichend fortgeschritten, um einige Differentialgleichungen zu lösen oder Multiplikation und Division mit schwimmenden Punktdarstellungen durchzuführen, alle auf Stanzkarten und Einheitsrekordmaschinen. Solche Maschinen wurden während des Zweiten Weltkriegs für die kryptographische statistische Verarbeitung verwendet, sowie eine Vielzahl von administrativen Anwendungen. Das Astronomische Computing Bureau, Columbia University, führte astronomische Berechnungen durch, die den Stand der Technik im Computing darstellen. Berechnungen Bis zum 20. Jahrhundert wurden frühere mechanische Rechner, Kassen, Buchhaltungsmaschinen usw. neu gestaltet, um Elektromotoren zu verwenden, mit Getriebestellung als Darstellung für den Zustand einer Variablen. Der Wortcomputer war ein Jobtitel, der hauptsächlich Frauen zugeordnet wurde, die diese Rechner zur Durchführung mathematischer Berechnungen verwendet haben. In den 1920er Jahren führte der britische Wissenschaftler Lewis Fry Richardsons Interesse an der Wetterprognose dazu, menschliche Computer und numerische Analyse vorzuschlagen, um das Wetter zu modellieren; bis heute werden die mächtigsten Computer auf der Erde benötigt, um sein Wetter mit den Navier-Stokes-Gleichungen angemessen zu modellieren. Firmen wie Friden, Marchant Calculator und Monroe machten Desktop-Mechanikrechner aus den 1930er Jahren, die hinzufügen, subtrahieren, multiplizieren und teilen könnten. 1948 wurde die Curta vom österreichischen Erfinder Curt Herzstark eingeführt. Es war ein kleiner, handkurbelter mechanischer Taschenrechner und als solcher ein Nachkomme von Gottfried Leibnizs Stepped Reckoner und Thomas's Arithmometer. Der weltweit erste all-elektronische Desktop-Rechner war der britische Bell Punch ANITA, der 1961 veröffentlicht wurde. Es verwendet Vakuumröhren, Kaltkathodenröhren und Dekatrons in seinen Schaltungen, mit 12 Kaltkathoden Nixie-Röhren für seine Anzeige. Der ANITA verkaufte sich gut, da es der einzige elektronische Desktop-Rechner zur Verfügung stand und war leise und schnell. Die Röhrentechnik wurde im Juni 1963 von der US-amerikanischen Firma Friden EC-130, die ein All-Transistor-Design, einen Stapel von vier 13-stelligen Zahlen auf einem 5-Zoll (13 cm) CRT, und eingeführt umgekehrte polnische Notation (RPN) Das erste Universalrechnergerät Charles Babbage, ein englischer Maschinenbauer und Polymath, hat das Konzept eines programmierbaren Computers hervorgebracht. Als der "Vater des Computers" betrachtete er den ersten mechanischen Computer im frühen 19. Jahrhundert. Nach der Arbeit an seinem revolutionären Differenz-Engine, entworfen, um in Navigation Berechnungen zu helfen, im Jahre 1833 erkannte er, dass ein viel allgemeineres Design, eine Analytical Engine, möglich war. Die Eingabe von Programmen und Daten war über Stanzkarten an die Maschine zu stellen, wobei zur Zeit ein Verfahren zur direkten mechanischen Webstühlen wie die Jacquardwebmaschine verwendet wird. Für die Ausgabe würde die Maschine einen Drucker, einen Kurvenplotter und eine Glocke haben. Die Maschine wäre auch in der Lage, Zahlen auf später einzulesende Karten zu stanzen. Es verwendet gewöhnliche Basis-10 Fixpunkt arithmetic. Der Motor integriert eine arithmetische Logikeinheit, Steuerstrom in Form von bedingten Verzweigungen und Schleifen, und integriertem Speicher, so dass es die erste Konstruktion für einen universellen Computer, der in modernen Begriffen als Turing-complete beschrieben werden könnte. Es sollte ein Speicher oder Speicher sein, in der Lage, 1.000 Zahlen von je 40 Dezimalziffern zu halten (ca.16.7 kB). Eine arithmetische Einheit, die als Mühle bezeichnet wird, würde in der Lage sein, alle vier arithmetischen Operationen, plus Vergleiche und optional quadratische Wurzeln durchzuführen. Zunächst wurde es als Differenzmotor konzipiert, der auf sich zurückgeschwungen ist, in einem allgemein kreisförmigen Layout, wobei der lange Speicher auf eine Seite austritt. (Letzte Zeichnungen zeigen ein regelmäßiges Rasterlayout.) Wie die zentrale Verarbeitungseinheit (CPU) in einem modernen Computer würde sich die Mühle auf eigene interne Prozeduren verlassen, etwa äquivalent zu Mikrocode in modernen CPUs, in Form von Kugeln in rotierende Trommeln, genannt Fässer, gespeichert werden, um einige der komplexeren Anweisungen, die das Programm des Benutzers angeben könnte. Die von den Benutzern zu verwendende Programmiersprache war den modernen Tag Montagesprachen zu entsprechen. Loops und bedingte Verzweigungen waren möglich, und so wäre die Sprache, wie konzipiert, Turing-komplete wie später von Alan Turing definiert gewesen. Es wurden drei verschiedene Typen von Stempelkarten verwendet: eine für arithmetische Operationen, eine für numerische Konstanten, und eine für Last- und Speichervorgänge, Übertragung von Zahlen vom Speicher auf die Recheneinheit oder zurück. Es gab drei separate Leser für die drei Kartentypen. Die Maschine war etwa ein Jahrhundert vor ihrer Zeit. Das Projekt wurde jedoch durch verschiedene Probleme verlangsamt, darunter Streitigkeiten mit den wichtigsten Machinisten-Bauteilen. Alle Teile für seine Maschine mussten von Hand gemacht werden – das war ein großes Problem für eine Maschine mit Tausenden von Teilen. Schließlich wurde das Projekt mit der Entscheidung der britischen Regierung aufgelöst, die Finanzierung einzustellen. Babbages Versäumnis, den analytischen Motor zu vervollständigen, kann vor allem auf Schwierigkeiten zurückzuführen sein, nicht nur auf Politik und Finanzierung, sondern auch auf seinen Wunsch, einen zunehmend anspruchsvolleren Computer zu entwickeln und schneller voranzutreiben als jeder andere. Ada Lovelace übersetzt und ergänzte Notizen zum "Sketch of the Analytical Engine" von Luigi Federico Menabrea. Dies scheint die erste veröffentlichte Beschreibung der Programmierung zu sein, so Ada Lovelace wird weithin als erster Computerprogrammierer angesehen. Nach Babbage, obwohl er sich seiner früheren Arbeit nicht bewusst war, war Percy Ludgate, ein Angestellter eines Maishändlers in Dublin, Irland. Er entwarf unabhängig einen programmierbaren mechanischen Computer, den er in einer 1909 veröffentlichten Arbeit beschrieben hat. Analogrechner In der ersten Hälfte des 20. Jahrhunderts wurden analoge Computer von vielen als die Zukunft des Computing betrachtet. Diese Geräte nutzten die kontinuierlich änderbaren Aspekte physikalischer Phänomene wie elektrische, mechanische oder hydraulische Größen, um das Problem zu modellieren, das gelöst wird, im Gegensatz zu digitalen Computern, die symbolisch unterschiedliche Größen repräsentierten, da sich ihre Zahlenwerte ändern. Da ein Analogrechner nicht diskrete Werte, sondern kontinuierliche Werte verwendet, können Prozesse nicht zuverlässig mit exakter Gleichwertigkeit wiederholt werden, wie sie bei Turingmaschinen möglich sind. Der erste moderne analoge Computer war eine von Sir William Thomson, später Lord Kelvin, 1872 erfundene Maschine. Es nutzte ein System von Riemenscheiben und Drähten, um automatisch prognostizierte Stromstärken für einen bestimmten Zeitraum an einem bestimmten Ort zu berechnen und war von großem Nutzen für die Navigation in flachen Gewässern. Sein Gerät war die Grundlage für weitere Entwicklungen im Analog Computing. Der Differential-Analysator, ein mechanischer Analog-Computer zur Lösung von Differentialgleichungen durch Integration mit Rad-und-Disc-Mechanismen, wurde 1876 von James Thomson, dem Bruder des berühmteren Lord Kelvin, konzipiert. Er erforschte den möglichen Aufbau solcher Rechner, wurde aber durch das begrenzte Abtriebsmoment der Ball-and-Disk-Integratoren gestochen. In einem Differenzanalysator fuhr der Ausgang eines Integrators den Eingang des nächsten Integrators oder einen Graphisierungsausgang. Ein wichtiger Fortschritt bei der Analog Computing war die Entwicklung der ersten Brandkontrollsysteme für die Langstrecken-Schiffsanlage. Als die Schießereien im späten 19. Jahrhundert dramatisch zunahmen, war es nicht mehr eine einfache Frage der Berechnung des richtigen Zielpunktes angesichts der Flugzeiten der Schalen. Verschiedene Spotter an Bord des Schiffes würden Distanzmaßnahmen und Beobachtungen an eine zentrale Plotstation weiterleiten. Dort wurden die Brandrichtungsteams in Position, Geschwindigkeit und Richtung des Schiffes und dessen Ziel, sowie verschiedene Anpassungen für Coriolis-Effekt, Wettereffekte auf die Luft und andere Anpassungen gespeist; der Computer würde dann eine Zündlösung ausgeben, die den Revolvern zur Verlegung zugeführt würde. 1912 entwickelte der britische Ingenieur Arthur Pollen den ersten elektrisch betriebenen mechanischen Analogrechner (im Zeitpunkt der Argo-Uhr). Es wurde von der kaiserlichen russischen Marine im Ersten Weltkrieg verwendet.Die alternative Dreyer Table Feuerleitsystem wurde bis Mitte 1916 an britische Kapitalschiffe montiert. Mechanische Geräte wurden auch verwendet, um die Genauigkeit der Luftbombung zu unterstützen. Drift Sight war die erste solche Hilfe, die von Harry Wimperis im Jahr 1916 für den Royal Naval Air Service entwickelt wurde; sie gemessen die Windgeschwindigkeit von der Luft und verwendet diese Messung, um die Auswirkungen des Windes auf die Flugbahn der Bomben zu berechnen. Das System wurde später mit der Course Setting Bomb Sight verbessert und erreichte einen Höhepunkt mit den Bombenanblicken des Zweiten Weltkriegs, dem Bombenanblick Mark XIV (RAF Bomber Command) und dem Norden (Luftstreitkräfte der Vereinigten Staaten von Amerika). Die Kunst des mechanischen Analog Computing erreichte seinen Zenit mit dem Differentialanalysator, gebaut von H. L. Hazen und Vannevar Bush am MIT ab 1927, der auf den mechanischen Integratoren von James Thomson und den von H. W. Nieman erfundenen Drehmomentverstärkern aufgebaut wurde. Ein Dutzend dieser Geräte wurden gebaut, bevor ihre Obsoleszenz offensichtlich wurde; die mächtigsten wurde an der Moore School of Electrical Engineering der University of Pennsylvania gebaut, wo die ENIAC gebaut wurde. Ein vollständig elektronischer Analogrechner wurde 1942 von Helmut Hölzer im Peenemünde Army Research Center gebaut. In den 1950er Jahren hatte der Erfolg von digitalen elektronischen Computern das Ende für die meisten analogen Computer geschrieben, aber hybride analoge Computer, gesteuert durch digitale Elektronik, blieb in erheblichem Gebrauch in den 1950er und 1960er Jahren, und später in einigen spezialisierten Anwendungen. Advent des digitalen Computers Das Prinzip des modernen Computers wurde zuerst von dem Informatiker Alan Turing beschrieben, der die Idee in seinem Halbnal 1936 Papier, On Computable Numbers. Turbulenzen reformiert Kurt Gödels allgemeine arithmetische formale Sprache durch die formalen und einfachen hypothetischen Geräte ersetzt, die als Turing-Maschinen bekannt wurden. Er bewies, dass eine solche Maschine in der Lage wäre, jede denkbare mathematische Berechnung durchzuführen, wenn sie als Algorithmus darstellbar wäre. Er ging weiter, um zu beweisen, dass es keine Lösung für das Entscheidungsproblem gab, indem er zunächst zeigte, dass das Stoppproblem für Turing-Maschinen unentbehrlich ist: generell ist es nicht möglich, algorithmisch zu entscheiden, ob eine bestimmte Turing-Maschine jemals aufhört. Er stellte auch den Begriff einer "Universalmaschine" vor (jetzt als Universal-Turniermaschine bekannt), mit der Idee, dass eine solche Maschine die Aufgaben einer anderen Maschine erfüllen könnte, oder mit anderen Worten, es ist nachweislich in der Lage, alles zu berechnen, was durch die Ausführung eines auf Band gespeicherten Programms berechnet wird, so dass die Maschine programmierbar ist. Von Neumann erkannte, dass das zentrale Konzept des modernen Computers auf diesem Papier beruhte. Turing Maschinen sind bis heute ein zentrales Objekt der Studie in der Theorie der Berechnung. Außer den Einschränkungen, die durch ihre endlichen Speicherplätze auferlegt werden, sollen moderne Computer Turing-komplete sein, d.h. sie haben Algorithmus-Ausführungsfähigkeit, die einer universellen Turing-Maschine entspricht.Elektromechanische Computer Die Ära des modernen Computing begann mit einem Grippe der Entwicklung vor und während des Zweiten Weltkriegs. Die meisten in dieser Zeit gebauten digitalen Computer waren elektromechanische – elektrische Schalter fuhren mechanische Relais, um die Berechnung durchzuführen. Diese Geräte hatten eine geringe Betriebsgeschwindigkeit und wurden schließlich von viel schnelleren All-elektrischen Computern übertroffen, ursprünglich mit Vakuumröhren. Das Z2 war eines der frühesten Beispiele eines elektromechanischen Relaisrechners und wurde 1940 vom deutschen Ingenieur Konrad Zuse erstellt. Es war eine Verbesserung seiner früheren Z1; obwohl es den gleichen mechanischen Speicher verwendet, ersetzt es die Rechen- und Steuerlogik durch elektrische Relaisschaltungen. Im selben Jahr wurden elektromechanische Geräte, die Bomben genannt wurden, von britischen Kryptologen gebaut, um deutsche Enigma-Maschine-verschlüsselte geheime Botschaften während des Zweiten Weltkriegs zu entschlüsseln. Das ursprüngliche Design der Bombe wurde 1939 in der britischen Regierungskodex und Cypher School (GC&CS) im Bletchley Park von Alan Turing geschaffen, mit einer wichtigen Verfeinerung, die 1940 von Gordon Welchman entworfen wurde. Das Engineering-Design und Konstruktion war die Arbeit von Harold Keen von der britischen Tabulator-Maschinenfirma. Es war eine wesentliche Entwicklung von einem Gerät, das 1938 vom polnischen Cipher Bureau Kryptologen Marian Rejewski entworfen worden war, und bekannt als die "Kryptologic bomb" (Polish: "bomba kryptologiczna"). 1941 folgte Zuse seiner früheren Maschine mit dem Z3, dem weltweit ersten arbeitenden elektromechanischen programmierbaren vollautomatischen digitalen Computer. Das Z3 wurde mit 2000 Relais aufgebaut, wobei eine 22-Bit-Wortslänge realisiert wurde, die mit einer Taktfrequenz von ca. 5–10 Hz betrieben wurde. Programmcode und Daten wurden auf Stanzfolie gespeichert. Es war ziemlich ähnlich wie moderne Maschinen in gewisser Hinsicht, wegweisend zahlreiche Fortschritte wie schwimmende Punktzahlen. Der Austausch des schwer zu implementierenden Dezimalsystems (in Charles Babbages früherem Design) durch das einfachere binäre System bedeutete, dass Zuses Maschinen leichter zu bauen und möglicherweise zuverlässiger waren, angesichts der damals verfügbaren Technologien. Das Z3 wurde 1998 von Raúl Rojas als Turing-komplete Maschine erwiesen. In zwei 1936 Patentanmeldungen erwartete Zuse auch, dass Maschinenanweisungen in der gleichen Datenspeicherung gespeichert werden könnten – die Schlüsseleinsicht dessen, was als von Neumann-Architektur bekannt wurde, die 1948 in Amerika im elektromechanischen IBM SSEC und in Großbritannien im vollelektrischen Manchester Baby umgesetzt wurde. Zuse erlitt Rückschläge während Zweiter Weltkrieg, als einige seiner Maschinen im Zuge der Bombenangriffe von Allied zerstört wurden. Offenbar blieb seine Arbeit weitgehend unbekannt für Ingenieure in Großbritannien und den USA bis viel später, obwohl zumindest IBM war sich dessen bewusst, wie es seine Nachkriegs-Start-Unternehmen im Jahr 1946 im Gegenzug für eine Option auf Zuses Patente finanziert. 1944 die Harvard Mark Ich wurde in IBMs Endicott Laboren gebaut. Es war ein ähnlicher allgemeiner Zweck elektromechanischer Computer zum Z3, aber war nicht ganz Turing-komplete. Digitale Berechnung Der Begriff Digital wurde zunächst von George Robert Stibitz vorgeschlagen und bezieht sich darauf, wo ein Signal, wie eine Spannung, nicht verwendet wird, um einen Wert (wie es in einem analogen Computer sein würde), sondern um ihn zu codieren. Im November 1937 absolvierte George Stibitz, der später bei Bell Labs (1930–1941) arbeitete, einen Relay-basierten Rechner, den er später den "Model K" (für "Küchentisch", auf dem er ihn montiert hatte), der zum ersten binären Addierer wurde. Typischerweise haben Signale zwei Zustände - niedrig (in der Regel 0) und hoch (in der Regel 1 dargestellt), aber manchmal wird dreiwertige Logik verwendet, insbesondere im High-Density-Speicher. Moderne Computer verwenden in der Regel binäre Logik, aber viele frühen Maschinen waren dezimale Computer. In diesen Maschinen war die Basiseinheit der Daten die Dezimalstelle, die in einem von mehreren Systemen kodiert wurde, einschließlich binärcodierter Dezimal- oder BCD, bi-quinär, überschüssiger-3 und zwei-out-of-five-Code. Die mathematische Grundlage des digitalen Computing ist Boolean Algebra, entwickelt von der britischen Mathematiker George Boole in seiner Arbeit The Laws of Thought, veröffentlicht 1854. Seine Boolesche Algebra wurde in den 1860er Jahren von William Jevons und Charles Sanders Peirce weiterentwickelt und wurde zunächst systematisch von Ernst Schröder und A. N. Whitehead präsentiert. 1879 Gottlob Frege entwickelt den formalen Ansatz zur Logik und schlägt die erste logische Sprache für logische Gleichungen vor. In den 1930er Jahren und unabhängig arbeitend, zeigten der amerikanische elektronische Ingenieur Claude Shannon und der sowjetische Logiker Victor Shestakov beide eine einmalige Korrespondenz zwischen den Konzepten der Booleschen Logik und bestimmten elektrischen Schaltungen, jetzt genannt Logik-Gatter, die jetzt in digitalen Computern ubiquitous. Sie zeigten, dass elektronische Relais und Schalter die Ausdrücke von Boolean Algebra erkennen können. Diese These gründete im Wesentlichen die praktische digitale Schaltung. Darüber hinaus gibt Shannons Papier ein richtiges Schaltbild für einen 4 Bit digitalen Binär-Addierer. Elektronische Datenverarbeitung Rein elektronische Schaltelemente ersetzten ihre mechanischen und elektromechanischen Äquivalente, gleichzeitig ersetzte die digitale Berechnung analog. Maschinen wie der Z3, der Atanasoff-Berry-Computer, die Colossus-Computer und die ENIAC wurden von Hand gebaut, mit Schaltungen, die Relais oder Ventile (Vakuumrohre) enthalten und oft gestanzte Karten oder gestanztes Papierband für die Eingabe und als Haupt (nichtflüchtige) Speichermedium verwendet. Der Ingenieur Tommy Flowers trat 1926 in die Telekommunikationsbranche des General Post Office ein. Während er in den 1930er Jahren an der Forschungsstelle in Dollis Hill arbeitete, begann er die mögliche Nutzung der Elektronik für den Telefonaustausch zu erkunden. Experimentelle Geräte, die er 1934 baute, gingen in Betrieb 5 Jahre später, Umwandlung eines Teils des Telefonaustauschnetzes in ein elektronisches Datenverarbeitungssystem, mit Tausenden von Vakuumröhren. In den USA erfand Arthur Dickinson (IBM) 1940 den ersten digitalen elektronischen Computer. Dieses Rechengerät war vollständig elektronisch – Steuerung, Berechnungen und Ausgabe (das erste elektronische Display). John Vincent Atanasoff und Clifford E. Berry der Iowa State University entwickelten 1942 den Atanasoff-Berry Computer (ABC), das erste binäre elektronische Rechengerät. Diese Konstruktion war halbelektronisch (elektromechanische Steuerung und elektronische Berechnungen) und verwendet etwa 300 Vakuumröhren, mit Kondensatoren in einer mechanisch rotierenden Trommel für den Speicher fixiert. Sein Papierkartenschreiber/Lesegerät war jedoch unzuverlässig und das regenerative Trommelkontaktsystem war mechanisch. Die Besonderheit der Maschine und der Mangel an änderbarem, gespeichertem Programm unterscheiden sie von modernen Computern. Computer, deren Logik in erster Linie mit Vakuumröhren gebaut wurde, sind jetzt als Computer der ersten Generation bekannt. Der elektronische programmierbare Computer Während des Zweiten Weltkriegs erreichten britische Codebreaker im Bletchley Park, 40 Meilen (64 km) nördlich von London, eine Reihe von Erfolgen beim Bruch von verschlüsselten feindlichen militärischen Kommunikationen. Die deutsche Verschlüsselungsmaschine Enigma wurde zunächst mit Hilfe der elektromechanischen Bomben angegriffen. Frauen haben diese Bombenmaschinen oft betrieben. Sie ausgeschlossenen mögliche Enigma-Einstellungen, indem sie Ketten logischer Abzüge ausführen, die elektrisch durchgeführt werden. Die meisten Möglichkeiten führten zu einem Widerspruch, und die wenigen verbleibenden konnten von Hand getestet werden. Die Deutschen entwickelten auch eine Reihe von Teleprinter-Verschlüsselungssystemen, ganz anders als Enigma. Die Lorenz SZ 40/42 Maschine wurde für High-Level-Armee-Kommunikation verwendet, kodiert Tunny von den Briten. Die ersten Abschnitte der Lorenz-Nachrichten begannen 1941. Als Teil eines Angriffs auf Tunny entwickelten Max Newman und seine Kollegen den Heath Robinson, eine Festfunktionsmaschine, die beim Code Breaking hilft. Tommy Flowers, ein Senior-Ingenieur an der Post Office Research Station wurde Max Newman von Alan Turing empfohlen und verbrachte elf Monate ab Anfang Februar 1943 die Gestaltung und den Aufbau des flexibleren Colossus-Computers (der den Heath Robinson übertraf). Nach einem Funktionstest im Dezember 1943 wurde Colossus nach Bletchley Park versendet, wo es am 18. Januar 1944 ausgeliefert wurde und seine erste Botschaft am 5. Februar angriff. Colossus war der weltweit erste elektronische digitale programmierbare Computer. Es verwendet eine große Anzahl von Ventilen (Vakuumrohre). Es hatte Papier-Tape-Eingabe und war in der Lage, eine Vielzahl von booleanischen logischen Operationen auf seinen Daten durchzuführen, aber es war nicht Turing-komplete. Die in Colossus eingegebenen Daten wurden durch photoelektrisches Lesen einer Papierband-Transkription der verschlüsselten abgefangenen Nachricht angezeigt. Dies wurde in einer kontinuierlichen Schleife angeordnet, so dass es mehrfach gelesen und wieder gelesen werden konnte – es gibt keinen internen Speicher für die Daten. Der Lesemechanismus lief bei 5.000 Zeichen pro Sekunde, wobei sich das Papierband mit 40 ft/s (12.2 m/s; 27.3 mph) bewegte. Colossus Mark 1 enthielt 1500 Thermionventile (Rohre), aber Mark 2 mit 2400 Ventilen und fünf Prozessoren parallel, war sowohl 5 mal schneller und einfacher zu bedienen als Mark 1, was den Decodiervorgang stark beschleunigte. Mark 2 wurde während Mark 1 konstruiert. Allen Coombs übernahm die Führung des Colossus Mark 2 Projekts, als Tommy Flowers in andere Projekte einzog. Der erste Mark 2 Colossus wurde am 1. Juni 1944 in Betrieb genommen, gerade rechtzeitig für die Alliierte Invasion der Normandie am D-Day. Der größte Teil der Verwendung von Colossus war die Bestimmung der Startpositionen der Tunny-Rotore für eine Nachricht, die als "Radeinstellung" bezeichnet wurde. Colossus umfasste die erste Verwendung von Schieberegistern und systolischen Arrays, die fünf gleichzeitige Tests ermöglichten, die jeweils bis zu 100 Boolesche Berechnungen beinhalten. Dadurch konnten fünf verschiedene mögliche Startpositionen auf einen Durchgang des Papierbandes untersucht werden. Neben der Radeinstellung enthalten einige spätere Colossi Mechanismen zur Bestimmung von Stiftmustern, die als "Radbruch" bekannt sind. Beide Modelle waren mit Schaltern und Steckern in einer Weise programmierbar, wie ihre Vorgänger nicht waren. Zehn Mk 2 Colossi waren am Ende des Krieges in Betrieb. Ohne die Verwendung dieser Maschinen wären die Alliierten von der sehr wertvollen Intelligenz beraubt worden, die aus dem Lesen der riesigen Menge an verschlüsselten hochrangigen telegraphischen Botschaften zwischen dem Deutschen Hochkommando (OKW) und ihren Armeebefehlen in ganz Europa gewonnen wurde. Details ihrer Existenz, ihres Designs und ihrer Verwendung wurden in den 1970er Jahren gut geheim gehalten. Winston Churchill gab persönlich einen Auftrag für ihre Zerstörung in Stücke, die nicht größer sind als die Hand eines Mannes, um geheim zu halten, dass die Briten in der Lage waren, Lorenz SZ-Kypern (aus deutschen Rotorstrom-Kiffermaschinen) während des kommenden Kalten Krieges zu knacken. Zwei der Maschinen wurden auf den neu gebildeten GCHQ übertragen und die anderen zerstört. Infolgedessen waren die Maschinen nicht in vielen Rechenwerken enthalten. Eine rekonstruierte Arbeitskopie einer der Colossus-Maschinen wird nun im Bletchley Park ausgestellt. Der US-amerikanische ENIAC (Electronic Numerical Integrator und Computer) war der erste elektronische programmierbare Computer in den USA. Obwohl das ENIAC dem Colossus ähnlich war, war es viel schneller und flexibler. Es war eindeutig ein Turing-komplete Gerät und konnte jedes Problem, das in seinen Speicher passen würde berechnen. Wie der Colossus wurde ein Programm auf der ENIAC durch die Zustände seiner Patchkabel und Schalter definiert, einen weiten Schrei von den gespeicherten Programm-Elektronikmaschinen, die später kamen. Nachdem ein Programm geschrieben wurde, musste es mit manuellem Rücksetzen von Steckern und Schaltern mechanisch in die Maschine eingestellt werden. Die Programmierer des ENIAC waren Frauen, die als Mathematiker ausgebildet worden waren. Es kombinierte die hohe Geschwindigkeit der Elektronik mit der Fähigkeit, für viele komplexe Probleme programmiert werden. Es könnte 5000 Mal pro Sekunde, tausend Mal schneller als jede andere Maschine hinzufügen oder subtrahieren. Es hatte auch Module zu multiplizieren, teilen und quadratische Wurzel. Der Hochgeschwindigkeitsspeicher war auf 20 Wörter (entsprechend etwa 80 Bytes) begrenzt. Unter der Leitung von John Mauchly und J. Presper Eckert an der University of Pennsylvania erbaut, dauerte die Entwicklung und der Bau des ENIAC von 1943 bis zum vollen Betrieb Ende 1945. Die Maschine war riesig, wiegte 30 Tonnen, mit 200 Kilowatt elektrischer Leistung und enthielt über 18.000 Vakuumröhren, 1.500 Relais, und Hunderttausende von Widerständen, Kondensatoren und Induktoren. Eines der wichtigsten technischen Ergebnisse war, die Auswirkungen von Rohrabbrand zu minimieren, die ein gemeinsames Problem in der Maschinensicherheit damals war. Die Maschine war in den nächsten zehn Jahren in fast konstantem Gebrauch. Speicherprogramm Computer Frührechner waren programmierbar in dem Sinne, dass sie der Reihenfolge der Schritte folgen konnten, die sie eingerichtet wurden, um auszuführen, aber das Programm oder die Schritte, die die Maschine ausführen sollte, wurden in der Regel durch Änderung der Art, wie die Drähte in ein Patch-Panel oder ein Plugboard eingesteckt wurden. Die Neuprogrammierung, wenn überhaupt möglich, war ein mühsamer Prozess, beginnend mit Ingenieuren, die Fließdiagramme ausarbeiten, den neuen Aufbau entwerfen, und dann der oft auslösende Prozess der physikalisch wiederverdrahtenden Patchpaneele. Stored-Programm-Computer hingegen wurden entwickelt, um eine Reihe von Anweisungen (ein Programm) im Speicher zu speichern – typischerweise der gleiche Speicher wie gespeicherte Daten. Theorie Die theoretische Basis für den Speicherprogrammrechner wurde von Alan Turing in seinem Papier 1936 vorgeschlagen. 1945 trat Turing dem National Physical Laboratory bei und begann seine Arbeit an der Entwicklung eines elektronischen, gespeicherten Programms digitalen Computers. Sein 1945er Bericht "Proposed Electronic Calculator" war die erste Spezifikation für ein solches Gerät. Mittlerweile zirkulierte John von Neumann an der Moore School of Electrical Engineering, University of Pennsylvania, seinen ersten Entwurf eines Berichts über den EDVAC 1945. Obwohl Turings Design im Wesentlichen ähnlich und vergleichsweise wenig technische Details enthält, wurde die von ihm umrissene Computerarchitektur als "von Neumann Architektur" bekannt. Turing hat dem National Physical Laboratory (NPL) ein detaillierteres Papier vorgelegt. Executive Committee im Jahr 1946, geben die erste recht vollständige Konstruktion eines gespeicherten Programms Computer, ein Gerät, das er die Automatische Computing Engine (ACE) nannte. Das bekannte EDVAC-Design von John von Neumann, der von Turings theoretischer Arbeit wusste, erhielt jedoch trotz seiner unvollkommenen Natur und fragwürdigem Mangel an Zuschreibung der Quellen einiger Ideen mehr Öffentlichkeit. Turing dachte, dass die Geschwindigkeit und die Größe des Computerspeichers entscheidende Elemente waren, so schlug er einen High-Speed-Speicher von dem, was heute genannt würde 25 KB, Zugriff auf eine Geschwindigkeit von 1 MHz. Die ACE implementierte Unterroutine-Anrufe, während der EDVAC nicht, und die ACE auch verwendet Abbreviated Computer Instructions, eine frühe Form der Programmiersprache. Manchester Baby The Manchester Baby war der weltweit erste elektronische Programmcomputer. Es wurde an der Victoria University of Manchester von Frederic C. Williams, Tom Kilburn und Geoff Tootill gebaut und lief sein erstes Programm am 21. Juni 1948. Die Maschine war nicht als praktischer Computer gedacht, sondern wurde als Testbett für die Williams-Röhre, die erste zufällig zugängliche digitale Speichereinrichtung, ausgebildet. Erfunden von Freddie Williams und Tom Kilburn an der Universität Manchester 1946 und 1947 war es eine Kathodenstrahlröhre, die einen Effekt namens Sekundäremission verwendet, um elektronische binäre Daten vorübergehend zu speichern, und wurde erfolgreich in mehreren frühen Computern verwendet. Obwohl der Computer klein und primitiv war, war es ein Beweis für das Konzept für die Lösung eines einzigen Problems; Baby war die erste Arbeitsmaschine, um alle Elemente, die für einen modernen elektronischen Computer essentiell enthalten. Sobald das Baby die Machbarkeit seines Designs demonstriert hatte, wurde an der Universität ein Projekt initiiert, um das Design zu einem nutzbareren Computer zu entwickeln, dem Manchester Mark 1. Das Mark 1 wurde wiederum schnell zum Prototyp des Ferranti Mark 1, dem weltweit ersten kommerziell erhältlichen Universalcomputer. Das Baby hatte eine 32-Bit-Wortlänge und eine Erinnerung an 32 Wörter. Da es sich um den einfachsten Speicherprogrammrechner handelte, waren die einzigen in Hardware implementierten Rechenoperationen Subtraktion und Negation; andere Rechenoperationen wurden in Software implementiert. Die erste von drei für die Maschine geschriebenen Programme fand die höchste richtige Divisor von 218 (262,144) eine Berechnung, die bekannt war, würde eine lange Zeit dauern, um zu laufen - und so beweisen, die Zuverlässigkeit des Computers -, indem jede ganze Zahl von 218 - 1 nach unten, wie Division durch wiederholte Subtraktion des Divisors durchgeführt wurde. Das Programm bestand aus 17 Anweisungen und lief für 52 Minuten, bevor die richtige Antwort von 131.072, nachdem das Baby 3.5 Millionen Operationen durchgeführt hatte (für eine effektive CPU-Geschwindigkeit von 1,1 kIPS). Die aufeinanderfolgenden Annäherungen an die Antwort wurden als die aufeinanderfolgenden Positionen eines hellen Punktes auf dem Williams-Röhrchen angezeigt. Manchester Mark 1Die Experimentalmaschine führte zur Entwicklung des Manchester Mark 1 an der Universität Manchester. Die Arbeit begann im August 1948, und die erste Version war bis April 1949 betriebsbereit; ein Programm geschrieben, um nach Mersenne Primes zu suchen, lief neun Stunden lang in der Nacht vom 16./17. Juni 1949 fehlerfrei. Die erfolgreiche Operation der Maschine wurde in der britischen Presse weit berichtet, die den Begriff "elektronisches Gehirn" verwendet, um sie ihren Lesern zu beschreiben. Der Computer ist aufgrund seiner wegweisenden Aufnahme von Indexregistern besonders historisch bedeutsam, eine Innovation, die es einem Programm erleichtert, sequentiell durch eine Reihe von Wörtern im Speicher zu lesen. Dreiunddreißig Patente resultierten aus der Entwicklung der Maschine, und viele der Ideen hinter ihrem Design wurden in spätere kommerzielle Produkte wie die IBM 701 und 702 sowie die Ferranti Mark 1 integriert. Die Chef-Designer, Frederic C. Williams und Tom Kilburn, aus ihren Erfahrungen mit dem Mark 1, dass Computer mehr in wissenschaftlichen Rollen als in reiner Mathematik verwendet werden. 1951 begannen sie mit der Entwicklung an Meg, dem Nachfolger von Mark 1, der eine Floating-Point-Einheit umfassen würde. EDSAC Der andere Kontender für den ersten erkennbar modernen digitalen Speicherprogramm-Computer war der EDSAC, entworfen und gebaut von Maurice Wilkes und sein Team an der University of Cambridge Mathematical Laboratory in England an der University of Cambridge in 1949. Die Maschine wurde von John von Neumanns halbnaler Erster Entwurf eines Berichts über den EDVAC inspiriert und war einer der ersten nützlich funktionsfähigen elektronischen Digitalspeicherprogrammcomputer. EDSAC lief seine ersten Programme am 6. Mai 1949, als es eine Tabelle der Quadrate und eine Liste der Hauptzahlen berechnete. Der EDSAC diente auch als Grundlage für den ersten kommerziell angewandten Computer, den LEO I, der von der Lebensmittelhersteller J. Lyons & Co. Ltd. verwendet wurde. Der EDSAC 1 wurde am 11. Juli 1958 endgültig stillgelegt, nachdem er von EDSAC 2 überholt wurde, der bis 1965 in Betrieb war. Das Gehirn [Computer] kann eines Tages auf unser Niveau [der gemeinsamen Menschen] kommen und mit unserer Einkommensteuer- und Buchhaltungsrechnung helfen. Aber das ist Spekulation und es gibt bisher kein Zeichen davon. EDVAC ENIAC Erfinder John Mauchly und J. Presper Eckert schlugen im August 1944 den Bau des EDVAC vor, und die Planungsarbeit für den EDVAC begann an der Moore School of Electrical Engineering der University of Pennsylvania, bevor die ENIAC voll funktionsfähig war. Das Design implementierte eine Reihe von wichtigen architektonischen und logischen Verbesserungen, die während des Baus von ENIAC konzipiert wurden, und einen Hochgeschwindigkeits-Serien-Access-Speicher. Eckert und Mauchly verließen jedoch das Projekt und seine Konstruktion floss. Es wurde schließlich im August 1949 an das Ballistiklabor der US-Armee am Aberdeen Proving Ground geliefert, aber aufgrund einer Reihe von Problemen begann der Computer erst 1951, und dann nur auf begrenzter Basis. Handelsrechner Der erste kommerzielle Computer war die Ferranti Mark 1, gebaut von Ferranti und lieferte an die Universität Manchester im Februar 1951. Es basierte auf dem Manchester Mark 1. Die wichtigsten Verbesserungen gegenüber dem Manchester Mark 1 waren in der Größe des Primärspeichers (unter Verwendung von zufälligen Zutritt Williams-Röhren), Sekundärspeicher (unter Verwendung einer Magnettrommel), ein schnellerer Multiplikator und zusätzliche Anweisungen.Die Grundzykluszeit betrug 1,2 Millisekunden und eine Multiplikation konnte in etwa 2,16 Millisekunden abgeschlossen werden. Der Multiplikator verwendet fast ein Viertel der 4,050 Vakuumröhren der Maschine (Ventile). Eine zweite Maschine wurde von der University of Toronto gekauft, bevor das Design in den Mark 1 Star überarbeitet wurde. Mindestens sieben dieser späteren Maschinen wurden zwischen 1953 und 1957 geliefert, einer davon in Shell Labors in Amsterdam. Im Oktober 1947 beschlossen die Direktoren von J. Lyons & Company, einem britischen Catering-Unternehmen, das für seine Teashops berühmt ist, aber mit starken Interessen an neuen Büroverwaltungstechniken, eine aktive Rolle bei der Förderung der kommerziellen Entwicklung von Computern zu spielen. Der LEO I Computer (Lyons Electronic Office) wurde im April 1951 in Betrieb genommen und führte die weltweit erste regelmäßige Routine-Office-Computer-Job. Am 17. November 1951 startete die Firma J. Lyons wöchentlichen Betrieb eines Bäckerei-Bewertungsauftrags am LEO – die erste Geschäftsanwendung, um auf einem gespeicherten Programmcomputer live zu gehen. Im Juni 1951 wurde der UNIVAC I (Universal Automatic Computer) an das US Census Bureau geliefert. Remington Rand verkaufte schließlich 46 Maschinen mit jeweils mehr als 1 Million US$ ($9,97 Millionen ab 2021). UNIVAC war der erste "Mass produziert" Computer. Es verwendete 5,200 Vakuumröhren und verbrauchte 125 kW Leistung. Seine primäre Speicherung war seriell zugängliche Quecksilber-Verzögerungslinien, die in der Lage waren, 1.000 Wörter von 11 Dezimalziffern plus Zeichen (72-Bit Wörter) zu speichern. IBM führte 1954 einen kleineren, kostengünstigeren Computer ein, der sich als sehr beliebt erwies. Die IBM 650 wiegte über 900 kg, die angeschlossene Stromversorgung wiegte etwa 1350 kg und beide wurden in separaten Schränken von etwa 1,5 Metern um 0,9 Meter um 1,8 Meter gehalten. Das System kostet 500.000 US$ ($4,82 Millionen ab 2021) oder könnte für 3.500 US$ pro Monat ($30.000 ab 2021) vermietet werden. Sein Trommelspeicher war ursprünglich 2.000 zehnstellige Wörter, später auf 4.000 Wörter erweitert. Memory-Beschränkungen wie diese waren, die Programmierung für Jahrzehnte danach zu dominieren. Die Programmanweisungen wurden von der Spinntrommel abgeholt, als der Code lief. Effiziente Ausführung mit Trommelspeicher wurde durch eine Kombination von Hardware-Architektur – das Instruktionsformat umfasste die Adresse der nächsten Instruktion – und Software: das Symbolische Optimal Assembly Program, SOAP, zugewiesenen Anweisungen an die optimalen Adressen (soweit durch statische Analyse des Quellprogramms möglich). So wurden bei Bedarf viele Anweisungen in der nächsten Zeile der zu lesenden Trommel gefunden und zusätzliche Wartezeit auf Trommeldrehung reduziert. Mikroprogrammierung 1951, britischer Wissenschaftler Maurice Wilkes entwickelte das Konzept der Mikroprogrammierung aus der Erkenntnis, dass die zentrale Verarbeitungseinheit eines Computers durch ein miniatures, hochspezialisiertes Computerprogramm in Highspeed-ROM gesteuert werden könnte. Die Mikroprogrammierung erlaubt es, den Basisanweisungssatz durch integrierte Programme (jetzt Firmware oder Mikrocode genannt) zu definieren oder zu erweitern. Dieses Konzept vereinfacht die CPU-Entwicklung. Er beschrieb dies zunächst an der Universität Manchester Computer Inaugural Conference 1951, dann in erweiterter Form in IEEE Spectrum 1955 veröffentlicht. Es wurde in den CPUs und Floating-Point-Einheiten von Mainframe und anderen Computern weit verbreitet; es wurde erstmals in EDSAC 2 implementiert, die auch mehrere identische "Bit-Slices" verwendet, um das Design zu vereinfachen. Für jedes Bit des Prozessors wurden austauschbare, austauschbare Rohranordnungen verwendet. Magnetische Erinnerung Magnetische Trommelspeicher wurden für die US-Marine während der WW II mit der Arbeit an Engineering Research Associates (ERA) in 1946 und 1947 entwickelt. ERA, dann ein Teil von Univac enthalten einen Trommelspeicher in seinem 1103, angekündigt im Februar 1953. Der erste Massenrechner, der 1953 ebenfalls angekündigte IBM 650, hatte etwa 8,5 Kilobyte Trommelspeicher. Magnetkernspeicher patentiert 1949 mit seiner ersten Verwendung für den Whirlwind-Computer im August 1953. Kommerzialisierung folgte schnell. Magnetkern wurde in Peripheriegeräten des im Juli 1955 gelieferten IBM 702 und später im 702 selbst verwendet. Der IBM 704 (1955) und der Ferranti Mercury (1957) verwendeten Magnetkernspeicher. Es ging weiter, um das Feld in die 1970er Jahre zu dominieren, als es durch Halbleiterspeicher ersetzt wurde. Der Magnetkern stieg im Volumen um 1975 und sank danach in der Nutzung und dem Marktanteil. Bereits 1980 waren PDP-11/45-Maschinen mit Magnetkern-Hauptspeicher und Trommeln zum Swapping noch an vielen der ursprünglichen UNIX-Standorte im Einsatz. Frühe digitale Computereigenschaften Transistoren Der Bipolartransistor wurde 1947 erfunden. Ab 1955 ersetzten die Transistoren die Vakuumröhren in Computerkonstruktionen, was die "zweite Generation" der Computer hervorruft. Im Vergleich zu Vakuumröhren haben Transistoren viele Vorteile: sie sind kleiner und erfordern weniger Leistung als Vakuumröhren, so geben weniger Wärme ab. Silicon-Übergangstransistoren waren viel zuverlässiger als Vakuumröhren und hatten eine längere Lebensdauer. Transistorisierte Rechner könnten Zehntausende binäre Logikschaltungen in einem relativ kompakten Raum enthalten. Die Transistoren reduzierten die Computergröße, die Anfangskosten und die Betriebskosten erheblich. Typischerweise bestanden Computer der zweiten Generation aus einer Vielzahl von Leiterplatten wie dem IBM Standard Modular System, die jeweils ein bis vier logische Gatter oder Flip-Flops tragen. An der Universität Manchester entwarf ein Team unter der Leitung von Tom Kilburn eine Maschine mit den neu entwickelten Transistoren anstelle von Ventilen. Ursprünglich waren die einzigen verfügbaren Geräte Germanium-Punkt-Kontakt-Transistoren, weniger zuverlässig als die von ihnen ausgetauschten Ventile, die aber viel weniger Leistung verbrauchten. Ihr erster Transistorrechner und der erste der Welt war bis 1953 in Betrieb, und dort wurde im April 1955 eine zweite Version fertiggestellt. Die 1955-Version verwendet 200 Transistoren, 1.300 Festkörperdioden und hatte einen Stromverbrauch von 150 Watt. Die Maschine nutzte jedoch Ventile, um ihre 125 kHz-Taktwellenformen zu erzeugen und in der Schaltung auf ihren magnetischen Trommelspeicher zu lesen und zu schreiben, so dass es nicht der erste vollständig Transistorrechner war. Diese Unterscheidung geht an die Harwell CADET von 1955, gebaut von der Elektronikabteilung der Atomenergieforschungsanstalt in Harwell. Das Design verfügte über einen 64-Klobyte Magnettrommelspeicher mit mehreren beweglichen Köpfen, die am National Physical Laboratory, UK, entworfen worden waren. Bis 1953 hatte dieses Team Transistorschaltungen, die zum Lesen und Schreiben einer kleineren Magnettrommel des Royal Radar Establishment tätig waren. Die Maschine verwendet eine geringe Taktgeschwindigkeit von nur 58 kHz, um zu vermeiden, dass alle Ventile zur Erzeugung der Taktwellenformen verwendet werden. CADET nutzte 324-Punkt-Kontakt-Transistoren der britischen Firma Standard Telefone und Kabel; 76 Anschluss-Transistoren wurden für die ersten Stufenverstärker für Daten aus der Trommel verwendet, da Punkt-Kontakt-Transistoren zu laut waren. Ab August 1956 bietet CADET einen regelmäßigen Rechendienst an, bei dem es oft kontinuierliche Rechenläufe von 80 Stunden oder mehr ausgeführt hat. Probleme mit der Zuverlässigkeit von frühen Chargen von Punktkontakt und legierten Verbindungstransistoren bedeuteten, dass die mittlere Zeit der Maschine zwischen Ausfällen etwa 90 Minuten betrug, aber dies verbesserte sich, sobald die zuverlässigeren bipolaren Verbindungstransistoren zur Verfügung standen. Das Design des Manchester University Transistor Computer wurde von der lokalen Ingenieurfirma von Metropolitan-Vickers in ihrem Metrovick 950, dem ersten kommerziellen Transistor Computer überall angenommen. Sechs Metrovick 950s wurden gebaut, die erste abgeschlossen 1956. Sie wurden erfolgreich in verschiedenen Abteilungen des Unternehmens eingesetzt und waren etwa fünf Jahre im Einsatz. Ein Computer der zweiten Generation, der IBM 1401, erfasste etwa ein Drittel des Weltmarktes. IBM installierte zwischen 1960 und 1964 mehr als zehntausend 1401s. Transistor-Peripheriegeräte Die transistorisierte Elektronik verbesserte nicht nur die CPU (Central Processing Unit), sondern auch die peripheren Geräte. Die Datenspeichereinheiten der zweiten Generation konnten zehn Millionen von Buchstaben und Ziffern speichern. Neben den Festplattenspeichern, die über Hochgeschwindigkeits-Datenübertragung mit der CPU verbunden sind, waren abnehmbare Datenspeichereinheiten für Datenträger. Ein abnehmbarer Scheibenpack lässt sich in wenigen Sekunden leicht mit einer anderen Packung austauschen. Auch wenn die abnehmbare Festplattenkapazität kleiner als feste Festplatten ist, garantiert ihre Austauschbarkeit eine nahezu unbegrenzte Anzahl von Daten nahe bei der Hand. Magnetband lieferte Archivfähigkeit für diese Daten, zu einem geringeren Kosten als Scheibe. Viele CPUs der zweiten Generation delegierten periphere Gerätekommunikationen an einen sekundären Prozessor. Zum Beispiel, während der Kommunikationsprozessor gesteuerte Kartenlesung und Stanzung, die Haupt-CPU ausgeführt Berechnungen und binäre Zweiganweisungen. Ein Datenbus würde bei der fetch-execute-Zyklusrate der CPU Daten zwischen der Haupt-CPU und dem Kernspeicher tragen, und andere Datenbusse würden in der Regel den peripheren Geräten dienen. Auf der PDP-1 betrug die Zykluszeit des Kernspeichers 5 Mikrosekunden; folglich dauerten die meisten arithmetischen Anweisungen 10 Mikrosekunden (100.000 Operationen pro Sekunde), da die meisten Operationen mindestens zwei Speicherzyklen dauerten; eine für die Instruktion, eine für den Operanden-Daten-Fetch. Während der zweiten Generation haben Remote-Terminaleinheiten (oft in Form von Teleprintern wie einem Friden Flexowriter) den Einsatz stark gesteigert. Telefonverbindungen lieferten ausreichend Geschwindigkeit für frühe Remote-Terminals und erlaubten Hunderte von Kilometern Trennung zwischen Remote-Terminals und Rechenzentrum. Letztendlich würden diese eigenständigen Computernetze in ein vernetztes Netz von Netzwerken - das Internet - verallgemeinert werden. Transistor-Supercomputer Die frühen 1960er Jahre sahen das Aufkommen von Supercomputing. Der Atlas war eine gemeinsame Entwicklung zwischen der University of Manchester, Ferranti und Plessey und wurde erstmals an der Manchester University installiert und im Jahr 1962 offiziell als einer der weltweit ersten Supercomputer in Auftrag gegeben – als der mächtigste Computer der Welt damals. Es wurde gesagt, dass, wenn Atlas offline ging die Hälfte der Computerkapazität des Vereinigten Königreichs verloren. Es war eine zweite Generation Maschine, mit diskreten Germanium-Transistoren. Atlas leitete auch den Atlas Supervisor, "berücksichtigt von vielen, um das erste erkennbare moderne Betriebssystem zu sein". In den USA wurde eine Reihe von Computern der Control Data Corporation (CDC) von Seymour Cray entwickelt, um innovative Designs und Parallelismus zu nutzen, um eine überlegene rechnerische Spitzenleistung zu erzielen. Die 1964 veröffentlichte CDC 6600 gilt in der Regel als erster Supercomputer. Der CDC 6600 hat seinen Vorgänger, den IBM 7030 Stretch, um etwa den Faktor 3 übertroffen. Mit einer Leistung von ca. 1 MegaFLOPS war die CDC 6600 von 1964 bis 1969 der schnellste Computer der Welt, als sie diesen Status seinem Nachfolger, der CDC 7600, zurückwies. Integrierte Schaltungsrechner Die dritte Generation von digitalen elektronischen Computern verwendet integrierte Schaltung (IC) Chips als Grundlage ihrer Logik. Die Idee einer integrierten Schaltung wurde von einem Radarwissenschaftler konzipiert, der für die Royal Radar Establishment des Ministeriums für Verteidigung, Geoffrey W.A Dummer arbeitet. Die ersten integrierten Schaltkreise wurden von Jack Kilby auf Texas Instruments und Robert Noyce auf Fairchild Semiconductor erfunden. Kilby hat seine ersten Ideen zum integrierten Schaltkreis im Juli 1958 aufgenommen und am 12. September 1958 erfolgreich das erste integrierte Arbeitsbeispiel gezeigt. Die Erfindung von Kilby war eine hybride integrierte Schaltung (Hybrid IC). Es hatte externe Drahtverbindungen, was es schwierig machte, massenweise herzustellen. Noyce kam mit seiner eigenen Idee einer integrierten Schaltung ein halbes Jahr nach Kilby. Die Erfindung von Noyce war ein monolithischer Chip (IC). Sein Chip löste viele praktische Probleme, die Kilby nicht hatte. Produziert auf Fairchild Semiconductor, es wurde aus Silizium, während Kilby's Chip aus Germanium. Die Basis für Noyces monolithisches IC war Fairchilds Planarprozess, der es ermöglichte, integrierte Schaltungen anhand der gleichen Prinzipien wie die der gedruckten Schaltungen festzulegen. Der Planarprozess wurde Anfang 1959 von Noyces Kollegin Jean Hoerni entwickelt, basierend auf Mohamed M. Atallas Arbeit an der Halbleiteroberflächenpassivation von Siliziumdioxid in Bell Labs in den späten 1950er Jahren. Die Computer der dritten Generation (integrierte Schaltung) erschienen in den frühen 1960er Jahren zunächst in Computern, die für Regierungszwecke entwickelt wurden, und dann in kommerziellen Computern, die Mitte der 1960er Jahre beginnen. Der erste Silizium-IC-Computer war der Apollo Guidance Computer oder AGC. Obwohl nicht der mächtigste Computer seiner Zeit, die extremen Zwänge auf Größe, Masse und Leistung des Apollo Raumfahrzeugs erforderte die AGC viel kleiner und dichter als jeder vorherige Computer, Gewicht in nur 70 Pfund (32 kg.) Jede Mondlandemission führte zwei AGCs, jeweils eine in den Befehls- und Mondansteigungsmodulen. Der MOSFET (Metalloxid-Halbleiter-Feldeffekttransistor oder MOS-Transistor) wurde 1959 von Mohamed M. Atalla und Dawon Kahng in Bell Labs erfunden. Neben der Datenverarbeitung ermöglichte der MOSFET den praktischen Einsatz von MOS-Transistoren als Speicherzellenspeicherelemente, einer bisher von magnetischen Kernen bedienten Funktion. Halbleiterspeicher, auch MOS-Speicher genannt, war billiger und verbraucht weniger Leistung als Magnetkern-Speicher. MOS-Zufallsspeicher (RAM,) in Form von statischem RAM (SRAM) wurde 1964 von John Schmidt auf Fairchild Semiconductor entwickelt. 1966 entwickelte Robert Dennard im IBM Thomas J. Watson Research Center MOS Dynamic RAM (DRAM). 1967 entwickelten Dawon Kahng und Simon Sze bei Bell Labs den Floating-Gate-MOSFET, die Basis für MOS-nichtflüchtige Speicher wie EPROM, EEPROM und Flash-Speicher. Mikroprozessoren Die vierte Generation von digitalen elektronischen Computern verwendet Mikroprozessoren als Grundlage ihrer Logik. Der Mikroprozessor hat Ursprünge im MOS-integrierten Schaltkreis (MOS IC) Chip. Aufgrund der schnellen MOSFET-Skalierung stiegen die MOS-IC-Chips mit einer von Moore's Gesetz vorhergesagten Rate rasch an Komplexität, was zu einer großflächigen Integration (LSI) mit Hunderten von Transistoren auf einem einzigen MOS-Chip bis Ende der 1960er Jahre führte. Die Anwendung von MOS LSI-Chips zur Berechnung war die Grundlage für die ersten Mikroprozessoren, da Ingenieure erkannten, dass ein vollständiger Computerprozessor auf einem einzigen MOS LSI-Chip enthalten sein könnte. Das Thema, welches Gerät der erste Mikroprozessor war, ist befriedigend, zum Teil durch mangelnde Übereinstimmung über die genaue Definition des Begriffs Mikroprozessor". Die frühesten Multichip-Mikroprozessoren waren die Four-Phase Systems AL-1 1969 und Garrett AiResearch MP944 1970, entwickelt mit mehreren MOS LSI-Chips. Der erste Single-Chip-Mikroprozessor war der Intel 4004, der auf einem einzigen PMOS LSI-Chip entwickelt wurde. Es wurde von Ted Hoff, Federico Faggin, Masatoshi Shima und Stanley Mazor bei Intel entworfen und realisiert und 1971 veröffentlicht. Tadashi Sasaki und Masatoshi Shima bei Busicom, einem Rechnerhersteller, hatten die ersten Erkenntnisse, dass die CPU ein einziger MOS LSI-Chip sein könnte, der von Intel geliefert wird. Während die frühesten Mikroprozessor-ICs buchstäblich nur den Prozessor, d.h. die zentrale Verarbeitungseinheit, eines Computers enthielten, führte ihre fortschreitende Entwicklung natürlich zu Chips, die die meisten oder alle internen elektronischen Teile eines Computers enthielten. Die integrierte Schaltung im Bild rechts, beispielsweise ein Intel 8742, ist ein 8-Bit-Mikrocontroller, der eine CPU mit 12 MHz, 128 Bytes RAM, 2048 Bytes EPROM und I/O im gleichen Chip umfasst. In den 1960er Jahren gab es erhebliche Überschneidungen zwischen Technologien der zweiten und dritten Generation. IBM hat 1964 seine IBM Solid Logic Technology Module in Hybridschaltungen für das IBM System/360 implementiert. Bereits 1975 setzte Sperry Univac die Herstellung von Maschinen der zweiten Generation wie der UNIVAC 494 fort. Die Burroughs große Systeme wie die B5000 waren Stapelmaschinen, die eine einfachere Programmierung erlaubten. Diese Pushdown-Automaten wurden später auch in Minicomputern und Mikroprozessoren implementiert, was das Programmiersprachendesign beeinflusste. Minicomputer dienten als kostengünstige Rechenzentren für Industrie, Wirtschaft und Universitäten. Es wurde möglich, analoge Schaltungen mit dem Simulationsprogramm mit integriertem Schaltungsbeton oder SPICE (1971) auf Minicomputern zu simulieren, eines der Programme für die elektronische Designautomatisierung (EDA). Der Mikroprozessor führte zur Entwicklung von Mikrocomputern, kleinen, kostengünstigen Computern, die Personen und kleinen Unternehmen gehören könnten. Mikrocomputer, die in den 1970er Jahren erstmals erschienen sind, wurden in den 1980er und darüber hinaus allgegenwärtig. Während das spezifische System als erster Mikrocomputer betrachtet wird, ist es eine Frage der Debatte, da es mehrere einzigartige Hobbysysteme auf der Basis des Intel 4004 und seines Nachfolgers entwickelt, der Intel 8008, der erste kommerziell verfügbare Mikrocomputer-Kit war der Intel 8080-basierte Altair 8800, der im Januar 1975 veröffentlicht wurde, Cover-Artikel von Popular Electronics. Dies war jedoch ein äußerst begrenztes System in seinen Anfangsstufen, mit nur 256 Bytes von DRAM in seinem Anfangspaket und keinen Eingang außer seinen Kniehebelschaltern und LED-Registeranzeige. Trotzdem war es zunächst überraschend beliebt, mit mehreren hundert Verkäufen im ersten Jahr, und Nachfrage schnell übertroffen Angebot. Mehrere frühere Drittanbieter wie Cromemco und Processor Technology begannen, für den Altair 8800 zusätzliche S-100 Bushardware zu liefern. Im April 1975 auf der Hannover Messe präsentierte Olivetti die P6060, das weltweit erste komplette, vormontierte Computersystem. Die zentrale Verarbeitungseinheit bestand aus zwei Karten, den Code PUCE1 und PUCE2, und im Gegensatz zu den meisten anderen Personalcomputern wurde mit TTL-Komponenten anstelle eines Mikroprozessors gebaut. Es hatte ein oder zwei 8" Diskettenlaufwerke, eine 32-Charakter-Plasmaanzeige, 80-Säulen-Grafik-Thermaldrucker, 48 Kbytes RAM und BASIC-Sprache. Es wiegt 40 kg (88 lb). Als komplettes System war dies ein bedeutender Schritt vom Altair, obwohl es nie den gleichen Erfolg erreichte. Es war im Wettbewerb mit einem ähnlichen Produkt von IBM, die ein externes Diskettenlaufwerk hatte. Von 1975 bis 1977 wurden die meisten Mikrocomputer wie die MOS Technology KIM-1, der Altair 8800 und einige Versionen des Apple I als Kits für Do-it-yourselfers verkauft. Vormontierte Systeme konnten bis 1977 nicht viel Boden gewinnen, mit der Einführung des Apple II, des Tandy TRS-80, der ersten SWTPC-Computer und des Commodore PET. Computing hat sich mit Mikrocomputer-Architekturen entwickelt, mit Funktionen von ihren größeren Brüdern hinzugefügt, jetzt dominant in den meisten Marktsegmenten. Ein NeXT Computer und seine objektorientierten Entwicklungstools und Bibliotheken wurden von Tim Berners-Lee und Robert Cailliau am CERN verwendet, um die weltweit erste Webserver-Software CERN httpd zu entwickeln und den ersten Webbrowser WorldWideWeb zu schreiben. Systeme so kompliziert wie Computer erfordern sehr hohe Zuverlässigkeit. Seit acht Jahren blieb ENIAC im laufenden Betrieb von 1947 bis 1955, bevor sie stillgelegt wurde. Obwohl ein Vakuumschlauch scheitern könnte, würde es ersetzt werden, ohne das System herunterzubringen. Durch die einfache Strategie, ENIAC nie abzuschalten, wurden die Fehler drastisch reduziert. Die Vakuum-Röhre SAGE Air-Defense-Computer wurden bemerkenswert zuverlässig – paarweise installiert, eine Offline, Rohre wahrscheinlich scheitern, so dass, wenn der Computer bewusst mit reduzierter Leistung laufen, um sie zu finden. Hot-pluggable Festplatten, wie die Hot-pluggable Vakuumröhren von jateryear, weiterhin die Tradition der Reparatur während des Dauerbetriebs. Halbleiterspeicher haben bei ihrem Betrieb routinemäßig keine Fehler, obwohl Betriebssysteme wie Unix beim Start-up Speichertests verwendet haben, um fehlerhafte Hardware zu erkennen. Heute wird die Anforderung der zuverlässigen Leistung noch strenger gemacht, wenn Serverfarmen die Lieferplattform sind. Google hat dies durch die Verwendung von fehlertoleranter Software geschafft, um sich von Hardware-Ausfällen zu erholen, und arbeitet sogar an dem Konzept des Ersetzens von gesamten Serverfarmen on-the-fly, während eines Service-Events. Im 21. Jahrhundert wurden Multi-Core-CPUs kommerziell erhältlich. Content-Addressable Memory (CAM) ist in der Vernetzung kostengünstig genug geworden und wird häufig für On-Chip-Cache-Speicher in modernen Mikroprozessoren verwendet, obwohl noch kein Computersystem Hardware CAMs zur Verwendung in Programmiersprachen implementiert hat.Derzeit sind CAMs (oder assoziative Arrays) in der Software programmiersprachlich-spezifisch. Halbleiter-Speicherzellen-Arrays sind sehr regelmäßige Strukturen, und Hersteller beweisen ihre Prozesse auf ihnen; dies ermöglicht Preissenkungen auf Speicherprodukten. In den 1980er Jahren entwickelten sich CMOS-Logik-Gatter zu Geräten, die so schnell wie andere Schaltungstypen gemacht werden konnten; der Computer-Stromverbrauch konnte daher drastisch gesenkt werden. Im Gegensatz zum kontinuierlichen Stromabzug eines auf anderen Logiktypen basierenden Gatters zieht ein CMOS-Gatter während des Übergangs zwischen logischen Zuständen nur mit Ausnahme von Leckage signifikanten Strom. CMOS-Schaltungen haben es ermöglicht, Computing zu einer Ware zu werden, die jetzt in vielen Formen eingebettet ist, von Grußkarten und Telefonen zu Satelliten. Die während des Betriebes abgeführte thermische Auslegungsleistung ist als Rechengeschwindigkeit des Betriebes wesentlich geworden. Im Jahr 2006 verbrauchten Server 1,5% des gesamten Energiebudgets der USA.Der Energieverbrauch von Rechenzentren wurde voraussichtlich bis 2011 auf 3% des Weltverbrauchs verdoppeln. Das SoC (System auf einem Chip) hat noch mehr der integrierten Schaltkreise in einen einzigen Chip komprimiert; SoCs ermöglichen Telefone und PCs, in einzelne handgehaltene drahtlose Geräte zu konvergieren. Quantum Computing ist eine neue Technologie im Bereich Computing. MIT Technologie Review berichtete vom 10. November 2017, dass IBM einen 50-Qubit-Computer erstellt hat; aktuell hält sein Quantenzustand 50 Mikrosekunden. Google-Forscher konnten die 50 Mikrosekunden-Zeitgrenze verlängern, wie es am 14. Juli 2021 in Nature berichtet wurde; die Stabilität wurde um das 100-fache erweitert, indem sie eine einzige logische Qubit-Ketten über Datenqubits zur Quantenfehlerkorrektur verteilten. Physikalische Untersuchung X berichtete am 26. November 2018 eine Technik zur 'Single-Gate-Sensierung als tragfähiges Ausleseverfahren für Spin-Qubits' (ein Singlet-Triplet-Spin-Zustand in Silizium). A Google-Team hat es geschafft, ihren HF-Impulsmodulatorchip bei 3 Kelvin zu betreiben, wodurch die Kryoogene ihres 72-Qubit-Computers vereinfacht werden, der auf 0,3 Kelvin ausgelegt ist, aber die Ausleseschaltung und ein anderer Treiber bleiben in die Kryogene gebracht werden. Siehe: Quantum supremacy Silicon Qubit-Systeme haben die Verschränkung in nicht-lokalen Abständen demonstriert. Computing Hardware und seine Software sind sogar eine Metapher für den Betrieb des Universums geworden. Epilog Ein Hinweis auf die Schnelligkeit der Entwicklung dieses Feldes kann aus der Geschichte des Halbnals 1947 Artikel von Burks, Goldstine und von Neumann abgeleitet werden. Als jemand Zeit hatte, etwas aufzuschreiben, war es veraltet. Nach 1945 lesen andere John von Neumanns Erster Entwurf eines Berichts über den EDVAC und begannen sofort mit der Umsetzung ihrer eigenen Systeme. Bis heute hat sich das rasante Entwicklungstempo weltweit fortgesetzt. Ein 1966er Artikel in der Zeit vorhergesagt, dass: "Durch 2000 werden die Maschinen so viel produzieren, dass jeder in den USA wird in der Tat unabhängig wohlhabend sein. Wie Freizeit zu nutzen, wird ein großes Problem sein." Siehe auch Antikythera-Mechanismus Geschichte des Computing Information Age IT History Society Timeline of Computing Liste der Pioniere in der Informatik Vacuum Tube Computer Notes Referenzen Weiter lesen "Online-Zugang". IEEE Annals der Geschichte des Computing. Archiviert aus dem Original auf 2006-05-23.Ceruzzi, Paul E. (1998,) Eine Geschichte des modernen Computing, The MIT Press Computers and Automation Magazine – Pictorial Report on the Computer Field: A PICTORIAL INTRODUCTION TO COMPUTERS – 06/1957 A PICTORIAL MANUAL ON COMPUTERS – 12/1957 A PICTORIAL MANUAL ON COMPUTERS, Part19 Pictorial Report on the Computer Field – Dezember-Ausgaben (195812.pdf, ..., 196712.pdf) Bit by Bit: Eine illustrierte Geschichte von Computern, Stan Augarten, 1984.OCR mit Erlaubnis des Autors Externe Links Obsolete Technologie – Alte Computer Geschichte der Rechentechnik Historische Computer in JapanDie Geschichte der japanischen mechanischen Berechnungsmaschinen Computergeschichte - eine Sammlung von Artikeln von Bob Bemer 25 Microchips, die die Welt schüttelten - eine Sammlung von Artikeln des Instituts für Elektro- und Elektronik Computer-Engine WahlsystemAlle 650 Mitglieder des britischen Parlaments werden mit dem ersten Wahlsystem der Post in den einzelnen Mitgliedskreisen des gesamten Vereinigten Königreichs gewählt, wo jeder Wahlkreis seinen eigenen Vertreter hat. Wahlen Alle MP-Positionen werden gleichzeitig frei für Wahlen, die an einem fünfjährigen Zyklus abgehalten werden. Die Parlamente Das Gesetz 2011 legt fest, dass am ersten Donnerstag im Mai alle fünf Jahre gewöhnliche allgemeine Wahlen abgehalten werden. Mit Zustimmung des Parlaments fanden jedoch sowohl die allgemeinen Wahlen 2017 als auch 2019 vor dem vom Gesetz festgelegten Zeitplan statt. Ergibt sich zu einem anderen Zeitpunkt aufgrund des Todes oder des Rücktritts eine Leerstelle, so kann eine Ersatzfreistellung durch eine Durchwahl befüllt werden. Unter der Darstellung des Volksgesetzes 1981 räumt jeder Abgeordnete, der zu mehr als einem Jahr Gefängnis verurteilt wurde, automatisch seinen Sitz ein. Für bestimmte Arten von weniger Handlungen, die das Recall of MPs Act 2015 falsch machen, ist es erforderlich, dass eine Rückruf-Petition eröffnet wird; wenn sie von mehr als 10% der registrierten Wähler innerhalb des Wahlkreises unterzeichnet wird, wird der Sitz freigelassen. Förderfähigkeit In der Vergangenheit könnten nur männliche erwachsene Eigentumsbesitzer für das Parlament stehen. 1918 erhielten die Frauen das Recht, für das Parlament zu stehen und abzustimmen. Um als Abgeordneter zu stehen, muss eine Person mindestens 18 Jahre alt sein und Bürger des Vereinigten Königreichs, einer Commonwealth Nation oder Irland sein. Eine Person ist nicht verpflichtet, zur Abstimmung registriert zu werden, noch gibt es Einschränkungen in Bezug auf den Aufenthalt eines Kandidaten. Das Haus der Unqualifikation Gesetz 1975 verbietet die Inhaber verschiedener Positionen von MPs. Dazu gehören Beamte, Polizisten, Mitglieder der Streitkräfte und Richter. Mitglieder des Hauses der Herren sind nicht gestattet, die Sitze des Commons zu halten. Mitglieder von Gesetzen außerhalb des Commonwealth sind ausgeschlossen, mit Ausnahme der irischen Gesetzgebung. Darüber hinaus sind die Mitglieder des Senedd (Welsh Parliament) oder der Northern Ireland Assembly auch für die Commons gemäß den Wales und Nordirland (Sonstige Bestimmungen) nicht geeignet. Rechtsakte, die im Jahr 2014 verabschiedet wurden. Leute, die bankrott sind, können nicht als Abgeordneter stehen. Die Vertretung des Volksgesetzes 1981 schließt Personen aus, die derzeit eine Haftstrafe von einem Jahr oder mehr dienen. Titel Mitglieder des Parlaments sind berechtigt, den Postn Nominaltitel MP zu verwenden. Die Abgeordneten werden während der Debatten im Parlament als höflich bezeichnet oder wenn sie das Kind eines Peers sind. Diejenigen, die Mitglieder des Privy-Rates sind, verwenden das Formular Das rechte Honourable (The Rt Hon. oder Rt Hon.) Name MP. Aufgaben Die erste Pflicht eines Parlaments besteht darin, zu tun, was sie in ihrem treuen und uninteressierten Urteil für die Ehre und Sicherheit Großbritanniens halten. Der zweite Zoll ist für ihre Bestandteile, von denen sie der Vertreter sind, aber nicht der Delegierte. Burkes berühmte Erklärung zu diesem Thema ist bekannt. Es ist nur an dritter Stelle, dass ihre Pflicht zur Parteiorganisation oder zum Programm auf Platz nimmt. All diese drei Loyalitäten sollten beobachtet werden, aber es gibt keinen Zweifel an der Ordnung, in der sie unter einer gesunden Manifestation der Demokratie stehen. Theoretisch, zeitgemäß Die Abgeordneten gelten als zwei Aufgaben, oder drei, wenn sie einer politischen Partei angehören. Ihre Hauptaufgabe besteht darin, im nationalen Interesse zu handeln. Sie müssen auch im Interesse ihrer Bestandteile handeln, wenn dies ihre primäre Verantwortung nicht überwindet. Schließlich, wenn sie einer politischen Partei angehören, können sie im Interesse dieser Partei handeln, die den beiden anderen Verantwortlichkeiten untergeordnet ist. Siehe auch Liste der in der Generalwahl 2019 gewählten Abgeordneten Vereinigtes Königreich Parlament Wahlkreise Mitglied des schottischen Parlaments Mitglied des senedd Mitglieds der Legislativversammlung (Nordirland) =Referenzen ==