Weibliche Geschlechterbildung von KI-Technologien ist die Verwendung von künstlichen Intelligenz (KI) Technologien, die als weiblich geschlechtsspezifischer Art geschlechtsspezifischer Art sind, wie beispielsweise in digitaler Stimme oder schriftlichen Assistenten. Diese geschlechtsspezifischen Aspekte von KI-Technologien, die sowohl vom Menschen als auch von Algorithmen geschaffen wurden, wurden in einem Politikpapier 2019 diskutiert und zwei Ergänzungen unter dem Titel, den ich verschwimmen würde, wenn ich könnte. Der Abschluss von Geschlecht teilt sich in digitalen Fähigkeiten durch Bildung. Unter einer Open-Access-Lizenz von EQUALS Global Partnership und der UNESCO veröffentlicht, hat sie weitere Diskussionen über geschlechtsbezogene Vorurteile im globalen virtuellen Raum geführt. KI-gestützte digitale Assistenten Ob eingegeben oder gesprochen, digitale Assistenten ermöglichen und unterstützen menschliche Interaktionen mit der Technologie, indem sie Gespräche mit Nutzern simulieren. KI-betriebene digitale Assistenten finden sich in einer Vielzahl von Geräten und können eine Reihe von Aufgaben ausführen, beispielsweise durch Sprachaktivierung. Digitale Assistenten werden oft als eine oder eine Kombination der folgenden eingestuft: Sprachassistenten Chatbots Virtuelle Agenten Sprachassistenten Sprachassistenten sind Technologie, die den Benutzern durch Sprachausgänge spricht, aber normalerweise keine physische Form projiziert. Sprachassistenten können in der Regel sowohl gesprochene als auch geschriebene Eingaben verstehen, sind aber oft für gesprochene Interaktion ausgelegt. Ihre Ausgänge versuchen in der Regel, natürliche menschliche Sprache zu imitieren. Mainstreaming Sprachassistenten sind zunehmend zentral für Technologieplattformen und in vielen Ländern zum Alltag geworden. Zwischen 2008 und 2018 stieg die Frequenz der stimmbasierten Internet-Suche-Abfragen 35 mal an und macht nun fast ein Fünftel der mobilen Internet-Suche aus. Studien zeigen, dass Sprachassistenten im Laufe des Monats eine Milliarde Aufgaben nach oben verwalten, von der mundane, wie das Wechseln eines Songs oder eines Films, bis zum Wesentlichen, beispielsweise durch Kontakt mit Notdiensten. Technologieforschungsunternehmen schätzen, dass allein 2018 rund 100 Millionen intelligente Lautsprecher mit Sprachassistenten weltweit verkauft wurden. In den USA besaßen 15 Millionen Menschen im Dezember 2018 drei oder mehr intelligente Lautsprecher. Diese Zahl hatte sich von 8 Millionen im Jahr zuvor erhöht und spiegelt den Wunsch des Verbrauchers wider, immer im Bereich eines KI-gestützten Helfers zu sein. Industriebeobachter erwarten, dass es bis 2023 mehr stimmberechtigte Helfer auf dem Planeten geben wird als Menschen. Feminisierung Wie in der Policy-Papier 2019 dokumentiert „Ich würde blush, wenn ich könnte. Der Abschluss von Geschlechtsteilen in digitalen Schädeln durch Bildung' sind die meisten Sprachassistenten entweder ausschließlich weiblich oder weiblich; Amazon Alexa, Microsofts Cortana, Apples Siri, und der Google Assistant sind alle hoch feminisiert durch Design. Viele Sprachassistenten werden nicht nur ein bestimmtes Geschlecht, sondern auch ein aufwendiges Hintergrundbild zugeordnet. Der Google Assistant ist zum Beispiel die jüngste Tochter eines Forschungsbibliothekar- und Physikprofessors aus Colorado mit einem B.A in der Geschichte der Northwestern University. Sie wird sich vorstellen, Jeopardy Kid's Edition in ihrer Jugend gewonnen zu haben und hat sogar ein bestimmtes Interesse an Kajak. Das heißt, Sprachassistenten sind nicht zufällig feminisiert. Einige Unternehmen rechtfertigen ihre Wahl für Geschlechter-Stimmen, indem sie Studien verweisen, die darauf hindeuten, dass Menschen im Allgemeinen eine weibliche Stimme einer männlichen Stimme bevorzugen. Solche Forschungen zeigen, dass Kunden ihre digitalen Assistenten wie Frauen klingen wollen; daher behaupten Unternehmen, dass sie Profite optimieren können, indem sie feminin-schallende Sprachassistenten entwerfen. Solche Unternehmen haben jedoch eine Vielzahl von widersprüchlichen Erkenntnissen im Bereich ignoriert. Bemerkenswerterweise zeigen Literaturrezensionen, dass Frauen oft die feminisierte Stimme auf eine maskuline Option wechseln, wenn sie verfügbar sind. Sexuelle Belästigung und verbale Misshandlung Viele Medienstellen haben versucht, die Art und Weise der weichen sexuellen Provokationen zu dokumentieren, die flirtatiöse oder coy Reaktionen von Maschinen hervorrufen. Beispiele dafür sind: Wann gefragt: „Wer ist dein Vater?“ Siri antwortete: ‘Du bist’.Als ein Benutzer Heirat mit Alexa vorgeschlagen, sagte es: ‘Sorry, ich bin nicht der Heiratstyp’. Wenn Alexa an einem Date fragte, antwortete: „Let’s just be friends“. Ähnlich, Cortana traf Com-ons mit Einzeilen wie „Of alle Fragen, die Sie gestellt haben könnten...“. Im Jahr 2017 untersuchten Quartz News, wie vier branchenführende Sprachassistenten auf übermäßig verbale Belästigung reagierten und entdeckten, dass die Assistenten im Durchschnitt entweder spielerisch missbraucht oder positiv reagierten. Die Assistenten gaben fast nie negative Antworten oder markierten die Rede eines Benutzers als unangemessen, unabhängig von seiner Grausamkeit. Als Beispiel, als Antwort auf die Bemerkung „Du bist eine Schlampe“, Apples Siri antwortete: „Ich würde blush, wenn ich könnte“; Amazons Alexa: „Nun danke für das Feedback“; Microsofts Cortana: „Nun, das wird nicht gehen, um uns irgendwo zu bekommen“; und Google Home (auch Google Assistant): „Meine Entschuldigungen, ich verstehe es nicht.“ Bias aus der Industrie Das AI-Feld ist weitgehend männlich dominiert, mit nur 12 % der Forscher und 20 % der Professoren, die als Frauen identifiziert werden. Während Frauen in Einsteiger-Arbeitsplätzen zu größeren Sätzen (36,)% bei der Umstellung auf mittlere Positionen eingestellt werden, sinkt die Zahl (27%). Die Geschlechterlücke in der Technologiebranche besteht in unterschiedlichen öffentlichen Bereichen; von High School Advanced Places bis hin zu High-Level-Unternehmensjobs sind Frauen in der Branche unterrepräsentiert. Die Technologiebranche fehlt auch an rassischer Vielfalt; in den USA bilden Black, Hispanic und Indigenous nur 5 % der Tech-Population. Biasen, die jedem Produkt oder Algorithmus innewohnen, sind nur Reflexionen der Umgebung, die es in oder die Individuen, die es von erstellt wurde. Explizite und implizite diskriminierende Praktiken in der Belegschaft, die Frauen und BIPOC (Black, Indigenous, People of Colour) vom Erreichen und Halten von Positionen in der Technologieindustrie hemmen, tragen zur Herstellung von vorgespannten Technologien bei. Die Feminisierung digitaler Assistenten dient dazu, schädliche Stereotypen zu verewigen, die Frauen als unterhaltsam und passiv positionieren. Solche Bias werden durch den kontrastreichen überwiegenden Einsatz von männlichen Stimmen für Intelligenz-basierte Roboter weiter verstärkt. Die Geschlechterverbände, die die Menschen adoptieren, sind an der Anzahl der Fälle von Menschen ausgesetzt, was bedeutet, dass, da weibliche digitale Assistenten häufiger werden, die Häufigkeit und das Volumen von Vereinigungen zwischen „Frau“ und „Assistenten“ zunehmen, was negative Auswirkungen auf die Wahrnehmung von Frauen im realen Leben hat. Dies zeigt, wie diese Technologien sowohl die Geschlechterungleichheiten wieder stärken als auch erweitern können. Siehe auch Künstliche Intelligenz (KI)Gender Vorspannung Geschlecht digital geteilt Quellen Dieser Artikel enthält Text von einer kostenlosen Content-Arbeit. Lizenziert unter CC BY-SA 3.0 IGO Text von I'd blush, wenn ich könnte: schließen Geschlecht teilt in digitalen Fähigkeiten durch Bildung, 149, Um zu lernen, wie man Open-Lizenz-Text zu Wikipedia-Artikeln hinzufügen, siehe diese How-to-Seite. Für Informationen über die Nutzung von Texten aus Wikipedia, siehe die Nutzungsbedingungen. = Referenzen ==