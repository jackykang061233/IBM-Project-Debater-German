AIXI '[aićkːsiý] ist ein theoretischer mathematischer Formalismus für künstliche allgemeine Intelligenz. Es kombiniert Solomonoff Induktion mit sequentiellen Entscheidungstheorie. AIXI wurde erstmals im Jahr 2000 von Marcus Hutter vorgeschlagen und in Hutters 2005er Buch Universal Artificial Intelligence werden mehrere Ergebnisse bezüglich AIXI nachgewiesen. AIXI ist ein Verstärkungslerner. Es maximiert die erwarteten Gesamtbelohnungen der Umwelt. Intuitiv betrachtet sie gleichzeitig jede rechnerische Hypothese (oder Umgebung). In jedem Schritt betrachtet es jedes mögliche Programm und wertet aus, wie viele Belohnungen dieses Programms je nach der nächsten Aktion generiert werden. Die versprochenen Belohnungen werden dann durch den subjektiven Glauben gewichtet, dass dieses Programm die wahre Umgebung darstellt. Dieser Glaube wird von der Länge des Programms berechnet: längere Programme werden als weniger wahrscheinlich betrachtet, im Einklang mit Occam Rasierer. AIXI wählt dann die Aktion, die die höchste erwartete Gesamtbelohnung in der gewichteten Summe aller diese Programme hat. Definition AIXI ist ein Verstärkungslerner, der mit einigen stochastischen und unbekannten, aber rechnerischen Umgebungen interagiert μ \{displaystyle \mu } . Die Interaktion verläuft in Zeitschritten, von t = 1 \{displaystyle t=1} bis t = m \{displaystyle t=m}, wobei m ε N \{displaystyle m\in \mathbb {N} ist die Lebensdauer des AIXI Agenten. Zum Zeitpunkt t wählt der Agent eine Aktion a t ε A \{displaystyle a_{t}\in \{mathcal {A} (z.B. Gliederbewegung) aus und führt sie in der Umgebung aus, und die Umgebung reagiert mit einem Percept e t ε E = O × R \{displaystyle e_{t}\in ) E}={\mathcal {O}\times \mathbb {R} , die aus einer Beobachtung o t ε O \{displaystyle o_{t}\in \{mathcal {O} (z.B. Kamerabild) und einer Belohnung r t ε R \{displaystyle r_{t}\in \mathbb {R} } bedingung verteilt nach , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . a_{1}o_{1}r_{1}...a_{t-1}o_{t-1}r_{t-1}a_{t ist die Geschichte von Handlungen, Beobachtungen und Belohnungen. Die Umgebung μ \{displaystyle \mu } ist somit mathematisch als Wahrscheinlichkeitsverteilung über Wahrnehmungen (Beobachten und Belohnungen) dargestellt, die von der vollen Geschichte abhängen, so gibt es keine Markov-Annahme (im Gegensatz zu anderen RL-Algorithmen). Beachten Sie erneut, dass diese Wahrscheinlichkeitsverteilung dem AIXI Agenten unbekannt ist. Darüber hinaus wird nochmals darauf hingewiesen, dass μ \{displaystyle \mu } berechnet werden kann, d.h. die Beobachtungen und Belohnungen, die der Agent aus der Umgebung erhalten μ \{displaystyle \mu } können durch ein Programm berechnet werden (das auf einer Turing-Maschine läuft), wenn die früheren Aktionen des AIXI-Agenten. Das einzige Ziel des AIXI-Agenten ist es, Σ t = 1 m r t \{displaystyle \sum t=1}^{m}r_{t, d.h. die Summe der Belohnungen von Zeitschritt 1 bis m zu maximieren. Der AIXI-Agent ist mit einer stochastischen Politik π assoziiert : ( A × E ) ♦ → A \{displaystyle \pi \:{(mathcal {A}\times \{mathcal E}})^{*}\rightarrow \{mathcal {A}, die die Funktion ist, die es nutzt, um Aktionen zu jedem Zeitschritt zu wählen, wo A \{displaystyle} Die Umwelt (oder die Wahrscheinlichkeitsverteilung) μ \{displaystyle \mu } kann auch als stochastische Politik betrachtet werden (die eine Funktion ist:) μ : (A × E ) χ A → E \{displaystyle \mu \:{(mathcal {A}\times \{mathcal E}}}^^^^ Im allgemeinen, zu dem Zeitpunkt t \{displaystyle t} (welche reichen von 1 bis m,) AIXI, mit zuvor ausgeführten Aktionen a 1 ... a t - 1 \{displaystyle a_{1}\dots a_{t-1} (welches oft in der Literatur als < t \{displaystyle a_{<t} abgekürzt wird) und die Geschichte der Perzepte o 1 r 1 . (die als e < t \{displaystyle e_{<t} abgekürzt werden kann), die Aktion in der Umgebung auswählt und ausführt, eine t \{displaystyle a_{t}, die wie folgt definiert ist:= arg ♦ max a m r t ... max a m r m r m r t + ... + r m a_{t}:=\arg \max a_{t}\sum o_{t}r_{t}\ldots \max a_{m}\sum o_{m}r_{m}[r_{t}+\ldots +r_{m}]\sum q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}2^{-{\textrm {Länge}(q) oder, mit Klammern, um die Vorhergehenden zu disambiguieren a t := arg ♦ max a t ... ( max a m r m r m [ r t + ... + r m ) ( Σ q : U ( q , a 1 ... a m ) = o 1 r 1 ... o m r m 2 − Länge ( q ) ) \{displaystyle a_{t}:=\arg \max a_{t}\left(\sum o_{t}r_{t}\ldots left(\max a_{m}\sum o_{m}r_{m}[r_{t}+\ldots (\sum q:\;U(q,a_{1}\ldots) a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}2^{-{\textrm {length}(q)}\right)\right)\right Intuitiv betrachtet AIXI in der obigen Definition die Summe der Gesamtbelohnung über alle möglichen Zukunften bis zu m - t \{displaystyle m-t} Zeitschritte voraus (d.h. von t \{displaystyle t} bis m \{displaystyle m}), die jeweils durch die Komplexität der Programme q \{displaystyle q} (d.h. durch 2 - Länge (q) Lassen Sie uns diese Definition abbrechen, um sie vollständig zu verstehen. o t r t \{displaystyle o_{t}r_{t ist der Begriff, der aus der Beobachtung o t \{displaystyle o_{t} und der Belohnung r t \{displaystyle r_{t} besteht, die der AIXI-Agent zum Zeitpunkt t \{displaystyle t} aus der Umgebung (die unbekannt und stochastisch ist) erhält. In ähnlicher Weise ist o m r m \{displaystyle o_{m}r_{m das von AIXI im Zeitschritt m \{displaystyle m} (der letzte Zeitschritt, in dem AIXI aktiv ist) r t + ... + r m \{displaystyle r_{t}+\ldots +r_{m} ist die Summe der Belohnungen vom Zeitschritt t \{displaystyle t} bis zum Zeitschritt m \{displaystyle m}, so AIXI muss in die Zukunft schauen, um seine Aktion im Zeitschritt t \{displaystyle t} zu wählen. U \{displaystyle U} bezeichnet eine monotone Universal-Turniermaschine, und q \{displaystyle q} Bereiche über alle (deterministischen) Programme auf der Universal-Maschine U \{displaystyle U}, die als Eingabe das Programm q \{displaystyle q} und die Reihenfolge der Aktionen a 1... a m \{displaystyle a_{1}\dots a_{m} und die Reihenfolge der Aktionen, ,,,,,, o_{1}r_{1}\ldots o_{m}r_{m . Die universelle Turing-Maschine U \{displaystyle U} dient somit dazu, die Umgebungsantworten oder -perzepte zu simulieren oder zu berechnen, da das Programm q \{displaystyle q} (welches die Umgebung modelt) und alle Aktionen des AIXI-Agenten: In diesem Sinne ist die Umgebung berechenbar (wie oben angegeben). Beachten Sie, dass im Allgemeinen das Programm, das die aktuelle und tatsächliche Umgebung (wo AIXI handeln muss) modelliert, unbekannt ist, weil die aktuelle Umgebung auch unbekannt ist. Länge (q ) \{displaystyle \{textrm {length}(q) ist die Länge des Programms q \{displaystyle q} (die als Zeichenfolge kodiert ist). Beachten Sie, dass 2 - Länge (q ) = 1 2 Länge (q ) \{displaystyle 2^{-{\textrm Länge}(q)}={\frac 1}{2^{\textrm {length}(q) . In der obigen Definition ist also Σ q : U (q , a 1 ... a m ) = o 1 r 1 ... o m r m 2 − Länge (q ) \{displaystyle \sum q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}2^{-{\textrm {length}(q) sollte als Mischung (in diesem Fall eine Summe) über alle rechnerischen Umgebungen (die mit der Vergangenheit des Agenten übereinstimmen) interpretiert werden, die jeweils durch seine Komplexität 2 - Länge (q) \{displaystyle 2^{-{\textrm {length}(q) gewichtet sind. Beachten Sie, dass a 1 ... a m \{displaystyle a_{1}\ldots a_{m} auch als 1 ... a t - 1 a t ... a m \{displaystyle a_{1}\ldots a_{t-1}a_{t\ldots a_{m} und a 1 ... a t - 1 = a < t \{display} a In ähnlicher Weise o 1 r 1 ... o m r m = o 1 r 1 ... o t - 1 r t - 1 o t r t r t ... o m r m r m o_{1}r_{1}\ldots o_{m}r_{m}=o_{1}r_{1}\ldots o_{t-1}r_{t-1}o_{t}r_{t}\ldots o_{m}r_{m und o 1 r 1 ... o t − 1 r t − 1 \{displaystyle o_{1}r_{1}\ldots o_{t-1}r_{t-1 ist die bisher von der Umwelt erzeugte Sequenz von Wahrnehmungen. Lassen Sie uns nun alle diese Komponenten zusammensetzen, um diese Gleichung oder Definition zu verstehen. Zum Zeitpunkt t wählt AIXI die Aktion a t \{displaystyle a_{t}, wobei die Funktion Σ o t r t ... max a m Σ o m r m [ r t + ... + r m ] Σ q : U ( q , a 1 ... a m ) = o 1 r 1 ... o max r m 2 - Länge ( q ) \{displaystyle \sum o_{t} o_{m}r_{m}[r_{t}+\ldots +r_{m}]\sum q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}2^{-{\textrm {length}(q) erreicht sein Maximum. Parameter Die Parameter zu AIXI sind die Universal-Turniermaschine U und die Lebensdauer m des Agenten, die gewählt werden müssen. Letzterer Parameter kann durch Rabatt entfernt werden. Die Bedeutung des Wortes AIXI Nach Hutter kann das Wort AIXI mehrere Interpretationen haben. AIXI kann für AI auf Basis der Salomooff-Distribution stehen, bezeichnet durch ξ \{displaystyle \xi } (das ist der griechische Buchstabe xi,) oder z.B. kann es für KI gekreuzt (X) mit Induktion (I.) stehen Es gibt andere Interpretationen. Die Leistung von AIXI wird durch die erwartete Gesamtzahl der Belohnungen gemessen. AIXI hat sich auf folgende Weise als optimal erwiesen. Pareto-Optimalität: Es gibt keinen anderen Agenten, der mindestens AIXI in allen Umgebungen durchführt und in mindestens einer Umgebung streng besser arbeitet. Balanced Pareto Optimität: Wie Pareto Optimität, aber unter Berücksichtigung einer gewichteten Summe von Umgebungen. Selbstoptimierend: Eine Politik p wird als selbstoptimierend für eine Umgebung μ \{displaystyle \mu } bezeichnet, wenn die Leistung von p dem theoretischen Maximum für μ \{displaystyle \mu } nähert, wenn die Länge der Lebensdauer des Agenten (nicht Zeit) auf Unendlichkeit geht. Für Umweltklassen, in denen selbstoptimierende Politiken existieren, ist AIXI selbstoptimierend. Es wurde später von Hutter und Jan Leike gezeigt, dass eine ausgewogene Pareto-Optimierung subjektiv ist und dass jede Politik als Pareto-optimal betrachtet werden kann, die sie als Unterminierung aller bisherigen Optimitätsansprüche für AIXI beschreiben. AIXI hat jedoch Einschränkungen. Es ist beschränkt auf die Maximierung von Belohnungen auf der Grundlage von Wahrnehmungen im Gegensatz zu externen Staaten. Sie geht auch davon aus, dass sie mit der Umwelt ausschließlich durch Handlungs- und Wahrnehmungskanäle interagiert und verhindert, dass sie die Möglichkeit hat, beschädigt oder verändert zu werden. Kolloquial bedeutet dies, dass es sich nicht als von der Umgebung, mit der es interagiert, enthalten betrachtet. Sie geht auch davon aus, dass die Umwelt berechenbar ist. Computational Aspekte Wie Solomonoff Induktion, AIXI ist unbestritten. Es gibt jedoch rechnerische Annäherungen. Eine solche Approximation ist AIXItl, die sowohl mindestens als auch die nachweislich beste Zeit t und Raum l begrenztes Mittel durchführt. Eine weitere Annäherung an AIXI mit eingeschränkter Umgebungsklasse ist MC-AIXI (FAC-CTW) (die für Monte Carlo AIXI FAC-Context-Tree Weighting steht), die einige Erfolge beim Spielen einfacher Spiele wie teilweise beobachtbarer Pac-Man hatte. Siehe auch Gödel-Maschinenreferenzen "Universal Algorithmic Intelligence: A mathematisch top->down approach", Marcus Hutter, arXiv:cs/0701125; auch in Künstliche Allgemeine Intelligenz, hrsg. B. Goertzel und C. Pennachin, Springer, 2007, ISBN 9783540237334, S. 227–290, doi:10.1007/978-3-540