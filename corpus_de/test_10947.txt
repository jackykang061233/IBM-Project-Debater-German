Maschinenethik (oder Maschinenethik, rechnerische Moral oder Rechenethik) ist Teil der Ethik künstlicher Intelligenz, die mit der Hinzufügung oder der Sicherstellung von Verhaltens von anthropogenen Maschinen, die künstliche Intelligenz verwenden, ansonsten als künstliche intelligente Agenten bekannt. Maschinenethik unterscheidet sich von anderen ethischen Bereichen im Bereich Technik und Technologie. Maschinenethik sollte nicht mit Computerethik verwechselt werden, die sich auf den menschlichen Gebrauch von Computern konzentriert. Man sollte auch von der Technologietheorie unterschieden werden, die sich mit den großen sozialen Auswirkungen der Technologie befasst. Geschichte vor dem 21. Jahrhundert war die Ethik der Maschinen größtenteils Gegenstand der wissenschaftlichen Literatur, vor allem aufgrund von Computer- und künstlichen Intelligenz (AI). Obwohl sich die Definition von "Maschinenethik" seit entwickelt hat, wurde der Begriff von Mitchell Waldrop im AI Magazine-Artikel "A Frage der Verantwortung" von 1987 angegeben: "Wie immer, eins, das aus der oben genannten Diskussion hervorgeht, ist, dass intelligente Maschinen Werte, Annahmen und Zwecke verkörpern, ob ihre Programmteilnehmer sie bewusst planen oder nicht. Da Computer und Roboter intelligenter werden, müssen wir sorgfältig und explizit darüber nachdenken, was die eingebauten Werte sind. vielleicht, was wir brauchen, ist eine Theorie und Praxis der Maschinenethik im Sinne der drei Gesetze von Asimov. " 2004 wurde auf dem AAAI-Workshop über die Arbeitsorganisation vorgestellt: Theorie und Praxis, in denen theoretische Grundlagen für maschinelle Ethik festgelegt wurden. Es war im AAAI Fall 2005 Symposium über Maschinenethik, in dem Forscher erstmals zusammengekommen sind, um die Umsetzung einer ethischen Dimension in autonomen Systemen zu erwägen. In der gesammelten Ausgabe "Maschinenethik", die sich auf das AAAI Fall 2005 Symposium über Maschinenethik stützt, ist eine Vielzahl von Perspektiven dieses nascent Felds zu finden. Im Jahr 2007 stellte AI Magazine Maschinenethik vor: Schaffung eines Ethicaln Intelligent Agents, eines Artikels, der die Bedeutung von Maschinenethik erörtert, die Notwendigkeit von Maschinen, die ethischen Prinzipien ausdrücklich entsprechen, und die Herausforderungen, denen sich diese auf Maschinenethik stellen. Es zeigte sich auch, dass es möglich ist, zumindest in beschränktem Bereich für eine Maschine ein ethisches Prinzip aus Beispielen ethischer Urteile zu wählen und dieses Prinzip zu verwenden, um ihr eigenes Verhalten zu steuern. Oxford University Press im Jahr 2009 veröffentlichten Moralmaschinen, Roboter Recht von Wrong, das es als "das erste Buch, um die Herausforderung des Aufbaus künstlicher Moralagenten zu untersuchen, in die Natur der menschlichen Entscheidungsfindung und der Ethik einzutreten. " Es nannte rund 450 Quellen, von denen etwa 100 wichtige Fragen der Maschinenethik angesprochen haben. Im Jahr 2011 veröffentlichte die Cambridge University Press eine Sammlung von Beiträgen zur von Michael und Susan Anderson herausgegebenen Maschinenethik, die auch 2006 ein spezielles Thema der intelligenten Computersysteme auf dem Thema veröffentlichte. Die Sammlung besteht aus den Herausforderungen, ethische Grundsätze für Maschinen hinzuzufügen. Im Jahr 2014 kündigte das US-Amt für Meeresforschung an, dass es 7,5 Millionen $ in Zuschüssen über fünf Jahre an Universitätsforscher verteilen würde, um Fragen der Maschinenethik zu untersuchen, die auf autonome Roboter angewandt werden, und Nick Bostrom's Superintelligence: Paths, Dangers, Strategien, die die Maschinenethik als die "wichtigste...issue Menschheit jemals" auf der New York Times Liste der besten wissenschaftlichen Bücher aufgebracht haben. Im Jahr 2016 veröffentlichte das Europäische Parlament ein Papier, (22 Seite PDF), um die Kommission zu ermuntern, die Frage des Rechtsstatus von Robotern zu behandeln, wie in der Presse kurz beschrieben. In diesem Papier wurden Abschnitte über die rechtlichen Verbindlichkeiten von Robotern aufgenommen, in denen die Verbindlichkeiten als proportional zur Höhe der Autonomie der Roboter argumentiert wurden. In dem Papier wurde auch die Zahl der Arbeitsplätze in Frage gestellt, die durch AI Roboter ersetzt werden könnten. Definitionen James H. Moor, einer der Pioniere der Zahnmedizin im Bereich der Computerethik, definieren vier Arten von ethischen Robotern. Als umfangreicher Forscher zu den Studien der Philosophie der künstlichen Intelligenz, der Philosophie der Natur, der Philosophie der Wissenschaft und der Logik definiert Moor Maschinen als ethische Wirkungsstoffe, implizite ethische Agenten, explizite ethische Agenten oder voll ethische Wirkstoffe. Eine Maschine kann mehr als eine Art von Agenten sein.: Wirkungsstoffe: Es handelt sich um Maschinensysteme, die ethische Auswirkungen haben, unabhängig davon, ob geplant oder nicht. Gleichzeitig haben diese Agenten das Potenzial, unethisch zu handeln. Moor gibt ein hypothetisches Beispiel namens „Goodman Agent“, benannt nach Philosoph Nelson Goodman. Der Goodman-Beauftragte vergleicht Daten, aber hat die Jahrtausendwende. Dieser Fehler resultierte aus Programmern, die nur die letzten zweistelligen des Jahres repräsentierten. Jegliche Daten über das Jahr 2000 würden daher irreführend behandelt als früher als im späten zwanzigsten Jahrhundert. So war der Goodman-Beauftragte vor dem Jahr 2000 ein ethisch wirkender Erreger und danach ein unethischer Wirkstoff. implizite ethische Wirkstoffe: Für die Prüfung der menschlichen Sicherheit sind diese Agenten programmiert, um einen Mangel an Sicherheit oder eine Bauleistung zu haben. Sie sind nicht ausschließlich ethischer Natur, sondern eher programmiert, um unethische Ergebnisse zu vermeiden. ethische Stoffe: Maschinen, die in der Lage sind, Szenarien zu verarbeiten und ethische Entscheidungen zu treffen. Maschinen, die Algorithmen ethisch wirken. Volle ethische Vertreter: Diese Maschinen sind mit expliziten ethischen Mitteln vergleichbar, um ethische Entscheidungen treffen zu können. Jedoch enthalten sie menschliche Metaphysen.(i.e haben freie Willen, Bewusstsein und Absichtserklärung)(Siehe künstliche Systeme und moralische Verantwortung). Konzentrationen von Maschinenethik AI-Kontrollproblemen Manche Wissenschaftler, wie Philosoph Nick Bostrom und AI-Forschunger Stuart Russell, weisen darauf hin, dass, wenn die AI die Menschheit in der allgemeinen Intelligenz überholt und überflüssig wird, diese neue Super-Intelligente zu einer wirksamen und schwierigen Kontrolle werden könnte: Wie das Schicksal des Berggorilla hängt von menschlichem Wohlwillen ab, so kann das Schicksal der Menschheit von den Handlungen eines zukünftigen Super-Inspektors abhängen. In ihren jeweiligen Büchern Superintelligence und Humanifizierte behaupten beide Wissenschaftler, dass trotz der großen Unsicherheit in Bezug auf die Zukunft der AI das Risiko für die Menschheit groß genug ist, um bedeutende Maßnahmen in der Gegenwart zu verdienen. Dies stellt das Problem der AI-Kontrolle dar: Wie soll ein intelligenter Agenten aufgebaut werden, der seine Urheber unterstützt, während gleichzeitig unabsichtlich einen Super-Indikator aufgebaut werden, der seine Schöpfer schädigen wird. Die Gefahr, die Kontrolle nicht richtig "das erste Mal" zu gestalten, ist, dass eine Super-Intelligence in der Lage sein kann, Macht über ihre Umwelt zu nehmen und den Menschen daran zu hindern. Potenzielle AI-Kontrollstrategien umfassen "Kapazitätskontrolle" (Begrenzung der Fähigkeit der AI, die Welt zu beeinflussen) und "Führungskontrollen" (eine Art, um eine AI aufzubauen, deren Ziele an menschliche oder optimale Werte angepasst sind). Es gibt eine Reihe von Organisationen, die das Problem der AI-Kontrolle untersuchen, einschließlich der Zukunft des Humanity Institute, des Instituts für Maschinen- und Nachrichtenforschung, des Center for Human-Compatible Künstliche Intelligenz und der Zukunft des Life Institute. Algorithms und Training AI Paradigmen wurden diskutiert, insbesondere im Hinblick auf ihre Wirksamkeit und Unparteilichkeit. Nick Bostrom und Eliezer Yudkowsky haben für Entscheidungsbäume (wie ID3) über Neuralnetze und genetische Algorithmen plädiert, weil die Entscheidungsbäume moderne soziale Normen für Transparenz und Berechenbarkeit einhalten (z.B. Sterne decisis). Chris Santos-Lang sprach sich dagegen für Neuralnetze und genetische Algorithmen aus, weil die Normen jedes Alters geändert werden müssen und dass natürliches Versagen dieser besonderen Normen wesentlich dazu beigetragen hat, dass Menschen weniger anfällig als Maschinen für kriminellen Hacker sind. 2009 wurden im Rahmen eines Experiments im Labor für intelligente Systeme im Ecole Polytechnique Fédérale von Lausanne in der Schweiz AI Roboter programmiert, um miteinander zusammenzuarbeiten und mit dem Ziel der Suche nach einer nutzbringenden Ressource unter Vermeidung einer giftigen Ressource zu beauftragen. Während des Experiments wurden die Roboter in Clans zusammengefasst, und der digitale Gencode der erfolgreichen Mitglieder wurde für die nächste Generation verwendet, eine Art von Algorithmus, der als genetischer Algorithmus bekannt ist. Nach 50 aufeinanderfolgenden Generationen in der AI entdeckten ein Clans Mitglieder, wie man die nutzbringenden Ressourcen vom Gift unterscheiden kann. Die Roboter haben daraufhin gelernt, einander zu liegen, um die nutzbringenden Ressourcen anderer Roboter zu nutzen. In demselben Experiment erfuhren die gleichen AI-Roboter auch, sich selbst zu verhalten und die Gefahr für andere Roboter zu signalisieren, und starben auch an den Kosten, um andere Roboter zu retten. Die Auswirkungen dieses Experiments wurden von Maschinenethikern angefochten. Im Ecole Polytechnique Fédérale Experiment wurden die Ziele der Roboter als Terminal programmiert. Humanmotiven verfügen hingegen in der Regel über eine Qualität, die ein nie-nachhaltiges Lernen erfordert. autonome Waffensysteme Im Jahr 2009 nahmen Wissenschaftler und technische Experten an einer Konferenz teil, um die möglichen Auswirkungen von Robotern und Computern und die Auswirkungen der hypothetischen Möglichkeit zu diskutieren, dass sie selbst reichen und ihre eigenen Entscheidungen treffen können. Sie erörterten die Möglichkeit und den Umfang, in dem Computer und Roboter alle Autonomiestufen erwerben können und in welchem Maße sie solche Fähigkeiten nutzen könnten, um eine Bedrohung oder Gefahr zu verursachen. Sie stellten fest, dass einige Maschinen verschiedene Formen der Halbautonomie erworben haben, darunter die Möglichkeit, eigene Stromquellen zu finden und eigenständige Ziele für den Angriff auf Waffen zu wählen. Sie wiesen auch darauf hin, dass einige Computerviren die Abschaffung verhindern können und „Cockroach Intelligence“ erreicht haben. " Sie wiesen darauf hin, dass Selbstbewusstsein, wie in Wissenschaft dargestellt, wahrscheinlich unwahrscheinlich ist, aber dass es andere potenzielle Gefahren und Gefahren gab. Manche Experten und Wissenschaftler haben die Verwendung von Robotern für militärische Kämpfe in Frage gestellt, insbesondere wenn solche Roboter einen gewissen Grad autonomer Funktionen erhalten. Die US-Katastrophe hat einen Bericht finanziert, in dem darauf hingewiesen wird, dass militärische Roboter komplexer werden, dass die Auswirkungen ihrer Fähigkeit, autonome Entscheidungen zu treffen, stärker berücksichtigt werden sollten. Der Präsident der Vereinigung für die Förderung der künstlichen Intelligenz hat eine Studie in Auftrag gegeben, um diese Frage zu prüfen. Sie weisen auf Programme wie das Instrument für den Erwerb von Sprachen hin, das die menschliche Interaktion verschmutzen kann. Integration künstlicher Generalnachrichten mit der Gesellschaft Vorliminary work wurde über Methoden zur Integration künstlicher allgemeiner Intelligenz (vollständige ethische Wirkstoffe, die oben definiert sind) mit bestehenden rechtlichen und sozialen Rahmenbedingungen durchgeführt. Konzepte haben sich auf die Prüfung ihrer Rechtslage und ihrer Rechte konzentriert. Leistungsstarke Big Data und maschinelle Lernalgorithmen sind in zahlreichen Branchen wie Online-Werbung, Ratings und strafrechtliche Verfolgung verbreitet, wobei das Versprechen, objektivere, datengesteuerte Ergebnisse zu liefern, aber als potenzielle Quelle für die Verfolgung sozialer Ungleichheiten und Diskriminierungen erkannt wurde. Eine Studie von 2015 ergab, dass Frauen weniger wahrscheinlich von der Google-Technik gezeigt werden. In einer weiteren Studie wurde festgestellt, dass der gleiche Tag der Lieferung von Amazon absichtlich in schwarzen Stadtteilen nicht verfügbar war. Google und Amazon waren nicht in der Lage, diese Ergebnisse zu einem einzigen Thema zu isolieren, sondern erklärten stattdessen, dass die Ergebnisse das Ergebnis der von ihnen verwendeten schwarzen Boxalgorithmen waren. Das Justizsystem der Vereinigten Staaten hat mit quantitativen Risikobewertungssoftware begonnen, wenn Entscheidungen über die Entmietung von Menschen auf Bail und die Verurteilung in einem Bemühen getroffen werden, fairer zu sein und eine bereits hohe Haftrate zu senken. Diese Werkzeuge analysieren die kriminelle Geschichte eines Angeklagten unter anderen Attributen. In einer Studie von 7.000 Personen, die in der Provinz Broward verhaftet wurden, haben nur 20 % der Befragten eine Straftat berechent, die das Risikobeurteilungssystem des Bezirks nutzte, um ein Verbrechen zu begehen. In einem Bericht von Pro Publica 2016 wurden die Risikowerte für die Gegenseitigkeit analysiert, die von einem der am häufigsten verwendeten Werkzeuge, dem nordpointe COMPAS-System, berechnet wurden, und die Ergebnisse über zwei Jahre untersucht. Laut dem Bericht sind nur 61 % der Befragten, die ein hohes Risiko aufweisen, während sie zusätzliche Verbrechen begangen haben. In diesem Bericht wurde auch darauf hingewiesen, dass afrikanische und amerikanische Angeklagte im Vergleich zu ihren weißen Beklagten weit wahrscheinlich höhere Risikowerte erhalten haben. 2016 berichtet die Big Data Working Group von Obama – eine übersehnliche Gruppe verschiedener ordnungspolitischer Großdaten –, die „das Potenzial der Kodierung von Diskriminierungen in automatisierten Entscheidungen“ und die Forderung nach „gleicher Möglichkeit durch Gestaltung“ für Anwendungen wie Bonitätsbewertung. In den Berichten wird der Diskurs zwischen politischen Entscheidungsträgern, Bürgerinnen und Bürgern und Wissenschaftlern gleichermaßen gefördert, doch erkennt er an, dass es keine mögliche Lösung für die Kodierung von Verzerrungen und Diskriminierungen in Algorithmussystemen.-Rahmens und -praktiken im März 2018 gibt, um die zunehmenden Bedenken hinsichtlich der Auswirkungen von maschinenlesbaren Lernprozessen auf die Menschenrechte, das Welt-Wirtschaftsforum und den Global Future Council on Human Rights mit detaillierten Empfehlungen zur bestmöglichen Vermeidung diskriminierender Ergebnisse im Maschinenbau auszuräumen. Weltwirtschaft Forum hat vier Empfehlungen entwickelt, die auf den Leitprinzipien der Vereinten Nationen für Menschenrechte basieren, um diskriminierende Ergebnisse im Maschinenbau zu bekämpfen und zu verhindern. Die Empfehlungen des Weltwirtschaftsforums lauten wie folgt: Aktive Integration: Die Entwicklung und Gestaltung von Anwendungen des maschinenlesbaren Lernens müssen sich aktiv um eine Vielzahl von Input bemühen, insbesondere von Normen und Werten bestimmter Bevölkerungsgruppen, die von der Produktion von AI-Systemen betroffen sind. Fairness: Personen, die an der Konzeption, Entwicklung und Implementierung von Maschinenlernsystemen beteiligt sind, sollten überlegen, welche Definition der Fairness am besten für ihren Kontext und ihre Anwendung gilt und sie in der Architektur des maschinenlesbaren Lernsystems und deren Bewertungsparameter Recht auf Vereinbarung legt: Beteiligung von Maschinenlernsystemen an Entscheidungsprozessen, die die individuellen Rechte betreffen, muss offengelegt werden, und die Systeme müssen in der Lage sein, ihre Entscheidungsfindung zu erklären, die für die Endnutzer verständlich ist und von einer zuständigen menschlichen Behörde überprüft werden kann. Wo dies nicht möglich ist und die Rechte auf dem Spiel stehen, müssen die Führer in der Gestaltung, Einführung und Regulierung der maschinellen Lerntechnik in Frage stellen, ob der Zugang zum Rechtsschutz genutzt werden sollte: Leaders, Designer und Entwickler von Maschinenlernsystemen sind für die Ermittlung der möglichen negativen Auswirkungen ihrer Systeme verantwortlich. Sie müssen für diejenigen, die von unterschiedlichen Auswirkungen betroffen sind, sichtbare Rechtsbehelfe schaffen und Verfahren für die rechtzeitige Behebung von diskriminierenden Outputs einrichten. Im Januar 2020 veröffentlichte die Harvard University’s Berkman Klein Center for Internet und Society eine Meta-Study von 36 prominenten Prinzipien für die AI, in der acht Schlüsselthemen aufgeführt sind: Privatsphäre, Rechenschaftspflicht, Sicherheit und Sicherheit, Transparenz und Erklärbarkeit, Fairness und Nichtdiskriminierung, menschliche Kontrolle von Technologie, professionelle Verantwortung und Förderung menschlicher Werte. Im Jahr 2019 wurde eine ähnliche Meta-Study von Forschern aus dem Swiss Federal Institute of Technology in Zürich durchgeführt. Konzepte wurden mehrere Versuche unternommen, um Ethik komutierbar oder zumindest formal zu machen. Isaac Asimovs drei Gesetze der Robotik gelten in der Regel nicht als geeignet für einen künstlichen moralischen Agenten, es wurde untersucht, ob die Kategorisierung von Kant verwendet werden kann. Jedoch wurde darauf hingewiesen, dass der menschliche Wert in einigen Aspekten sehr komplex ist. Eine Möglichkeit, diese Schwierigkeit ausdrücklich zu überwinden, besteht darin, menschliche Werte direkt vom Menschen durch einen Mechanismus zu erhalten, beispielsweise durch das Lernen. Ein weiterer Ansatz besteht darin, aktuelle ethische Erwägungen auf früheren ähnlichen Situationen zu stützen. Dies wird als Kasuist bezeichnet und könnte durch Forschung im Internet umgesetzt werden. Der Konsens einer Million vergangener Entscheidungen würde zu einer neuen Entscheidung führen, die von Demokratie abhängig ist. Dies könnte jedoch zu Entscheidungen führen, die Verzerrungen und unethischen Verhaltens in der Gesellschaft widerspiegeln. Die negativen Auswirkungen dieses Ansatzes sind in Microsofts Tay (bot) zu sehen, wo der Chatterbot gelernt hat, rasssistische und sexuell übertragbare Nachrichten von Twitter-Nutzern zu wiederholen. Ein Gedankenversuch konzentriert sich auf einen Genie Golem mit unbegrenzten Befugnissen, die sich dem Leser stellen. Dieser Genie erklärt, dass er in 50 Jahren zurückkommen wird und verlangt, dass er mit einem eindeutigen Satz von Morals geliefert wird, dass er dann sofort handeln wird. Zweck dieses Experiments ist es, einen Diskurs darüber einzuleiten, wie man am besten mit der Definition eines vollständigen Satzes von Ethik umgehen kann, die Computer verstehen können. In der Science-Fiction haben Filme und Romane mit der Idee der Entsendung in Roboter und Maschinen gespielt. Neil Blomkamp's Chappie (2015) verabschiedete ein Szenario, um ein Bewusstsein in einen Computer zu übertragen. Ex Machina (2014) von Alex Garland, gefolgt von einer künstlichen Intelligenz, die einer Variante des Turing Test unterzogen wurde, einem Test, der an eine Maschine verabreicht wird, um zu sehen, ob ihr Verhalten vom Menschen unterscheiden kann. Arbeiten wie beispielsweise Alfa (1984) und The Matrix (1999) umfassen das Konzept der Maschinen, die auf ihren menschlichen Masters (siehe künstliche Intelligenz) umsteigen. Isaac Asimov betrachtete das Thema in den 1950er Jahren in I, Roboter. John W. Campbell Jr. schlug er vor, die drei Gesetze der Robotik für künstlich intelligente Systeme zu regeln. Viele seiner Arbeiten wurden dann untersucht, welche Grenzen seine drei Gesetze sehen, um zu sehen, wo sie sich brechen, oder wo sie paradoxes oder unvorhergesehenes Verhalten schaffen würden. Seine Arbeit deutet darauf hin, dass keine festen Gesetze alle möglichen Umstände ausreichend antizipieren können. In Philip K. Dicks Roman, Do Androids Traum von Electric Schaf?(1968) untersucht er, was es bedeutet, Menschen zu sein. In seinem postokularen Zypern-Szenario bezweifelte er, ob Emopathie ein rein menschliches Merkmal sei. Seine Geschichte ist die Grundlage für den Science-Fiction-Film, das Spiel für Videospiele (1982). verwandte Bereiche Affective Computerformale Ethik der Bioeethik Theorie des Denkens Computerethik Ethik der künstlichen Intelligenz Moral Philosophie des Denkens Siehe auch künstliche Intelligenz in der Fiktion künstlicher Intelligenz, die die technische Entscheidungsfindung von Google car Military Maschinen Research Institute Robot Space Law Self-replikating Spacecraft Watson Projekt für die Automatisierung medizinischer Entscheidungshilfen Tay (bot) Externer Maschinenethik, interdisziplinäres Projekt zur Ethik. Pharmaethik Podcast, Podcast über Maschinenethik, AI und Techethik. Verweise Wallach, Wendell; Allen, Colin (November 2008). Moralmaschinen: Lernroboter von Wrong. USA: Oxford University Press. Anderson, Michael; Anderson, Susan Leigh, eds (Juli 2011). Maschinenethik. Cambridge University Press. Storrs J. (Mai 30, 2007). Abgesehen von der AI: Aufbau der Conscience of the Maschinen Prometheus Bücher. Moor, J. (2006) Natur, Import und Schwierigkeit der Maschinenethik. Intelligente Systeme, 21 (4), S.18–21. Anderson, M. und Anderson, S. (2007). Erstellung eines Ethicalen Smart Agent.AI Magazine, Band 28 (4). Weitere Lesung Hagendorff, Thilo (2020). " ins Verhalten bei Menschen und Maschinen -- Bewertung der Qualität der Ausbildungsdaten für nützliches maschinelernen".arXiv:200811463 Anderson, Michael; Anderson, Susan Leigh, eds (Juli/August 2006)."Besondere Frage zur Maschinenethik"IEEE Intelligent Systems 21 (4): 10–63.Bendel, Oliver (Dezember 11 2013). Überlegungen zur Beziehung zwischen Tier- und Maschinenethik. AI & SOCIETY, doi:10.1007/s00146-013-0526-3.Dalink, Gerhard, ed. (2010)"Ethische und rechtliche Aspekte unmanned Systems". Interviews".Österreichisches Ministerium für Verteidigung und Sport, Wien 2010, [www.3-902761-04-0.Gardner, A. (1987). Ein künstliches Konzept für die rechtliche Begründung. Cambridge, MA: MIT Presse. Georges, T. M. (2003). Digital Soul: Intelligente Maschinen und menschliche Werte. Cambridge, MA: Westview Presse. Singer, P.W (Dezember 29, 2009). Krieg: Die Revolution und der Konflikt im 21. Jahrhundert: Penguin.