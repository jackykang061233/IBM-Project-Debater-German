Künstliche allgemeine Intelligenz (AGI) ist die hypothetische Fähigkeit eines intelligenten Agenten, jede geistige Aufgabe zu verstehen oder zu lernen, die ein Mensch kann. Es ist ein primäres Ziel einer künstlichen Intelligenzforschung und eines gemeinsamen Themas in Science Fiction und Futures-Studien. AGI kann auch als starke KI, vollständige KI oder allgemeine intelligente Aktion bezeichnet werden. Einige akademische Quellen behalten den Begriff "starke KI" für Computerprogramme vor, die Verschwörung, Selbstbewusstsein und Bewusstsein erleben können. Seit den späten 2010er Jahren wird AI auf Jahrzehnte von AGI entfernt spekuliert. Im Gegensatz zu starker KI ist schwache KI (auch als schmale KI bezeichnet) nicht dazu bestimmt, human-ähnliche kognitive Fähigkeiten und Persönlichkeit zu haben, sondern schwache KI beschränkt sich auf die Verwendung von Software, um bestimmte vorveröffentlichte Problemlösungs- oder Argumentationsaufgaben (Expertensysteme) zu studieren oder zu erfüllen. Seit 2017 forschen über vierzig Organisationen aktiv an AGI. Merkmale Verschiedene Kriterien für die Intelligenz wurden vorgeschlagen (meistens der Turing-Test), aber bisher gibt es keine Definition, die jedem gerecht wird. Es gibt jedoch eine große Übereinstimmung zwischen den Forschern der künstlichen Intelligenz, dass Intelligenz erforderlich ist, um die folgenden zu tun: Grund, Verwendung Strategie, lösen Rätsel und Urteile unter Ungewissheit; stellen Wissen, einschließlich des Verständnisses des Gemeinwohls; Plan; lernen; kommunizieren in der natürlichen Sprache; und integrieren alle diese Fähigkeiten zu gemeinsamen Zielen. Weitere wichtige Fähigkeiten umfassen die Fähigkeit, Objekte (z.B. sehen) und die Handlungsfähigkeit (z.B. bewegen und manipulieren) in der Welt, in der intelligentes Verhalten beobachtet werden soll. Dies würde eine Fähigkeit zur Erkennung und Reaktion auf Gefahren beinhalten. Viele interdisziplinäre Ansätze zur Intelligenz (z.B. kognitive Wissenschaft, rechnerische Intelligenz und Entscheidungsfindung) neigen dazu, die Notwendigkeit zu betonen, zusätzliche Eigenschaften wie Phantasie (als Fähigkeit zur Bildung von geistigen Bildern und Konzepten, die nicht programmiert wurden) und Autonomie zu berücksichtigen. Computerbasierte Systeme, die viele dieser Fähigkeiten zeigen, existieren (z.B. rechnergestützte Kreativität, automatisierte Argumentation, Entscheidungsunterstützungssystem, Roboter, evolutionäre Berechnung, intelligenter Agent), aber noch nicht auf menschlicher Ebene. Tests zur Bestätigung des Personals Folgende Tests zur Bestätigung von AGI auf menschlicher Ebene wurden berücksichtigt: Der Turing Test (Turing)A-Maschine und ein Mensch konversieren sich beide mit einem zweiten Menschen, die auswerten müssen, welche von beiden die Maschine ist, die den Test passiert, wenn es den Evaluator einen signifikanten Bruchteil der Zeit täuschen kann. Hinweis: Turing verschreibt nicht, was als Intelligenz gelten sollte, nur dass das Wissen, dass es sich um eine Maschine handelt, sie disqualifizieren sollte. Der Kaffeetest (Wozniak) Eine Maschine ist erforderlich, um ein durchschnittliches amerikanisches Haus zu betreten und herauszufinden, wie man Kaffee macht: die Kaffeemaschine finden, den Kaffee finden, Wasser hinzufügen, einen Becher finden und den Kaffee durch Drücken der richtigen Knöpfe. Der Robot College Student Test (Goertzel) Eine Maschine wickelt sich an einer Universität ein, nimmt und leitet die gleichen Klassen, die der Mensch würde, und erhalten einen Abschluss. Der Beschäftigungstest (Nilsson) Eine Maschine leistet einen wirtschaftlich wichtigen Job mindestens sowie Menschen in der gleichen Arbeit. Probleme, die AGI zur Lösung Die schwierigsten Probleme für Computer sind informell als KI-komplete oder KI-Hard bekannt, was bedeutet, dass die Lösung dem allgemeinen Wert der menschlichen Intelligenz oder starken KI über die Fähigkeiten eines zweckspezifischen Algorithmus entspricht. KI-komplette Probleme werden hypothetisiert, um allgemeine Computer Vision, natürliches Sprachverständnis und Umgang mit unerwarteten Umständen bei der Lösung eines realen Problems. KI-komplete Probleme können nicht allein mit der aktuellen Computertechnologie gelöst werden, und erfordern auch menschliche Berechnungen. Diese Eigenschaft könnte z.B. nützlich sein, um auf das Vorhandensein von Menschen zu testen, wie CAPTCHAs darauf abzielen; und für die Computersicherheit, brutale Angriffe abzuwehren. Geschichte Klassische KI Moderne KI-Forschung begann Mitte der 1950er Jahre. Die erste Generation von KI-Forschern war überzeugt, dass künstliche allgemeine Intelligenz möglich war und dass sie in wenigen Jahrzehnten existieren würde. KI-Pionier Herbert A. Simon schrieb 1965: "Maschinen werden innerhalb von zwanzig Jahren in der Lage sein, jede Arbeit zu leisten, die ein Mann tun kann." Ihre Vorhersagen waren die Inspiration für Stanley Kubrick und Arthur C. Clarkes Charakter HAL 9000, die verkörperten, was AI-Forscher glaubten, sie könnten bis zum Jahr 2001 schaffen. KI-Pionier Marvin Minsky war ein Berater für das Projekt, HAL 9000 so realistisch wie möglich nach den Konsensvorhersagungen der Zeit zu machen; Crevier zitiert ihn, wie es 1967 zu dem Thema sagte: "In einer Generation ...das Problem, "künstliche Intelligenz" zu schaffen, wird im Wesentlichen gelöst werden", obwohl Minsky sagt, dass er falsch zitiert wurde. In den frühen 1970er-Jahren wurde jedoch deutlich, dass Forscher die Schwierigkeit des Projekts stark unterschätzt hatten. Die Förderagenturen wurden skeptisch von AGI und setzen Forscher unter zunehmendem Druck, nützliche "Applied AI" zu produzieren. Als die 1980er Jahre begann, hat Japans Computerprojekt der fünften Generation an der AGI wieder Interesse geweckt und eine zehnjährige Zeitlinie aufgestellt, die AGI-Ziele wie "Werbe auf einem lässigen Gespräch" umfasste. Als Reaktion auf diese und den Erfolg von Expertensystemen haben sowohl Industrie als auch Regierung Geld zurück in den Bereich gepumpt. In den späten 1980er Jahren kollabierte jedoch das Vertrauen in AI spektakulär und die Ziele des Computerprojekts der Fünften Generation wurden nie erfüllt. Zum zweiten Mal in 20 Jahren hatten sich KI-Forscher, die die bevorstehende Leistung von AGI vorhergesagt hatten, als grundsätzlich falsch erwiesen. In den 1990er Jahren hatten AI-Forscher einen Ruf gewonnen, vergebliche Versprechen zu machen. Sie zögerten, überhaupt Vorhersagen zu machen und jegliche Erwähnung von "menschlicher Ebene" künstlicher Intelligenz zu vermeiden, weil sie Angst davor hatten, als "wild-eyed dreamer[s]" bezeichnet zu werden. KI-Forschung In den 1990er und Anfang des 21. Jahrhunderts erreichte die Mainstream-KI einen weitaus größeren kommerziellen Erfolg und die akademische Respektivität, indem sie sich auf bestimmte Teilprobleme konzentrierte, in denen sie überprüfbare Ergebnisse und kommerzielle Anwendungen wie künstliche neuronale Netze und statistisches maschinelles Lernen produzieren können. Diese "Applied AI"-Systeme werden inzwischen in der Technologieindustrie weit verbreitet eingesetzt, und die Forschung in dieser Vene ist in Wissenschaft und Industrie sehr stark gefördert. Derzeit gilt die Entwicklung auf diesem Gebiet als ein aufstrebender Trend, und eine reife Phase wird in mehr als 10 Jahren erwartet. Die meisten KI-Forscher hoffen, dass eine starke KI durch die Kombination der Programme entwickelt werden kann, die verschiedene Teilprobleme lösen. Hans Moravec schrieb 1988: "Ich bin zuversichtlich, dass dieser Bottom-up-Route zur künstlichen Intelligenz eines Tages die traditionelle Top-Down-Route mehr als die Hälfte erreichen wird, bereit, die reale Weltkompetenz und das Commonsense-Wissen zu bieten, das so frustrierend in Argumentationsprogrammen empfunden wurde. Voll intelligente Maschinen werden entstehen, wenn der metaphorische goldene Spieß angetrieben wird, der die beiden Bemühungen vereint. " Doch auch diese Grundphilosophie ist bestritten worden; zum Beispiel hat Stevan Harnad von Princeton seine 1990er-Jahre über die Symbol Grounding Hypothese abgeschlossen: "Die Erwartung ist oft geäußert worden, dass sich Top-down (symbolische) Ansätze zur Modellierung der Kognition irgendwie mit Bottom-up (sensorische) Ansätze irgendwo dazwischen treffen werden. Wenn die Erdungsüberlegungen in diesem Papier gültig sind, dann ist diese Erwartung hoffnungslos modular und es gibt wirklich nur einen tragfähigen Weg von Sinn zu Symbolen: von Grund auf. Eine frei fließende symbolische Ebene wie die Software-Ebene eines Computers wird nie auf dieser Strecke erreicht werden (oder umgekehrt) – noch ist es klar, warum wir sogar versuchen sollten, eine solche Ebene zu erreichen, da es aussieht, als würde es einfach dazu kommen, unsere Symbole von ihren intrinsischen Bedeutungen zu entschlüsseln (dadurch reduzieren wir uns einfach auf das funktionelle Äquivalent eines programmierbaren Computers)." Moderne künstliche allgemeine Intelligenz Forschung Der Begriff "künstliche allgemeine Intelligenz" wurde bereits 1997 von Mark Gubrud in einer Diskussion über die Auswirkungen vollautomatischer militärischer Produktion und Operationen verwendet. Der Begriff wurde um 2002 von Shane Legg und Ben Goertzel neu eingeführt und populär gemacht. Das Forschungsziel ist viel älter, zum Beispiel das Cyc-Projekt von Doug Lenat (das 1984 begann), und das Projekt von Allen Newell's Soar gilt als im Rahmen von AGI. Die AGI-Forschungstätigkeit im Jahr 2006 wurde von Pei Wang und Ben Goertzel als "Produktion von Publikationen und vorläufigen Ergebnissen" beschrieben. Die erste Sommerschule in AGI wurde 2009 vom künstlichen Gehirnlabor der Xiamen-Universität und OpenCog in Xiamen, China, organisiert. Der erste Hochschulkurs wurde 2010 und 2011 an der Plovdiv University, Bulgarien von Todor Arnaudov. Das MIT präsentierte 2018 einen Kurs in der AGI, organisiert von Lex Fridman und mit einer Reihe von Gastdozenten. Dennoch haben die meisten KI-Forscher der AGI wenig Aufmerksamkeit gewidmet, wobei einige behaupten, dass Intelligenz zu komplex ist, um in naher Zukunft vollständig repliziert zu werden. Eine kleine Anzahl von Informatikern sind jedoch in der AGI-Forschung aktiv, und viele dieser Gruppen tragen zu einer Reihe von AGI-Konferenzen bei.Die Forschung ist äußerst vielfältig und oft Pionier in der Natur. In der Einführung seines Buches sagt Goertzel, dass die Schätzungen der Zeit, die vor der Errichtung einer wirklich flexiblen AGI benötigt wird, von 10 Jahren bis über ein Jahrhundert variieren, aber der Konsens in der AGI-Forschungsgemeinschaft scheint, dass die von Ray Kurzweil in The Singularity diskutierte Zeitlinie in der Nähe (d.h. zwischen 2015 und 2045) ist plausibel. Die wichtigsten KI-Forscher haben jedoch eine breite Palette von Meinungen darüber abgegeben, ob die Fortschritte so schnell sein werden. Eine 2012 Meta-Analyse von 95 solcher Meinungen fand eine Voreingenommenheit, dass der Beginn von AGI innerhalb von 16–26 Jahren für moderne und historische Vorhersagen gleichermaßen auftreten würde. Es wurde später gefunden, dass der Datensatz einige Experten als Nicht-Experten aufgelistet und umgekehrt. Organisationen, die AGI ausdrücklich verfolgen, umfassen das Schweizer AI-Labor IDSIA, Nnaisense, Vikarious, Maluuba, die OpenCog Foundation, Adaptive AI, LIDA und Numenta sowie das dazugehörige Redwood Neuroscience Institute. Zudem wurden Organisationen wie das Machine Intelligence Research Institute und OpenAI gegründet, um den Entwicklungspfad von AGI zu beeinflussen. Schließlich haben Projekte wie das Human Brain Projekt das Ziel, eine funktionierende Simulation des menschlichen Gehirns aufzubauen. Eine 2017 Umfrage von AGI kategorisiert fünfzig bekannte "aktive FuE-Projekte", die explizit oder implizit (durch veröffentlichte Forschung) Forschung AGI, mit den größten drei Wesen DeepMind, das Human Brain Project, und OpenAI. Im Jahr 2017 gründete Ben Goertzel die AI-Plattform SingularityNET mit dem Ziel, die demokratische, dezentrale Kontrolle der AGI bei ihrer Ankunft zu erleichtern. Im Jahr 2017 führten die Forscher Feng Liu, Yong Shi und Ying Liu Geheimdienstprüfungen über öffentlich zugängliche und frei zugängliche schwache AI wie Google AI oder Apples Siri und andere durch. Höchstens erreichte diese KI einen IQ-Wert von etwa 47, der etwa einem sechsjährigen Kind in der ersten Klasse entspricht. Ein Erwachsener kommt im Durchschnitt auf etwa 100. Ähnliche Tests wurden 2014 durchgeführt, wobei die IQ-Score einen Maximalwert von 27 erreichte. Im Jahr 2019 gaben Videospiel-Programmierer und Luft- und Raumfahrtingenieur John Carmack Pläne zur Forschung AGI bekannt. Im Jahr 2020 entwickelte OpenAI GPT-3, ein Sprachmodell, das in der Lage ist, viele verschiedene Aufgaben ohne spezielles Training durchzuführen. Laut Gary Grossman in einem VentureBeat Artikel, während es Konsens gibt, dass GPT-3 kein Beispiel von AGI ist, wird es von einigen als zu fortgeschritten angesehen, um als ein schmales KI-System zu klassifizieren. Bremssimulation Ganze Gehirnemulsion Ein populär diskutierter Ansatz zur Erzielung allgemeiner intelligenter Maßnahmen ist die Gehirnemulsion. Ein Low-Level-Gehirnmodell wird durch Scannen und Kartieren eines biologischen Gehirns im Detail und Kopieren seines Zustands in ein Computersystem oder ein anderes Rechengerät erstellt. Der Computer betreibt ein Simulationsmodell, das dem Original so treu ist, dass er sich im Wesentlichen genauso wie das ursprüngliche Gehirn oder für alle praktischen Zwecke unzweifelhaft verhalten wird. Ganze Hirnemulation wird in der rechnerischen Neurowissenschaft und Neuroinformatik im Kontext der Hirnsimulation für medizinische Forschungszwecke diskutiert. Es wird in der künstlichen Intelligenzforschung als Ansatz für starke KI diskutiert. Neuroimaging-Technologien, die das notwendige Detailverständnis liefern könnten, verbessern sich schnell und futuristisch Ray Kurzweil im Buch Der Singular Is Near prognostiziert, dass eine Karte ausreichender Qualität auf einem ähnlichen Zeitmaß zur erforderlichen Rechenleistung zur Verfügung steht. Frühere Schätzungen Für die niedere Hirnsimulation wäre ein extrem leistungsfähiger Computer erforderlich. Das menschliche Gehirn hat eine große Anzahl von Synapsen. Jede der 1011 (einhundert Milliarden) Neuronen hat im Durchschnitt 7.000 synaptische Verbindungen (Synapsen) zu anderen Neuronen. Es wurde geschätzt, dass das Gehirn eines dreijährigen Kindes etwa 1015 Synapsen (1 Quadrillion) aufweist. Diese Zahl nimmt mit dem Alter ab und stabilisiert sich durch das Erwachsenenalter. Die Schätzungen variieren für Erwachsene, von 1014 bis 5 x 1014 Synapsen (100 bis 500 Trillion). Eine Schätzung der Verarbeitungsleistung des Gehirns, basierend auf einem einfachen Schaltermodell für die Neuronaktivität, beträgt ca. 1014 (100 Trillion) synaptische Updates pro Sekunde (SUPS). 1997 untersuchte Kurzweil verschiedene Schätzungen für die Hardware, die für die Gleichberechtigung des menschlichen Gehirns erforderlich war und eine Zahl von 1016 Berechnungen pro Sekunde cps annahm. (Vergleich, wenn eine Berechnung einem "Floating Point Operation" entspricht – einer Maßnahme zur Rate von aktuellen Supercomputern – dann würden 1016 Berechnungen 10 petaFLOPS entsprechen, die 2011 erreicht wurden). Er nutzte diese Figur, um vorherzusagen, dass die notwendige Hardware irgendwann zwischen 2015 und 2025 zur Verfügung stehen würde, wenn das exponentielle Wachstum der Computerleistung zum Zeitpunkt des Schreibens fortgesetzt.Modellierung der Neuronen im Detail Das künstliche Neuronmodell, das von Kurzweil angenommen und in vielen aktuellen künstlichen neuronalen Netzwerk-Implementierungen verwendet wird, ist im Vergleich zu biologischen Neuronen einfach. Eine Gehirnsimulation müsste wahrscheinlich das detaillierte zelluläre Verhalten biologischer Neuronen erfassen, die derzeit nur in den weitesten Umrissen verstanden werden. Die durch vollständige Modellierung der biologischen, chemischen und physikalischen Details des neuronalen Verhaltens (insbesondere auf molekularer Ebene) eingeführte Oberleitung würde Rechenleistungen in mehreren Größenordnungen erfordern, die größer sind als Kurzweils Schätzung. Darüber hinaus berücksichtigen die Schätzungen keine Glialzellen, die mindestens so zahlreich sind wie Neuronen, und die bis zu 10:1 Neuronen übersteigen können, und sind nun bekannt, eine Rolle in kognitiven Prozessen zu spielen. Aktuelle Forschung Es gibt einige Forschungsprojekte, die die Gehirnsimulation anhand komplexer neuraler Modelle untersuchen, die auf konventionellen Computerarchitekturen umgesetzt werden. Das Projekt Artificial Intelligence System implementierte 2005 nicht-reale Zeitsimulationen eines Gehirns (mit 1011 Neuronen). Es dauerte 50 Tage auf einem Cluster von 27 Prozessoren zu simulieren 1 Sekunde eines Modells. Das Projekt Blue Brain nutzte eine der schnellsten Supercomputer-Architekturen der Welt, die Plattform Blue Gene von IBM, um eine Echtzeit-Simulation einer einzigen Ratten-Neocortical-Säule zu schaffen, die aus etwa 10.000 Neuronen und 108 Synapsen im Jahr 2006 besteht. Ein langfristiges Ziel ist es, eine detaillierte, funktionelle Simulation der physiologischen Prozesse im menschlichen Gehirn aufzubauen: "Es ist nicht unmöglich, ein menschliches Gehirn aufzubauen und wir können es in 10 Jahren tun", sagte Henry Markram, Leiter des Blue Brain Project 2009 auf der TED-Konferenz in Oxford. Es gab auch umstrittene Behauptungen, ein Katzenhirn simuliert zu haben. Neuro-Silizium-Schnittstellen wurden als alternative Umsetzungsstrategie vorgeschlagen, die besser skaliert werden kann. Hans Moravec adressierte die obigen Argumente ("Hirn sind komplizierter", "Neuronen müssen genauer modelliert werden") in seinem 1997er Papier "Wann wird Computerhardware dem menschlichen Gehirn entsprechen?". Er gemessen die Fähigkeit der vorhandenen Software, die Funktionalität des neuronalen Gewebes zu simulieren, und zwar die Netzhaut. Seine Ergebnisse hängen nicht von der Anzahl der Glialzellen ab, noch von welchen Arten der Verarbeitung von Neuronen wo. Die eigentliche Komplexität der Modellierung biologischer Neuronen wurde im OpenWorm-Projekt erforscht, das auf eine vollständige Simulation einer Schnecke zielte, die nur 302 Neuronen in ihrem neuronalen Netzwerk hat (insgesamt etwa 1000 Zellen). Das neuronale Netz des Tieres wurde vor Beginn des Projekts gut dokumentiert. Obwohl die Aufgabe zu Beginn einfach schien, funktionierten die auf einem generischen neuronalen Netz basierenden Modelle nicht. Derzeit konzentrieren sich die Bemühungen auf eine präzise Emulation biologischer Neuronen (teilweise auf molekularer Ebene), aber das Ergebnis kann noch nicht als Gesamterfolg bezeichnet werden. Auch wenn die Anzahl der in einem human-brain-Skala-Modell zu lösenden Probleme nicht proportional zur Anzahl der Neuronen ist, ist die Arbeitsmenge auf diesem Weg offensichtlich. Kritiken an Simulations-basierten Ansätzen Eine grundlegende Kritik an dem simulierten Gehirnansatz ergibt sich aus verkörperter Erkenntnis, wo die menschliche Ausführungsform als wesentlicher Aspekt der menschlichen Intelligenz betrachtet wird. Viele Forscher glauben, dass eine Verkörperung notwendig ist, um Bedeutung zu stiften. Wenn diese Ansicht richtig ist, muss jedes vollfunktionelle Gehirnmodell mehr umfassen als nur die Neuronen (d.h. einen Roboterkörper). Goertzel schlägt eine virtuelle Ausführungsform vor (wie in Second Life), aber es ist noch nicht bekannt, ob dies ausreichend wäre. Desktop-Computer mit Mikroprozessoren, die in der Lage sind, mehr als 109 cps (Kurzweils nicht standardmäßige Einheit "Berechnungen pro Sekunde", siehe oben) sind seit 2005 verfügbar. Laut den von Kurzweil (und Moravec) verwendeten Hirnleistungsschätzungen sollte dieser Computer in der Lage sein, eine Simulation eines Bienenhirns zu unterstützen, aber trotz einiger Interesses besteht keine solche Simulation. Dafür gibt es mindestens drei Gründe: Das Neuronmodell scheint zu vereinfachen (siehe nächste Abschnitt). Es gibt nicht genügend Verständnis für höhere kognitive Prozesse, um genau zu ermitteln, was die neuronale Aktivität des Gehirns, beobachtet mit Techniken wie funktionale Magnetresonanz-Bildgebung, korreliert mit. Auch wenn unser Verständnis von Kognition ausreichend vorankommt, sind frühe Simulationsprogramme sehr ineffizient und brauchen daher erheblich mehr Hardware. Das Gehirn eines Organismus, während kritisch, kann keine geeignete Grenze für ein kognitives Modell sein.Um ein Bienenhirn zu simulieren, kann es notwendig sein, den Körper und die Umgebung zu simulieren. Die Extended Mind-Thesis formalisiert das philosophische Konzept, und die Forschung an Cephalopods hat deutliche Beispiele für ein dezentrales System gezeigt. Darüber hinaus ist die Skala des menschlichen Gehirns derzeit nicht gut eingeschränkt. Eine Schätzung legt das menschliche Gehirn bei etwa 100 Milliarden Neuronen und 100 Trillion-Synapsen. Eine weitere Schätzung ist 86 Milliarden Neuronen, von denen 16,3 Milliarden in der zerebralen Kortex und 69 Milliarden im cerebellum sind. Gliale Zellsynapsen sind derzeit nichtquantifiziert, sind aber als äußerst zahlreich bekannt. philosophische Perspektive 1980 prägte der Philosoph John Searle den Begriff "starke KI" als Teil seines chinesischen Raumarguments. Er wollte zwischen zwei verschiedenen Hypothesen über künstliche Intelligenz unterscheiden: Ein künstliches Intelligenzsystem kann denken und einen Geist haben. (Der Begriff Geist hat eine bestimmte Bedeutung für Philosophen, wie in "der Geist-Körper-Problem" oder "die Philosophie des Geistes" verwendet.) Ein künstliches Intelligenzsystem kann (nur) wie es denkt und hat einen Verstand. Die erste wird als "die starke AI-Hypothese" bezeichnet und die zweite ist "die schwache AI-Hypothese", weil die erste die stärkere Aussage macht: es nimmt an, dass etwas Besonderes mit der Maschine passiert ist, die über alle seine Fähigkeiten hinausgeht, die wir testen können. Searle bezeichnete die "starke AI-Hypothese" als "starke KI". Diese Nutzung ist auch in akademischen AI-Forschung und Lehrbüchern üblich. Die schwache AI-Hypothese entspricht der Hypothese, dass künstliche allgemeine Intelligenz möglich ist. Laut Russell und Norvig, "Die meisten AI-Forscher nehmen die schwache AI-Hypothese für selbstverständlich, und kümmern sich nicht um die starke AI-Hypothese. " Im Gegensatz zu Searle verwendet Ray Kurzweil den Begriff "starke KI", um jedes künstliche Intelligenzsystem zu beschreiben, das wie es einen Verstand hat, unabhängig davon, ob ein Philosoph in der Lage wäre, zu bestimmen, ob es tatsächlich einen Geist hat oder nicht. In der Science-Fiction ist AGI mit Eigenschaften wie Bewusstsein, Geschicklichkeit, Sapienz und Selbstbewusstsein verbunden, die in Lebewesen beobachtet werden. Laut Searle ist es jedoch eine offene Frage, ob allgemeine Intelligenz für das Bewusstsein ausreicht. "Strong AI" (wie oben von Kurzweil definiert) sollte nicht mit Searles "starke AI-Hypothese" verwechselt werden. Die starke AI-Hypothese ist der Anspruch, dass ein Computer, der sich so intelligent wie eine Person verhält, auch notwendigerweise einen Geist und Bewusstsein haben muss. AGI bezieht sich nur auf die Höhe der Intelligenz, die die Maschine mit oder ohne Geist zeigt. Bewußtsein Es gibt andere Aspekte des menschlichen Geistes neben der Intelligenz, die für das Konzept der starken KI relevant sind, die eine große Rolle in der Science-Fiction und der Ethik der künstlichen Intelligenz spielen: Bewusstsein: subjektive Erfahrung und Gedanken zu haben. Selbstbewusstsein: Sich selbst als separates Individuum bewusst zu sein, vor allem um sich der eigenen Gedanken bewusst zu sein. Versand: Die Fähigkeit, Wahrnehmungen oder Emotionen subjektiv zu fühlen. sapience: Die Fähigkeit zur Weisheit. Diese Eigenschaften haben eine moralische Dimension, weil eine Maschine mit dieser Form der starken KI Rechte haben kann, analog zu den Rechten von nicht-menschlichen Tieren. Als solche wurden vorläufige Arbeiten an Ansätzen zur Integration vollständiger ethischer Akteure mit bestehenden rechtlichen und sozialen Rahmenbedingungen durchgeführt. Diese Ansätze haben sich auf die rechtliche Position und Rechte der starken KI konzentriert. Bill Joy, unter anderem, argumentiert eine Maschine mit diesen Eigenschaften kann eine Bedrohung für das menschliche Leben oder die Würde sein. Es bleibt zu zeigen, ob eine dieser Eigenschaften für eine starke KI notwendig ist. Die Rolle des Bewusstseins ist nicht klar, und derzeit gibt es keinen vereinbarten Test für seine Anwesenheit. Wird eine Maschine mit einem Gerät gebaut, das die neuralen Korrelate des Bewusstseins simuliert, hätte sie automatisch Selbstbewusstsein? Es ist auch möglich, dass einige dieser Eigenschaften, wie z.B. Versand, natürlich aus einer voll intelligenten Maschine hervorgehen oder dass es selbstverständlich wird, diese Eigenschaften an Maschinen zu akribieren, wenn sie beginnen, in einer Weise zu handeln, die eindeutig intelligent ist. Zum Beispiel kann intelligente Handlungen für die Verschwörung ausreichend sein, anstatt umgekehrt. Künstliche Bewusstseinsforschung Obwohl die Rolle des Bewusstseins in starkem AI/AGI debatierbar ist, betrachten viele AGI-Forscher die Forschung, die Möglichkeiten zur Umsetzung von Bewusstsein als lebenswichtig erforscht. In einer frühen Anstrengung argumentierte Igor Aleksander, dass die Prinzipien für die Schaffung einer bewussten Maschine bereits existierte, aber dass es vierzig Jahre dauern würde, eine solche Maschine zu trainieren, um Sprache zu verstehen.Mögliche Erklärungen für den langsamen Fortschritt der KI-Forschung Seit der Einführung der KI-Forschung im Jahr 1956 hat sich das Wachstum dieses Feldes im Laufe der Zeit verlangsamt und hat die Ziele der Schaffung von Maschinen mit intelligenten Aktionen auf dem menschlichen Niveau geebnet. Eine mögliche Erklärung für diese Verzögerung ist, dass Computer einen ausreichenden Umfang an Speicher- oder Verarbeitungsleistung haben. Darüber hinaus kann die Komplexität, die sich mit dem Prozess der KI-Forschung verbindet, auch den Fortschritt der KI-Forschung begrenzen. Während die meisten KI-Forscher glauben, dass eine starke KI in Zukunft erreicht werden kann, gibt es einige Personen wie Hubert Dreyfus und Roger Penrose, die die Möglichkeit verweigern, eine starke KI zu erreichen. John McCarthy war einer von verschiedenen Informatikern, die glauben, dass KI auf menschlicher Ebene erreicht wird, aber ein Datum kann nicht genau vorhergesagt werden. Konzeptionelle Einschränkungen sind ein weiterer möglicher Grund für die Langsamkeit in der KI-Forschung. KI-Forscher müssen den konzeptionellen Rahmen ihrer Disziplin ändern, um eine stärkere Basis und einen Beitrag zur Suche nach einer starken KI zu leisten. Wie William Clocksin 2003 schrieb: "Der Rahmen beginnt von Weizenbaums Beobachtung, dass Intelligenz sich nur relativ zu bestimmten sozialen und kulturellen Kontexten manifestiert". Darüber hinaus haben AI-Forscher in der Lage, Computer zu schaffen, die für die Menschen komplizierte Jobs ausführen können, wie Mathematik, aber umgekehrt haben sie versucht, einen Computer zu entwickeln, der in der Lage ist, Aufgaben auszuführen, die einfach für den Menschen zu tun sind, wie Gehen (Moravec paradox). Ein Problem, das von David Gelernter beschrieben wird, ist, dass einige Leute denken und denken, sind gleichwertig. Doch die Idee, ob Gedanken und der Schöpfer dieser Gedanken einzeln isoliert werden, hat KI-Forscher fasziniert. Die Probleme, die in der AI-Forschung in den letzten Jahrzehnten aufgetreten sind, haben den Fortschritt der KI weiter behindert. Die gescheiterten Vorhersagen, die von KI-Forschern versprochen wurden, und das Fehlen eines vollständigen Verständnisses menschlicher Verhaltensweisen haben dazu beigetragen, die primäre Idee der KI auf menschlicher Ebene zu verringern. Obwohl der Fortschritt der KI-Forschung sowohl Verbesserung als auch Enttäuschung hervorgebracht hat, haben die meisten Ermittler einen Optimismus etabliert, um das Ziel der KI im 21. Jahrhundert zu erreichen. Weitere mögliche Gründe wurden für die langwierige Forschung im Fortschritt der starken KI vorgeschlagen. Die Intriktität der wissenschaftlichen Probleme und die Notwendigkeit, das menschliche Gehirn durch Psychologie und Neurophysiologie vollständig zu verstehen, haben viele Forscher begrenzt, um die Funktion des menschlichen Gehirns in Computerhardware zu emulieren. Viele Forscher neigen dazu, jeden Zweifel zu unterschätzen, der mit zukünftigen Vorhersagen von KI involviert ist, aber ohne diese Probleme ernst zu nehmen, können die Menschen dann Lösungen für problematische Fragen übersehen. Clocksin sagt, dass eine konzeptionelle Einschränkung, die den Fortschritt der KI-Forschung behindern könnte, ist, dass Menschen die falschen Techniken für Computerprogramme und die Implementierung von Geräten verwenden können. Als die KI-Forscher zunächst auf das Ziel der künstlichen Intelligenz zielen, war ein Hauptinteresse menschlicher Überlegungen. Forscher hofften, rechnerische Modelle des menschlichen Wissens durch Begründung zu etablieren und herauszufinden, wie man einen Computer mit einer bestimmten kognitiven Aufgabe entwerfen kann. Die Praxis der Abstraktion, die Menschen bei der Arbeit mit einem bestimmten Kontext in der Forschung neigen, bietet Forschern eine Konzentration auf nur wenige Konzepte. Der produktivste Einsatz der Abstraktion in der KI-Forschung kommt von der Planung und Problemlösung. Obwohl es darum geht, die Geschwindigkeit einer Berechnung zu erhöhen, hat die Rolle der Abstraktion Fragen zur Einbeziehung der Abstraktionsbetreiber gestellt. Ein möglicher Grund für die Verlangsamung in KI bezieht sich auf die Anerkennung vieler KI-Forscher, dass Heuristik ein Abschnitt ist, der einen erheblichen Bruch zwischen Computerleistung und menschlicher Leistung enthält. Die spezifischen Funktionen, die auf einen Computer programmiert werden, können in der Lage sein, viele der Anforderungen zu berücksichtigen, die es ermöglichen, menschliche Intelligenz anzupassen. Diese Erklärungen sind nicht unbedingt gewährleistet, dass sie die grundlegenden Ursachen für die Verzögerung bei der Erzielung einer starken KI sind, aber sie werden von zahlreichen Forschern weitgehend vereinbart. Es gab viele KI-Forscher, die über die Idee diskutieren, ob Maschinen mit Emotionen erstellt werden sollten. Es gibt keine Emotionen in typischen Modellen der KI und einige Forscher sagen, Programmierung Emotionen in Maschinen erlaubt es ihnen, einen eigenen Verstand zu haben. Emotion fasst die Erfahrungen der Menschen zusammen, weil es ihnen erlaubt, sich an diese Erfahrungen zu erinnern.David Gelernter schreibt: "Kein Computer wird kreativ sein, es sei denn, er kann alle Nuancen menschlicher Emotion simulieren." Diese Sorge um Emotion hat Probleme für KI-Forscher gestellt, und sie verbindet sich mit dem Konzept der starken KI, wie ihre Forschung in die Zukunft vorangeht. Kontroversen und Gefahren FeasibilityAs August 2020 bleibt AGI spekulativ, da noch kein solches System nachgewiesen wurde. Die Meinungen unterscheiden sich sowohl darüber, ob und wann künstliche allgemeine Intelligenz ankommt. An einem Extrem spekulierte der KI-Pionier Herbert A. Simon 1965: "Maschinen werden innerhalb von zwanzig Jahren in der Lage sein, irgendeine Arbeit zu leisten, die ein Mann tun kann". Diese Vorhersage konnte jedoch nicht wahr werden. Microsoft Mitbegründer Paul Allen glaubte, dass solche Intelligenz im 21. Jahrhundert unwahrscheinlich sei, weil sie "unvorhersehbare und grundsätzlich unvorhersehbare Durchbrüche" und ein "wissenschaftlich tiefes Verständnis der Kognition" erfordern würde. Der Roboter Alan Winfield, der in The Guardian geschrieben wurde, behauptete, dass der Golf zwischen modernem Computing und künstlicher Intelligenz auf menschlicher Ebene so breit ist, wie der Golf zwischen aktuellem Raumflug und praktischer schneller als leichter Raumflug. Die Ansichten von KI-Experten über die Machbarkeit von AGI-Wachs und -Wachstum haben in den 2010er Jahren möglicherweise eine Aufständung erlebt. Vier Umfragen, die 2012 und 2013 durchgeführt wurden, schlugen vor, dass die mediane Vermutung unter Experten, wenn sie 50 % zuversichtliche AGI ankommen würden, 2040 bis 2050 betrug, je nach Umfrage, mit dem Mittelwert 2081. Von den Experten antworteten 16,5% mit nie, wenn die gleiche Frage gestellt, aber mit einem 90% Vertrauen statt. Weitere aktuelle AGI-Fortschrittsüberlegungen finden Sie unter Tests zur Bestätigung der human-level AGI. Mögliche Bedrohung der menschlichen Existenz Die These, dass KI ein existentielles Risiko darstellt, und dass dieses Risiko viel mehr Aufmerksamkeit braucht als es derzeit wird, wurde von vielen öffentlichen Figuren unterstützt; vielleicht sind die berühmtesten Elon Musk, Bill Gates und Stephen Hawking. Der bemerkenswerteste KI-Forscher, der die These unterstützt, ist Stuart J. Russell. Endorser der Dissertation exprimieren manchmal Schlichtung bei Skeptikern: Gates sagt, er weiß nicht, warum einige Leute nicht betroffen sind, und Hawking kritisierte weit verbreitete Gleichgültigkeit in seinem 2014 Editorial: "So tun die Experten angesichts möglicher Zukunften unschätzbarer Vorteile und Risiken sicher alles Mögliche, um das beste Ergebnis zu gewährleisten, richtig? Falsch. Wenn eine überlegene fremde Zivilisation uns eine Nachricht schickte, die besagt, dass wir in ein paar Jahrzehnte ankommen, würden wir nur antworten, "OK, rufen Sie uns an, wenn Sie herkommen - wir lassen das Licht auf?" Wahrscheinlich nicht – aber das ist mehr oder weniger was mit KI passiert.'Viele der Gelehrten, die über existentielles Risiko besorgt sind, glauben, dass der beste Weg nach vorn wäre, die (möglicherweise massiven) Forschung zur Lösung des schwierigen "Kontrollproblems" zu führen, um die Frage zu beantworten: Welche Arten von Schutz-, Algorithmen oder Architekturen können Programmierer implementieren, um die Wahrscheinlichkeit zu maximieren, dass ihre wiederkehrende AI-im-imieren, dass ihre wieder-im-improving-improving KIhmig-imieren würde, würde, als KIde KI-ime weiterhin freundlich Die These, dass KI existentielle Risiken darstellen kann, hat auch viele starke Ablenker. Skeptiker verlangen manchmal, dass die These krypto-religiös ist, mit einem irrationalen Glauben an die Möglichkeit der Superintelligenz ersetzt einen irrationalen Glauben an einen allmächtigen Gott; extrem argumentiert Jaron Lanier, dass das gesamte Konzept, dass aktuelle Maschinen in irgendeiner Weise intelligent sind, "eine Illusion" und ein "stupendous con" von den Reichen ist. Viele bestehende Kritik argumentiert, dass AGI kurzfristig unwahrscheinlich ist. Informatik Gordon Bell argumentiert, dass sich die menschliche Rasse bereits zerstören wird, bevor sie die technologische Einzigartigkeit erreicht. Gordon Moore, der ursprüngliche Befürworter des Moore's Law, erklärt: "Ich bin eine Skepsis. Ich glaube nicht, dass [eine technologische Singularität] zumindest für eine lange Zeit passieren wird. Und ich weiß nicht, warum ich mich so fühle. " Ehemaliger Baidu Vizepräsident Andrew Ng sagt AI existentielles Risiko ist "wie Sorgen um Überbevölkerung auf dem Mars, wenn wir noch nicht einmal Fuß auf dem Planeten gesetzt haben." Siehe auch Hinweise Referenzen Quellen Externe Links Das AGI-Portal von Pei WangThe Genesis Group bei MIT's CSAIL – Moderne Forschung zu den Berechnungen, die die menschliche Intelligenz OpenCog untergraben – Open Source-Projekt zur Entwicklung eines human-level AI Simulieren logischen menschlichen Denkens Was wissen wir über AI Timelines?– Literaturrezension