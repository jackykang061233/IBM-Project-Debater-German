Die Grenzen der Berechnung werden durch eine Reihe verschiedener Faktoren geregelt. Insbesondere gibt es mehrere physikalische und praktische Grenzen für die Menge der Rechen- oder Datenspeicher, die mit einer bestimmten Menge von Masse, Volumen oder Energie durchgeführt werden kann. Hardwaregrenzen oder physische Grenzen Verarbeitung und Speicherdichte Der Bekenstein begrenzt die Informationsmenge, die innerhalb eines Kugelvolumens gespeichert werden kann, auf die Entropie eines schwarzen Loches mit der gleichen Oberfläche. Thermodynamik begrenzt die Datenspeicherung eines Systems basierend auf seiner Energie, Anzahl der Partikel und Partikelmodi. In der Praxis ist es eine stärkere Grenze als die Bekenstein gebunden. Die Verarbeitungsgeschwindigkeit Bremermanns Grenze ist die maximale Rechengeschwindigkeit eines in sich geschlossenen Systems im materiellen Universum und basiert auf Massenenergie- und Quantenunsicherheitszwängen. Kommunikationsverzögerungen Der Margolus-Levitin-Theorem setzt eine Begrenzung auf die maximale Rechengeschwindigkeit pro Energieeinheit: 6 × 1033 Operationen pro Sekunde pro Joule. Diese Begrenzung kann jedoch vermieden werden, wenn der Zugriff auf den Quantenspeicher erfolgt. Es können dann Rechenalgorithmen konstruiert werden, die beliebig kleine Mengen an Energie/Zeit pro einen elementaren Rechenschritt benötigen. Das Prinzip der Energieversorgung Landauer definiert eine geringere theoretische Grenze für den Energieverbrauch: kT ln 2 verbraucht pro irreversibler Zustandsänderung, wobei k die Boltzmann-Konstante und T die Betriebstemperatur des Rechners ist. Reversible Computing unterliegt nicht dieser unteren Grenze. T kann auch in der Theorie nicht unter 3 Kelvin, die ungefähre Temperatur der kosmischen Mikrowelle Hintergrundstrahlung gemacht werden, ohne mehr Energie auf Kühlung zu verbringen, als in der Berechnung gespeichert wird. Auf einer Zeitskala von 109 - 1010 Jahren wird jedoch die kosmische Mikrowellen-Hintergrundstrahlung exponentiell abnehmen, was argumentiert wurde, um schließlich 1030 so viel Berechnungen pro Energieeinheit zu ermöglichen. Wichtige Teile dieses Arguments wurden bestritten. Baugeräte, die physikalische Grenzen annähern Mehrere Verfahren wurden vorgeschlagen, um Rechengeräte oder Datenspeichergeräte herzustellen, die physische und praktische Grenzen erreichen: Ein kalter degenerierender Stern könnte als riesiger Datenspeicher verwendet werden, indem er ihn sorgfältig auf verschiedene angeregte Zustände, in der gleichen Weise wie ein Atom oder Quanten gut für diese Zwecke verwendet. Ein solcher Stern müsste künstlich aufgebaut werden, da keine natürlichen degenerierten Sterne für eine extrem lange Zeit auf diese Temperatur abkühlen. Es ist auch möglich, dass Keime auf der Oberfläche von Neutronensterne komplexe Moleküle bilden könnten, die einige für Rechenzwecke vorgeschlagen haben, wodurch eine Art Rechenonium auf der Basis von Femtotechnologie entsteht, die schneller und dichter wäre als Rechenzentrum auf der Basis von Nanotechnologie. Es kann möglich sein, ein schwarzes Loch als Datenspeicher oder Rechengerät zu verwenden, wenn ein praktischer Mechanismus zur Extraktion von enthaltenen Informationen gefunden werden kann. Eine solche Extraktion kann grundsätzlich möglich sein (Stephen Hawkings vorgeschlagene Lösung für die Schwarzlochinformation paradox). Dies würde eine Speicherdichte erreichen, die genau gleich der Bekenstein gebunden ist. Seth Lloyd berechnete die Rechenfähigkeiten eines "ultimate Laptops", der durch das Zusammendrücken eines Kilogramm Materie in ein schwarzes Loch des Radius 1.485 × 10-27 Meter gebildet wird, wobei der Schluss gezogen wird, dass er nur etwa 10 - 19 Sekunden vor dem Verdampfen durch Hawking-Strahlung dauern würde, aber während dieser kurzen Zeit mit einer Rate von etwa 5 × 1050 Operationen pro Sekunde berechnen könnte, letztendlich etwa 1032 Operationen auf 10~1 Lloyd stellt fest, dass "Interessierend, obwohl diese hypothetische Berechnung bei ultrahohen Dichten und Geschwindigkeiten durchgeführt wird, ist die Gesamtzahl der zu verarbeitenden Bits nicht weit von der Anzahl, die den aktuellen Computern in vertrauterer Umgebung zur Verfügung steht. " In The Singularity is Near, Ray Kurzweil zitiert die Berechnungen von Seth Lloyd, dass ein universeller Computer in der Lage ist 1090 Operationen pro Sekunde. Die Masse des Universums kann auf 3 × 1052 Kilogramm geschätzt werden. Wenn alle Materie im Universum in ein schwarzes Loch verwandelt wurde, hätte es eine Lebensdauer von 2,8 × 10139 Sekunden, bevor es durch Hawking-Strahlung verdampft. Während dieser Lebensdauer würde ein solcher universeller Schwarzlochcomputer 2,8 × 10229 Operationen durchführen. Abstrakte Grenzen der Informatik Im Bereich der theoretischen Informatik werden oft die Rechenfähigkeit und Komplexität von Rechenproblemen gesucht. Die Berechnungstheorie beschreibt den Grad, in dem Probleme rechnerisch sind, während die Komplexitätstheorie den asymptotischen Grad des Ressourcenverbrauchs beschreibt.Computerprobleme sind daher in Komplexitätsklassen beschränkt. Die arithmetische Hierarchie und Polynomhierarchie klassifizieren den Grad, in dem Probleme jeweils in Polynomzeit berechenbar und berechenbar sind. Zum Beispiel die Ebene Σ 0 = D 0 = Δ 0 0 {\displaystyle (Sigma) Pi Delta _0}^{0 der arithmetischen Hierarchie klassifiziert rechnerische Teilfunktionen. Darüber hinaus ist diese Hierarchie streng so, dass in jeder anderen Klasse in der arithmetischen Hierarchie streng unbestrittene Funktionen klassifiziert werden. Loose und enge Grenzen Viele Grenzen, die in Bezug auf physikalische Konstanten und abstrakte Modelle der Berechnung in der Informatik abgeleitet werden, sind lose. Sehr wenige bekannte Grenzen behindern direkt Spitzentechnologien, aber viele technische Hindernisse können derzeit nicht durch geschlossene Grenzen erklärt werden. Siehe auch Transcomputational problem Programmable matter Hypercomputation Supertask Digitale Physik Quantum Berechnung Matrioshka Gehirn Bremermann's limit Referenzen Externe Links Sandberg, Anders (22. Dezember 1999). "The Physics of Information Processing Superobjects: Daily Life Unter den Jupiter Brains" (PDF). Journal of Evolution and Technology.5 (1:) 1–34.Ein Adenosin-Reuptake-Inhibitor (AdoRI) ist eine Art Medikament, das als Reuptake-Inhibitor für den Purinkern und Neurotransmitter-Adenosin wirkt, indem die Wirkung eines oder mehrerer der äquilibrativen Nucleosid-Transporter (ENTs) blockiert wird. Dies wiederum führt zu erhöhten extrazellulären Konzentrationen von Adenosin und damit zu einer Erhöhung der adenosinergischen Neurotransmission. Liste der AdoRIs Siehe auch Adenosinergic Reuptake Inhibitor == Referenzen ==