Semi-Supervised Learning ist ein Konzept für das maschinelle Lernen, das einen geringen Anteil an gekennzeichneten Daten mit einer großen Anzahl von nichtlabelierten Daten während der Ausbildung kombiniert. Halbgesteuertes Lernen fällt zwischen unüberwachtem Lernen (mit no Labeled Training data) und überwachtem Lernen (mit nur gekennzeichneten Ausbildungsdaten). Es ist ein besonderes Beispiel für eine schwache Überwachung. Ungedeckte Daten, wenn sie in Verbindung mit einer kleinen Menge gekennzeichneter Daten verwendet werden, können eine erhebliche Verbesserung der Lerngenauigkeit bewirken. Der Erwerb von Etikettendaten für ein Lernproblem erfordert häufig einen qualifizierten Humanagenten (z.B. ein Audio-Segment) oder ein physisches Experiment (z.B. die Bestimmung der 3D-Struktur eines Proteins oder die Bestimmung, ob es Öl an einem bestimmten Standort gibt). Die mit dem Etikettierungsprozess verbundenen Kosten können daher große, vollständig gekennzeichnete Ausbildungseinrichtungen unbrauchbar machen, während der Erwerb von nichtlabelierten Daten relativ kostengünstig ist. In solchen Situationen kann das halbgedeckte Lernen von großem praktischen Wert sein. Halbgesteuertes Lernen ist auch theoretisches Interesse am Maschinenbau und als Modell für das menschliche Lernen. L HANAstyle l} unabhängige identische Beispiele x 1 , ... , x l  of X KINGstyle x_{1},\dots ,x_{l}\in X} mit entsprechenden Etiketten y 1 , ... , y l  Y Y KINGstyle y_{1},\dots ,y_{l}\in Y} und Urandisplaystyle u} unlabel Beispiele x l + 1 , ... , x l + u ∈ X KINGstyle x_{l+1},\dots ,x_{l+u}\in X} wird verarbeitet. Halbgesteuertes Lernen kombiniert diese Informationen, um die Einstufungsleistung zu übertreffen, die entweder durch Rückwürfe der nichtlabelierten Daten und durch die Überwachung des Lernens oder durch Rückwurf der Etiketten und durch unüberwachtes Lernen erreicht werden kann. Halbgesteuertes Lernen kann entweder auf transduktives Lernen oder induktives Lernen verweisen. Ziel des transduktiven Lernens ist es, die richtigen Etiketten für die angegebenen nichtlabelierten Daten x l + 1 , ... , x l + u HANAstyle x_{l+1},\dots ,x_{l+u} nur zu verwenden. Ziel des induktiven Lernens ist es, die richtige Kartierung von X VINdisplaystyle X} bis Y displaystyle Y} zu untergraben, das Lernproblem kann als eine Prüfung und Kennzeichnung von Daten als Stichprobenprobleme angesehen werden, die der Lehrer für die Klasse als Hilfe bei der Lösung eines anderen Problems lösen kann. In der transduktiven Umgebung wirken diese ungelösten Probleme als Prüfungsfragen. In der induktiven Einrichtung werden sie Probleme der Art, die die Prüfung durchführen wird. Es ist unnötig (und, nach dem Prinzip von Vapnik, imprudent), das transduktive Lernen durch die Einmischung einer Klassifikationsregel über den gesamten Inputraum zu führen; in der Praxis werden jedoch Algorithmen, die formell für die Einführung oder Einführung bestimmt sind, häufig interaktiv verwendet. Vermutungen Zur Nutzung von nichtlabelierten Daten müssen einige Beziehungen zur zugrunde liegenden Datenverteilung bestehen. Halbgesteuerte Lernalgorithmen verwenden mindestens eine der folgenden Annahmen: Künftige Annahmepunkte, die in der Nähe von einander liegen, sind eher geeignet, ein Etikett zu teilen. In der Regel wird dies auch in kontrollierten Lernprozessen angenommen und gibt eine Präferenz für geometrische einfache Entscheidungsgrenzen. Im Falle eines semi-revisierten Lernens führt die Annahme zusätzlich zu einer Präferenz für Entscheidungsgrenzen in Regionen mit geringem Charakter, so dass nur wenige Punkte in der Nähe von einander, aber in verschiedenen Klassen liegen. Cluster-Annahme Die Daten werden in der Regel getrennte Cluster bilden, und die Punkte im gleichen Cluster sind eher geeignet, ein Label zu teilen (obwohl Daten, die ein Label über mehrere Cluster verbreiten können). Es handelt sich um einen besonderen Fall der Annahme von Rechtsvorschriften, die das Lernen mit Cluster-Algorithmen erfassen. Manipulierende Annahme Die Daten liegen ungefähr auf einer Vielzahl von viel niedrigeren Dimensionen als dem Eingangsraum. In diesem Fall kann der vielfältige Einsatz sowohl der gekennzeichneten als auch der unlabelierten Daten den Verlust der Dimension vermeiden. Lernen kann dann mit Entfernungen und Verweigerungen, die auf der Vielfalt definiert sind, fortfahren. Mehrdimensionale Daten werden durch einen gewissen Prozess generiert, der schwer zu modellieren ist, aber nur ein paar Grad an Freiheit hat. Beispielsweise wird die menschliche Stimme von einigen wenigen Stimmen beherrscht, und die Bilder verschiedener Gesichtstexte werden von einigen Muskeln kontrolliert. In diesen Fällen sind die Entfernungen und die Einfachheit im natürlichen Raum des erzeugenden Problems überlegen, ob alle möglichen akustischen Wellen oder Bilder im Raum liegen. Geschichte Sein touristischer Ansatz der Selbstausbildung (auch bekannt als Selbstlernen oder Selbstkennzeichnung) ist historisch der älteste Ansatz für das semi-regulierte Lernen mit Beispielen für Anwendungen, die in den 1960er Jahren beginnen. Der transduktive Lernrahmen wurde von Vladimir Vapnik in den siebziger Jahren förmlich eingeführt. Interesse an einem induktiven Lernen mit generativen Modellen begann auch in den siebziger Jahren. Wahrscheinlich wurde 1995 von Ratsaby und Venkatesh ein etwa korrektes Lernen nachgewiesen, das für das halbgesteuerte Lernen einer Gausischen Mischung gebunden ist. Halbgesteuertes Lernen ist vor kurzem populärer und praktisch relevant, weil die Vielzahl von Problemen, für die große Mengen unlabelter Daten verfügbar sind – z.B. Text auf Websites, Proteinsequenzen oder Bilder. Methoden Generative Modelle Generative Ansätze für statistisches Lernen suchen zunächst auf die Schätzung von p ( x | y ) Memestyle p(x|y)}, die Verteilung der Datenpunkte, die jeder Klasse angehören. Wahrscheinlichkeit p ( y ) Kaffeedisplaystyle p(y|x)}, dass ein bestimmter Punkt x Memestyle x} etikett y {\displaystyle y} ist dann proportional zu p ( x | y ) p ( y ) Memestyle p(x|y)p(y} von Bayes' Regel. Halbgesteuertes Lernen mit generativen Modellen kann entweder als Ausweitung des beaufsichtigten Lernens (Klasse plus Informationen über p ( x ) Memestyle p(x)} oder als Erweiterung des nicht überwachten Lernens (Clustering plus einige Etiketten) angesehen werden. Generative Modelle gehen davon aus, dass die Vertriebenen einige besondere Form p ( x | y ,   ) Memestyle p(x|y,\theta )} durch den Vektor θ HANAstyle \theta } . Wenn diese Annahmen falsch sind, können die unlabelierten Daten die Richtigkeit der Lösung im Vergleich zu den Ergebnissen, die allein aus den Etikettendaten gewonnen wurden, tatsächlich verringern. Wenn die Annahmen korrekt sind, verbessern die nicht angemeldeten Daten zwangsläufig die Leistung. Laut einer Mischung von Einzelvertrieben werden die nicht eingetragenen Daten verteilt. Um die Mischung aus den nichtlabelierten Daten zu lernen, muss es bestimmbar sein, d. h. unterschiedliche Parameter müssen verschiedene zusammenfassende Verteilungen liefern. Gausssische Mischungsverteilungen sind für generative Modelle erkennbar und häufig verwendet. Die parameterierte gemeinsame Verteilung kann als p ( x , y   ) = p ( y[   ) p ( x [ y ,   ) {\displaystyle p(x,y|\theta )=p(y|\theta )p(x|y,\theta )} mit der Regel der Kette geschrieben werden. Jeder Parametervektor   Memedisplaystyle \theta } ist mit einer Entscheidungsfunktion f θ ( x ) = argmax y p ( y ,   ) {\displaystyle f_ggiotheta (}x)=Getunderset y}{\operator {argmax}  p p(y|x,\theta ) } verbunden. Der Parameter wird dann auf der Grundlage der etikettierten und unlabelten Daten ausgewählt, gewichtet durch   HANAstyle \lambda }: argmax ) (Log  i p ( { x i , y i } i = 1 l [ ) ] + ) Log } p ( { x i } i = l + 1 l + u [ θ   ) ) {\ ) ) ) . 7.8Theta )operator {argmax} } Link(\log p(\{x_{i},y_{i__{i=1}^{l}\theta )+\lambda \log p(\{x_{i}\}_{i=l+1}^{l+u}\theta \right} Trennung von Lowdensity Eine weitere große Klasse von Methoden versucht, Grenzen in Regionen mit wenigen Datenpunkten (gezeichnen oder nicht gekennzeichnet) zu schaffen. Eines der am häufigsten verwendeten Algorithmen ist die transduktive Unterstützungsvektormaschine oder TSVM (die trotz ihres Namens auch für induktives Lernen verwendet werden kann). Während die Unterstützung von Vektorgeräten für die Überwachung des Lernens eine Entscheidungsgrenze mit maximaler Marge über die gekennzeichneten Daten anstreben sollte, ist das Ziel des TSVM ein Zeichen für die unlabelierten Daten, so dass die Entscheidungsgrenze über alle Daten verfügt. Neben dem Standardabfall ( 1 − y f ( x ) ) ) + KINGstyle 1-yf(x)}_{+ für gekennzeichnete Daten, eine Verlustfunktion ( 1 − e) ( x ) * ) ) + Memestyle 1-195f(x)}_{+ wird über die unlabelierten Daten eingeführt, indem y = Zeichen f ( x ) displaystyle y=operator } {f) {f(f) {f) {f) {f) {f) {fx) {f) {f) {f) {f) {f) {f) {f) {f) {f) {f) {f) {f) } TSVM wählt dann f s ( x ) = h  ( ( x ) + b displaystyle f**}(x)=h**}(x)+b von einem Reproducingkern Hilbert Space H WELLdisplaystyle {H} bis zur Minimierung des regelmäßigen empirischen Risikos: f  = = argmin f (  i i = 1 l ( 1 − y i f ( x i) ) +  1 1  H 2 +  H 2  i i = l + 1 l + u ( 1  - ) 7.8displaystyle f**}= 050unterset f)operator {argmin} } Link(\style \sum i=1}^{l}(1-y_{i}f(x_{i}))_{+} 1||h\· H22}+\lambda {_2}\sum i=l+1}^{l+u}(1-ml(x_{i})_{+}\right) Eine genaue Lösung ist aufgrund des nicht-konvexen Begriffs ( 1  - ( x ) [ )] + HANAdisplaystyle 1- · }_{+, so dass die Forschung auf nützliche Annäherungen konzentriert. Andere Ansätze, die eine geringe Haftungstrennung anwenden, umfassen Gausssian-Prozessmodelle, Informations Regularisierung und entropy minimization (davon ist TSVM ein Sonderfall). Laplacian Regularisation Lapplacian Regularisation wurde historischer Ansatz durch Graph-Laplacian. Graphikbasierte Methoden für das halbgedeckte Lernen verwenden eine grafische Darstellung der Daten, mit einem node für jedes gekennzeichnete und unlabelte Beispiel. Die Graphik kann unter Verwendung von Domain-Know-how oder ähnlichen Beispielen erstellt werden; zwei gemeinsame Methoden sind, um jeden Datenpunkt an seine k Memestyle k} naheesten Nachbarn oder Beispiele innerhalb einer Entfernung ε KINGstyle \epsilon } zu verbinden. Gewicht W i j {\displaystyle W_{ij} von einem Rand zwischen x i {\displaystyle x_{i} und x j {\displaystyle x_{j} wird dann auf e −  i x i − x j  2 2 ε {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ {\ fracstyle e efrac x_{i}-x_{j|2^{epsilon  . Mit dem Rahmen der vielfältigen Regularisierung dient die Graphik als Ersatz für die Vielfalt. Ein Begriff wird dem Standard-Tikhonov-Regulierungsproblem hinzugefügt, um die Loyalität der Lösung im Verhältnis zu der Vielzahl (im eigenen Raum des Problems) sowie im Vergleich zum Raum des Eingangs zu gewährleisten. Das Minimisierungsproblem wird zu einemrgmin f  H H ( 1 l  i i = 1 l V ( x i , y i ) +  A A  H F 2 + λ I  M M  M M f ( x )  2 2 d p ( x ) ) Memestyle Memeset {f\in) 7.8 Mathematik H)operatorname {argmin} } Link(Ehefrac 1 1l idisplaystyle \sum i=1 il}V(fx_{i}),y_{i})+\lambda A||f\|_gronomcal H22}+\lambda {_IIint ggio_ Mathematik {M|\nabla hol_thecal Mxf(x)\||2}dp(x)\right, wo H {\displaystyle oc {H} ein Reproduzieren von Hilbert Space und M {\displaystyle {M} ist die Menge, auf der die Daten liegen. Parameter für die Regularisierung  A A WELLdisplaystyle \lambda {_A} und  I I {\displaystyle \lambda {_I} Kontrolle der Unversehrtheit in der Umgebung bzw. in den zugehörigen Räumen.Die Graphik wird verwendet, um den inhärenten Regularisierungstermin anzugleichen. Definition des GraphLaplacian L = D − W KINGstyle L=D-W}, wo D i =  j j = 1 l + u W i j {\displaystyle D_{ii}=\sum j=1}^{l+u}W_{ij und f {\displaystyle \ Mathematik {f} } der Vektor [ f ( x 1 ) ... f ( x l + u ) ] faserstil [f(x_{1})\dots f(x_{l+u)] , wir haben f T L f =  i i , j = 1 l + u W i j ( f i  - f j) 2  M M f ( x )  2 2 d p ( x ) {\ 2 d p ( x ) Memestyle \th\ k {f}L\dl} {f} \style \style \style i\sum,j=1ll+u}W {ij(f_f_f) {f_f_f_jf {j) {d- in Ebenso wie bei den Maschenöffnungen in PDE ist es möglich, dank einer fortgeschritteneren funktionellen Analyse das erwartete reibungslose Funktionieren der Funktion zu mobilisieren, was eine stärkere statistische Eigenschaft insbesondere zur Überwindung des Problems der Dimension gewährleistet. Les Laplacian kann auch verwendet werden, um die überwachten Lernalgorithmen auszuweiten: regulärisierte mindestens Quadrate und unterstützende Vektor-Maschinen (SVM) auf halbgesteuerte Versionen Lapplacian, die mindestens Quadrate und Lapplacian SVM sind. He touristische Ansätze Manche Methoden für das halbgedeckte Lernen sind nicht inhärent auf das Erlernen von nichtlabelten und gekennzeichneten Daten ausgerichtet, sondern verwenden nicht etikettierte Daten innerhalb eines beaufsichtigten Lernrahmens. Zum Beispiel können die etikettierten und unlabelierten Beispiele x 1 , ... , x l + u {\displaystyle x_{1},\dots ,x_{l+u} eine Auswahl an Darstellungen, Entfernungsparametern oder Kernmaterial für die Daten in einem nicht überwachten ersten Schritt angeben. Dann wird das Lernen von nur den etikettierten Beispielen kontrolliert. In diesem Zusammenhang lernen einige Methoden eine niedrigdimensionale Darstellung unter Verwendung der überwachten Daten und gelten dann entweder für eine geringe Haftungstrennung oder Graphik-basierte Methoden. Iterative Raffinerie der Darstellung und anschließendes halbgewaltetes Lernen an dieser Vertretung kann die Leistung weiter verbessern. Selbstausbildung ist eine ganzheitliche Methode für das halbgesteuerte Lernen. Erstens wird ein überwachter Lerngorithmus auf der Grundlage der einzigen Daten ausgebildet. Letztere wird dann auf die unlabelierten Daten angewendet, um mehr als ein Input für den überwachten Lerngorithmus zu erzeugen. Insgesamt sind nur die Etiketten, die der Klassenprüfer am sichersten ist, in jedem Schritt hinzugefügt. Co-Fortbildung ist eine Erweiterung der Selbstausbildung, bei der mehrere Klassenprüfer auf verschiedenen (ideumfassenden) Merkmalen ausgebildet werden und für eine andere Kennzeichnungsbeispiele erstellen. Menschenrechtsreaktionen auf formale, halbgedeckte Lernprobleme haben unterschiedliche Schlussfolgerungen über den Grad des Einflusses der nicht angemeldeten Daten gezogen. Mehr natürliche Lernprobleme können auch als Instanzen des halben Lernens angesehen werden. Vieles menschliches Konzept Lernen umfasst eine kleine Menge direkter Anweisung (z.B. elterliche Kennzeichnung von Gegenständen während der Kindheit) in Kombination mit großen Mengen unlabelter Erfahrung (z.B. Beobachtung von Gegenständen ohne Benennung oder Zählung) oder zumindest ohne Feedback. menschliche Säuglinge sind empfindlich auf die Struktur ungeschützter natürlicher Kategorien wie Bilder von Hunden und Katzen oder männliche und weibliche Gesichter. Kinder und Kinder berücksichtigen nicht nur ungeschützte Beispiele, sondern das Probenahmeverfahren, aus dem etikettierte Beispiele hervorgehen. Siehe auch PU Learning Weak-Beaufsichtigungspunkte Quelles Chapelle, Olivier; Schölkopf, Bernhard; Zien, Alexander (2006). Halbgesteuertes Lernen. Cambridge, Mass.: MIT Presse.ISBNUR0-262-03358-9. Externe Links Manifold Regularisierung Eine frei verfügbare modulare Umsetzung der Graphen-basierten halbgewalteten Algorithmen Lapplacian unterstützen Vektormaschinen und normalisierte zumindest Quadrate. KEEL: Software-Werkzeug zur Bewertung von Entwicklungsgorithmen für Data Mining-Probleme (Regression, Klassifizierung, Clusterbildung, Musterbau und so on) KEEL-Modul für das halbgedeckte Lernen. Halbleiter-Trivised Learning Software Semi-vised Learning Software 1.14.Semi-Supervised – Scikit-learn 0,22.1 Dokumentation halbviseder Algorithmen in Skikit-learn .