In Statistiken und Maschinenlernen ist der unvoreingenommene -varianz-Ausstieg das Eigentum eines Modells, dass die Varianz des Parameters, der für die einzelnen Proben geschätzt wird, dadurch verringert werden kann, dass die Verzerrung in den geschätzten Parametern erhöht wird. Das einseitige -varianz-Problem oder das einseitige -varianzproblem ist der Konflikt, um diese beiden Fehlerquellen gleichzeitig zu minimieren, die verhindern, dass Lernalgorithmen über ihre Ausbildungseinrichtung verbreitet werden: Fehler sind ein Fehler von falschen Annahmen im Lerngorithmus. Hohe Verzerrungen können einen Algorithmus verursachen, um die relevanten Beziehungen zwischen den Merkmalen und den angestrebten Outputs (Nachrüstung) zu verpassen. Die Varianz ist ein Fehler von der Empfindlichkeit bis zu kleinen Schwankungen im Ausbildungsgang. Hohe Varianz kann aus einem Algorithmus resultieren, der das zufällige Lärm in den Ausbildungsdaten (Überrüstung) modelliert. Es handelt sich um eine Methode zur Analyse des erwarteten Generalisierungsfehlers eines Lern-Algorithmus in Bezug auf ein bestimmtes Problem als Summe von drei Begriffen, der Unparteilichkeit, der Varianz und einer Menge, die als unerträglicher Fehler bezeichnet wird, der sich aus dem Lärm im Problem selbst ergibt. Motivation Ein zentrales Problem bei der Überwachung des Lernens ist der unparteiische Handel. Idealerweise möchte man ein Modell wählen, das sowohl die Ordnungsmäßigkeiten in ihren Ausbildungsdaten genau erfasst, als auch die Daten allgemeinisiert. Leider ist es in der Regel unmöglich, beide gleichzeitig zu tun. Lernmethoden mit hoher Varianz können ihre Ausbildung gut repräsentieren, sind jedoch Gefahr, dass sie zu lauten oder unrepräsentativen Ausbildungsdaten überreichen. Kontrastlich produzieren Algorithmen mit hohen Verzerrungen in der Regel einfachere Modelle, die in den Daten keine wichtigen Ordnungsmäßigkeiten (z.B. unzureichend) erfassen können. Es ist ein häufig gemachter Rückgang zu erwarten, dass komplexe Modelle hohe Varianz aufweisen müssen; Hochvarianzmodelle sind in gewisser Hinsicht komplex, aber die umgekehrten Bedürfnisse sind nicht wahr. Darüber hinaus muss man darauf achten, wie die Komplexität definiert wird: Insbesondere die Anzahl der Parameter, die zur Beschreibung des Modells verwendet werden, ist eine schlechte Maß an Komplexität. Dies wird durch ein Beispiel illustriert von: Das Modell f a , b ( x ) = eine S. (b x ) Memedisplaystyle f_{a,b}(x)=a\sin(bx) hat nur zwei Parameter (eine , b HANAdisplaystyle a,b} ), kann jedoch jede Reihe von Punkten durch eine hohe Frequenz, die sowohl zu einer hohen Verzerrung als auch zu hohen Varianz führt. Künftig wird die Verzerrung durch die Nutzung nur lokaler Informationen verringert, während die Varianz nur durch durchschnittliche Über mehrere Beobachtungen verringert werden kann, was inhärent bedeutet, dass Informationen aus einer größeren Region verwendet werden. Beispiel für ein erlesenes Beispiel ist der Abschnitt über k-norest Nachbarn oder die Zahl auf dem richtigen Weg. Um zu gewährleisten, wie viele Informationen von benachbarten Beobachtungen verwendet werden, kann ein Modell durch explizite Regularisierung, wie Schrumpfung, gelockert werden. Bias – Differenzierung des durchschnittlichen Quadratkilometern Fehlers: Wir verfügen über eine Ausbildung, die aus einer Reihe von Punkten x 1 , ... , x n displaystyle x_{1},\dots ,x_{n} und realen Werten y i {\displaystyle y_{i} besteht, die mit jedem Punkt x i Memestyle x_{i} assoziiert sind . Wir gehen davon aus, dass es eine Funktion mit Lärm y = f ( x ) +   WELLdisplaystyle y=f(x)+\varepsilon } gibt, in der die Lärme,   \varepsilon } , Null bedeutet und Varianz  2 2 \sigma \sigma {^2} . Wir wollen eine Funktion f ^ ( x ; D) KING KING {f) {f( f(f )f · )f )f  ff {\x) finden. durch einen Lerngorithmus, der auf einem Ausbildungsdatenset (sample) D = { ( x 1 , y 1 ) ... , ( x n , y n ) } basiert KINGstyle D=((x_{1},y_{1})\dots (x_{n},y_{n)\ .Wir machen "so gut wie möglich" durch Messung des durchschnittlichen Fehlers zwischen y {\displaystyle y} und f ^ ( x ; D ) Memestyle {f}}(x;D) : Wir wollen ( y f ^ ( x ; D) 2 ) KING (Systyle, · ·  ...,  ...,  ..., ), ), ), ), ), ), ), ), ), )) Man kann natürlich nicht hoffen, dies zu tun, da der Yendisplaystyle y_{i} Lärm   Memedisplaystyle \varepsilon } enthält; dies bedeutet, dass wir bereit sein müssen, einen unerträglichen Fehler in jeder Funktion zu akzeptieren, die wir mit sich bringen. Suche nach einem f ^ {\displaystyle {f}, dass die Generalisierungen auf Punkte außerhalb der Ausbildungseinrichtung mit allen zahllosen Algorithmen, die zur Überwachung des Lernens verwendet werden, erfolgen können. Es stellt sich heraus, dass wir den erwarteten Fehler auf einer uneen Probe x Memestyle x} wie folgt entschlüsseln können: E D  [ [ ( y − f ^ ( x ; D ) ] 2 ] = (Sekkel D  [ [ f ^ ( x ; D ) ] ) 2 + Var D  [ [ f ^ ( x ; D ) ] +  2 2 displaystyle \operatorname {E} _D{\Big SSO[}big (}y-Barhat f((x;D) {\) ) ]2}{\Big }= } ({\name {Bias}D}{\big }[hat f((x;D) fib] ] {\ ] ])] ] {Var} _D)big Meme[}hat f)(x;D) cubig +\}]sigma {^2}, wo Bias D  [ [ f ^ ( x ; D )] = E D ) [ f ^ ( x ; D ) ] − f ( x ) \operatorname {Bias} _D)big WELL[7]hat f)(x;D) ]\=}operatorname {E} _D.big Meme[hat f((x;D) cubig -]f(x) und Var D  [ [ f ^ ( x ; D )] = E D  D [ (E D  [ [ f ^ ( x ; D ) ] − f ^ ( x ; D ) 2 ] ] KINGstyle \operatorname {Var} _D)big Meme[}hat f)(x;D) ]\=}operatorname {E} _D}[8]big ()ator {E} _D}[8][8] Fihat f((x;D)]-Gethat f((x;D)) {\2.] Die Erwartung reicht über die verschiedenen Wahlmöglichkeiten der Ausbildungseinrichtung D = { ( x 1 , y 1 ) ... ( x n , y n ) } displaystyle D=\{(x_{1},y_{1})\dots (x_{n},y_{n)\ )\ , alle Probenahmen aus dem gleichen gemeinsamen Vertrieb P ( x , y ) HANAstyle P(x,y)]) .Die drei Begriffe sind: E.g, wenn eine nicht-lineare Funktion f ( x ) Memestyle f(x)} mittels einer Lernmethode für lineare Modelle wird es Fehler in den Schätzungen f ^ ( x ) Memedisplaystyle {f((x) aufgrund dieser Annahme geben; die Varianz der Lernmethode oder, offensichtlich, wie viel die Lernmethode f ^ ( x ) Memestyle {f{\(x) {f)(x) {f)(x) {f{\(x) \sigma } .Sinceall drei Begriffe sind nicht-negativ, der unwiderrufliche Fehler ist ein geringerer Teil des erwarteten Fehlers bei uneen Proben. Je komplexer das Modell f ^ ( x ) Memedisplaystyle {ff(x) ist, desto mehr Datenpunkte, die es erfassen wird, und desto geringer wird die Verzerrung sein. Komplexität wird jedoch das Modell mehr bewegen, um die Datenpunkte zu erfassen, so dass seine Varianz größer sein wird. Derivat Die Ableitung der einseitigen -varianz-Ablagerung für flächendeckende Fehler erfolgt wie folgt. Kurzinformation: f = f ( x ) {\displaydisplaystyle f=f(x)}, f ^ = f ^ ( x ; D ) {\displaystyle WELLhat f== {ff(x;D) und wir nehmen die D Memestyle D} Unterbeschreibung für unsere Erwarteungsanbieter ab. Erstens erinnern wir daran, dass wir nach der Definition für jede zufällige variable X Memestyle X} Var  [ [ X ] = E  [ [ X 2 ] - E  [ [ X ] ] , [ X ] 2 . KINGstyle \operatorname {Var} [X]=\operatorname {E} [X^{2}]-\operatorname {E} [X]^{2}. Nach hinten: E  [ [ X 2 ] = Var  [ [ X ] + E  [ [ X ] 2 . KINGstyle \operatorname {E} [X^{2}]=\operatorname {Var} [X]+\operatorname {E} [X]^{2}. Seit f Memedisplaystyle f} ist deterministisch, d.h. unabhängig von D Memestyle D}, E  [ [ f] = f. KINGstyle \operatorname {E} [f]=f.} y = f +   WELLdisplaystyle y=f+\varepsilon } und E , [ ] = 0 {\displaystyle \operator {E} \[varepsilon ]=0} (da ε KINGstyle \varepsilon \varepsilon } ist Lärm); [ y ] = E  [ [ f + ε ] = E  [ [ f ] = f . KINGstyle \operatorname {E} [y]=\operatorname {E} [f+\varepsilon =\]operatorname {E} [f]=f.} Seit Var  [ [   ] =  2 2 , JPYstyle \operatorname {Var} \[varepsilon =\]sigma {^2,} Var   [ y ] = E   [] ( y − E   [ y ] ) 2 ] = E [] ( y − f ) 2 ] = E ) [ ( f + ε − f ) 2 ] = E [  2 2 ] = Var   [   ] + E  [ [   ] 2 =  2 2 + 0 2 =  2 2 . {Var} [y]=\operatorname {E} ([y-\operatorname {E} [y])^{2}] {E} ([y-f)^{2}]=\operatorname {E} ([f+\varepsilon -f)f2}]=\operatorname {E} \[varepsilon {^2}]=\operatorname {Var} +\] {E}  ]2} 2}+0^{2}=\sigma {^2}. So können wir, da   \varepsilon } und f ^ WELLdisplaystyle {f} unabhängig sind, E ) schreiben [ ( y − f ^ ) 2 ] = E ) [ ( f + ε − f ^ ) 2 ] = E ) [ ( f + ε − f ^ + E ⁡ [ f ^ ] − E ) [ f ^ ] ) 2 ] = E [ (f − E  [ [ f ^ ] ) 2 ] + E   [  2 2 ] + E) [ (E  [ [ f ^ ] − f ^ ) 2 ] + 2 E [ (f − E  [ [ f ^ ] ) ] + 2 E ) [  E (E  [ [ f ^ ] − f ^ ] + 2 E ) [ (E  E [ f ^ ] − f ^ ) f − E  [ [ f ^ ] ] = ( f  - E ) [ f ^ ] ) 2 + E [  2 2 ] + E  [ [ (E  [ [ f ^ ] − f ^ ) 2 ] + 2 ( f - E  [ [ f ^ ] ) E ) [   ] + 2 E [   ] E  [ [ E  [ [ f ^ ] − f ^ ] + 2 E ⁡ [ E ^ [ f ^ ] − f ^ ] ( f − E  [ [ f ^ ] ) = ( f  E [ f ^ ] ] ) 2 + E [  2 2 ] + E  [ [ (E  [ [ f ^ ] − f ^ ) 2 ] = ( f - E  [ [ f ^ ] ) 2 + Var   [   ] + Var  [ [ f ^ ] = Bias ⁡ [ f ^ ] 2 + Var   [   ] + Var  [ [ f ^] = Bias ⁡ [ ^ ] 2 +   2 + Var  [ [ f ^ ] . beginnen {E} WELLbig ([}y-Kaffeehat f}}2}{\big) } {E} WELLbig ([}f+\varepsilon {-\hat f))^{2}{\big [}5pt] {E} WELLbig ([}f+\varepsilon {-\hat {f++\operatorname {E} Meme[hat {f}}]-\operatorname {E} WELL[hat f}}]^{2}{\big [}5pt] {E} WELLbig ([}f-\operatorname {E} WELL[hat f]]^{2 +big +\}] {E} \[varepsilon {^2}]+\operatorname {E} WELLbig (\[}operatorname {E} Meme[hat f]]- steuerlicher Zw)2}{\big +}]2\operatornameatorname {E} Memebig ([}f-\operatorname {E} Meme[hat {fvar])\varepsilon SSObig +} {E} Memebig \[}varepsilon (\operatorname {E} Meme[hat f]]-Barhat f}}) 7.8big +} {E} Memebig (\[}operatorname {E} 7.8[hat f}}]- {f))(f-\operator {E} Meme[hat f f])) 7.8big [}5pt] &=(f-\operator {E} 574[hat f]] {E} \[varepsilon {^2}]+\operatorname {E} WELLbig (\[}operatorname {E} WELL[hat f}}]- cuhat f}}2}{\big +}2(f-\operatornameatorname) {E} WELL[hat {f]]\operatorname {E} +2\operatorname {E} \[varepsilon \]operatorname {E} Memebig \[}operatorname {E} Meme[hat f]]-Fithat f +big +} {E} Memebig \[}operatorname {E} Meme[hat f}}]- cuhat f}}{\big (}f-\operator {E} Meme[hat {f]])[[5pt] &=(f-\operator {E} 7.8[hat f]] {E} \[varepsilon {^2}]+\operatorname {E} WELLbig (\[}operatorname {E} WELL[hat f{\]- cuhat f}}2 [big [}5pt] &=(f-\operator {E} 7.8[hat f]])^{2} {Var} +\] {Var} Memebig Meme[}hat f}}{\big [}5pt] {Bias} Meme[hat f}}]^{2}+\operatorname {Var} {Var} Memebig Meme[}hat f}}{\big [}5pt] &=\operatorname {Bias} Meme[hat f]]^{2}+\sigma {^2}+\operatorname {Var} WELLbig WELL[}hat f}}{\big ]\.end Schließlich wird die MSE-Verlustfunktion (oder negative Log-likelihood) durch Übernahme des erwarteten Werts über x  P P {\displaystyle x\sim P}: MSE = E x   { Bias D  [ [ f ^ ( x ; D ]) ] 2 + Var D  [ [ f ^ ( x ; D ) ] } +  2 2 . KINGstyle Text{MSE==\operatorname {E} _x}{\bigg  operatorname {Bias} _D}[ñhat f f(x;D)]^{2}+\operatorname {Var} _D.big Meme[}hat f f(x;D) cubig {\}]bigg +\}sigma {^2}. Maßgeblich für die Verringerung der Dimension und die Auswahl von Merkmalen kann die Varianz durch Vereinfachung der Modelle verringern. In ähnlicher Weise verringert sich ein größeres Ausbildungsangebot die Varianz. Leistungspunkte (Vorbescheider) verringern tendenziell Verzerrungen auf Kosten der Einführung zusätzlicher Schwankungen. Lernalgorithmen haben in der Regel einige Parameter, die Verzerrungen und Varianz kontrollieren; zum Beispiel können lineare und allgemeinisierte lineare Modelle regelmäßig eingeführt werden, um ihre Varianz bei den Kosten einer zunehmenden Verzerrung zu verringern. In künstlichen Neuralnetzen nimmt die Varianz zu und die Unparteilichkeit nimmt ab, da die Zahl der versteckten Einheiten steigt, obwohl diese klassische Annahme Gegenstand der jüngsten Debatte war. Wie in GLMs wird die Regularisierung in der Regel angewendet. In k-norest-Nachbarmodellen führt ein hoher Wert von k zu hohen Verzerrungen und geringen Schwankungen (siehe unten). In der Praxis lässt sich die Regularisierung unterschiedlich aus der Mischung von Prototypen und Beispielen erreichen. Die Tiefe des Baumes bestimmt die Varianz. Entscheidungen Bäume werden häufig zur Kontrolle der Varianz geführt. Ein Weg zur Lösung des Handels ist die Verwendung von Mischungsmodellen und einem breiten Lernen. Beispielsweise kombiniert die Ankurbelung vieler schwacher (hoher) Modelle in einem Paket, das niedrigere Verzerrungen aufweist als die einzelnen Modelle, während die Beatmung starke Lernende auf eine Weise kombiniert, die ihre Varianz verringert. Methoden zur Validierung von Modellen wie Cross-validierung (statistics) können zur Optimierung des Handels verwendet werden. k-norest Nachbarn Im Falle der Regression von k-norest-Nachbarn, wenn die Erwartung über die mögliche Kennzeichnung einer festen Ausbildungseinrichtung erfolgt, besteht ein geschlossener Ausdruck, der sich auf die einseitige Abweichung vom Parameter k: E  [ [ ( y − f ^ ( x )) ) ) 2 ) X = x ] = ( x ) − 1 k  i i = 1 k f (N i ( x ) ) 2 +  2 2 k + {\ 2 {\displaystyle \operatorname {E} ([y-Fithat f}}(x) X2 Xmid X=x]=\left(fx)- 7.8fkk}}\k Xk) i=1}^{k}f(N_{i}(x))\right)^{2} + Fifrac ggiosigma ^2++\sigma {^2}, wo N 1 ( x ) , ... , N k ( x ) HANAstyle N_{1}(x),\dots ,N_{k}(x) die k nächstgelegenen Nachbarn von x in der Ausbildung sind. Die Verzerrung (erste Bezeichnung) ist eine monotone wachsende Funktion von k, während die Varianz (zweiter Begriff) abnimmt, da k zunimmt. Unter "annehmbaren Annahmen" ist die Unparteilichkeit der ersten Nachbarn (1-NN) Estimator vanishes ganz so groß wie die Größe der Ausbildungskonzepte infinity. Anwendungen Regression Die einseitige -varianz-Ablagerung bildet die konzeptuelle Grundlage für Regressions-Regulierungsmethoden, wie z.B. die Regression von Kühlen und Dornen. Regularisierungsmethoden führen Verzerrungen in die Regressionslösung ein, die die Varianz erheblich im Vergleich zu den normalen, am wenigsten Quadraten (OLS)-Lösung verringern kann. Obwohl die OLS-Lösung nicht-voreingenommene Regressionsschätzungen vorsieht, bieten die geringeren Varianzlösungen, die durch Regularisierungstechniken hergestellt werden, höhere MSE-Leistung. Klassifizierung Die einseitige -varianz-Ablagerung wurde ursprünglich für die Rückführung von mindestens Quadratmetern formuliert. Im Falle einer Klassifizierung nach dem Verlust von 0-1 (Reclassification Rate) ist es möglich, eine ähnliche Ablagerung zu finden. Wenn das Einstufungsproblem als probstabilistische Einstufung bezeichnet werden kann, kann der erwartete geschätzte Fehler der vorhergesagten Probabilities im Hinblick auf die wahren Probabilities wie früher abgebaut werden. In der Stärkung des Lernens, auch wenn die einseitige – Varianz-Ablagerung nicht direkt im verstärkten Lernen Anwendung findet, kann ein ähnlicher Abspiel auch die allgemeine Ausrichtung bestimmen. Wenn ein Agenten begrenzte Informationen über seine Umwelt hat, kann die Suboptimalität eines RL-Algorithmus in die Summe zweier Begriffe einfließen: ein Begriff, der mit einer asymptotischen Verzerrung und einem späteren Termin zusammenhängt. Die asymptotische Verzerrung ist direkt mit dem Lerngorithmus (unabhängig von der Datenmenge) verbunden, während die Nachrüstungsfrist aus der begrenzten Datenmenge stammt. Im Bereich des menschlichen Lernens, während im Rahmen des maschinenlesbaren Lernens weit diskutiert wurde, wurde im Zusammenhang mit der menschlichen Kognis insbesondere von Gerd Gigerenzer und Mitarbeitnehmern im Zusammenhang mit erlernten Hetourismus untersucht. Sie haben (siehe unten) argumentiert, dass das menschliche Gehirn das Dilemma im Falle der in der Regel dünnen, schlecht beladenen Ausbildungsangebote durch die Annahme von Hoch- und Tiefkulturen gelöst.Dies spiegelt die Tatsache wider, dass ein Null-Prozeß eine schlechte Allgemeinheit für neue Situationen hat und auch unzumutbar vermutet, dass der wahre Zustand der Welt genau bekannt ist. Die daraus resultierenden Hetourismus sind relativ einfach, produzieren aber bessere Gleichgültigkeiten in einer größeren Vielfalt von Situationen. Geman et al.argue, dass das unparteiische Dilemma bedeutet, dass Fähigkeiten wie die Anerkennung von Generika nicht von Kratzern gelernt werden können, sondern ein gewisses Maß an „harter Verkabelung“ erfordern, das später durch Erfahrung geprägt ist. Dies ist, weil modellfreie Ansätze zur Gleichgültigkeit unpraktisch große Ausbildungssets erfordern, wenn sie eine hohe Varianz vermeiden wollen. Siehe auch: